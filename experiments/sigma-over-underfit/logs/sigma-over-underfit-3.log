Start time: 2023-07-09 21:06:06.340890
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:2 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 3.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0447,  0.0756, -0.2127,  ...,  0.2609,  0.4910,  0.2665],
        [-0.2647, -0.0771,  0.2573,  ..., -0.1631,  0.2988,  0.2101],
        [ 0.0411, -0.3286, -0.2541,  ..., -0.6626, -0.0275,  0.2559],
        ...,
        [-0.4244,  0.2789, -0.2359,  ...,  0.0649, -0.5205, -0.0421],
        [-0.4508, -0.1568,  0.2182,  ..., -0.4388, -0.3859, -0.0195],
        [-0.3507,  0.0489,  0.4109,  ..., -0.0855, -0.2056, -0.2932]],
       device='cuda:2', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6166e-02, -1.3915e-01,  2.9564e-01,  2.9611e-01,  2.2214e-01,
        -9.7888e-02,  5.8897e-01, -1.8524e-01,  2.0443e-01,  5.8446e-02,
        -1.7745e-01, -1.9895e-01, -1.1682e-01, -5.3616e-01,  2.0789e-01,
        -6.4598e-02,  3.3362e-01,  1.3427e-01, -3.1360e-02, -3.8676e-01,
         5.7533e-01,  9.6737e-02, -3.1188e-01, -1.9318e-01, -3.3081e-01,
        -1.8537e-01,  3.6419e-01, -1.3505e-01,  3.0185e-01,  5.7582e-01,
        -3.7531e-01, -4.6969e-01, -6.4841e-02,  4.0309e-01, -4.3244e-01,
        -1.9433e-01,  1.6605e-01, -2.1830e-01,  2.8050e-01, -9.4567e-02,
         1.5417e-01,  1.7902e-01, -2.3644e-01, -2.4925e-01, -2.9956e-01,
        -3.7044e-02,  1.6814e-01, -1.9854e-01, -2.5638e-01,  2.6855e-01,
        -9.5205e-02,  3.6366e-01, -8.0318e-01, -1.6955e-01, -2.2713e-01,
        -2.0035e-02,  1.3097e-01,  2.3065e-01,  3.7070e-01, -1.3243e-01,
        -3.6160e-02, -4.8206e-03, -1.1376e-03,  2.0086e-01, -2.2492e-01,
         3.9010e-01, -2.8781e-01,  4.2783e-02, -3.1186e-01,  2.7993e-01,
         3.9311e-01,  7.0192e-02, -9.7189e-02,  8.0272e-01, -3.1546e-01,
        -5.5958e-01,  2.1423e-02, -1.2455e-01, -3.3646e-01,  2.7934e-01,
        -1.5209e-01,  2.7482e-01, -1.1987e-01,  4.2689e-01,  1.3793e-02,
         2.3633e-01, -3.1819e-01,  5.3045e-02,  1.5081e-01,  3.4590e-01,
        -3.7396e-01, -7.3865e-02, -9.2203e-02,  6.5114e-02,  3.8257e-01,
        -1.5689e-01,  1.7788e-01,  5.1722e-01, -4.2938e-01,  8.0669e-02,
        -6.2920e-03, -1.3110e-01, -2.6427e-02,  4.4446e-01, -5.4677e-01,
        -1.6423e-01,  3.0083e-02,  9.8959e-02, -1.1722e-01,  2.3326e-01,
         1.7362e-01, -4.6287e-01, -5.5670e-01, -9.9926e-02, -3.0622e-01,
         1.4751e-01, -6.1184e-01, -5.0763e-01, -1.6882e-02, -5.2880e-01,
        -7.5939e-01, -1.9816e-01, -1.5566e-01,  3.9489e-01,  3.2330e-01,
        -1.3048e-01,  2.4041e-01, -2.6492e-01, -1.5796e-01,  7.9179e-01,
         1.4048e-01,  5.2655e-01,  2.2442e-01, -9.1619e-02, -1.0366e+00,
         2.6553e-01,  2.0384e-01, -1.7155e-01, -1.6481e-03,  3.2141e-01,
        -5.8581e-01,  6.4256e-01, -3.9943e-01, -1.9391e-01, -2.6091e-01,
         1.1797e-01,  4.4223e-02,  1.6194e-01, -3.5289e-01, -3.2432e-01,
        -2.6665e-01,  3.0188e-01, -8.1698e-02, -1.5214e-01, -6.1498e-01,
        -3.0815e-02, -6.0532e-01,  4.0075e-01, -7.2847e-02,  2.7709e-01,
        -2.6974e-01,  2.5113e-01,  4.1658e-01,  3.3207e-01,  6.8874e-02,
        -5.5198e-02, -4.3679e-01,  5.4634e-01, -1.6460e-01, -1.7971e-01,
         3.5597e-01,  5.3952e-01, -2.0925e-01, -4.3910e-01, -3.2569e-01,
         7.6319e-01, -4.8673e-01,  2.1589e-02,  3.6273e-01, -3.1636e-01,
         9.0259e-02, -1.8414e-02, -2.3747e-01, -5.9817e-02, -1.9344e-01,
        -1.7293e-01, -9.0646e-02, -2.1341e-01, -4.3948e-01, -7.6056e-01,
        -2.0861e-01,  6.6792e-01, -1.3956e-01, -3.2485e-01,  1.1441e-01,
         1.7686e-01, -2.2298e-01,  4.3501e-01, -2.1019e-01, -1.8008e-02,
         3.9982e-01, -1.0706e-01,  3.2638e-01, -5.9527e-01,  1.1342e-01,
        -5.9372e-01, -9.7981e-02, -4.6723e-02,  1.7939e-01, -1.9921e-01,
         6.0286e-01, -5.7475e-01, -2.8755e-01, -2.1078e-02,  3.3662e-01,
        -5.4672e-01, -8.4574e-02,  7.0585e-01, -3.7891e-02, -2.0209e-01,
        -4.6297e-02, -1.0724e-01, -3.7633e-02,  3.6016e-01,  6.9463e-01,
         8.4260e-02,  4.2845e-01, -2.1915e-01,  2.0365e-01,  3.4857e-01,
         2.7607e-01, -4.2154e-01, -7.8276e-01,  3.9964e-02, -1.5784e-02,
        -6.7150e-02,  7.5284e-02,  2.0118e-01,  4.6005e-01,  1.2877e-01,
         1.7722e-01,  2.7403e-01, -4.4400e-01, -3.5898e-01,  3.2615e-01,
        -2.8858e-01,  1.9766e-01,  3.4877e-01,  3.3363e-01, -3.0321e-02,
        -1.4267e-01,  7.5654e-03, -4.5793e-01,  9.4683e-02, -7.8115e-01,
         3.5276e-01, -2.6607e-02, -1.3633e-01,  1.3946e-01, -1.6989e-01,
         9.7561e-02,  1.9092e-01,  1.1244e-01, -1.5142e-02, -2.4859e-01,
         2.6107e-01,  1.3460e-01, -1.9601e-01,  2.2171e-01, -1.1813e-01,
        -2.7271e-02,  2.2744e-01, -6.0848e-02,  1.0853e-01,  2.9351e-01,
         2.4204e-02, -1.1631e-01,  2.2765e-01,  3.5871e-01,  2.9450e-01,
        -5.8593e-01,  3.7996e-02, -4.5191e-01,  2.0098e-01,  1.1031e-01,
         2.5319e-01,  3.8350e-01,  4.1628e-01,  2.2083e-01, -5.2145e-01,
        -1.2084e-04, -2.9774e-01,  5.9605e-01, -1.4718e-01, -2.0328e-01,
        -2.7510e-01,  6.2981e-02,  2.8263e-01,  1.0762e-01, -3.1115e-02,
        -8.6717e-02,  1.2602e-01,  3.4481e-01, -3.1291e-01,  4.4573e-02,
        -5.4093e-02,  1.2785e-02,  5.9368e-01, -9.3458e-02, -3.0025e-02,
        -8.2114e-01,  3.5871e-02, -1.2330e-01, -2.9151e-02, -1.7028e-02,
        -6.4747e-01,  6.7500e-01, -9.3829e-02, -3.5783e-01, -1.1899e-01,
        -1.5495e-01,  1.4848e-01, -1.3111e-02, -4.6007e-01,  7.6984e-02,
        -3.0186e-02,  5.6222e-01, -2.4689e-01, -9.7050e-02, -1.6777e-01,
        -3.7750e-01, -2.4305e-01, -4.1383e-01,  3.1174e-01, -3.7243e-01,
        -7.9324e-02,  1.1726e-01,  6.9679e-01,  1.9514e-01, -5.7582e-02,
        -2.1583e-01,  1.9784e-02,  1.2355e-01,  2.7329e-01,  9.2189e-02,
         1.3324e-01,  1.3457e-01,  1.5220e-01,  5.6737e-01, -4.6108e-01,
        -1.4723e-01,  3.6228e-01,  3.5389e-01, -1.3471e-01, -4.3502e-01,
         5.7319e-02,  6.4020e-01, -3.4931e-01,  5.9252e-01, -3.7241e-01,
         4.8289e-01, -7.1217e-01, -1.7626e-01,  5.8545e-02, -4.4086e-01,
         4.1514e-01,  3.5856e-02,  2.0005e-01, -7.7625e-02,  4.2968e-01,
         4.4622e-01, -2.8619e-01, -5.0256e-01, -6.0599e-01, -1.0697e-01,
        -6.6589e-03, -1.3865e-01, -1.9513e-01, -1.1505e-01, -1.1677e-01,
         1.4042e-01,  1.0355e-01, -1.9033e-01,  2.3525e-02, -3.3168e-01,
         3.7288e-01,  3.2279e-01,  3.5255e-01, -1.1326e-01,  3.3135e-01,
        -1.7131e-01,  3.8987e-01, -1.6718e-01,  2.1797e-02, -6.6744e-01,
        -3.9690e-01, -6.4341e-01,  1.5612e-01,  1.4624e-01, -1.5568e-01,
         1.6977e-02, -1.1612e-01, -2.9578e-01,  3.9115e-01, -1.8719e-01,
         2.2748e-01,  1.3767e-01,  3.3728e-01,  1.0920e-01, -3.8483e-01,
         2.2107e-01,  2.1028e-01,  1.3182e-01,  1.1805e-01,  1.4939e-02,
         4.6891e-01, -3.7607e-01, -1.6891e-01, -1.7116e-01, -6.4544e-02,
        -3.4030e-01,  4.5838e-01,  3.0842e-01,  2.1312e-01, -5.4339e-02,
         2.4640e-01,  9.0786e-02, -6.6366e-01,  3.6862e-02,  3.8134e-01,
        -1.4490e-01, -5.9302e-02, -1.5054e-01,  3.4661e-02, -2.3997e-01,
        -2.0295e-01, -5.0164e-01, -2.1711e-02, -5.0930e-01, -2.2989e-01,
        -1.3395e-01, -7.2049e-02,  5.1076e-01, -1.9724e-01,  1.3791e-01,
         2.4521e-01,  4.4357e-02, -7.0624e-01, -1.9231e-01, -4.9890e-01,
        -3.3431e-01,  9.3956e-02,  3.9203e-02, -3.3491e-01, -7.7078e-01,
        -3.9517e-01, -7.0076e-03, -4.3714e-01, -5.3338e-01, -1.1568e-01,
        -7.7663e-01, -1.9353e-01,  1.6875e-01,  1.5197e-01,  3.1701e-01,
        -2.0656e-01,  2.4972e-01,  6.6190e-02, -2.9960e-02, -8.0230e-01,
        -4.1852e-02,  2.3524e-02,  3.8774e-01, -3.4516e-01,  2.3149e-01,
        -8.1876e-02, -7.5342e-04,  2.5952e-01,  1.2734e-01, -1.9654e-01,
        -2.9427e-02, -4.5514e-02,  5.0196e-01,  4.5032e-01,  4.2538e-01,
         2.0322e-01,  7.1951e-01, -5.9948e-01,  3.0338e-01, -2.7482e-02,
        -9.1157e-02, -2.4218e-01, -2.1302e-02,  2.9174e-01,  3.2023e-02,
         4.5233e-02, -1.0554e-01,  4.9416e-01,  1.0603e-01, -2.3242e-01,
        -1.0399e-01, -2.7018e-01,  2.0762e-01,  3.4917e-01, -7.6422e-02,
         1.4725e-01, -5.7476e-02, -7.6483e-02,  3.3816e-01, -5.8852e-01,
         1.6630e-02,  8.2448e-03], device='cuda:2', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1347,  0.2989, -0.5339,  ..., -0.5265, -0.2761, -0.0241],
        [-0.4257,  0.3315,  0.2079,  ..., -0.1017,  0.1317,  0.5264],
        [-0.6097, -0.1783,  0.2613,  ...,  0.3783, -0.4814, -0.0344],
        ...,
        [ 0.0203, -0.2584, -0.2590,  ..., -0.4645, -0.5785, -0.1913],
        [-0.1921,  0.9825,  0.0440,  ...,  0.2357, -0.6674,  0.3438],
        [ 0.4051, -0.2642,  0.7370,  ...,  0.3095,  0.2294,  0.2429]],
       device='cuda:2', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 5.1383e-01,  2.2676e-01, -1.8156e-01, -2.9724e-01, -1.5685e-01,
         6.0414e-01,  6.1232e-02,  2.7250e-02, -2.3418e-01, -4.5196e-02,
        -2.3660e-01, -6.8167e-01,  5.7750e-02,  3.6466e-01,  3.1874e-01,
         5.3432e-02,  1.8438e-01,  8.8736e-02,  3.0512e-01,  5.2947e-02,
         8.0293e-03,  6.4511e-02,  1.7163e-01,  7.7425e-02,  2.7578e-01,
        -2.7774e-01,  6.8803e-01,  4.6933e-01,  2.0090e-01, -4.3950e-01,
        -2.8161e-01,  3.8266e-01, -3.2890e-02, -6.4451e-01,  6.4740e-01,
        -2.4258e-01, -5.4273e-01, -2.0864e-01, -1.6346e-01,  4.2535e-01,
         6.1291e-02, -1.3541e-01,  3.5634e-01,  2.0267e-01, -3.8041e-01,
         4.7205e-01, -1.3605e-02,  3.2818e-01, -5.6446e-01, -2.6980e-01,
        -3.6690e-01,  1.6149e-01, -2.4639e-01, -4.7106e-02,  4.9669e-02,
        -1.6383e-01, -6.6454e-01, -2.0559e-01,  3.5866e-01, -6.5202e-01,
        -2.5192e-01, -1.2759e-01, -1.0010e-01,  5.5464e-01,  2.6002e-01,
         6.1209e-01,  4.5297e-02, -2.3984e-01,  2.4185e-01, -5.3261e-01,
        -1.5795e-03,  3.7480e-01,  2.2998e-01,  2.5373e-01,  6.2845e-01,
        -4.8846e-01, -2.6556e-01,  1.0543e-01, -1.7961e-01, -4.7442e-02,
         2.0987e-01,  1.4915e-01, -5.4043e-01,  6.3419e-01, -1.4536e-02,
        -1.3952e-01,  1.4541e-03, -1.5868e-01, -4.4381e-02, -7.2536e-03,
         3.0550e-01,  1.7987e-01,  2.4523e-01, -2.6376e-01, -5.4468e-01,
         8.4813e-02, -1.6345e-01, -9.2069e-02,  3.3448e-01,  8.8672e-02,
        -4.2712e-01, -7.8307e-02,  1.5671e-01, -1.4268e-01, -2.6539e-01,
         1.5692e-02, -1.4193e-01,  6.3396e-01, -1.6775e-01, -4.3899e-01,
        -2.9766e-01,  2.0886e-01,  6.2453e-02,  1.4515e-01, -1.7576e-01,
        -5.2782e-01,  2.8647e-01, -3.6118e-01, -1.1295e-01, -1.5333e-01,
         3.8649e-02, -2.3035e-01,  4.8449e-01, -6.5390e-01,  1.7446e-01,
        -2.0998e-01, -2.4014e-01,  3.8094e-01,  5.4638e-01, -9.2301e-02,
         7.2799e-01, -5.4894e-02, -2.6638e-01,  1.8212e-01,  6.3314e-02,
         4.4116e-02, -1.3119e-01,  7.7694e-01,  1.6987e-02, -1.0065e-01,
        -4.2424e-01,  5.3672e-01, -1.0983e-01,  1.8426e-01, -1.5716e-01,
         1.5363e-01,  9.4920e-02, -3.5707e-01, -3.0785e-02, -4.7559e-02,
        -6.1601e-01,  1.6270e-01,  1.1011e-01, -1.8307e-01,  2.0297e-01,
        -3.7307e-01,  1.1780e-01, -8.7353e-01, -6.3581e-01, -9.1177e-02,
         4.5015e-01,  5.8748e-01, -2.0362e-02,  5.2032e-01,  1.2865e-01,
        -2.7387e-01,  3.3855e-01, -8.1991e-02, -3.3930e-02, -2.2129e-01,
        -1.1106e-01,  2.9059e-01,  3.2387e-01, -3.3815e-02, -3.7728e-01,
         5.8272e-01,  6.2464e-01, -2.5074e-01, -4.3780e-01,  5.6216e-01,
         1.2451e-01, -1.2899e-01,  1.2758e-01, -9.5528e-02,  2.0078e-01,
        -2.1147e-01, -1.9996e-01,  2.1658e-01, -4.6028e-01,  1.7152e-01,
        -1.7944e-01,  2.5667e-02,  2.6994e-01, -3.3626e-01, -3.3286e-01,
         2.1351e-01,  3.3059e-01,  8.8407e-02,  1.1184e-01, -9.3410e-01,
        -3.5365e-01, -4.0851e-01,  1.9238e-01,  1.9179e-01,  6.0050e-01,
        -6.1300e-01, -7.7526e-02,  4.3369e-01,  1.9656e-01,  7.5422e-02,
         1.2600e-01,  1.8432e-01,  1.6680e-01,  9.4564e-02,  2.1989e-01,
         6.5338e-02,  3.0670e-01,  2.1004e-01, -3.4678e-02,  3.7545e-01,
        -7.9084e-02, -6.3368e-02,  5.0477e-01,  2.8279e-01, -6.6957e-02,
        -8.2414e-02, -4.5131e-01,  8.7372e-01, -1.2054e-01,  2.9910e-01,
        -7.9745e-04,  1.4130e-01,  2.4211e-01,  1.4722e-01,  2.2276e-02,
         6.4288e-01,  1.9376e-01,  2.3107e-01,  4.2620e-02, -1.0629e-01,
         4.9965e-01,  1.2873e-01, -5.4799e-02,  4.2276e-01, -2.7031e-01,
        -1.9287e-01, -2.8321e-01, -4.5494e-01, -1.1602e-01, -1.0290e-01,
        -1.4740e-01,  4.4534e-01,  5.1917e-01, -1.4431e-01, -7.7550e-01,
         1.6413e-01, -2.1547e-01,  3.4814e-01,  2.9550e-02,  1.2023e-01,
         2.7038e-01, -2.4514e-01, -2.5994e-01, -4.0943e-01,  6.1809e-01,
        -6.4172e-02,  1.4789e-01,  4.9981e-01,  5.8535e-01,  2.1067e-01,
        -7.5197e-02, -2.0989e-01, -6.3907e-02, -4.6315e-01,  2.4584e-01,
        -6.1935e-01,  1.9323e-02,  5.7194e-01, -2.9602e-01, -5.6871e-02,
        -4.8013e-01, -2.9334e-02,  4.9256e-02, -4.2844e-01,  1.4215e-01,
        -1.9628e-01, -5.6353e-01,  2.3770e-01, -1.9504e-01, -6.0946e-02,
         8.6788e-01,  1.5729e-01, -3.3453e-01,  4.2605e-02,  3.4771e-01,
         2.2894e-01,  8.9872e-02,  2.3474e-01,  3.7149e-01, -2.5347e-01,
         1.2892e-01, -3.7038e-01,  6.8718e-02, -2.8476e-01,  3.6732e-01,
        -1.4778e-01,  3.5754e-01,  5.1770e-02, -1.2465e-01,  1.4894e-02,
        -5.7676e-03, -4.7557e-01, -5.1856e-01,  1.0399e-01,  8.1136e-02,
         5.3078e-01, -4.8298e-01,  2.6217e-01,  1.1708e-01, -2.2655e-02,
         3.2414e-01,  3.2335e-01, -3.2487e-01, -3.6452e-01,  4.9279e-01,
         2.7096e-01, -2.5982e-01, -2.0117e-01, -2.4121e-01, -1.1426e-01,
        -7.0331e-02, -3.8127e-01, -3.8204e-01, -1.4516e-01, -3.1634e-01,
         2.4916e-01,  2.1327e-01, -2.6620e-01, -1.7288e-01,  2.1561e-02,
         2.7197e-01, -1.3148e-01,  8.5358e-02, -2.5469e-01,  1.9918e-01,
        -8.2631e-02, -3.2942e-01,  4.1063e-01, -2.2063e-01,  1.4496e-01,
         1.9767e-01, -1.2407e-01, -1.5407e-01, -5.4703e-01,  7.7805e-02,
        -4.6431e-01, -1.4431e-01,  9.7252e-02,  6.0515e-01, -5.5042e-01,
        -2.1915e-01, -2.3620e-01,  6.5295e-02,  8.8855e-01,  1.2050e-01,
        -2.8610e-01,  1.6722e-01,  4.6341e-01,  9.4008e-03, -9.6328e-02,
        -3.3636e-01,  4.1791e-01,  1.4727e-01,  4.5041e-01, -4.3296e-01,
        -3.3456e-02,  6.8088e-02, -1.9341e-01, -2.2595e-01,  1.5026e-01,
        -1.9233e-01, -2.3071e-02, -1.5759e-01,  2.4411e-01, -1.4933e-01,
        -7.8673e-02, -5.1893e-01, -1.8274e-01, -3.6652e-01,  3.9821e-01,
        -1.1308e-03,  1.1077e-01,  1.1272e-01,  1.4247e-01, -2.5590e-01,
        -2.6423e-01, -3.1982e-01,  5.4112e-01,  4.2725e-01,  4.2929e-02,
        -1.4493e-01,  4.0068e-01, -1.5884e-01, -4.5532e-01, -3.6434e-01,
        -2.6256e-01, -1.9314e-01, -3.0976e-01,  2.1947e-01,  1.2527e+00,
         4.6440e-02,  5.4746e-02, -4.3317e-02,  3.7618e-01, -3.7077e-01,
        -2.0447e-01, -1.2383e-01,  2.0692e-01,  1.3606e-01,  1.9516e-01,
         2.2275e-01,  1.7480e-01,  3.2938e-01, -5.1290e-02, -7.7177e-01,
        -2.7293e-01,  3.2536e-01, -1.2914e-01,  8.3979e-02,  3.6308e-02,
        -3.2365e-02, -2.9454e-01,  3.3357e-01, -1.0323e-01,  2.0189e-01,
         2.5102e-02,  4.2577e-01, -3.1884e-01,  4.0641e-01,  8.1616e-03,
         3.8972e-01, -1.5253e-01,  6.5812e-01, -3.0501e-01,  1.1704e-01,
        -3.4881e-01,  1.1053e-02, -3.0909e-01, -7.7253e-02,  3.6673e-01,
        -7.6565e-02,  9.1087e-03, -1.7329e-01,  2.6309e-01,  8.9734e-02,
        -4.5330e-01,  1.5570e-02, -4.8282e-01,  4.0811e-02, -3.2425e-02,
        -3.0552e-01,  9.6641e-02, -1.8799e-02,  1.7777e-02,  7.3005e-01,
         1.4098e-01,  1.0050e-01, -1.0706e-01, -1.7136e-01,  2.0919e-01,
         2.9493e-01, -8.6930e-01, -8.4521e-02, -4.6610e-01,  4.6881e-01,
         2.8357e-01,  2.9123e-01,  5.6292e-01, -7.6437e-01,  9.9124e-02,
        -1.5738e-01,  4.0906e-01,  1.5995e-01, -3.8363e-01,  1.4078e-02,
         2.1916e-01,  2.0725e-01, -1.0849e-01, -4.3832e-01, -4.3885e-01,
        -2.4597e-02,  3.0685e-01, -4.8883e-01, -4.4606e-02, -8.3244e-01,
        -3.0004e-01,  3.6187e-01, -1.8884e-01,  1.1498e-01, -2.2350e-01,
         1.8464e-01, -5.0595e-02,  3.7592e-01, -1.6456e-01,  9.8069e-02,
         2.6785e-01,  2.1655e-01,  2.4135e-01, -4.7972e-01,  1.9272e-01,
         1.7350e-01, -4.1612e-01], device='cuda:2', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1730, -0.2277, -0.1308,  ..., -0.0064,  0.0714,  0.0089],
        [-0.3097, -0.3952,  0.1457,  ...,  0.1606, -0.3123,  0.2392],
        [-0.3884,  0.1280, -0.2651,  ..., -0.3274,  0.0380,  0.5866],
        ...,
        [-0.1138,  0.1471, -0.2607,  ..., -0.1059,  0.1240, -0.2748],
        [ 0.1022, -0.0441,  0.4888,  ...,  0.2420, -0.3309, -0.0214],
        [ 0.3723, -0.2546, -0.5360,  ...,  0.0103,  0.2929, -0.4245]],
       device='cuda:2', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 1.4179e-01,  3.2210e-01, -5.6643e-01,  2.7856e-01,  4.0780e-01,
        -5.2465e-01, -4.4860e-01,  6.3692e-01,  1.2261e-01,  3.3203e-01,
         1.1851e-01,  3.5166e-01,  4.1000e-01, -6.7416e-02,  5.7536e-01,
         9.8060e-02,  4.7483e-02, -9.5358e-02,  5.6268e-02,  4.9899e-02,
         2.9323e-01,  5.6371e-01,  2.5355e-01, -7.4552e-02, -3.9356e-01,
         1.9312e-01, -4.2989e-03, -7.0626e-02, -1.7859e-01,  4.3795e-01,
        -3.8399e-01,  5.2805e-02, -1.2964e-01,  5.6573e-02, -1.4092e-01,
         1.6990e-01,  6.7457e-02, -3.0747e-01,  4.2022e-01,  3.0776e-01,
        -3.1603e-02,  1.7747e-01,  1.3542e-01, -2.8546e-01,  6.4476e-01,
         2.1349e-02, -7.8576e-01,  4.7399e-02, -1.9146e-02,  8.6159e-04,
         4.6737e-01,  3.0309e-01, -8.5541e-02, -2.5931e-02, -1.7506e-01,
        -3.3630e-01,  1.6496e-01, -2.1160e-01,  8.8874e-02,  9.7483e-02,
         1.7767e-01,  8.5455e-01,  3.7553e-01,  1.7289e-01,  3.1729e-01,
        -1.1959e-01,  6.3505e-01, -3.6701e-01, -3.2456e-01,  4.0400e-01,
         3.2255e-01, -6.0352e-01,  2.1656e-02, -3.1416e-01, -3.4468e-01,
        -3.2838e-01,  1.2244e-01,  2.3571e-01,  5.9361e-01,  2.3576e-01,
         2.9457e-01,  2.5872e-01,  5.8149e-01, -4.4340e-01, -5.3872e-01,
        -2.0802e-01,  4.6460e-01, -2.5942e-01, -1.6369e-01, -3.9992e-01,
        -4.5108e-01,  1.5733e-01,  1.2554e-03,  1.2605e-01,  1.7727e-01,
        -1.0899e-01,  4.6134e-01, -1.1725e-01, -1.8230e-01, -3.0823e-01,
         1.0486e-02,  6.9209e-04, -1.7649e-01,  2.0212e-01, -1.1128e-01,
        -2.9200e-01, -4.3355e-01, -2.6220e-02, -3.9349e-01,  7.9136e-02,
         1.3552e-01, -5.1725e-01, -1.9342e-01, -9.7791e-02,  7.8979e-03,
        -3.1055e-01, -3.4341e-01, -1.3593e-01,  1.9896e-01, -1.7927e-01,
        -5.4761e-01, -2.2198e-01,  4.2610e-01,  8.2436e-02, -2.0173e-01,
         2.4040e-02, -1.8343e-01, -2.1978e-01,  3.5884e-01, -7.7458e-01,
         2.3324e-01,  1.3607e-01, -3.9449e-01,  4.1063e-01, -5.3288e-01,
         1.1797e-01, -3.6661e-01,  5.9781e-01, -2.7411e-01,  8.7778e-02,
         1.4884e-01, -3.0281e-01, -6.3868e-01, -1.1967e-01,  2.4771e-01,
         5.4744e-01, -1.4941e-01,  2.3260e-02,  3.9616e-04, -4.2770e-01,
        -4.0471e-01, -2.0044e-01,  3.2872e-01,  4.6651e-02,  2.4371e-01,
         1.9984e-01,  1.5637e-01, -8.4627e-02,  8.0435e-02,  1.6755e-01,
         2.4172e-01, -5.1212e-01,  1.8761e-01, -2.4661e-01,  1.5017e-01,
        -4.1426e-01,  3.1818e-01, -2.4743e-02, -6.2680e-02,  1.5470e-01,
         1.4934e-01,  2.2034e-01,  1.2142e-01,  2.1281e-01,  5.2308e-01,
         1.7412e-01,  2.7777e-01,  1.9550e-01,  5.3673e-01,  1.1650e-01,
        -1.3916e-01, -6.9720e-01,  6.8880e-01,  2.5903e-01, -1.0851e-01,
         1.5277e-01,  3.2483e-01, -2.9587e-02, -2.5160e-01, -8.1416e-02,
         7.9530e-02, -2.6971e-01,  7.0683e-01, -9.2652e-02, -2.7599e-01,
        -2.8310e-01, -3.0244e-01,  2.5042e-01, -2.0662e-01, -1.3467e-01,
        -3.8732e-01, -3.3861e-01, -2.8108e-01, -3.0866e-01, -6.7359e-02,
         3.5845e-01, -1.1492e-01, -2.6174e-01,  7.9780e-01,  6.6983e-02,
        -3.5975e-01,  4.4830e-01,  2.3262e-01, -3.8860e-01,  3.2931e-01,
        -5.0148e-01,  3.9907e-01,  6.3266e-01,  1.0864e-01,  3.5626e-02,
         4.6151e-01, -4.1601e-01,  2.0126e-01,  2.8901e-01, -2.9050e-01,
         1.5636e-01,  1.5314e-01, -2.6690e-01,  2.0631e-01, -1.2344e-01,
         5.0111e-02, -2.8826e-01, -1.3203e-01, -1.0933e-01, -1.2740e-01,
        -1.6363e-01, -3.2539e-02, -4.1435e-02,  2.0555e-01,  2.7631e-01,
        -1.3128e-01,  1.9418e-01,  2.1531e-01,  3.0464e-01,  1.2553e-01,
        -4.3558e-03,  3.5148e-03,  4.5660e-02, -2.6346e-01, -1.2613e-01,
        -1.6006e-01,  2.5396e-01,  2.3544e-03,  2.7709e-01,  2.7365e-01,
         1.5975e-01, -2.0429e-01,  2.6308e-01,  1.1232e-01, -1.7520e-01,
        -2.9343e-02,  1.9986e-01,  3.7351e-01,  1.8321e-03,  2.8144e-01,
         1.6961e-01,  1.1166e-01, -4.7342e-01,  2.5215e-01,  1.2465e-01,
        -1.7574e-01, -2.3487e-01, -3.2847e-02, -3.3511e-01, -1.6397e-01,
         9.2866e-02,  6.8842e-01, -1.2720e-01,  3.8358e-03, -1.8727e-01,
         1.1597e-01,  1.0454e-01, -2.6154e-01,  2.1201e-01, -4.5223e-01,
         2.0397e-01, -6.3676e-01, -1.0293e-01, -2.7385e-01, -3.9719e-01,
         1.6923e-01,  8.8046e-02,  1.1130e-01, -3.8459e-01, -3.7879e-01,
         5.1172e-01,  4.1529e-01, -3.8174e-01, -1.2938e-01,  3.2678e-01,
         4.4141e-01,  5.7247e-01,  5.7335e-01,  2.2937e-01,  1.7478e-01,
         3.0146e-01,  2.0555e-01, -8.7688e-01,  2.6807e-01,  3.9275e-01,
         3.8590e-02, -3.1944e-01, -5.3004e-02,  1.4634e-02,  1.4402e-01,
        -2.0849e-01,  1.3237e-01,  1.5445e-01, -2.2459e-02,  2.4120e-01,
         1.2028e-01, -1.7279e-01,  4.2801e-01, -7.6152e-01, -2.1103e-01,
         6.2899e-01, -1.6014e-01,  5.1742e-02,  4.7674e-01,  5.3797e-02,
        -1.6155e-01,  1.0281e-01, -5.7464e-01,  2.1592e-01, -1.2658e-01,
         4.8350e-02,  3.5443e-02,  1.1428e-01, -4.5882e-01, -9.1370e-02,
        -8.0386e-02, -5.6888e-02,  7.8916e-02,  1.6889e-02,  9.4458e-02,
        -1.0846e-02,  1.4602e-01,  2.9974e-01, -1.2817e-01,  1.4236e-01,
        -5.1285e-01, -4.5173e-01,  3.7114e-01, -6.4035e-02,  3.9342e-01,
         3.0148e-01, -8.0483e-02, -7.5345e-02, -3.7438e-01, -1.1560e-01,
        -7.6414e-01,  1.1941e-01,  3.2946e-01,  1.6418e-01,  2.5149e-01,
         2.8158e-01, -3.1064e-01, -2.1117e-01, -3.6134e-01, -1.7946e-01,
        -3.8067e-01, -1.8442e-02,  5.4178e-01,  1.0826e-01, -4.2489e-01,
         9.8786e-02, -1.9886e-01, -1.9944e-01, -2.6189e-01,  3.2915e-01,
         2.7132e-01, -2.7107e-01,  1.9721e-01, -3.4363e-02, -3.4736e-01,
         7.6883e-01,  7.6609e-01,  1.3300e-01,  1.0317e-01, -9.9651e-02,
        -1.5886e-01,  3.3379e-01,  1.1817e-01,  6.1088e-01, -9.8981e-02,
         3.2033e-02, -3.2494e-01,  6.3448e-01, -1.8965e-02, -1.0558e-01,
         1.8887e-01,  2.4920e-02, -4.7867e-01,  1.6645e-03,  1.6404e-01,
        -2.1174e-01,  5.4358e-01, -1.9561e-01, -6.2061e-01, -5.3127e-01,
         1.8306e-01,  5.1041e-01,  2.6210e-01,  4.7968e-01,  4.0350e-01,
         1.7007e-01, -6.5120e-02,  3.2445e-02, -2.4497e-01,  3.8961e-02,
         3.3875e-01, -2.3760e-01,  4.1712e-01,  1.6046e-01, -2.4134e-01,
         1.4197e-01, -1.9834e-01,  4.0619e-01,  2.6777e-01,  2.4601e-01,
         9.6592e-02,  2.2297e-02,  4.6209e-01,  3.8926e-01,  1.6503e-01,
         1.6308e-01, -2.7902e-01, -1.7507e-01, -1.0870e-01,  4.1737e-02,
         1.2515e-01, -1.6289e-01,  2.8835e-01, -5.0282e-01,  2.9335e-01,
         5.7595e-01, -2.0366e-01,  4.6764e-02, -2.6312e-01,  4.7355e-01,
        -3.3565e-01,  2.4745e-01, -8.0281e-02,  3.2135e-02, -6.0093e-01,
         4.9367e-01, -3.1144e-01,  2.6010e-01,  6.3402e-02, -1.1066e-02,
        -2.8310e-01, -5.9138e-01,  1.9894e-02, -5.6878e-01,  4.3710e-01,
        -1.8511e-01,  1.4252e-01, -4.7774e-01, -1.0077e-01, -2.5265e-01,
         1.0201e-01, -2.4135e-01, -3.3036e-01,  4.7532e-02,  1.9244e-01,
        -1.9187e-01,  3.5796e-02,  2.8442e-01, -2.9295e-01,  2.3329e-02,
         2.8451e-01, -1.1799e-01,  2.6453e-01,  1.4019e-01, -7.1528e-02,
         1.0160e+00,  2.1072e-01,  1.9396e-01, -1.2166e-01, -6.4238e-02,
        -1.6593e-01,  2.4005e-01, -2.9599e-01, -1.6909e-01,  9.1479e-03,
        -5.0724e-02, -3.1212e-01,  4.8560e-01,  3.9701e-01,  1.2635e-01,
        -5.3069e-01, -8.0266e-02,  3.7003e-01, -3.5217e-01,  8.7458e-02,
         5.4936e-01, -4.9864e-01,  1.0496e-01,  1.4389e-01, -7.2095e-01,
         1.7354e-01, -1.0492e-01], device='cuda:2', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 0.7570,  0.0071,  0.1389,  0.6667,  0.1712,  0.3561,  0.6226, -0.3891,
          0.5041, -0.1839, -0.3202, -0.1969,  0.1615,  0.0672,  0.6710, -0.3528,
         -0.3141, -0.2882, -0.8871,  0.1790, -0.3063,  0.4413, -1.0605, -0.2951,
         -0.3255, -0.0646,  0.2149, -0.5881, -0.2235, -0.0753,  0.5737, -0.5544,
         -0.0257, -0.1990, -0.4254, -0.1341,  0.0489, -0.3904, -0.2764, -0.3566,
          0.1979,  0.6079, -0.1551, -0.4622,  0.5872, -0.5354, -0.3222, -0.0805,
         -0.3098, -0.0448,  0.7059,  0.6188,  0.2833,  0.2381, -0.2853, -0.1509,
         -0.4050, -0.2043, -0.2229, -0.4423,  0.1936, -0.2756, -0.6272, -0.0563,
          0.0129,  0.0949,  0.0776,  0.0665,  0.0221, -0.1633, -0.2864,  0.3297,
          0.3653,  0.2385, -0.1601,  0.0213,  0.0414,  0.4344, -0.1426,  0.3209,
         -0.1330, -0.0715,  0.1667, -0.1738, -0.5722, -0.5953, -0.0185, -0.6191,
          0.4194,  0.3419,  0.0859,  0.2053,  0.2619, -0.2133, -0.4736,  0.0040,
         -0.3127, -0.3731, -0.2702,  0.1135,  0.2918,  0.2723, -0.2393, -0.0356,
          0.3593, -0.0179, -0.0283,  0.2146,  0.2183,  0.4363, -0.0966,  0.5011,
          0.4738, -0.2301,  0.3377,  0.5182,  0.2462, -0.2835, -0.2046, -0.0658,
          0.4032,  0.2224, -0.2755,  0.8335,  0.2519, -0.2333,  0.0282,  0.0173,
         -0.2267,  0.3252, -0.0499,  0.0456, -0.2739,  0.3119, -0.1075, -0.3342,
          0.2032, -0.3366, -0.2031,  0.1704, -0.1464, -0.3186,  0.1348, -0.2662,
         -0.4975,  0.1851,  0.2307,  0.0125, -0.1342,  0.5029, -0.3441, -0.0833,
         -0.5997,  0.1109,  0.1963,  0.0987, -0.6184,  0.0578,  0.1665,  0.3195,
          0.2226,  0.3615, -0.7867, -0.3353, -0.0744,  0.2562, -0.1247,  0.2649,
         -0.3539, -0.0216,  0.0795,  0.0297, -0.0140,  0.0654,  0.0853, -0.0177,
          0.1761, -0.1112,  0.1291, -0.4507, -0.3451, -0.2394,  0.4435,  0.4105,
         -0.4136, -0.0931, -0.5896,  0.0572, -0.3367, -0.3010,  0.0908, -0.1063,
         -0.5687,  0.1835, -0.1890, -0.0747,  0.2271,  0.1515, -0.0937,  0.0105,
         -0.5979,  0.5121,  0.2226,  0.2394, -0.1796, -0.5596, -0.1476, -0.3604,
          0.0613, -0.6463, -0.2884,  0.0498,  0.0476, -0.1238,  0.0033, -0.1810,
          0.2991, -0.7201, -0.0066,  0.6760,  0.0838, -0.2200,  0.5599, -0.4012,
          0.0079,  0.4337,  0.0768,  0.5233,  0.0051, -0.1023, -0.2747,  0.1599,
         -0.0071,  0.3136,  0.1999, -0.0581,  0.1474, -0.4202,  0.3128, -0.0721,
          0.1800, -0.6434,  0.2795,  0.0963,  0.1675, -0.2505,  0.5946,  0.0881,
         -0.3420, -0.4583,  0.2318, -0.1727,  0.3149, -0.2287, -0.0798, -0.1796,
          0.2559,  0.4442, -0.1899, -0.2371, -0.5489, -0.4815,  0.0435, -0.3169,
          0.4664,  0.0213,  0.0719, -0.0919,  0.0049,  0.0717, -0.2498, -0.1512,
          0.0769, -0.3439,  0.0776, -0.3771,  0.3978, -0.3865,  0.0013,  0.3145,
         -0.2653,  0.3279, -0.5856, -0.6437, -0.3371,  0.2609, -0.0104, -0.0156,
          0.1098,  0.2341,  0.1565,  0.2076,  0.0886, -0.3759, -0.1986,  0.2679,
          0.2184,  0.0187, -0.2609, -0.0485,  0.2321,  0.2120, -0.3796, -0.1598,
         -0.2731, -0.2402, -0.7589, -0.0833,  0.2278,  0.1961,  0.3413, -0.0987,
          0.4173, -0.0623, -0.5167, -0.5240, -0.3195, -0.2283,  0.4674, -0.0255,
         -0.5547,  0.4798, -0.4382, -0.0420, -0.1176,  0.0931, -0.0501,  0.4722,
          0.6665, -0.0861,  0.6480, -0.0445, -0.3785, -0.0294,  0.3172, -0.1142,
          0.0714, -0.1573,  0.0901, -0.3999,  0.2734,  0.3352,  0.1221,  0.1154,
         -0.2944,  0.4117, -0.2558, -0.0271, -0.0529, -0.1143, -0.3698,  0.3019,
         -0.3937, -0.0406,  0.4142, -0.0227,  0.1544,  0.1405,  0.6259, -0.3281,
         -0.1419,  0.6239,  0.0778, -0.2243, -0.2499,  0.0725, -0.4267, -0.4464,
          0.5441, -0.0022, -0.2501,  0.1621, -0.2522,  0.2555, -0.1617, -0.1284,
         -0.0315,  0.1421, -0.1258, -0.0346,  0.0374,  0.2211, -0.6120,  0.1248,
         -0.3892, -0.1977,  0.4000,  0.2652,  0.1389, -0.1081,  0.6099,  0.2467,
         -0.0990,  0.2355,  0.4636, -0.2268,  0.4451, -0.2121, -0.5002, -0.0408,
          0.2536,  0.3768, -0.6709,  0.2756, -0.2226, -0.4895, -0.5475,  0.0174,
          0.4232,  0.3590,  0.6767,  0.3510, -0.0845, -0.0845, -0.4329, -0.2155,
         -0.0831,  0.1408,  0.3465,  0.3161,  0.3256,  0.1817, -0.4837,  0.2695,
         -0.1407, -0.2695,  0.2053,  0.8556, -0.1175, -0.1025,  0.1077,  0.5011,
         -0.3269,  0.2269, -0.1758, -0.2570,  0.0990, -0.4238, -0.2143, -0.4339,
          0.5493,  0.0367, -0.0246,  0.2124,  0.3997, -0.2968, -0.6228,  0.4566,
          0.2689, -0.6811,  0.2121,  0.0886,  0.2624,  0.4336,  0.1599,  0.4324,
          0.5957, -0.5103,  0.8678, -0.3761, -0.3902, -0.2045, -0.1148,  0.1110,
         -0.3288, -0.1500,  0.7500, -0.5143,  0.4097, -0.2637,  0.2661, -0.3321,
          0.1945,  0.0906, -0.0694,  0.2230, -0.0518,  0.0263,  0.7312,  0.0204,
         -0.3837, -0.0236, -0.0973,  0.4279, -0.1067, -0.4604,  0.3151,  0.2388,
          0.0919,  0.3406, -0.2363, -0.3606,  0.5200,  0.1322, -0.2743,  0.1592,
         -0.4984,  0.0436, -0.2330, -0.0258,  0.0537, -0.4252,  0.3755, -0.1834,
         -0.2058, -0.5455,  0.1601, -0.1842, -0.2745, -0.3696, -0.3067,  0.3493]],
       device='cuda:2', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.1671], device='cuda:2', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:2', grad_fn=<AddBackward0>)
Using device: cuda:2
===== Training profile tensin-3x512-s3 - 1 =====
[0:00:01.784701] epoch: 0 | elbo: 6249055.96 | train_rmse: 66.8983 | val_rmse: 69.1621 | val_ll: -28.1492
[0:01:34.148863] epoch: 50 | elbo: 2387344.4825 | train_rmse: 16.3015 | val_rmse: 19.605 | val_ll: -5.5445
[0:03:05.144224] epoch: 100 | elbo: 2282901.8525 | train_rmse: 11.2056 | val_rmse: 16.4224 | val_ll: -4.9667
[0:04:36.584977] epoch: 150 | elbo: 2226769.2600000002 | train_rmse: 8.4547 | val_rmse: 14.8735 | val_ll: -4.6849
[0:06:09.617881] epoch: 200 | elbo: 2184805.6625 | train_rmse: 6.6296 | val_rmse: 13.9437 | val_ll: -4.5404
[0:07:40.520841] epoch: 250 | elbo: 2149579.9625 | train_rmse: 5.2732 | val_rmse: 13.1975 | val_ll: -4.3925
[0:09:11.488816] epoch: 300 | elbo: 2117407.115 | train_rmse: 4.2826 | val_rmse: 12.6009 | val_ll: -4.3059
[0:10:43.161462] epoch: 350 | elbo: 2087874.525 | train_rmse: 3.5403 | val_rmse: 12.0733 | val_ll: -4.2127
[0:12:15.516256] epoch: 400 | elbo: 2058437.0225000002 | train_rmse: 2.9327 | val_rmse: 11.5031 | val_ll: -4.1243
[0:13:47.242018] epoch: 450 | elbo: 2029869.6849999998 | train_rmse: 2.5288 | val_rmse: 10.9748 | val_ll: -4.0313
[0:15:18.643739] epoch: 500 | elbo: 2001433.7525 | train_rmse: 2.2231 | val_rmse: 10.3852 | val_ll: -3.9315
[0:16:49.932412] epoch: 550 | elbo: 1973203.7850000001 | train_rmse: 2.0035 | val_rmse: 9.7823 | val_ll: -3.8203
[0:18:21.018572] epoch: 600 | elbo: 1945036.1149999998 | train_rmse: 1.8998 | val_rmse: 9.1529 | val_ll: -3.7249
[0:19:52.034208] epoch: 650 | elbo: 1916490.5912499998 | train_rmse: 1.7482 | val_rmse: 8.4995 | val_ll: -3.6266
[0:21:22.812714] epoch: 700 | elbo: 1887824.8450000002 | train_rmse: 1.6922 | val_rmse: 7.8463 | val_ll: -3.5115
[0:22:54.175209] epoch: 750 | elbo: 1858892.1024999998 | train_rmse: 1.6281 | val_rmse: 7.2165 | val_ll: -3.4192
[0:24:26.687287] epoch: 800 | elbo: 1830120.68625 | train_rmse: 1.5619 | val_rmse: 6.6517 | val_ll: -3.3043
[0:25:57.989429] epoch: 850 | elbo: 1801466.3849999998 | train_rmse: 1.5362 | val_rmse: 6.07 | val_ll: -3.2126
[0:27:31.020108] epoch: 900 | elbo: 1772865.5650000002 | train_rmse: 1.4796 | val_rmse: 5.58 | val_ll: -3.1156
[0:29:02.720046] epoch: 950 | elbo: 1744361.08125 | train_rmse: 1.4525 | val_rmse: 5.1625 | val_ll: -3.0216
[0:30:35.795411] epoch: 1000 | elbo: 1716030.5987500001 | train_rmse: 1.4193 | val_rmse: 4.7867 | val_ll: -2.9319
[0:32:08.054814] epoch: 1050 | elbo: 1687935.1225 | train_rmse: 1.3664 | val_rmse: 4.4765 | val_ll: -2.8523
[0:33:40.742318] epoch: 1100 | elbo: 1659994.7974999999 | train_rmse: 1.3293 | val_rmse: 4.1982 | val_ll: -2.786
[0:35:11.716627] epoch: 1150 | elbo: 1632179.8975 | train_rmse: 1.2757 | val_rmse: 3.9902 | val_ll: -2.7349
[0:36:43.112841] epoch: 1200 | elbo: 1604495.7025 | train_rmse: 1.2411 | val_rmse: 3.7815 | val_ll: -2.6872
[0:38:16.507722] epoch: 1250 | elbo: 1576949.365 | train_rmse: 1.2058 | val_rmse: 3.6291 | val_ll: -2.6434
[0:39:48.709865] epoch: 1300 | elbo: 1549422.5499999998 | train_rmse: 1.1648 | val_rmse: 3.494 | val_ll: -2.6136
[0:41:20.797129] epoch: 1350 | elbo: 1521909.7187500002 | train_rmse: 1.1381 | val_rmse: 3.3618 | val_ll: -2.5796
[0:42:52.988365] epoch: 1400 | elbo: 1494458.2200000002 | train_rmse: 1.1232 | val_rmse: 3.2373 | val_ll: -2.5496
[0:44:25.087005] epoch: 1450 | elbo: 1467004.5600000003 | train_rmse: 1.0908 | val_rmse: 3.1232 | val_ll: -2.5224
[0:45:58.167622] epoch: 1500 | elbo: 1439535.2762499999 | train_rmse: 1.0795 | val_rmse: 3.0289 | val_ll: -2.5027
[0:47:29.695987] epoch: 1550 | elbo: 1412150.1949999998 | train_rmse: 1.0752 | val_rmse: 2.9275 | val_ll: -2.4811
[0:49:00.955263] epoch: 1600 | elbo: 1384691.69125 | train_rmse: 1.0569 | val_rmse: 2.8213 | val_ll: -2.4599
[0:50:32.877489] epoch: 1650 | elbo: 1357245.0349999997 | train_rmse: 1.0544 | val_rmse: 2.7361 | val_ll: -2.4419
[0:52:03.879409] epoch: 1700 | elbo: 1329848.21 | train_rmse: 1.0551 | val_rmse: 2.6469 | val_ll: -2.4215
[0:53:34.822557] epoch: 1750 | elbo: 1302456.45625 | train_rmse: 1.0556 | val_rmse: 2.5658 | val_ll: -2.4052
[0:55:06.014287] epoch: 1800 | elbo: 1275084.25 | train_rmse: 1.0677 | val_rmse: 2.4863 | val_ll: -2.3887
[0:56:37.490737] epoch: 1850 | elbo: 1247732.55375 | train_rmse: 1.0645 | val_rmse: 2.4054 | val_ll: -2.3771
[0:58:08.849241] epoch: 1900 | elbo: 1220433.99 | train_rmse: 1.0719 | val_rmse: 2.317 | val_ll: -2.3635
[0:59:39.905332] epoch: 1950 | elbo: 1193205.7825 | train_rmse: 1.0809 | val_rmse: 2.2386 | val_ll: -2.3478
[1:01:12.600977] epoch: 2000 | elbo: 1165990.175 | train_rmse: 1.0871 | val_rmse: 2.1702 | val_ll: -2.3366
[1:02:46.355854] epoch: 2050 | elbo: 1138817.93125 | train_rmse: 1.0905 | val_rmse: 2.0922 | val_ll: -2.3246
[1:04:19.453251] epoch: 2100 | elbo: 1111713.38625 | train_rmse: 1.0969 | val_rmse: 2.0222 | val_ll: -2.3139
[1:05:50.536174] epoch: 2150 | elbo: 1084664.92625 | train_rmse: 1.1138 | val_rmse: 1.9578 | val_ll: -2.3048
[1:07:22.256727] epoch: 2200 | elbo: 1057662.3037500002 | train_rmse: 1.121 | val_rmse: 1.887 | val_ll: -2.2952
[1:08:54.613471] epoch: 2250 | elbo: 1030786.2525000001 | train_rmse: 1.1361 | val_rmse: 1.8389 | val_ll: -2.2899
[1:10:25.065129] epoch: 2300 | elbo: 1003887.8300000001 | train_rmse: 1.1442 | val_rmse: 1.7775 | val_ll: -2.2818
[1:11:56.797502] epoch: 2350 | elbo: 977135.009375 | train_rmse: 1.1536 | val_rmse: 1.7306 | val_ll: -2.2741
[1:13:27.780247] epoch: 2400 | elbo: 950420.6606250001 | train_rmse: 1.1593 | val_rmse: 1.6793 | val_ll: -2.2693
[1:14:58.171425] epoch: 2450 | elbo: 923796.96875 | train_rmse: 1.1705 | val_rmse: 1.6409 | val_ll: -2.2643
[1:16:29.601124] epoch: 2500 | elbo: 897274.8456249998 | train_rmse: 1.1839 | val_rmse: 1.6078 | val_ll: -2.2607
[1:18:01.015666] epoch: 2550 | elbo: 870870.6981250001 | train_rmse: 1.194 | val_rmse: 1.5849 | val_ll: -2.2598
[1:19:32.936114] epoch: 2600 | elbo: 844523.5806250001 | train_rmse: 1.2105 | val_rmse: 1.5667 | val_ll: -2.2594
[1:21:06.697718] epoch: 2650 | elbo: 818275.22 | train_rmse: 1.2151 | val_rmse: 1.548 | val_ll: -2.2577
[1:22:39.476936] epoch: 2700 | elbo: 792150.6406249999 | train_rmse: 1.2459 | val_rmse: 1.539 | val_ll: -2.2558
[1:24:11.430724] epoch: 2750 | elbo: 766100.78125 | train_rmse: 1.2573 | val_rmse: 1.5386 | val_ll: -2.2599
[1:25:46.948442] epoch: 2800 | elbo: 740185.651875 | train_rmse: 1.2736 | val_rmse: 1.5303 | val_ll: -2.2616
[1:27:21.370156] epoch: 2850 | elbo: 714371.165 | train_rmse: 1.2978 | val_rmse: 1.5328 | val_ll: -2.2632
[1:28:55.478115] epoch: 2900 | elbo: 688741.200625 | train_rmse: 1.3327 | val_rmse: 1.5382 | val_ll: -2.2637
[1:30:29.120643] epoch: 2950 | elbo: 663156.368125 | train_rmse: 1.3544 | val_rmse: 1.5471 | val_ll: -2.2657
[1:32:02.357561] epoch: 3000 | elbo: 637736.825625 | train_rmse: 1.3803 | val_rmse: 1.5543 | val_ll: -2.27
[1:33:35.343799] epoch: 3050 | elbo: 612506.61875 | train_rmse: 1.4072 | val_rmse: 1.5719 | val_ll: -2.2734
[1:35:08.733669] epoch: 3100 | elbo: 587398.1875 | train_rmse: 1.4268 | val_rmse: 1.5836 | val_ll: -2.2778
[1:36:42.048613] epoch: 3150 | elbo: 562453.766875 | train_rmse: 1.4505 | val_rmse: 1.5987 | val_ll: -2.2808
[1:38:14.346659] epoch: 3200 | elbo: 537769.1712499999 | train_rmse: 1.4935 | val_rmse: 1.6218 | val_ll: -2.2839
[1:39:46.312772] epoch: 3250 | elbo: 513229.16937500006 | train_rmse: 1.5031 | val_rmse: 1.6292 | val_ll: -2.2847
[1:41:18.098723] epoch: 3300 | elbo: 488871.86437500006 | train_rmse: 1.54 | val_rmse: 1.6606 | val_ll: -2.2908
[1:42:49.301919] epoch: 3350 | elbo: 464829.01156250003 | train_rmse: 1.5724 | val_rmse: 1.6779 | val_ll: -2.2967
[1:44:21.200429] epoch: 3400 | elbo: 441054.26812500006 | train_rmse: 1.5946 | val_rmse: 1.6945 | val_ll: -2.3011
[1:45:52.345358] epoch: 3450 | elbo: 417511.28874999995 | train_rmse: 1.6354 | val_rmse: 1.722 | val_ll: -2.3035
[1:47:22.663017] epoch: 3500 | elbo: 394304.2321875 | train_rmse: 1.6472 | val_rmse: 1.7372 | val_ll: -2.3089
[1:48:53.127136] epoch: 3550 | elbo: 371414.8665625 | train_rmse: 1.6874 | val_rmse: 1.7679 | val_ll: -2.3146
[1:50:23.964610] epoch: 3600 | elbo: 348958.51625 | train_rmse: 1.7478 | val_rmse: 1.812 | val_ll: -2.319
[1:51:55.594895] epoch: 3650 | elbo: 326903.9165625 | train_rmse: 1.7645 | val_rmse: 1.8347 | val_ll: -2.3262
[1:53:27.057808] epoch: 3700 | elbo: 305257.881875 | train_rmse: 1.7886 | val_rmse: 1.848 | val_ll: -2.3281
[1:54:58.213639] epoch: 3750 | elbo: 284143.73812500003 | train_rmse: 1.8452 | val_rmse: 1.8943 | val_ll: -2.3337
[1:56:29.230463] epoch: 3800 | elbo: 263556.88156249997 | train_rmse: 1.8854 | val_rmse: 1.9293 | val_ll: -2.3427
[1:57:59.539207] epoch: 3850 | elbo: 243544.33562499998 | train_rmse: 1.8999 | val_rmse: 1.9498 | val_ll: -2.3462
[1:59:30.024212] epoch: 3900 | elbo: 224130.27015624996 | train_rmse: 1.9306 | val_rmse: 1.9714 | val_ll: -2.3524
[2:01:00.844230] epoch: 3950 | elbo: 205459.97640625 | train_rmse: 1.9887 | val_rmse: 2.0195 | val_ll: -2.3577
[2:02:32.575875] epoch: 4000 | elbo: 187574.84375000003 | train_rmse: 2.0004 | val_rmse: 2.0384 | val_ll: -2.3634
[2:04:04.022188] epoch: 4050 | elbo: 170406.96468749997 | train_rmse: 2.0394 | val_rmse: 2.0774 | val_ll: -2.3699
[2:05:35.556804] epoch: 4100 | elbo: 154121.56453124998 | train_rmse: 2.0814 | val_rmse: 2.1101 | val_ll: -2.3777
[2:07:06.732062] epoch: 4150 | elbo: 138820.85031250003 | train_rmse: 2.1394 | val_rmse: 2.1576 | val_ll: -2.3814
[2:08:37.803217] epoch: 4200 | elbo: 124585.66960937502 | train_rmse: 2.1443 | val_rmse: 2.1731 | val_ll: -2.3843
[2:10:08.077331] epoch: 4250 | elbo: 111315.861328125 | train_rmse: 2.1771 | val_rmse: 2.2025 | val_ll: -2.3912
[2:11:38.943169] epoch: 4300 | elbo: 99239.54632812498 | train_rmse: 2.1995 | val_rmse: 2.2215 | val_ll: -2.3979
[2:13:09.199821] epoch: 4350 | elbo: 88376.81343750001 | train_rmse: 2.239 | val_rmse: 2.2542 | val_ll: -2.4031
[2:14:39.912522] epoch: 4400 | elbo: 78731.61875 | train_rmse: 2.2567 | val_rmse: 2.2863 | val_ll: -2.4087
[2:16:11.686298] epoch: 4450 | elbo: 70336.77882812501 | train_rmse: 2.2763 | val_rmse: 2.2948 | val_ll: -2.408
[2:17:43.295255] epoch: 4500 | elbo: 63314.63003906249 | train_rmse: 2.3081 | val_rmse: 2.3234 | val_ll: -2.4081
[2:19:13.898957] epoch: 4550 | elbo: 57563.913242187504 | train_rmse: 2.305 | val_rmse: 2.3208 | val_ll: -2.412
[2:20:45.103911] epoch: 4600 | elbo: 52990.387851562504 | train_rmse: 2.3262 | val_rmse: 2.3353 | val_ll: -2.4082
[2:22:15.838613] epoch: 4650 | elbo: 49647.4412109375 | train_rmse: 2.3185 | val_rmse: 2.3372 | val_ll: -2.409
[2:23:46.868788] epoch: 4700 | elbo: 47290.0634375 | train_rmse: 2.3087 | val_rmse: 2.3237 | val_ll: -2.41
[2:25:20.040945] epoch: 4750 | elbo: 45768.6768359375 | train_rmse: 2.3081 | val_rmse: 2.3164 | val_ll: -2.4102
[2:26:50.930211] epoch: 4800 | elbo: 44820.02437499999 | train_rmse: 2.2932 | val_rmse: 2.3038 | val_ll: -2.4026
[2:28:22.422989] epoch: 4850 | elbo: 44166.4580078125 | train_rmse: 2.2882 | val_rmse: 2.2927 | val_ll: -2.3999
[2:29:53.600935] epoch: 4900 | elbo: 43795.390742187505 | train_rmse: 2.2443 | val_rmse: 2.2685 | val_ll: -2.3968
[2:31:24.377040] epoch: 4950 | elbo: 43519.207773437505 | train_rmse: 2.2099 | val_rmse: 2.2346 | val_ll: -2.3925
[2:32:56.055694] epoch: 5000 | elbo: 43210.968203125 | train_rmse: 2.1987 | val_rmse: 2.2199 | val_ll: -2.3919
[2:34:26.653902] epoch: 5050 | elbo: 42979.212382812504 | train_rmse: 2.186 | val_rmse: 2.2012 | val_ll: -2.3859
[2:35:58.392134] epoch: 5100 | elbo: 42749.8758203125 | train_rmse: 2.1628 | val_rmse: 2.1829 | val_ll: -2.3827
[2:37:30.524595] epoch: 5150 | elbo: 42500.7654296875 | train_rmse: 2.1455 | val_rmse: 2.1707 | val_ll: -2.377
[2:39:01.560261] epoch: 5200 | elbo: 42327.34343750001 | train_rmse: 2.1327 | val_rmse: 2.1627 | val_ll: -2.3785
[2:40:32.286396] epoch: 5250 | elbo: 42256.6937109375 | train_rmse: 2.1044 | val_rmse: 2.1337 | val_ll: -2.3753
[2:42:03.640521] epoch: 5300 | elbo: 42026.315664062495 | train_rmse: 2.0934 | val_rmse: 2.1307 | val_ll: -2.3725
[2:43:34.041348] epoch: 5350 | elbo: 41931.6484765625 | train_rmse: 2.0814 | val_rmse: 2.1062 | val_ll: -2.3664
[2:45:05.047971] epoch: 5400 | elbo: 41741.4984375 | train_rmse: 2.0686 | val_rmse: 2.0931 | val_ll: -2.367
[2:46:36.488383] epoch: 5450 | elbo: 41629.5555859375 | train_rmse: 2.0721 | val_rmse: 2.0898 | val_ll: -2.3649
[2:48:09.139141] epoch: 5500 | elbo: 41532.8541796875 | train_rmse: 2.0483 | val_rmse: 2.0755 | val_ll: -2.3619
[2:49:43.009823] epoch: 5550 | elbo: 41378.207304687494 | train_rmse: 2.0415 | val_rmse: 2.0685 | val_ll: -2.359
[2:51:15.374880] epoch: 5600 | elbo: 41243.7878515625 | train_rmse: 2.0349 | val_rmse: 2.0686 | val_ll: -2.3585
[2:52:46.813055] epoch: 5650 | elbo: 41179.6304296875 | train_rmse: 2.0208 | val_rmse: 2.0473 | val_ll: -2.3571
[2:54:17.712810] epoch: 5700 | elbo: 41015.188203125006 | train_rmse: 2.002 | val_rmse: 2.0257 | val_ll: -2.3553
[2:55:48.458658] epoch: 5750 | elbo: 40920.59328125 | train_rmse: 1.9954 | val_rmse: 2.0205 | val_ll: -2.3527
[2:57:19.368673] epoch: 5800 | elbo: 40880.84957031249 | train_rmse: 1.9835 | val_rmse: 2.0162 | val_ll: -2.3509
[2:58:51.393379] epoch: 5850 | elbo: 40708.225976562506 | train_rmse: 1.9735 | val_rmse: 2.0036 | val_ll: -2.3521
[3:00:22.128932] epoch: 5900 | elbo: 40734.5684375 | train_rmse: 1.9664 | val_rmse: 2.0032 | val_ll: -2.3482
[3:01:55.517244] epoch: 5950 | elbo: 40609.406015625005 | train_rmse: 1.9569 | val_rmse: 1.9883 | val_ll: -2.3456
[3:03:28.299295] epoch: 6000 | elbo: 40477.507421874994 | train_rmse: 1.9478 | val_rmse: 1.9782 | val_ll: -2.3431
[3:05:00.629489] epoch: 6050 | elbo: 40416.606015625 | train_rmse: 1.9245 | val_rmse: 1.9642 | val_ll: -2.3421
[3:06:31.772420] epoch: 6100 | elbo: 40407.381875 | train_rmse: 1.9238 | val_rmse: 1.9511 | val_ll: -2.3401
[3:08:02.792038] epoch: 6150 | elbo: 40239.307734375 | train_rmse: 1.9126 | val_rmse: 1.9458 | val_ll: -2.3399
[3:09:35.776300] epoch: 6200 | elbo: 40151.477070312496 | train_rmse: 1.9016 | val_rmse: 1.9446 | val_ll: -2.3404
[3:11:08.682058] epoch: 6250 | elbo: 40047.3332421875 | train_rmse: 1.9063 | val_rmse: 1.9333 | val_ll: -2.3354
[3:12:43.270804] epoch: 6300 | elbo: 39968.480859375 | train_rmse: 1.901 | val_rmse: 1.9352 | val_ll: -2.3372
[3:14:15.256346] epoch: 6350 | elbo: 39897.1480078125 | train_rmse: 1.881 | val_rmse: 1.9123 | val_ll: -2.3345
[3:15:47.159198] epoch: 6400 | elbo: 39840.926484375 | train_rmse: 1.8833 | val_rmse: 1.9192 | val_ll: -2.3321
[3:17:19.931313] epoch: 6450 | elbo: 39800.336640625006 | train_rmse: 1.8627 | val_rmse: 1.8939 | val_ll: -2.3314
[3:18:51.998781] epoch: 6500 | elbo: 39817.2984375 | train_rmse: 1.8689 | val_rmse: 1.8988 | val_ll: -2.3287
[3:20:23.566301] epoch: 6550 | elbo: 39650.15359375 | train_rmse: 1.8597 | val_rmse: 1.8925 | val_ll: -2.3279
[3:21:56.375471] epoch: 6600 | elbo: 39614.18621093749 | train_rmse: 1.8511 | val_rmse: 1.8782 | val_ll: -2.3248
[3:23:29.020707] epoch: 6650 | elbo: 39519.359921875 | train_rmse: 1.8351 | val_rmse: 1.873 | val_ll: -2.3258
[3:25:01.058984] epoch: 6700 | elbo: 39464.55375000001 | train_rmse: 1.8346 | val_rmse: 1.8728 | val_ll: -2.3223
[3:26:31.731778] epoch: 6750 | elbo: 39356.456796875005 | train_rmse: 1.824 | val_rmse: 1.8567 | val_ll: -2.3232
[3:28:03.070660] epoch: 6800 | elbo: 39289.300273437504 | train_rmse: 1.8095 | val_rmse: 1.8425 | val_ll: -2.3185
[3:29:33.429566] epoch: 6850 | elbo: 39240.202656249996 | train_rmse: 1.8076 | val_rmse: 1.8491 | val_ll: -2.3184
[3:31:04.937138] epoch: 6900 | elbo: 39254.30664062499 | train_rmse: 1.7953 | val_rmse: 1.8359 | val_ll: -2.3176
[3:32:36.693372] epoch: 6950 | elbo: 39122.925624999996 | train_rmse: 1.79 | val_rmse: 1.8297 | val_ll: -2.3156
[3:34:09.442663] epoch: 7000 | elbo: 39099.8467578125 | train_rmse: 1.7837 | val_rmse: 1.8211 | val_ll: -2.3142
[3:35:40.994609] epoch: 7050 | elbo: 39025.2599609375 | train_rmse: 1.7873 | val_rmse: 1.8365 | val_ll: -2.313
[3:37:13.498364] epoch: 7100 | elbo: 38953.194257812494 | train_rmse: 1.7628 | val_rmse: 1.8036 | val_ll: -2.3121
[3:38:46.655929] epoch: 7150 | elbo: 38955.11093750001 | train_rmse: 1.7611 | val_rmse: 1.8035 | val_ll: -2.3116
[3:40:17.901981] epoch: 7200 | elbo: 38862.6507421875 | train_rmse: 1.7576 | val_rmse: 1.7892 | val_ll: -2.31
[3:41:48.673233] epoch: 7250 | elbo: 38812.6448046875 | train_rmse: 1.747 | val_rmse: 1.7909 | val_ll: -2.3101
[3:43:19.948507] epoch: 7300 | elbo: 38710.738359375 | train_rmse: 1.7339 | val_rmse: 1.7723 | val_ll: -2.3059
[3:44:50.838778] epoch: 7350 | elbo: 38716.07953125 | train_rmse: 1.7279 | val_rmse: 1.7701 | val_ll: -2.3069
[3:46:22.159585] epoch: 7400 | elbo: 38589.749726562506 | train_rmse: 1.7218 | val_rmse: 1.7697 | val_ll: -2.3041
[3:47:52.934058] epoch: 7450 | elbo: 38513.338359375004 | train_rmse: 1.7269 | val_rmse: 1.7536 | val_ll: -2.3029
[3:49:23.446972] epoch: 7500 | elbo: 38451.3253515625 | train_rmse: 1.7135 | val_rmse: 1.7526 | val_ll: -2.3037
[3:50:54.258519] epoch: 7550 | elbo: 38472.40792968749 | train_rmse: 1.7013 | val_rmse: 1.7404 | val_ll: -2.3041
[3:52:25.562359] epoch: 7600 | elbo: 38353.35484375 | train_rmse: 1.6993 | val_rmse: 1.7371 | val_ll: -2.2999
[3:53:56.154660] epoch: 7650 | elbo: 38308.3808203125 | train_rmse: 1.681 | val_rmse: 1.7331 | val_ll: -2.296
[3:55:27.853465] epoch: 7700 | elbo: 38274.95214843751 | train_rmse: 1.6804 | val_rmse: 1.7105 | val_ll: -2.2951
[3:57:00.508588] epoch: 7750 | elbo: 38182.17984375 | train_rmse: 1.676 | val_rmse: 1.7207 | val_ll: -2.296
[3:58:32.160194] epoch: 7800 | elbo: 38125.19015625 | train_rmse: 1.6801 | val_rmse: 1.712 | val_ll: -2.2937
[4:00:03.527005] epoch: 7850 | elbo: 38065.902890625 | train_rmse: 1.6687 | val_rmse: 1.7102 | val_ll: -2.2913
[4:01:36.575903] epoch: 7900 | elbo: 38018.3197265625 | train_rmse: 1.6466 | val_rmse: 1.6968 | val_ll: -2.2935
[4:03:09.946834] epoch: 7950 | elbo: 37943.4908203125 | train_rmse: 1.656 | val_rmse: 1.7003 | val_ll: -2.2913
[4:04:42.885127] epoch: 8000 | elbo: 37938.3936328125 | train_rmse: 1.6404 | val_rmse: 1.6965 | val_ll: -2.2882
[4:06:15.406878] epoch: 8050 | elbo: 37905.42511718751 | train_rmse: 1.6453 | val_rmse: 1.6937 | val_ll: -2.2894
[4:07:47.344319] epoch: 8100 | elbo: 37807.164921875 | train_rmse: 1.6188 | val_rmse: 1.6704 | val_ll: -2.2863
[4:09:19.211621] epoch: 8150 | elbo: 37831.17589843749 | train_rmse: 1.6104 | val_rmse: 1.6637 | val_ll: -2.2868
[4:10:50.783222] epoch: 8200 | elbo: 37770.149531250005 | train_rmse: 1.6183 | val_rmse: 1.6593 | val_ll: -2.2851
[4:12:22.955323] epoch: 8250 | elbo: 37685.877695312505 | train_rmse: 1.6129 | val_rmse: 1.6575 | val_ll: -2.2837
[4:13:53.390759] epoch: 8300 | elbo: 37696.659687499996 | train_rmse: 1.6091 | val_rmse: 1.658 | val_ll: -2.2832
[4:15:24.445065] epoch: 8350 | elbo: 37561.92964843749 | train_rmse: 1.5993 | val_rmse: 1.6479 | val_ll: -2.2835
[4:16:56.407158] epoch: 8400 | elbo: 37535.089687499996 | train_rmse: 1.6082 | val_rmse: 1.6544 | val_ll: -2.2838
[4:18:28.647009] epoch: 8450 | elbo: 37512.275859375 | train_rmse: 1.5908 | val_rmse: 1.6378 | val_ll: -2.2804
[4:20:00.880209] epoch: 8500 | elbo: 37424.531796875 | train_rmse: 1.5763 | val_rmse: 1.6249 | val_ll: -2.2812
[4:21:32.997583] epoch: 8550 | elbo: 37352.49546875 | train_rmse: 1.5836 | val_rmse: 1.6256 | val_ll: -2.2789
[4:23:04.278518] epoch: 8600 | elbo: 37359.1219921875 | train_rmse: 1.5621 | val_rmse: 1.6156 | val_ll: -2.2748
[4:24:35.760477] epoch: 8650 | elbo: 37345.336054687505 | train_rmse: 1.571 | val_rmse: 1.6166 | val_ll: -2.2759
[4:26:08.110700] epoch: 8700 | elbo: 37287.832265624995 | train_rmse: 1.565 | val_rmse: 1.6133 | val_ll: -2.2733
[4:27:39.827215] epoch: 8750 | elbo: 37223.364960937506 | train_rmse: 1.5463 | val_rmse: 1.6028 | val_ll: -2.2752
[4:29:12.226335] epoch: 8800 | elbo: 37122.302500000005 | train_rmse: 1.5481 | val_rmse: 1.5988 | val_ll: -2.2722
[4:30:43.283342] epoch: 8850 | elbo: 37088.5841015625 | train_rmse: 1.5485 | val_rmse: 1.5897 | val_ll: -2.27
[4:32:14.763867] epoch: 8900 | elbo: 37089.2903125 | train_rmse: 1.5374 | val_rmse: 1.5926 | val_ll: -2.2725
[4:33:46.531624] epoch: 8950 | elbo: 37003.8053125 | train_rmse: 1.5274 | val_rmse: 1.5819 | val_ll: -2.2704
[4:35:18.332252] epoch: 9000 | elbo: 36953.465937500005 | train_rmse: 1.5325 | val_rmse: 1.5755 | val_ll: -2.2687
[4:36:50.581369] epoch: 9050 | elbo: 36894.955390625 | train_rmse: 1.5215 | val_rmse: 1.5669 | val_ll: -2.2641
[4:38:21.419152] epoch: 9100 | elbo: 36886.05671875 | train_rmse: 1.5104 | val_rmse: 1.565 | val_ll: -2.2644
[4:39:52.445102] epoch: 9150 | elbo: 36851.1176171875 | train_rmse: 1.5017 | val_rmse: 1.5509 | val_ll: -2.2644
[4:41:23.343296] epoch: 9200 | elbo: 36765.098164062496 | train_rmse: 1.5068 | val_rmse: 1.5624 | val_ll: -2.2627
[4:42:54.269787] epoch: 9250 | elbo: 36749.093359375 | train_rmse: 1.4991 | val_rmse: 1.551 | val_ll: -2.2622
[4:44:25.772302] epoch: 9300 | elbo: 36732.096835937504 | train_rmse: 1.4957 | val_rmse: 1.5462 | val_ll: -2.2597
[4:45:56.408361] epoch: 9350 | elbo: 36641.75015625001 | train_rmse: 1.4827 | val_rmse: 1.544 | val_ll: -2.2609
[4:47:28.365924] epoch: 9400 | elbo: 36611.481250000004 | train_rmse: 1.4815 | val_rmse: 1.5296 | val_ll: -2.2595
[4:49:00.026718] epoch: 9450 | elbo: 36597.3619140625 | train_rmse: 1.4798 | val_rmse: 1.5359 | val_ll: -2.2592
[4:50:31.224837] epoch: 9500 | elbo: 36564.825312500005 | train_rmse: 1.4795 | val_rmse: 1.5257 | val_ll: -2.2562
[4:52:02.337457] epoch: 9550 | elbo: 36520.95828125001 | train_rmse: 1.4613 | val_rmse: 1.5251 | val_ll: -2.2573
[4:53:33.993546] epoch: 9600 | elbo: 36404.708164062504 | train_rmse: 1.4705 | val_rmse: 1.5213 | val_ll: -2.2563
[4:55:04.522329] epoch: 9650 | elbo: 36392.5367578125 | train_rmse: 1.4583 | val_rmse: 1.5114 | val_ll: -2.2533
[4:56:34.626889] epoch: 9700 | elbo: 36372.3175 | train_rmse: 1.4404 | val_rmse: 1.513 | val_ll: -2.2519
[4:58:06.754734] epoch: 9750 | elbo: 36326.588437499995 | train_rmse: 1.4507 | val_rmse: 1.4996 | val_ll: -2.2498
[4:59:39.016811] epoch: 9800 | elbo: 36286.11980468751 | train_rmse: 1.44 | val_rmse: 1.4998 | val_ll: -2.2514
[5:01:10.662378] epoch: 9850 | elbo: 36248.7109375 | train_rmse: 1.441 | val_rmse: 1.5074 | val_ll: -2.2543
[5:02:42.555958] epoch: 9900 | elbo: 36143.8876171875 | train_rmse: 1.4379 | val_rmse: 1.4887 | val_ll: -2.2494
[5:04:14.329585] epoch: 9950 | elbo: 36086.4276171875 | train_rmse: 1.4267 | val_rmse: 1.4902 | val_ll: -2.2496
Training finished in 5:05:43.612231 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-s3/checkpoint_1.pt
File Size is 4.0595598220825195 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:2 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 3.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-1.2482e-02,  1.0787e-02,  1.4589e-02,  ..., -8.8618e-03,
         -6.4866e-03, -1.4695e-04],
        [-1.4177e-02,  1.3724e-02,  1.4491e-02,  ..., -1.3397e-02,
         -8.5654e-03, -2.8757e-03],
        [-1.2118e-02,  1.1444e-02,  1.7092e-02,  ..., -1.0889e-02,
         -6.6340e-03, -1.6519e-03],
        ...,
        [-1.6037e-02,  9.2867e-03,  1.1879e-02,  ..., -1.1949e-02,
         -7.3565e-03, -4.5316e-03],
        [-5.9758e-03, -7.2097e-04, -1.2571e-02,  ..., -2.3533e-01,
         -2.4784e-01,  1.9132e-02],
        [-1.3283e-02,  9.2209e-03,  1.0978e-02,  ..., -9.9826e-03,
         -5.3587e-03, -5.6016e-03]], device='cuda:2', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.4218, 0.4334, 0.4337,  ..., 0.4321, 0.4294, 0.4238],
        [0.4235, 0.4343, 0.4347,  ..., 0.4319, 0.4311, 0.4241],
        [0.4222, 0.4332, 0.4345,  ..., 0.4309, 0.4294, 0.4222],
        ...,
        [0.4225, 0.4340, 0.4344,  ..., 0.4312, 0.4284, 0.4225],
        [0.4048, 0.4171, 0.4196,  ..., 0.4228, 0.4196, 0.4010],
        [0.4220, 0.4340, 0.4349,  ..., 0.4323, 0.4293, 0.4238]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-2.3309, -2.3352, -2.3379, -2.3379, -2.3378, -2.3362, -1.4067, -2.3293,
        -2.3392, -2.3378, -2.3364, -2.3396, -2.3389, -2.3342, -2.3372, -2.3358,
        -2.3398, -2.3371, -2.3365, -2.3359, -2.3384, -1.7637, -2.3371, -2.3370,
        -2.3388, -2.3385, -2.3366, -2.3374, -2.2291, -2.3366, -2.3386, -2.3372,
        -2.3370, -2.1320, -2.3873, -2.3366, -2.3376, -2.3364, -2.3374, -2.3369,
        -2.3378, -2.3378, -2.3352, -2.3365, -2.3369, -2.3354, -2.3375, -2.3361,
        -2.3374, -2.3364, -2.3373,  4.8802, -2.3351, -2.3374, -2.3371, -1.0369,
        -2.3364, -2.3378, -2.3349, -1.9485, -2.3358, -1.9112, -1.8185, -2.3376,
        -2.3383, -2.1031, -2.3355, -2.3370, -1.3661, -2.3380, -2.3339, -2.3354,
        -2.3370,  2.7997, -2.3364, -2.3346, -2.3362, -2.3368, -2.3392, -2.3381,
        -2.3360, -2.3354, -2.0362,  3.7280, -2.3358, -2.3364, -2.3349, -2.3370,
        -2.3377, -2.3381, -2.3369, -2.3373, -2.1815, -2.3368, -2.3384, -2.3387,
        -2.3361, -2.3374, -2.3360, -2.3374, -1.5752, -2.3378, -2.3380, -2.3368,
        -2.4072, -2.3398, -2.3315, -2.3374, -2.3360, -2.3373, -2.3366, -2.3369,
        -2.3382, -2.3355, -2.3371, -1.9580, -2.2976, -1.1337, -2.0398, -1.4199,
        -2.3370, -2.3362, -2.3548, -2.3389, -2.3351, -2.3415, -2.3374, -2.3384,
        -1.1947,  4.5976, -2.3379, -2.3370, -2.3336, -2.3363, -2.3434, -2.3369,
        -2.0373, -2.3379, -2.1758, -1.8013, -2.3377,  5.2934, -2.3356, -2.3380,
        -2.3344, -2.3390, -2.3363, -2.3347, -2.3381, -2.3359, -2.3378, -2.3399,
        -2.3357, -2.2812, -1.5996, -2.3376, -2.3370, -2.3385, -2.3401, -2.3374,
        -2.3364, -2.3379, -2.3368, -2.3386, -1.3099, -2.3380, -2.3388, -2.3376,
        -2.3788, -2.3391, -2.3365, -2.3374, -2.3370, -2.3366, -2.3392,  4.4437,
        -2.3377, -2.3369, -2.3386, -1.9937, -2.3385, -2.3353, -2.3387, -2.3371,
        -2.3397, -2.3373, -2.3375, -2.3365, -2.3375, -2.3372, -2.3369, -2.3374,
        -2.3375, -1.8233, -2.3364, -2.3385, -2.3374, -2.3314, -2.3374, -2.3369,
        -2.3338, -2.3358, -2.3311, -2.3127, -2.3376, -1.9661, -2.3360, -2.3403,
        -2.3369, -2.3374,  4.2610, -2.6351, -2.3376, -1.9852, -2.3363, -2.3383,
        -2.3365, -2.3349, -2.3377, -2.3378, -2.3381, -2.3353, -2.3367, -2.3400,
         4.8207, -2.3355,  2.4125, -2.3357, -2.3360, -2.3373, -2.3393, -2.1050,
        -2.3359, -2.3382, -2.3372, -1.0655, -2.3343, -1.9765,  1.5658, -2.3352,
        -2.0016, -2.3379, -2.3381, -2.3373, -2.4194, -2.3388, -1.5309,  3.2339,
        -2.3370, -1.5172, -2.3379, -2.3357, -2.3372, -2.3386, -2.5026, -2.3396,
        -2.3388, -2.3352, -2.3377, -1.9492, -2.3363, -2.3356, -2.3369, -2.3370,
        -2.3379, -2.3369, -2.1393, -2.3366, -2.3371, -2.3358, -2.3366, -2.3396,
        -2.3377, -2.3389, -2.3384, -2.3376, -2.3370, -2.3369, -1.5173,  0.9841,
        -2.3355, -1.8104, -1.7688, -2.3361, -2.3376, -1.3748, -2.1972, -1.6826,
        -2.3354, -2.4069, -2.3371, -1.9220, -2.3375, -2.3369, -2.3359, -2.3368,
        -2.1745, -2.3371, -2.3390, -2.3330, -2.3362, -2.3368, -2.3369, -2.1490,
        -2.3397, -2.3371, -1.0104, -2.2581, -2.3371, -2.3360, -2.4823, -2.2964,
        -2.3400, -2.3366, -2.3387, -2.3110, -2.3380, -1.9338, -2.2621, -2.3358,
        -1.4217, -2.3368, -2.3382, -2.3363, -2.3363, -2.3369, -2.3276, -2.3351,
        -2.3381, -2.3365, -2.3360, -2.3367, -2.3374, -2.3381, -2.3349, -2.2095,
        -2.0786,  4.1375, -2.3360, -2.3378, -2.3352, -2.3372, -2.3400, -2.3371,
        -2.3352, -2.0351, -2.3338, -2.0061, -2.3358, -2.3372, -2.3356, -2.3375,
        -1.7927, -2.3360, -2.3377, -2.3363,  4.3398, -2.3402,  3.6666, -2.3387,
         1.1509, -2.3374, -2.3375, -2.3083, -2.3365,  2.9915, -2.3378, -2.3373,
        -2.3374, -2.3370, -2.3384, -2.3368, -2.3377, -2.3376, -2.3360, -2.3348,
        -2.0168, -2.3388, -2.3374, -2.3358, -2.3374, -2.3379, -2.3402, -2.3363,
        -2.3368, -2.3342, -2.3379, -2.3373, -2.3361, -2.3383, -2.3363, -2.3364,
        -2.3368, -2.3396, -2.3393, -1.1456, -2.3383, -2.2216, -2.3366, -2.3369,
        -2.3365, -2.4073, -2.3794, -0.5617, -2.3374, -2.3366, -2.3370, -2.3376,
        -2.3390, -2.3372, -2.3343, -2.3368, -2.3366, -2.3364, -2.3367, -2.1640,
        -2.3373, -2.0854, -0.8112, -1.0191, -2.3857, -2.3372, -2.3384, -2.3369,
        -2.3379, -2.3363, -2.3365, -2.3371, -2.3375, -2.3394, -2.3359, -2.3375,
        -2.3359, -2.2785, -2.3396, -2.3376, -2.0091, -2.3395, -2.3377, -2.3354,
        -2.3364, -2.3369, -2.3374, -2.3380, -2.3385, -2.3358, -2.3355, -2.3363,
        -2.3368, -2.3341, -2.3373, -2.3349, -2.3361, -2.3379, -2.3421, -2.3386,
        -1.1066, -2.3365, -2.3386, -2.3355, -2.3379, -2.2841, -2.3386, -2.3378,
         0.7227, -2.3376, -2.3400, -2.3352, -2.3366, -2.3378, -2.3361, -2.3368,
        -2.1253, -2.2562, -2.3381, -1.7632, -1.7134, -2.3378, -2.2561, -2.3356,
        -2.3376, -2.3362, -2.3373, -2.3232,  2.4254, -2.3394,  4.7486, -2.3352,
        -2.3344, -1.9821, -2.3360, -2.3376, -2.3367, -2.3341, -2.3391, -2.3375,
        -2.3387, -2.3384, -2.3356, -2.3359, -2.3359, -2.3367, -1.8022, -2.3379,
        -2.3365, -2.2876, -2.2420, -2.3384,  2.1413, -2.3369, -2.3015, -2.3375],
       device='cuda:2', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.4995, 0.4998, 0.4982, 0.4987, 0.4992, 0.4991, 0.4054, 0.4965, 0.4996,
        0.4982, 0.4996, 0.4984, 0.4989, 0.4997, 0.4998, 0.4985, 0.4986, 0.4995,
        0.4993, 0.4992, 0.4992, 0.1840, 0.4989, 0.4993, 0.4984, 0.4990, 0.4990,
        0.4990, 0.4546, 0.4998, 0.4986, 0.4989, 0.4997, 0.2835, 0.4715, 0.4988,
        0.4996, 0.4986, 0.4992, 0.4990, 0.4985, 0.4988, 0.4999, 0.4994, 0.4994,
        0.5000, 0.5001, 0.4998, 0.4995, 0.4991, 0.4988, 0.1467, 0.5000, 0.4988,
        0.4999, 0.0888, 0.4996, 0.4993, 0.4996, 0.0981, 0.4999, 0.2059, 0.1338,
        0.4991, 0.4990, 0.3488, 0.5001, 0.4988, 0.0972, 0.4996, 0.4997, 0.4997,
        0.4992, 0.2272, 0.4998, 0.5003, 0.4980, 0.4995, 0.4984, 0.4982, 0.4994,
        0.4999, 0.1187, 0.1846, 0.4999, 0.4998, 0.4999, 0.4990, 0.4988, 0.4983,
        0.4990, 0.4994, 0.2494, 0.4995, 0.4988, 0.4984, 0.4996, 0.4989, 0.4995,
        0.4981, 0.1129, 0.4993, 0.4985, 0.4991, 0.4761, 0.4991, 0.4992, 0.4994,
        0.4995, 0.4990, 0.4993, 0.4988, 0.4997, 0.4994, 0.4992, 0.1968, 0.1133,
        0.0833, 0.2180, 0.0814, 0.4988, 0.5002, 0.4383, 0.4990, 0.4997, 0.4973,
        0.4987, 0.4985, 0.0933, 0.1479, 0.4987, 0.4995, 0.4982, 0.4990, 0.5002,
        0.4995, 0.2767, 0.4987, 0.1992, 0.4436, 0.4997, 0.1313, 0.4995, 0.4989,
        0.5001, 0.4991, 0.4993, 0.4993, 0.4990, 0.4992, 0.4995, 0.4984, 0.4993,
        0.4902, 0.1012, 0.4985, 0.4991, 0.4993, 0.4976, 0.4991, 0.4996, 0.4989,
        0.4995, 0.4988, 0.1757, 0.4990, 0.4984, 0.4992, 0.4754, 0.4988, 0.4993,
        0.4992, 0.4991, 0.4993, 0.4987, 0.1663, 0.4992, 0.4995, 0.4980, 0.1506,
        0.4989, 0.4999, 0.4989, 0.4992, 0.4985, 0.4993, 0.4990, 0.5002, 0.4984,
        0.4993, 0.4992, 0.4990, 0.4991, 0.1294, 0.5003, 0.4994, 0.4990, 0.4978,
        0.4995, 0.4992, 0.5004, 0.4988, 0.4987, 0.1427, 0.4992, 0.1254, 0.5002,
        0.4985, 0.5000, 0.4996, 0.1751, 0.2628, 0.4996, 0.2478, 0.4993, 0.4990,
        0.4993, 0.4999, 0.4992, 0.4988, 0.4984, 0.5001, 0.4996, 0.4981, 0.1454,
        0.4998, 0.0653, 0.5000, 0.5002, 0.4988, 0.4986, 0.1053, 0.4996, 0.4992,
        0.5002, 0.0833, 0.5000, 0.2194, 0.1065, 0.4997, 0.1834, 0.4992, 0.4993,
        0.4992, 0.2593, 0.4985, 0.4179, 0.0619, 0.4988, 0.1375, 0.4993, 0.4993,
        0.4996, 0.4987, 0.3995, 0.4982, 0.4979, 0.4996, 0.4996, 0.2189, 0.5001,
        0.5001, 0.4992, 0.4990, 0.4993, 0.4997, 0.2647, 0.4996, 0.4993, 0.5001,
        0.4988, 0.4976, 0.4992, 0.4985, 0.4983, 0.4988, 0.4993, 0.4992, 0.1547,
        0.1238, 0.4996, 0.1923, 0.1112, 0.4996, 0.4992, 0.1791, 0.3969, 0.4321,
        0.4994, 0.3891, 0.4984, 0.1086, 0.4989, 0.4997, 0.4995, 0.4993, 0.2376,
        0.4994, 0.4982, 0.4996, 0.4990, 0.5001, 0.4997, 0.2154, 0.4988, 0.4988,
        0.0932, 0.4540, 0.4997, 0.5003, 0.3789, 0.4135, 0.4978, 0.4993, 0.4993,
        0.3115, 0.4992, 0.1479, 0.3592, 0.4993, 0.1056, 0.5001, 0.4992, 0.4993,
        0.4997, 0.4990, 0.4981, 0.5002, 0.4989, 0.4988, 0.4995, 0.4994, 0.4981,
        0.4985, 0.5003, 0.2837, 0.3067, 0.1785, 0.4996, 0.4987, 0.4998, 0.4995,
        0.4980, 0.4993, 0.4990, 0.1387, 0.5008, 0.2497, 0.4994, 0.4995, 0.4997,
        0.4990, 0.2246, 0.4994, 0.4995, 0.4990, 0.1725, 0.4985, 0.2001, 0.4984,
        0.1126, 0.4999, 0.4990, 0.4497, 0.4999, 0.0633, 0.4990, 0.4992, 0.4986,
        0.4990, 0.4981, 0.4997, 0.4994, 0.4994, 0.4997, 0.4999, 0.1647, 0.4985,
        0.4987, 0.4990, 0.4991, 0.4994, 0.4984, 0.4994, 0.4996, 0.4997, 0.4987,
        0.4985, 0.4998, 0.4988, 0.4995, 0.4995, 0.4999, 0.4988, 0.4982, 0.0791,
        0.4985, 0.2543, 0.4992, 0.4995, 0.4986, 0.3713, 0.4096, 0.3175, 0.4991,
        0.4979, 0.4989, 0.4994, 0.4986, 0.4992, 0.5005, 0.5002, 0.4996, 0.4990,
        0.4995, 0.4820, 0.4996, 0.1166, 0.0890, 0.0847, 0.4694, 0.4992, 0.4989,
        0.4989, 0.4995, 0.4997, 0.4992, 0.4983, 0.4992, 0.4976, 0.4996, 0.4990,
        0.5004, 0.4809, 0.4982, 0.4988, 0.1378, 0.4984, 0.4998, 0.4995, 0.4992,
        0.4992, 0.4992, 0.4990, 0.4991, 0.4996, 0.4999, 0.4999, 0.4990, 0.5007,
        0.4994, 0.5005, 0.4998, 0.4989, 0.4984, 0.4992, 0.1056, 0.4993, 0.4990,
        0.4999, 0.4985, 0.2140, 0.4987, 0.4992, 0.1487, 0.4986, 0.4977, 0.4991,
        0.4989, 0.4986, 0.4996, 0.4995, 0.2824, 0.1922, 0.4988, 0.1267, 0.1670,
        0.4986, 0.3239, 0.4987, 0.4988, 0.4994, 0.4993, 0.4987, 0.0622, 0.4991,
        0.1529, 0.5002, 0.5002, 0.2421, 0.4993, 0.4989, 0.4999, 0.5004, 0.4985,
        0.4989, 0.4991, 0.4984, 0.4994, 0.4994, 0.4994, 0.4994, 0.1779, 0.4995,
        0.4999, 0.4548, 0.3105, 0.4989, 0.0809, 0.4991, 0.4836, 0.4990],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.0091, -0.0110, -0.0081,  ..., -0.0084, -0.0097, -0.0088],
        [-0.0237, -0.0175, -0.0128,  ..., -0.0124,  0.1251, -0.0076],
        [-0.0099, -0.0097, -0.0091,  ..., -0.0087, -0.0098, -0.0084],
        ...,
        [-0.0116, -0.0113, -0.0118,  ..., -0.0141, -0.0130, -0.0136],
        [-0.0098, -0.0082, -0.0118,  ..., -0.0100, -0.0110, -0.0114],
        [ 0.1151,  0.0773,  0.0737,  ...,  0.0802,  0.3907,  0.0752]],
       device='cuda:2', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.9993, 0.9992, 0.9995,  ..., 0.9994, 0.9997, 0.9995],
        [0.9184, 0.9172, 0.9170,  ..., 0.9162, 0.9158, 0.9183],
        [0.9991, 0.9994, 0.9995,  ..., 0.9993, 0.9993, 0.9995],
        ...,
        [0.9993, 0.9994, 0.9989,  ..., 0.9990, 0.9989, 0.9991],
        [0.9992, 0.9992, 0.9984,  ..., 0.9993, 0.9988, 0.9988],
        [0.9203, 0.9242, 0.9210,  ..., 0.9186, 0.9066, 0.9199]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.1251,  0.1746, -0.1248, -0.1242, -0.1320,  0.3205, -0.1248, -0.1244,
        -0.1243, -0.1259, -0.1251, -0.1493, -0.1255, -0.1256, -0.1265, -0.1259,
        -0.1244, -0.1258, -0.1258, -0.1261, -0.1248, -0.1262, -0.1247, -0.1261,
        -0.1228, -0.1243, -0.0212, -0.1260, -0.1231, -0.1261, -0.1247, -0.1254,
        -0.1236, -0.1264, -0.1276, -0.1242, -0.1251, -0.1244, -0.2809, -0.1238,
        -0.1250, -0.1277, -0.1242, -0.1272, -0.1241, -0.1253, -0.1240, -0.1260,
        -0.1257, -0.1244, -0.1268, -0.1257, -0.1268, -0.1250, -0.1239, -0.2552,
        -0.1254, -0.1240, -0.1243, -0.1260, -0.1255, -0.1241, -0.1255, -0.1270,
        -0.1246, -0.1248, -0.1253, -0.1266, -0.1251, -0.1246, -0.1231, -0.1266,
        -0.1243, -0.1241, -0.1257, -0.1256, -0.1268, -0.1251, -0.1253, -0.2316,
        -0.1273, -0.1251, -0.1253, -0.1256, -0.1254, -0.1238, -0.1248, -0.1273,
        -0.1251, -0.1251,  0.3654, -0.1249, -0.1256, -0.1233, -0.1256, -0.1239,
        -0.1243, -0.1256,  0.3540, -0.1257, -0.1271, -0.1245, -0.1244, -0.1410,
        -0.1257, -0.1250, -0.1242, -0.1246, -0.1243, -0.1250, -0.1244, -0.1241,
        -0.1728, -0.1258, -0.1245, -0.1249, -0.1255, -0.1247, -0.1251, -0.1962,
        -0.1246, -0.1255, -0.1254, -0.1263, -0.1249, -0.1262, -0.1238, -0.1243,
        -0.1263, -0.1252, -0.1251, -0.1245, -0.1267, -0.1246, -0.1250, -0.1247,
        -0.1246, -0.1251, -0.1248, -0.1252, -0.1262, -0.1257, -0.1251, -0.1257,
        -0.1245, -0.1241, -0.2019, -0.1630,  0.2394, -0.1260, -0.1264, -0.1255,
        -0.1248, -0.1253, -0.1249, -0.1229, -0.1280, -0.1240, -0.1228, -0.1255,
        -0.1278, -0.1243, -0.1254, -0.1267, -0.1248, -0.1251, -0.1237, -0.1264,
        -0.1258, -0.1265, -0.1243, -0.1241, -0.1266, -0.1257, -0.1241, -0.1241,
        -0.1254, -0.1251, -0.1252, -0.1244, -0.1441, -0.1258, -0.1268, -0.1248,
        -0.1238, -0.1258, -0.1247, -0.1247, -0.1256, -0.1242, -0.1242, -0.1252,
        -0.1254, -0.1246, -0.1241, -0.1247, -0.1267, -0.1245, -0.1250, -0.1270,
        -0.1241, -0.1260, -0.1245, -0.1236, -0.1236, -0.1243, -0.1246, -0.1248,
        -0.1242, -0.1405, -0.1261, -0.1262, -0.1228, -0.1254, -0.1249, -0.1249,
        -0.1281, -0.1254, -0.1246, -0.1255, -0.1241, -0.1232, -0.1261, -0.3577,
        -0.1254, -0.1236, -0.1250, -0.1270, -0.1248, -0.1277, -0.1245, -0.1671,
        -0.1263, -0.1245, -0.1239, -0.1249,  0.2517, -0.1259, -0.1231, -0.1244,
        -0.1288, -0.1252, -0.1257, -0.1245, -0.1256, -0.1241, -0.1712, -0.0760,
        -0.1241, -0.1250, -0.1240, -0.1237, -0.1670, -0.1248, -0.1248, -0.1234,
        -0.1233, -0.1250, -0.1266, -0.1246, -0.1263, -0.1263, -0.1233, -0.1244,
        -0.1249, -0.1257, -0.1248,  0.1060, -0.1252, -0.1270, -0.1267,  0.4238,
        -0.1262, -0.1257, -0.1247, -0.1250, -0.1238, -0.1251, -0.1260, -0.1272,
        -0.1273, -0.1261, -0.1240, -0.1264, -0.1247, -0.1234, -0.1253, -0.1251,
        -0.1254, -0.1242, -0.1248, -0.1269, -0.1257, -0.1254, -0.1240, -0.1263,
        -0.1259, -0.1258, -0.1281, -0.1280, -0.1250, -0.1254, -0.1263, -0.1240,
        -0.1251, -0.1253, -0.1248, -0.1246, -0.1261, -0.1617, -0.1255, -0.1233,
        -0.1249, -0.1269, -0.1257, -0.1238, -0.1246, -0.1256, -0.1250, -0.1250,
        -0.1230, -0.1258, -0.1242, -0.1261, -0.1810, -0.1245, -0.1243, -0.1267,
        -0.1269, -0.1255, -0.1238, -0.1263, -0.1253, -0.1238, -0.1253, -0.1239,
        -0.1246, -0.1256, -0.1256, -0.1246, -0.1249, -0.1249, -0.1247, -0.1255,
        -0.1253, -0.1250, -0.1252, -0.1242, -0.1261, -0.1258, -0.1243, -0.1238,
        -0.1252, -0.1258, -0.1255, -0.2004, -0.1259, -0.1261, -0.2340, -0.1252,
        -0.1254, -0.1234, -0.1264, -0.1250, -0.1237, -0.1239, -0.1262, -0.1264,
        -0.1234, -0.1254, -0.1257, -0.1234, -0.1261, -0.1269, -0.1264, -0.1246,
        -0.1253, -0.1805,  2.1229, -0.1239, -0.1251, -0.1243, -0.1257, -0.1246,
        -0.1524, -0.1266, -0.1251,  0.1537, -0.1244, -0.1258, -0.1251, -0.1254,
        -0.1249, -0.1252, -0.1253,  0.2130, -0.1256, -0.1244, -0.1253, -0.1263,
        -0.1275, -0.1252, -0.1240, -0.1640, -0.2620, -0.1240, -0.1247, -0.1254,
        -0.1247, -0.1259, -0.1243, -0.1255, -0.1237, -0.1253, -0.1244, -0.1257,
        -0.1252, -0.1246, -0.1252, -0.1254, -0.1241, -0.1252, -0.1257, -0.1245,
        -0.1266, -0.1748, -0.1255, -0.1246, -0.1249, -0.1827, -0.1252, -0.1258,
        -0.1237, -0.1260, -0.1232, -0.1267, -0.1246, -0.1714, -0.1246, -0.1262,
        -0.1256, -0.1238, -0.1254, -0.1250, -0.1253, -0.1256, -0.1244,  0.1235,
        -0.1265, -0.1250, -0.1253, -0.1264, -0.1237, -0.1252, -0.1255, -0.1239,
        -0.1255, -0.1233, -0.1240, -0.1264, -0.1249, -0.1251, -0.1250, -0.1260,
        -0.1254, -0.1242, -0.1252, -0.1250, -0.1462, -0.1257, -0.1243, -0.1241,
        -0.1254, -0.1247, -0.1263, -0.1252, -0.1248, -0.1233, -0.1251, -0.1244,
        -0.1256, -0.1242, -0.1266, -0.1254, -0.1253, -0.1248, -0.1613, -0.1260,
        -0.1255, -0.1233, -0.1255, -0.1260, -0.1252, -0.1233, -0.1258, -0.1252,
        -0.1267, -0.1247, -0.1270, -0.1248, -0.1270, -0.1249, -0.1257, -0.1256,
        -0.1232, -0.1264, -0.1243, -0.1268, -0.1242, -0.1593, -0.1250, -0.2993],
       device='cuda:2', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.9929, 0.5231, 0.9928, 0.9935, 0.9911, 0.4683, 0.9925, 0.9934, 0.9929,
        0.9924, 0.9923, 0.8033, 0.9928, 0.9926, 0.9924, 0.9932, 0.9927, 0.9921,
        0.9922, 0.9932, 0.9929, 0.9923, 0.9922, 0.9931, 0.9930, 0.9924, 0.9639,
        0.9929, 0.9929, 0.9927, 0.9926, 0.9931, 0.9929, 0.9916, 0.9923, 0.9926,
        0.9929, 0.9923, 0.6803, 0.9926, 0.9925, 0.9916, 0.9928, 0.9925, 0.9929,
        0.9927, 0.9934, 0.9930, 0.9928, 0.9931, 0.9920, 0.9927, 0.9928, 0.9930,
        0.9924, 0.6576, 0.9925, 0.9930, 0.9923, 0.9924, 0.9919, 0.9927, 0.9932,
        0.9925, 0.9929, 0.9928, 0.9927, 0.9931, 0.9928, 0.9931, 0.9923, 0.9927,
        0.9931, 0.9928, 0.9926, 0.9929, 0.9931, 0.9925, 0.9921, 0.6852, 0.9927,
        0.9925, 0.9931, 0.9916, 0.9927, 0.9923, 0.9930, 0.9930, 0.9927, 0.9926,
        0.5397, 0.9931, 0.9926, 0.9929, 0.9929, 0.9930, 0.9925, 0.9923, 0.4565,
        0.9927, 0.9926, 0.9927, 0.9924, 0.8434, 0.9932, 0.9930, 0.9929, 0.9926,
        0.9930, 0.9924, 0.9928, 0.9929, 0.9712, 0.9924, 0.9931, 0.9925, 0.9927,
        0.9922, 0.9916, 0.9733, 0.9926, 0.9926, 0.9927, 0.9924, 0.9926, 0.9926,
        0.9927, 0.9927, 0.9930, 0.9930, 0.9929, 0.9926, 0.9915, 0.9929, 0.9930,
        0.9921, 0.9928, 0.9927, 0.9920, 0.9923, 0.9927, 0.9929, 0.9928, 0.9931,
        0.9926, 0.9929, 0.9780, 0.8844, 0.4292, 0.9928, 0.9926, 0.9929, 0.9929,
        0.9922, 0.9929, 0.9933, 0.9923, 0.9931, 0.9929, 0.9927, 0.9923, 0.9925,
        0.9929, 0.9926, 0.9930, 0.9927, 0.9925, 0.9924, 0.9922, 0.9925, 0.9928,
        0.9929, 0.9922, 0.9926, 0.9930, 0.9930, 0.9931, 0.9927, 0.9927, 0.9928,
        0.9905, 0.9925, 0.9912, 0.9932, 0.9928, 0.9924, 0.9929, 0.9923, 0.9926,
        0.9930, 0.9927, 0.9921, 0.9920, 0.9927, 0.9922, 0.9923, 0.9920, 0.9923,
        0.9924, 0.9927, 0.9928, 0.9930, 0.9928, 0.9928, 0.9924, 0.9930, 0.9924,
        0.9927, 0.9928, 0.9903, 0.9930, 0.9924, 0.9925, 0.9924, 0.9929, 0.9924,
        0.9904, 0.9928, 0.9928, 0.9929, 0.9930, 0.9933, 0.9925, 0.6475, 0.9928,
        0.9930, 0.9931, 0.9924, 0.9924, 0.9931, 0.9928, 0.9804, 0.9931, 0.9930,
        0.9924, 0.9926, 0.5762, 0.9928, 0.9929, 0.9930, 0.9924, 0.9932, 0.9927,
        0.9925, 0.9929, 0.9927, 0.9868, 0.7956, 0.9933, 0.9928, 0.9928, 0.9931,
        0.7876, 0.9931, 0.9923, 0.9931, 0.9929, 0.9926, 0.9926, 0.9922, 0.9925,
        0.9931, 0.9931, 0.9927, 0.9920, 0.9931, 0.9930, 0.9664, 0.9926, 0.9917,
        0.9930, 0.9512, 0.9923, 0.9924, 0.9923, 0.9927, 0.9931, 0.9924, 0.9922,
        0.9919, 0.9924, 0.9928, 0.9934, 0.9926, 0.9924, 0.9927, 0.9932, 0.9928,
        0.9933, 0.9928, 0.9926, 0.9929, 0.9925, 0.9924, 0.9930, 0.9926, 0.9928,
        0.9923, 0.9925, 0.9927, 0.9921, 0.9925, 0.9923, 0.9929, 0.9925, 0.9931,
        0.9926, 0.9930, 0.9929, 0.9566, 0.9924, 0.9928, 0.9927, 0.9913, 0.9926,
        0.9928, 0.9926, 0.9924, 0.9924, 0.9927, 0.9926, 0.9928, 0.9931, 0.9928,
        0.9917, 0.9933, 0.9931, 0.9931, 0.9926, 0.9926, 0.9933, 0.9927, 0.9923,
        0.9934, 0.9932, 0.9930, 0.9927, 0.9926, 0.9927, 0.9928, 0.9929, 0.9924,
        0.9930, 0.9928, 0.9917, 0.9926, 0.9926, 0.9927, 0.9928, 0.9925, 0.9927,
        0.9928, 0.9929, 0.9919, 0.9925, 0.8952, 0.9928, 0.9926, 0.7211, 0.9925,
        0.9928, 0.9926, 0.9924, 0.9917, 0.9933, 0.9926, 0.9928, 0.9923, 0.9930,
        0.9926, 0.9931, 0.9927, 0.9923, 0.9919, 0.9930, 0.9924, 0.9927, 0.7597,
        0.9315, 0.9929, 0.9923, 0.9929, 0.9925, 0.9929, 0.9898, 0.9920, 0.9926,
        0.5396, 0.9931, 0.9924, 0.9924, 0.9919, 0.9927, 0.9929, 0.9924, 0.5168,
        0.9928, 0.9929, 0.9927, 0.9928, 0.9921, 0.9929, 0.9930, 0.9113, 0.7355,
        0.9931, 0.9927, 0.9927, 0.9926, 0.9929, 0.9927, 0.9927, 0.9926, 0.9931,
        0.9930, 0.9922, 0.9929, 0.9926, 0.9932, 0.9927, 0.9926, 0.9926, 0.9922,
        0.9930, 0.9927, 0.9908, 0.9927, 0.9926, 0.9932, 0.9869, 0.9926, 0.9930,
        0.9924, 0.9926, 0.9933, 0.9917, 0.9925, 0.9794, 0.9926, 0.9930, 0.9929,
        0.9928, 0.9925, 0.9927, 0.9929, 0.9923, 0.9930, 0.8832, 0.9929, 0.9927,
        0.9927, 0.9927, 0.9929, 0.9928, 0.9922, 0.9926, 0.9930, 0.9929, 0.9930,
        0.9923, 0.9927, 0.9930, 0.9929, 0.9924, 0.9924, 0.9931, 0.9928, 0.9924,
        0.9903, 0.9925, 0.9928, 0.9931, 0.9930, 0.9928, 0.9919, 0.9928, 0.9928,
        0.9928, 0.9925, 0.9929, 0.9928, 0.9928, 0.9926, 0.9920, 0.9926, 0.9927,
        0.9887, 0.9930, 0.9932, 0.9931, 0.9930, 0.9926, 0.9928, 0.9926, 0.9927,
        0.9920, 0.9921, 0.9928, 0.9920, 0.9928, 0.9928, 0.9924, 0.9927, 0.9930,
        0.9935, 0.9925, 0.9930, 0.9926, 0.9926, 0.9916, 0.9931, 0.6126],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-9.0145e-04, -1.0138e-01, -7.0086e-04,  ..., -1.5841e-03,
         -1.0036e-03, -2.6048e-02],
        [-3.3980e-03, -2.0272e-01, -4.7833e-05,  ..., -2.5848e-03,
         -7.5631e-04, -7.7063e-02],
        [-3.7192e-03, -2.3434e-01, -3.7777e-03,  ..., -4.1596e-03,
         -6.3667e-04, -9.7868e-02],
        ...,
        [-8.9122e-05, -1.6445e-01, -7.9121e-04,  ..., -2.3088e-03,
         -1.2552e-03, -7.4027e-02],
        [-7.0116e-04, -1.4617e-01, -5.3953e-04,  ..., -3.0672e-03,
         -9.2216e-04, -5.4590e-02],
        [-6.7528e-04, -1.0865e-01, -2.1021e-03,  ..., -9.6139e-04,
         -3.1072e-04, -3.6067e-02]], device='cuda:2', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.9998, 0.9662, 1.0000,  ..., 0.9998, 1.0001, 0.9940],
        [0.9976, 0.8996, 1.0000,  ..., 0.9981, 1.0000, 0.9694],
        [1.0000, 0.8828, 0.9978,  ..., 0.9999, 1.0000, 0.9659],
        ...,
        [1.0000, 0.9217, 1.0000,  ..., 0.9993, 1.0000, 0.9704],
        [1.0000, 0.9256, 1.0000,  ..., 1.0000, 0.9997, 0.9850],
        [0.9997, 0.9575, 0.9999,  ..., 1.0000, 1.0000, 0.9912]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.0030, -0.0063, -0.0077, -0.0036, -0.0050, -0.0037, -0.0028, -0.0035,
        -0.0025, -0.0046, -0.0053, -0.0042, -0.0031, -0.0046, -0.0064, -0.0032,
        -0.0037, -0.0059, -0.0354, -0.0037, -0.0034, -0.0026, -0.0039, -0.0963,
        -0.0060, -0.0025, -0.0049, -0.0078, -0.0024, -0.0038, -0.0029, -0.0102,
        -0.0049, -0.0049, -0.0032, -0.0073, -0.0055, -0.0039, -0.0034, -0.0058,
        -0.0039, -0.0042, -0.0043, -0.0041, -0.0061, -0.0036, -0.0056, -0.0029,
        -0.0035, -0.0049, -0.0023, -0.0044, -0.0050, -0.0029, -0.0042, -0.0049,
        -0.0059, -0.0026, -0.0044, -0.0037, -0.0045, -0.0064, -0.0094, -0.0040,
        -0.0035, -0.0042, -0.0043, -0.0043, -0.0067, -0.0035, -0.0039, -0.0059,
        -0.0052, -0.0033, -0.0036, -0.0036, -0.0047, -0.0035, -0.0035, -0.0030,
        -0.0045, -0.0026, -0.0040, -0.0034, -0.0047, -0.0059, -0.0054, -0.0047,
        -0.0062, -0.0029, -0.0029, -0.0039, -0.0039, -0.0092, -0.0040, -0.0047,
        -0.0022, -0.0042, -0.0027, -0.0039, -0.0045, -0.0036, -0.0058, -0.0043,
        -0.0065, -0.0038, -0.0045, -0.0043, -0.0033, -0.0048, -0.0073, -0.0785,
        -0.0032, -0.0035, -0.0031, -0.0031, -0.0053, -0.0039, -0.0035, -0.0054,
        -0.0071, -0.0051, -0.0060, -0.0043, -0.0050, -0.0029, -0.0056, -0.0026,
        -0.0032, -0.0050, -0.0036, -0.0042, -0.0046, -0.0059, -0.0031, -0.0051,
        -0.0037, -0.0026, -0.0046, -0.0037, -0.0037, -0.0047, -0.0042, -0.0049,
        -0.0031, -0.0041, -0.0066, -0.0035, -0.0056, -0.0020, -0.0027, -0.0049,
        -0.0051, -0.0040, -0.0029, -0.0031, -0.0070, -0.0045, -0.0030, -0.0024,
        -0.0046, -0.0030,  0.1359, -0.0049, -0.0041, -0.0028, -0.0038, -0.0032,
        -0.0033, -0.0034, -0.0025, -0.0030, -0.0037, -0.0029, -0.0035, -0.0038,
        -0.0051, -0.0040, -0.0040, -0.0042, -0.0046, -0.0047, -0.0042, -0.0028,
        -0.0029, -0.0072, -0.0041, -0.0051, -0.0038, -0.0044, -0.0036, -0.0044,
        -0.0801, -0.0048, -0.0040, -0.0039, -0.0032, -0.0035, -0.0028, -0.0045,
        -0.0037, -0.0042, -0.0032, -0.0050, -0.0058, -0.0038, -0.0040, -0.0041,
        -0.0046, -0.0065, -0.0045, -0.0033, -0.0047, -0.0082, -0.0041, -0.0049,
        -0.0044,  0.0807, -0.0036, -0.0076, -0.0083, -0.0038,  0.0450, -0.0045,
        -0.0045, -0.0161, -0.0056, -0.0037, -0.0036, -0.0065, -0.0031, -0.0063,
        -0.0074, -0.0036, -0.0048, -0.0042, -0.0031, -0.0038, -0.0042, -0.0025,
        -0.0033, -0.0078, -0.0027, -0.0042, -0.0073, -0.0064, -0.0059, -0.0087,
        -0.0045, -0.0033, -0.0033, -0.0031, -0.0046, -0.0079, -0.0018, -0.0036,
        -0.0025, -0.0036, -0.0034, -0.0066, -0.0115, -0.0068, -0.0039, -0.0028,
        -0.0029, -0.0036, -0.0038, -0.0030, -0.0074, -0.0056, -0.0038, -0.0053,
        -0.0038, -0.0035, -0.0027, -0.0039, -0.0054, -0.0043, -0.0092, -0.0054,
        -0.0042, -0.0034, -0.0035, -0.0034, -0.0049, -0.0021, -0.0055, -0.0038,
        -0.0026, -0.0024, -0.0032, -0.0049, -0.0039, -0.0044, -0.0056, -0.0529,
        -0.0030, -0.0026, -0.0034, -0.0070, -0.0050, -0.0039, -0.0026, -0.0030,
        -0.0027, -0.0032, -0.0055, -0.0027, -0.0033, -0.0032, -0.0077, -0.0050,
        -0.0048, -0.0025, -0.0041, -0.0055, -0.0045, -0.0047, -0.0071, -0.0031,
        -0.0049, -0.0049, -0.0036, -0.0041, -0.0055, -0.0052, -0.0044, -0.0055,
        -0.0038, -0.0033, -0.0055, -0.0045, -0.0028, -0.0041, -0.0077, -0.0047,
        -0.0043, -0.0067, -0.0051, -0.0053, -0.0055, -0.0060, -0.0036, -0.0045,
        -0.0063, -0.0077, -0.0037, -0.0069, -0.0055, -0.0089, -0.0081, -0.0051,
        -0.0038, -0.0031, -0.0028, -0.0044, -0.0043, -0.0039, -0.0032, -0.0027,
        -0.0029, -0.0109, -0.0043, -0.0027, -0.0045, -0.0053, -0.0046, -0.0040,
        -0.0025, -0.0031, -0.0060, -0.0035, -0.0036, -0.0032, -0.0035, -0.0074,
        -0.0041, -0.0044, -0.0061, -0.0025, -0.0036, -0.0040, -0.0027, -0.0043,
        -0.0044, -0.0023, -0.0023, -0.0027, -0.0042, -0.0025, -0.0051, -0.0050,
        -0.0031, -0.0041, -0.0069, -0.0041, -0.0064, -0.0039, -0.0043, -0.0034,
        -0.0040, -0.0059, -0.0033, -0.0042, -0.0046, -0.0031,  0.0483, -0.0060,
        -0.0028, -0.0026, -0.0062, -0.0057, -0.0055, -0.0031, -0.0030, -0.0076,
        -0.0025, -0.0023, -0.0041, -0.0043, -0.0038, -0.0047, -0.0050, -0.0022,
        -0.0035, -0.0028, -0.0060, -0.0080, -0.0059, -0.0027, -0.0035, -0.0042,
        -0.0066, -0.0055, -0.0027, -0.0036, -0.0043, -0.0033, -0.0074, -0.0039,
        -0.0032, -0.0044, -0.0037, -0.0035, -0.0068, -0.0040, -0.0033, -0.0034,
        -0.0030, -0.0025, -0.0082, -0.0025, -0.0035, -0.0039, -0.0039, -0.0028,
        -0.0037, -0.0026,  0.0394, -0.0033, -0.0053, -0.0027, -0.0031, -0.0040,
        -0.0061, -0.0050, -0.0050, -0.0032, -0.0050, -0.0069, -0.0032, -0.0062,
        -0.0077, -0.0037, -0.0037, -0.0057, -0.0038, -0.0042, -0.0089, -0.0046,
        -0.0071, -0.0029, -0.0033, -0.0052, -0.0048, -0.0086, -0.0047, -0.0036,
        -0.0057, -0.0021, -0.0048, -0.0042, -0.0033, -0.0052, -0.0031, -0.0059,
        -0.0018, -0.0024, -0.0049, -0.0029, -0.0044, -0.0048, -0.0035, -0.0048,
        -0.0031, -0.0043, -0.0034, -0.0033, -0.0043, -0.0047, -0.0033, -0.0039],
       device='cuda:2', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([1.0000, 0.9993, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000,
        1.0001, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001,
        0.9868, 0.9998, 1.0000, 1.0000, 1.0000, 0.9783, 1.0000, 1.0000, 0.9999,
        0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 1.0000, 1.0002,
        0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9990,
        1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0003, 1.0000, 1.0001, 0.9999, 1.0000,
        1.0002, 0.9999, 1.0000, 1.0000, 0.9997, 0.9999, 1.0000, 0.9999, 1.0000,
        1.0000, 1.0000, 0.9999, 1.0000, 0.9999, 0.9998, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000,
        0.9999, 1.0000, 1.0001, 0.9998, 1.0000, 0.9999, 1.0000, 1.0001, 0.9999,
        1.0000, 0.9999, 1.0002, 0.9719, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999,
        1.0000, 0.9999, 0.9998, 0.9997, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9998, 1.0000, 1.0000,
        0.9998, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 1.0002, 0.9998, 0.9999,
        0.9999, 1.0000, 1.0002, 1.0000, 1.0000, 1.0000, 0.9997, 0.9998, 1.0000,
        0.9999, 1.0000, 1.0001, 0.9999, 1.0000, 1.0000, 0.9999, 1.0001, 0.9999,
        0.9564, 0.9999, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,
        0.9999, 0.9998, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,
        0.9998, 1.0000, 1.0003, 0.9796, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0002, 1.0000, 0.9999, 1.0000, 1.0000, 0.9996,
        0.9999, 1.0000, 1.0000, 0.9999, 0.9998, 0.9996, 1.0000, 1.0001, 1.0000,
        1.0000, 0.9693, 1.0000, 0.9999, 1.0000, 1.0000, 0.9834, 1.0000, 1.0000,
        0.9941, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0001,
        1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 0.9999, 1.0001, 1.0000,
        1.0000, 0.9999, 1.0000, 0.9996, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999,
        0.9997, 0.9999, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000, 0.9999, 0.9992,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,
        1.0001, 1.0000, 1.0000, 1.0000, 0.9998, 0.9999, 1.0000, 0.9998, 1.0001,
        0.9998, 0.9998, 1.0000, 0.9999, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9999, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 0.9785, 0.9999,
        0.9999, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 1.0000, 1.0000, 0.9999,
        0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,
        1.0000, 0.9999, 0.9999, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,
        1.0002, 1.0000, 1.0000, 0.9997, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000,
        0.9997, 0.9999, 0.9999, 1.0000, 1.0000, 1.0001, 0.9999, 1.0000, 1.0000,
        1.0000, 1.0001, 0.9998, 1.0000, 0.9998, 0.9997, 1.0000, 0.9998, 1.0000,
        1.0000, 0.9999, 0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001,
        1.0000, 0.9996, 1.0000, 0.9999, 0.9998, 0.9999, 1.0001, 1.0001, 1.0000,
        1.0000, 1.0001, 1.0000, 1.0000, 0.9999, 1.0000, 1.0001, 1.0000, 0.9999,
        1.0004, 1.0000, 0.9995, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999,
        1.0000, 1.0001, 1.0000, 1.0000, 0.9998, 1.0000, 0.9999, 0.9994, 1.0000,
        0.9999, 0.9999, 0.9999, 1.0001, 0.9999, 1.0000, 1.0000, 0.9999, 0.9999,
        1.0000, 0.9892, 1.0001, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9994, 1.0000, 0.9999, 1.0000, 0.9999, 1.0000, 0.9994, 1.0000,
        1.0000, 0.9993, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0001,
        0.9999, 0.9999, 1.0001, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 0.9308,
        1.0000, 0.9999, 1.0000, 1.0000, 0.9998, 1.0001, 1.0000, 1.0000, 1.0000,
        0.9999, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0002, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0001,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 0.9999],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[-2.4565e-04,  2.1217e-03, -5.0159e-04, -1.5570e-02, -2.6155e-03,
          2.2398e-03,  1.7758e-03,  1.4672e-03, -7.5654e-03,  2.3097e-03,
          1.6607e-03,  4.1795e-03, -3.9515e-03, -2.6250e-03, -1.6178e-02,
         -1.3407e-03,  6.4719e-04, -4.7410e-03, -1.4546e-02,  7.4872e-05,
         -2.3745e-03, -1.8308e-05, -6.2152e-04, -1.8001e-02, -1.4477e-03,
          1.6794e-03,  2.1980e-03,  7.1800e-03, -2.0669e-03,  2.3478e-03,
          7.8568e-04, -1.1174e-02, -2.9880e-03, -1.1969e-02, -1.8293e-04,
         -1.8481e-03,  9.6813e-04,  3.6815e-03, -2.2380e-03, -1.7591e-03,
         -2.8316e-03, -2.2824e-02, -4.3828e-03,  2.5814e-03,  4.0771e-03,
         -3.2121e-03, -3.0459e-03, -4.4748e-04, -7.3077e-03,  1.0519e-02,
         -8.7026e-03,  1.7426e-03, -6.1014e-03, -1.0344e-03, -5.3921e-04,
         -2.2006e-03,  4.1408e-03,  1.4736e-03, -1.0052e-03,  5.9157e-04,
         -2.5682e-03, -7.3338e-03, -2.8215e-03,  2.2982e-03,  9.4533e-04,
         -5.1913e-03,  1.6283e-03,  3.4091e-03,  4.2415e-03, -4.2441e-03,
          6.8442e-03,  9.4349e-03, -4.5161e-03,  4.0454e-03,  1.4439e-03,
         -6.8809e-03, -1.0415e-02, -2.8978e-03, -4.4839e-05, -3.7612e-03,
          2.2045e-03,  1.1426e-03, -1.6791e-03,  3.6329e-03,  2.5820e-03,
         -2.5685e-03, -3.5498e-03,  7.8295e-03,  4.0850e-03, -1.3815e-04,
          1.5195e-03, -8.3564e-04, -2.7908e-06,  4.4964e-03, -3.9507e-05,
         -5.8779e-04, -4.7474e-03,  2.5688e-03, -3.1478e-03, -6.3846e-04,
          1.2603e-03, -1.1558e-04, -2.7685e-03,  4.4397e-03,  2.2591e-03,
          3.0491e-04, -4.3624e-03, -1.9875e-03,  1.3390e-03, -2.2650e-03,
          1.1012e-02,  2.0934e-02, -6.1462e-04, -2.6968e-03,  5.2895e-05,
          2.4839e-04,  5.9584e-03, -3.6896e-04,  1.1179e-03, -7.3518e-03,
          1.2125e-03, -5.6439e-04,  1.9892e-03, -1.2227e-02, -1.7440e-03,
          1.5517e-03, -7.7031e-03,  6.6501e-04, -7.0154e-03, -3.6152e-04,
          3.4153e-04,  3.0612e-03, -6.3058e-03,  4.4823e-03, -1.2367e-03,
          1.9128e-03,  5.1316e-04,  2.6855e-04,  2.6923e-03, -3.1917e-04,
         -7.3432e-04, -2.9798e-03,  4.4861e-04, -2.1038e-03, -4.8109e-04,
          1.8020e-03, -4.2520e-03,  6.1807e-03, -1.1869e-03, -5.3598e-03,
          6.6111e-03, -5.4087e-03,  1.2776e-03, -1.6708e-03,  1.8697e-03,
         -9.9527e-03,  4.1377e-03,  5.6985e-03, -6.4506e-03, -6.3509e-03,
          7.6297e-03, -4.8533e-03, -1.0117e-02, -4.3897e-03, -9.3415e-04,
         -6.8073e-04,  1.8587e-03,  1.0216e-03,  1.3234e-04,  6.7027e-03,
         -5.1857e-03,  4.5955e-05, -6.9069e-03, -2.9175e-03,  5.9035e-04,
          5.7879e-03, -5.5486e-04,  3.0382e-03, -1.6562e-03,  8.8282e-04,
         -2.9769e-03,  5.4606e-03,  2.3212e-05, -2.7823e-04, -4.7034e-03,
         -6.7642e-03, -4.8033e-04,  3.7930e-03,  4.5744e-03, -7.6985e-03,
          1.0129e-03,  2.7697e-03, -1.8034e-02, -2.7127e-03,  5.0864e-03,
          3.8210e-03,  7.4097e-04, -5.0441e-04,  8.6468e-04,  1.5779e-03,
          8.9742e-04, -6.4571e-03,  3.4070e-03, -1.3233e-02,  2.3606e-03,
         -4.0592e-03, -2.9378e-03,  2.6097e-03,  7.6214e-04, -9.3239e-04,
         -5.1617e-03, -5.0483e-04, -5.3965e-03,  1.2227e-02,  3.1344e-04,
          6.1342e-04, -9.2728e-04, -9.0833e-03, -3.2046e-03,  1.7353e-03,
         -1.4446e-04, -1.6500e-03,  8.7249e-03, -7.4309e-03, -3.8950e-03,
          1.2655e-02,  1.5237e-03,  1.1642e-03, -2.6350e-04,  1.5419e-03,
          2.1891e-03, -2.4189e-03,  6.7237e-03,  3.1665e-04, -7.5656e-03,
         -2.6793e-04, -1.1461e-05, -2.8325e-04, -7.3717e-03, -1.5434e-03,
          1.5312e-04, -7.4620e-03, -4.9689e-03, -1.5028e-03, -1.0078e-02,
          6.9678e-03, -7.9720e-03,  6.5112e-03,  3.2633e-03,  1.8877e-03,
          1.5390e-03, -3.8820e-03,  9.5117e-03, -3.8647e-03,  3.9671e-03,
         -9.4823e-03, -2.2241e-05,  3.8040e-04,  8.4186e-04, -7.0185e-03,
         -5.3542e-03,  6.0367e-04, -9.6697e-04,  2.5356e-04, -4.7266e-04,
          2.3663e-03, -1.9867e-03, -3.3897e-03, -2.6389e-03,  8.6035e-03,
         -2.3014e-03,  9.1351e-03, -5.6059e-03,  6.1452e-05, -2.6395e-03,
          2.9404e-03,  3.3949e-03, -6.6950e-03, -1.2540e-02,  5.0714e-03,
          4.1803e-04, -2.1512e-03,  3.4292e-04,  7.5903e-03, -8.9038e-03,
         -7.0141e-04, -7.3276e-03,  5.4493e-03,  3.9742e-03, -4.9736e-03,
          3.6138e-03,  5.7530e-04, -3.8882e-03,  2.8541e-03,  2.8585e-03,
          1.9184e-02, -2.5020e-03,  6.7485e-04, -2.2018e-03, -5.2256e-04,
         -4.2986e-03,  3.6416e-03,  2.7624e-03, -1.0416e-04, -1.5211e-03,
          3.4205e-03, -4.8705e-03,  8.7053e-04, -9.5039e-03,  1.6962e-03,
         -8.9307e-04,  4.2579e-03,  4.5600e-03,  9.3269e-05,  4.1723e-03,
          4.3753e-03,  4.2999e-03, -9.5729e-03,  1.1858e-02, -1.6742e-03,
         -5.2211e-03, -4.8207e-03, -6.6603e-03,  7.9248e-04, -1.7687e-03,
          3.1359e-03, -2.1646e-02, -5.3285e-03, -1.1752e-02, -2.6304e-03,
         -1.2540e-03, -4.9396e-03,  5.6491e-03,  4.5358e-03, -2.9104e-03,
         -6.1227e-03,  5.1025e-04, -3.3715e-03,  4.9853e-04, -1.3932e-03,
          3.6152e-03,  6.2276e-03, -3.5835e-03,  2.7106e-03,  4.1965e-03,
          1.3592e-02,  2.9451e-03,  3.9580e-03, -5.0566e-03,  5.2371e-03,
          3.5334e-03, -2.3960e-03, -7.9007e-03,  7.4309e-03, -3.2834e-04,
         -5.5479e-03, -3.5222e-03, -1.1793e-03, -8.8391e-04,  1.1448e-03,
          1.1097e-03,  8.2285e-03, -8.6262e-04, -1.7669e-03, -6.2544e-03,
         -2.1787e-03, -6.0379e-03,  1.8630e-03, -6.2989e-04,  7.0048e-03,
         -1.1222e-02,  6.5895e-04,  5.7653e-03, -4.7024e-03,  4.5446e-03,
         -1.4057e-03, -3.6761e-03, -9.2350e-03, -9.1827e-03,  1.3129e-03,
         -2.5676e-03,  1.2887e-03,  5.2682e-04, -1.0249e-04,  1.4471e-03,
          3.9572e-04, -4.8747e-03, -2.3157e-04,  4.3671e-03,  1.1306e-03,
          1.6041e-02, -3.3736e-03,  4.9522e-03, -1.0373e-03, -7.6226e-03,
         -6.5351e-03,  5.2816e-03, -1.3818e-03, -9.3294e-05,  6.7219e-03,
          8.7989e-04,  6.4248e-03, -2.3653e-04, -3.1201e-03, -1.0371e-03,
         -9.3911e-04, -9.2567e-03,  8.5027e-04,  5.3072e-05, -9.3070e-04,
          6.8644e-03,  3.7198e-03, -6.0334e-03, -1.2082e-03, -3.7227e-03,
          3.7504e-03, -3.6241e-03,  2.1014e-04, -4.3596e-04, -1.7839e-03,
          2.5420e-03, -1.1001e-04,  4.9760e-04,  6.0787e-04, -4.5509e-04,
          3.6351e-03, -1.3837e-02,  1.6410e-02, -3.7162e-03, -1.1491e-04,
          4.0731e-03,  8.2846e-04, -8.3458e-03,  4.9710e-03, -6.8137e-03,
          2.5116e-03,  2.3753e-03, -9.7605e-04,  8.0082e-03,  2.3546e-04,
          2.7696e-04,  2.0004e-03,  3.3233e-03,  5.7417e-03,  1.4625e-03,
         -1.5690e-03,  9.8333e-04, -4.1527e-04,  2.3841e-03,  1.6671e-04,
         -8.9850e-03,  4.1204e-04, -2.9043e-04, -7.3776e-04, -4.2903e-04,
          5.6028e-04, -1.1533e-03, -8.4504e-04,  1.2468e-02,  9.3343e-05,
          7.8378e-04, -9.0935e-04, -1.3910e-03,  1.2429e-04,  1.8542e-04,
         -4.5211e-03,  3.7629e-03, -9.1964e-05,  3.5598e-03, -6.6871e-04,
          1.2388e-03, -1.2247e-02, -6.4686e-03,  7.2629e-03, -2.6612e-03,
         -3.4426e-03, -1.4171e-03, -4.2856e-03, -7.4453e-03, -1.9739e-03,
         -1.2102e-02, -3.9688e-03,  1.0960e-03, -4.6014e-03, -2.8026e-04,
          1.2441e-02, -8.6412e-04, -1.3658e-03,  3.9589e-03, -1.0413e-03,
          2.2644e-03,  1.0886e-02,  6.2832e-05, -9.7417e-03,  1.3542e-05,
          4.7955e-03,  1.3996e-05,  7.4300e-03,  7.6881e-04, -4.9217e-03,
          1.0979e-02, -1.7564e-03,  1.8712e-04,  3.1509e-03,  4.6358e-03,
          4.4438e-04,  1.5578e-02,  3.7495e-03, -3.9136e-03, -4.5436e-03,
         -6.0661e-03, -6.7532e-04]], device='cuda:2', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[2.0230e-03, 1.1450e-01, 1.6170e-01, 2.2597e-01, 8.4656e-02, 1.8268e-01,
         1.2137e-01, 1.2619e-02, 1.2198e-01, 6.1996e-02, 2.1181e-01, 9.3175e-02,
         7.3173e-02, 7.6725e-02, 1.7136e-01, 1.3922e-02, 1.0137e-01, 9.5047e-02,
         4.0318e-04, 7.7327e-02, 4.5718e-02, 1.6558e-03, 8.1992e-03, 3.8646e-04,
         5.4922e-02, 8.9601e-02, 1.3769e-01, 2.2219e-01, 2.8426e-02, 4.9503e-02,
         7.1528e-03, 2.2626e-01, 9.8573e-02, 1.1388e-01, 7.9312e-03, 7.7125e-02,
         7.1472e-02, 8.2232e-02, 3.0697e-02, 8.7377e-02, 5.9758e-02, 1.6403e-01,
         6.2526e-02, 1.1477e-02, 1.4700e-01, 7.7496e-02, 4.8697e-02, 7.9887e-02,
         6.1714e-02, 6.3852e-02, 5.7110e-02, 7.9115e-02, 1.2852e-01, 5.1895e-02,
         6.4537e-02, 1.9641e-01, 8.2656e-02, 8.5599e-03, 4.9696e-02, 1.2232e-01,
         7.5042e-02, 8.4741e-02, 1.9061e-01, 9.1258e-02, 7.7615e-02, 6.7269e-02,
         6.6034e-02, 5.4887e-02, 2.1375e-01, 6.3505e-02, 5.0385e-02, 9.6587e-02,
         1.0295e-01, 5.3863e-02, 6.6277e-02, 7.1704e-02, 7.0906e-02, 1.7948e-02,
         8.7554e-03, 3.5933e-02, 2.1863e-02, 1.4928e-02, 7.8517e-03, 6.6872e-02,
         4.1022e-02, 1.0667e-01, 8.0810e-02, 1.1884e-01, 1.1545e-01, 2.6598e-03,
         5.6964e-02, 7.4157e-02, 6.6029e-03, 2.0214e-01, 5.9461e-02, 9.4999e-02,
         6.5520e-02, 4.2807e-02, 5.7588e-02, 4.9302e-02, 1.7357e-01, 2.5215e-02,
         1.7901e-01, 7.8222e-02, 1.3646e-01, 1.7265e-02, 7.2437e-02, 1.0337e-02,
         6.6583e-02, 2.7776e-02, 1.7062e-01, 4.1686e-04, 3.0768e-03, 1.6110e-02,
         1.1624e-02, 2.2353e-03, 7.4381e-02, 1.1266e-01, 9.7551e-03, 1.7743e-01,
         6.9994e-02, 5.6030e-02, 1.2330e-01, 1.0083e-01, 4.7496e-02, 4.4400e-02,
         6.6871e-02, 6.3973e-02, 8.3626e-02, 7.9181e-03, 7.0770e-02, 7.9395e-02,
         5.7820e-02, 1.5353e-01, 2.1068e-02, 3.5883e-02, 2.9148e-02, 3.5790e-03,
         5.6792e-02, 2.3914e-02, 3.3121e-02, 7.3197e-02, 9.4688e-02, 6.5071e-02,
         2.0026e-02, 5.2237e-02, 6.0923e-02, 7.3830e-02, 6.1528e-02, 4.8502e-02,
         7.9950e-02, 1.2810e-01, 1.0700e-01, 6.6109e-02, 1.2212e-02, 1.2245e-01,
         9.1038e-02, 7.4283e-02, 6.0910e-02, 5.4452e-02, 9.5597e-02, 5.7371e-02,
         6.7565e-05, 8.8623e-02, 7.2104e-02, 7.5121e-02, 5.5209e-02, 2.6282e-02,
         1.4495e-02, 7.4371e-02, 5.6587e-02, 7.8598e-02, 9.8701e-02, 6.2205e-02,
         8.4151e-02, 9.3948e-02, 6.4501e-02, 5.4935e-02, 6.1636e-02, 8.9369e-02,
         6.7721e-02, 5.9307e-02, 7.4600e-02, 7.3003e-02, 1.2340e-01, 2.2294e-01,
         3.5984e-02, 6.1949e-02, 1.7235e-01, 6.4130e-02, 4.3244e-02, 6.2185e-02,
         4.1409e-04, 2.0703e-01, 8.1467e-02, 5.1261e-02, 2.2640e-02, 6.9325e-02,
         9.7792e-02, 9.9110e-02, 1.3354e-01, 9.0943e-02, 6.4128e-02, 1.5790e-01,
         4.6458e-02, 1.0351e-01, 6.8109e-02, 8.1738e-02, 6.7046e-02, 1.2475e-01,
         8.7878e-02, 7.6078e-02, 4.9238e-02, 1.0763e-01, 4.2631e-02, 9.4959e-02,
         6.7917e-02, 9.5232e-05, 1.2996e-01, 2.0996e-01, 1.9829e-01, 9.6239e-02,
         1.8068e-04, 1.0724e-01, 1.0371e-01, 7.5966e-04, 6.0505e-02, 5.9804e-02,
         1.2203e-02, 6.5703e-02, 1.1824e-02, 1.0366e-01, 8.2675e-02, 5.1647e-02,
         9.8538e-02, 7.1560e-03, 3.8754e-03, 4.0081e-03, 1.7276e-01, 4.8551e-02,
         3.1990e-02, 2.0774e-01, 4.5452e-02, 9.7128e-02, 2.0425e-01, 1.9682e-01,
         1.2298e-01, 2.0352e-01, 2.2200e-01, 8.2918e-02, 2.3308e-02, 5.8127e-02,
         1.2208e-01, 7.4306e-02, 6.0500e-02, 1.2303e-01, 3.3238e-03, 1.0401e-01,
         5.1567e-02, 2.2504e-01, 2.1899e-01, 1.5412e-01, 8.3564e-03, 4.5113e-03,
         1.7944e-03, 6.4100e-02, 2.1042e-01, 6.7251e-02, 9.5500e-02, 9.2260e-02,
         5.7243e-02, 4.8711e-02, 1.0425e-01, 2.6225e-02, 6.3639e-02, 9.0054e-02,
         8.6147e-02, 6.9624e-02, 2.2687e-01, 7.8878e-02, 6.8625e-02, 3.5229e-02,
         4.8139e-03, 1.8099e-01, 1.8527e-01, 4.7638e-03, 1.1937e-01, 8.3822e-02,
         5.7574e-02, 1.2708e-01, 7.0372e-02, 1.1550e-01, 1.2299e-01, 5.8825e-02,
         1.1300e-01, 4.3651e-04, 4.5226e-02, 7.6239e-02, 4.7422e-02, 8.3377e-02,
         3.4544e-02, 3.3432e-02, 3.0341e-02, 1.4592e-02, 4.5353e-02, 6.5224e-02,
         1.3309e-01, 7.2479e-02, 5.5558e-02, 6.1562e-02, 2.0424e-01, 8.2181e-02,
         3.3924e-02, 5.0700e-02, 1.1082e-01, 2.2832e-01, 1.3722e-01, 7.4096e-02,
         1.5723e-01, 9.2535e-02, 2.1536e-01, 6.9976e-02, 9.8296e-02, 7.7147e-02,
         6.4658e-02, 6.7754e-02, 1.5937e-01, 1.4792e-01, 1.1270e-01, 6.1399e-02,
         1.4126e-01, 7.0842e-02, 5.4473e-02, 1.6271e-01, 1.5415e-01, 8.6095e-02,
         6.0291e-02, 1.8163e-01, 9.0620e-02, 6.9967e-02, 1.2640e-01, 6.8162e-02,
         2.8931e-02, 7.1978e-02, 7.7471e-02, 1.7231e-01, 8.3588e-02, 9.3223e-02,
         9.6025e-02, 1.2655e-01, 2.2554e-01, 7.3692e-02, 8.6958e-02, 6.9835e-02,
         3.3387e-03, 7.4110e-02, 4.9904e-02, 5.1321e-02, 1.1667e-02, 2.8625e-02,
         3.5682e-02, 1.7917e-01, 7.2492e-02, 3.0648e-02, 5.9729e-02, 7.3441e-02,
         1.4927e-01, 3.0316e-02, 1.9311e-03, 1.0826e-01, 1.0176e-01, 6.9447e-02,
         1.1032e-01, 6.3680e-02, 9.1249e-02, 1.0147e-01, 1.1799e-01, 1.1795e-01,
         1.4250e-01, 7.0617e-02, 8.0019e-02, 1.4833e-02, 2.1412e-03, 6.8915e-02,
         5.6345e-02, 3.8692e-03, 6.6348e-02, 4.6666e-03, 6.1524e-02, 8.0339e-02,
         2.0966e-01, 7.8241e-02, 6.6574e-02, 7.5578e-02, 1.7741e-01, 5.5287e-02,
         6.7839e-02, 4.1269e-02, 1.9159e-02, 1.2751e-01, 1.0504e-01, 9.5701e-02,
         2.0086e-03, 5.8083e-02, 6.7536e-02, 5.4042e-02, 3.1595e-04, 5.8564e-02,
         2.5093e-03, 4.4297e-02, 1.3833e-01, 1.1897e-01, 1.4406e-01, 1.1960e-02,
         5.0540e-02, 1.3277e-01, 3.8313e-02, 8.1577e-03, 1.0445e-02, 2.6198e-02,
         3.8993e-02, 9.8359e-02, 2.2510e-01, 1.5271e-02, 9.3246e-02, 6.2766e-02,
         1.1576e-01, 2.1915e-01, 6.8354e-02, 2.6003e-03, 5.1442e-02, 2.7326e-02,
         1.1912e-01, 2.8804e-02, 7.6856e-02, 2.0877e-01, 5.3066e-02, 1.5066e-02,
         7.2739e-02, 3.7616e-02, 1.7743e-04, 7.5117e-02, 8.3469e-02, 6.9630e-02,
         1.3852e-01, 1.5166e-02, 1.4461e-02, 1.9037e-03, 8.7971e-02, 1.1062e-03,
         1.6937e-01, 6.0109e-02, 5.3805e-02, 8.6913e-03, 9.4512e-03, 5.6131e-03,
         2.6220e-02, 3.6625e-03, 5.4497e-05, 2.8745e-03, 5.7389e-02, 1.1571e-02,
         9.3374e-02, 5.6185e-02, 6.6358e-02, 1.0954e-01, 1.1566e-01, 3.1366e-03,
         1.4273e-01, 9.0547e-02, 2.7288e-02, 1.9876e-01, 1.8294e-01, 5.7271e-02,
         6.1633e-02, 1.6231e-01, 6.3967e-02, 6.0859e-02, 1.8825e-01, 8.0915e-02,
         9.9955e-02, 7.4110e-02, 6.8127e-02, 1.2132e-01, 1.1777e-01, 2.1206e-01,
         8.9946e-02, 6.4407e-02, 1.2684e-01, 7.0304e-02, 1.1088e-01, 1.5335e-01,
         3.4028e-03, 8.8672e-02, 4.3645e-02, 8.9120e-02, 1.2475e-03, 1.1361e-01,
         1.8034e-01, 7.1949e-02, 6.9424e-02, 1.5361e-01, 3.1977e-03, 8.6176e-02,
         1.0996e-01, 9.0116e-02, 1.1536e-01, 4.5834e-02, 1.0143e-01, 9.5620e-02,
         6.1081e-02, 7.2833e-03]], device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.2325], device='cuda:2', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0300], device='cuda:2', grad_fn=<AddBackward0>)
Using device: cuda:2
===== Training profile tensin-3x512-s3 - 2 =====
[0:00:01.725698] epoch: 0 | elbo: 36125.7612890625 | train_rmse: 1.44 | val_rmse: 1.4994 | val_ll: -2.2455
[0:01:32.983654] epoch: 50 | elbo: 36060.251445312504 | train_rmse: 1.4231 | val_rmse: 1.4854 | val_ll: -2.2489
[0:03:04.149777] epoch: 100 | elbo: 36045.884921875 | train_rmse: 1.4172 | val_rmse: 1.4839 | val_ll: -2.248
[0:04:35.933012] epoch: 150 | elbo: 35987.60671875 | train_rmse: 1.4123 | val_rmse: 1.4726 | val_ll: -2.2467
[0:06:08.820242] epoch: 200 | elbo: 35922.792109375005 | train_rmse: 1.414 | val_rmse: 1.4737 | val_ll: -2.2445
[0:07:41.206593] epoch: 250 | elbo: 35879.99019531249 | train_rmse: 1.4083 | val_rmse: 1.4601 | val_ll: -2.2426
[0:09:13.591861] epoch: 300 | elbo: 35884.9643359375 | train_rmse: 1.3934 | val_rmse: 1.4607 | val_ll: -2.2438
[0:10:45.208635] epoch: 350 | elbo: 35818.56328125 | train_rmse: 1.3873 | val_rmse: 1.4559 | val_ll: -2.2417
[0:12:16.615662] epoch: 400 | elbo: 35766.2344921875 | train_rmse: 1.393 | val_rmse: 1.4466 | val_ll: -2.2439
[0:13:49.026335] epoch: 450 | elbo: 35761.31902343749 | train_rmse: 1.3923 | val_rmse: 1.4514 | val_ll: -2.2406
[0:15:22.378875] epoch: 500 | elbo: 35823.698320312506 | train_rmse: 1.3752 | val_rmse: 1.4442 | val_ll: -2.2407
[0:16:55.664582] epoch: 550 | elbo: 35667.9737109375 | train_rmse: 1.384 | val_rmse: 1.4365 | val_ll: -2.2394
[0:18:27.704492] epoch: 600 | elbo: 35653.9053515625 | train_rmse: 1.3768 | val_rmse: 1.4486 | val_ll: -2.2401
[0:20:00.986540] epoch: 650 | elbo: 35628.8655078125 | train_rmse: 1.376 | val_rmse: 1.4415 | val_ll: -2.239
[0:21:33.961646] epoch: 700 | elbo: 35574.21292968751 | train_rmse: 1.3711 | val_rmse: 1.4427 | val_ll: -2.2359
[0:23:08.300917] epoch: 750 | elbo: 35535.64847656251 | train_rmse: 1.3637 | val_rmse: 1.4293 | val_ll: -2.2394
[0:24:42.055214] epoch: 800 | elbo: 35496.58757812501 | train_rmse: 1.3691 | val_rmse: 1.4287 | val_ll: -2.2357
[0:26:14.540546] epoch: 850 | elbo: 35470.563359375 | train_rmse: 1.3541 | val_rmse: 1.4202 | val_ll: -2.2362
[0:27:46.756665] epoch: 900 | elbo: 35473.158046875 | train_rmse: 1.3582 | val_rmse: 1.4104 | val_ll: -2.2343
[0:29:19.389600] epoch: 950 | elbo: 35403.9380859375 | train_rmse: 1.3521 | val_rmse: 1.4195 | val_ll: -2.2351
[0:30:51.501219] epoch: 1000 | elbo: 35424.623281249995 | train_rmse: 1.3457 | val_rmse: 1.4111 | val_ll: -2.2349
[0:32:22.818662] epoch: 1050 | elbo: 35356.5081640625 | train_rmse: 1.35 | val_rmse: 1.414 | val_ll: -2.2324
[0:33:54.700325] epoch: 1100 | elbo: 35322.6721484375 | train_rmse: 1.3437 | val_rmse: 1.4063 | val_ll: -2.2329
[0:35:26.249521] epoch: 1150 | elbo: 35253.5265625 | train_rmse: 1.336 | val_rmse: 1.4121 | val_ll: -2.2331
[0:36:58.779200] epoch: 1200 | elbo: 35263.7992578125 | train_rmse: 1.3466 | val_rmse: 1.4077 | val_ll: -2.2335
[0:38:30.248893] epoch: 1250 | elbo: 35215.16078125 | train_rmse: 1.3315 | val_rmse: 1.4007 | val_ll: -2.2323
[0:40:01.275960] epoch: 1300 | elbo: 35200.822265625 | train_rmse: 1.322 | val_rmse: 1.3874 | val_ll: -2.2301
[0:41:32.074209] epoch: 1350 | elbo: 35166.764179687496 | train_rmse: 1.3337 | val_rmse: 1.3947 | val_ll: -2.229
[0:43:02.864212] epoch: 1400 | elbo: 35133.04234375 | train_rmse: 1.329 | val_rmse: 1.3857 | val_ll: -2.231
[0:44:33.722382] epoch: 1450 | elbo: 35134.3516796875 | train_rmse: 1.3199 | val_rmse: 1.3922 | val_ll: -2.2278
[0:46:05.923654] epoch: 1500 | elbo: 35069.3834375 | train_rmse: 1.3245 | val_rmse: 1.3849 | val_ll: -2.2269
[0:47:37.814854] epoch: 1550 | elbo: 35079.74671875 | train_rmse: 1.3161 | val_rmse: 1.3816 | val_ll: -2.2303
[0:49:09.713843] epoch: 1600 | elbo: 35010.056210937495 | train_rmse: 1.3194 | val_rmse: 1.3874 | val_ll: -2.227
[0:50:41.088761] epoch: 1650 | elbo: 34980.0405859375 | train_rmse: 1.302 | val_rmse: 1.3731 | val_ll: -2.2269
[0:52:12.015110] epoch: 1700 | elbo: 34950.79101562501 | train_rmse: 1.3007 | val_rmse: 1.3717 | val_ll: -2.2291
[0:53:44.814741] epoch: 1750 | elbo: 34919.125507812496 | train_rmse: 1.2999 | val_rmse: 1.3675 | val_ll: -2.2263
[0:55:15.960778] epoch: 1800 | elbo: 34893.54816406251 | train_rmse: 1.2977 | val_rmse: 1.3572 | val_ll: -2.2233
[0:56:47.654053] epoch: 1850 | elbo: 34881.64484375001 | train_rmse: 1.2936 | val_rmse: 1.3673 | val_ll: -2.2258
[0:58:18.821307] epoch: 1900 | elbo: 34866.51203124999 | train_rmse: 1.2858 | val_rmse: 1.3604 | val_ll: -2.2245
[0:59:49.995443] epoch: 1950 | elbo: 34839.3433203125 | train_rmse: 1.2839 | val_rmse: 1.3531 | val_ll: -2.225
[1:01:21.244589] epoch: 2000 | elbo: 34798.8711328125 | train_rmse: 1.2834 | val_rmse: 1.3521 | val_ll: -2.2241
[1:02:52.602668] epoch: 2050 | elbo: 34728.47527343749 | train_rmse: 1.2817 | val_rmse: 1.3498 | val_ll: -2.2219
[1:04:24.257697] epoch: 2100 | elbo: 34736.922421875 | train_rmse: 1.2762 | val_rmse: 1.349 | val_ll: -2.2225
[1:05:55.492406] epoch: 2150 | elbo: 34764.16140625001 | train_rmse: 1.2745 | val_rmse: 1.3392 | val_ll: -2.2211
[1:07:27.369073] epoch: 2200 | elbo: 34701.58015625 | train_rmse: 1.2807 | val_rmse: 1.3478 | val_ll: -2.2224
[1:08:58.403880] epoch: 2250 | elbo: 34653.79363281249 | train_rmse: 1.2703 | val_rmse: 1.3337 | val_ll: -2.2212
[1:10:30.097974] epoch: 2300 | elbo: 34613.235624999994 | train_rmse: 1.2848 | val_rmse: 1.3437 | val_ll: -2.2218
[1:12:01.191778] epoch: 2350 | elbo: 34604.9158203125 | train_rmse: 1.2693 | val_rmse: 1.3351 | val_ll: -2.2194
[1:13:32.284927] epoch: 2400 | elbo: 34562.5393359375 | train_rmse: 1.2668 | val_rmse: 1.3435 | val_ll: -2.2209
[1:15:03.187094] epoch: 2450 | elbo: 34563.87468749999 | train_rmse: 1.2678 | val_rmse: 1.341 | val_ll: -2.22
[1:16:34.092682] epoch: 2500 | elbo: 34531.90078124999 | train_rmse: 1.2458 | val_rmse: 1.3192 | val_ll: -2.2189
[1:18:04.809730] epoch: 2550 | elbo: 34529.760585937496 | train_rmse: 1.2526 | val_rmse: 1.3235 | val_ll: -2.216
[1:19:35.723569] epoch: 2600 | elbo: 34474.4694140625 | train_rmse: 1.2548 | val_rmse: 1.3185 | val_ll: -2.2189
[1:21:06.268330] epoch: 2650 | elbo: 34435.0735546875 | train_rmse: 1.2559 | val_rmse: 1.323 | val_ll: -2.2168
[1:22:37.973165] epoch: 2700 | elbo: 34448.4060546875 | train_rmse: 1.2482 | val_rmse: 1.3149 | val_ll: -2.217
[1:24:09.797230] epoch: 2750 | elbo: 34437.7387109375 | train_rmse: 1.2548 | val_rmse: 1.3192 | val_ll: -2.2173
[1:25:40.901019] epoch: 2800 | elbo: 34420.83015625 | train_rmse: 1.2478 | val_rmse: 1.3186 | val_ll: -2.2167
[1:27:11.482447] epoch: 2850 | elbo: 34377.9716796875 | train_rmse: 1.2426 | val_rmse: 1.3068 | val_ll: -2.2168
[1:28:43.768202] epoch: 2900 | elbo: 34340.766835937495 | train_rmse: 1.2427 | val_rmse: 1.3167 | val_ll: -2.215
[1:30:16.190608] epoch: 2950 | elbo: 34366.22527343751 | train_rmse: 1.2388 | val_rmse: 1.3155 | val_ll: -2.2147
[1:31:48.415407] epoch: 3000 | elbo: 34331.774765625 | train_rmse: 1.2383 | val_rmse: 1.3099 | val_ll: -2.2146
[1:33:22.255314] epoch: 3050 | elbo: 34308.117656250004 | train_rmse: 1.2301 | val_rmse: 1.2962 | val_ll: -2.2143
[1:34:54.580778] epoch: 3100 | elbo: 34294.639609375 | train_rmse: 1.2407 | val_rmse: 1.3128 | val_ll: -2.2158
[1:36:26.705098] epoch: 3150 | elbo: 34232.657656250005 | train_rmse: 1.2296 | val_rmse: 1.2989 | val_ll: -2.2134
[1:37:59.384975] epoch: 3200 | elbo: 34340.3984765625 | train_rmse: 1.2331 | val_rmse: 1.2976 | val_ll: -2.2134
[1:39:31.652702] epoch: 3250 | elbo: 34251.00453125 | train_rmse: 1.2262 | val_rmse: 1.3004 | val_ll: -2.2148
[1:41:02.428380] epoch: 3300 | elbo: 34290.10203125 | train_rmse: 1.2249 | val_rmse: 1.2852 | val_ll: -2.2131
[1:42:32.846058] epoch: 3350 | elbo: 34205.2439453125 | train_rmse: 1.2276 | val_rmse: 1.3034 | val_ll: -2.2133
[1:44:05.264120] epoch: 3400 | elbo: 34215.703281250004 | train_rmse: 1.2268 | val_rmse: 1.2944 | val_ll: -2.2118
[1:45:37.180703] epoch: 3450 | elbo: 34167.30113281249 | train_rmse: 1.2177 | val_rmse: 1.2968 | val_ll: -2.2137
[1:47:09.898540] epoch: 3500 | elbo: 34147.92515625001 | train_rmse: 1.2182 | val_rmse: 1.2891 | val_ll: -2.2142
[1:48:41.253234] epoch: 3550 | elbo: 34128.9469140625 | train_rmse: 1.2279 | val_rmse: 1.2961 | val_ll: -2.212
[1:50:12.038828] epoch: 3600 | elbo: 34151.13734375 | train_rmse: 1.2207 | val_rmse: 1.2854 | val_ll: -2.2116
[1:51:43.127138] epoch: 3650 | elbo: 34089.5252734375 | train_rmse: 1.2123 | val_rmse: 1.2839 | val_ll: -2.2109
[1:53:14.639591] epoch: 3700 | elbo: 34100.456484375 | train_rmse: 1.2125 | val_rmse: 1.2797 | val_ll: -2.21
[1:54:46.311259] epoch: 3750 | elbo: 34107.9522265625 | train_rmse: 1.2226 | val_rmse: 1.289 | val_ll: -2.2113
[1:56:17.975694] epoch: 3800 | elbo: 34021.605039062495 | train_rmse: 1.2135 | val_rmse: 1.2828 | val_ll: -2.2116
[1:57:49.895421] epoch: 3850 | elbo: 34057.438046874995 | train_rmse: 1.2057 | val_rmse: 1.2708 | val_ll: -2.2096
[1:59:23.124290] epoch: 3900 | elbo: 34025.3832421875 | train_rmse: 1.2099 | val_rmse: 1.2741 | val_ll: -2.2091
[2:00:54.744999] epoch: 3950 | elbo: 34087.0376953125 | train_rmse: 1.2048 | val_rmse: 1.2804 | val_ll: -2.2105
[2:02:27.278387] epoch: 4000 | elbo: 33988.3736328125 | train_rmse: 1.2129 | val_rmse: 1.2801 | val_ll: -2.211
[2:04:01.759516] epoch: 4050 | elbo: 34050.634921875004 | train_rmse: 1.2033 | val_rmse: 1.2718 | val_ll: -2.208
[2:05:35.419682] epoch: 4100 | elbo: 33959.239531249994 | train_rmse: 1.2025 | val_rmse: 1.2703 | val_ll: -2.2098
[2:07:08.372916] epoch: 4150 | elbo: 33979.81 | train_rmse: 1.1932 | val_rmse: 1.2723 | val_ll: -2.2095
[2:08:40.673784] epoch: 4200 | elbo: 33950.1130859375 | train_rmse: 1.1933 | val_rmse: 1.2618 | val_ll: -2.208
[2:10:13.943295] epoch: 4250 | elbo: 33890.235625 | train_rmse: 1.1936 | val_rmse: 1.2604 | val_ll: -2.2074
[2:11:46.526440] epoch: 4300 | elbo: 33945.989375 | train_rmse: 1.1959 | val_rmse: 1.2635 | val_ll: -2.209
[2:13:19.715302] epoch: 4350 | elbo: 33883.63808593749 | train_rmse: 1.1979 | val_rmse: 1.2653 | val_ll: -2.2086
[2:14:52.245539] epoch: 4400 | elbo: 33852.33441406251 | train_rmse: 1.1958 | val_rmse: 1.2608 | val_ll: -2.2086
[2:16:24.477728] epoch: 4450 | elbo: 33830.1033203125 | train_rmse: 1.1829 | val_rmse: 1.256 | val_ll: -2.2075
[2:17:57.411635] epoch: 4500 | elbo: 33859.319921875 | train_rmse: 1.1869 | val_rmse: 1.2589 | val_ll: -2.2082
[2:19:30.005357] epoch: 4550 | elbo: 33813.7587109375 | train_rmse: 1.1953 | val_rmse: 1.2582 | val_ll: -2.2088
[2:21:02.534365] epoch: 4600 | elbo: 33841.1153125 | train_rmse: 1.193 | val_rmse: 1.2621 | val_ll: -2.2083
[2:22:35.461784] epoch: 4650 | elbo: 33816.335234375 | train_rmse: 1.1814 | val_rmse: 1.2505 | val_ll: -2.2065
[2:24:07.962679] epoch: 4700 | elbo: 33806.105195312506 | train_rmse: 1.1904 | val_rmse: 1.2512 | val_ll: -2.2071
[2:25:39.762966] epoch: 4750 | elbo: 33762.8930078125 | train_rmse: 1.1768 | val_rmse: 1.2454 | val_ll: -2.2075
[2:27:11.816214] epoch: 4800 | elbo: 33873.965234375 | train_rmse: 1.1819 | val_rmse: 1.2586 | val_ll: -2.2082
[2:28:43.188834] epoch: 4850 | elbo: 33762.65390625 | train_rmse: 1.1859 | val_rmse: 1.2548 | val_ll: -2.2055
[2:30:15.596810] epoch: 4900 | elbo: 33807.2921875 | train_rmse: 1.1864 | val_rmse: 1.2496 | val_ll: -2.2064
[2:31:48.566297] epoch: 4950 | elbo: 33767.9095703125 | train_rmse: 1.1838 | val_rmse: 1.2472 | val_ll: -2.2054
[2:33:21.823943] epoch: 5000 | elbo: 33729.673007812504 | train_rmse: 1.1851 | val_rmse: 1.2509 | val_ll: -2.2052
[2:34:53.625158] epoch: 5050 | elbo: 33698.382070312495 | train_rmse: 1.1863 | val_rmse: 1.2507 | val_ll: -2.2054
[2:36:25.069545] epoch: 5100 | elbo: 33723.0834375 | train_rmse: 1.1811 | val_rmse: 1.25 | val_ll: -2.206
[2:37:56.876336] epoch: 5150 | elbo: 33706.3333984375 | train_rmse: 1.1794 | val_rmse: 1.251 | val_ll: -2.2041
[2:39:27.034077] epoch: 5200 | elbo: 33696.11062500001 | train_rmse: 1.1728 | val_rmse: 1.2449 | val_ll: -2.206
[2:40:57.705674] epoch: 5250 | elbo: 33662.512773437506 | train_rmse: 1.1751 | val_rmse: 1.2427 | val_ll: -2.2024
[2:42:30.676868] epoch: 5300 | elbo: 33687.933320312506 | train_rmse: 1.1718 | val_rmse: 1.2336 | val_ll: -2.203
[2:44:05.219335] epoch: 5350 | elbo: 33629.98265625 | train_rmse: 1.1832 | val_rmse: 1.243 | val_ll: -2.2041
[2:45:37.332051] epoch: 5400 | elbo: 33751.6241015625 | train_rmse: 1.1652 | val_rmse: 1.227 | val_ll: -2.207
[2:47:09.180400] epoch: 5450 | elbo: 33634.005351562504 | train_rmse: 1.1709 | val_rmse: 1.2378 | val_ll: -2.205
[2:48:41.160900] epoch: 5500 | elbo: 33588.233906249996 | train_rmse: 1.1694 | val_rmse: 1.235 | val_ll: -2.2034
[2:50:12.138242] epoch: 5550 | elbo: 33696.76843750001 | train_rmse: 1.1781 | val_rmse: 1.2331 | val_ll: -2.2027
[2:51:44.602626] epoch: 5600 | elbo: 33591.4259765625 | train_rmse: 1.1682 | val_rmse: 1.2369 | val_ll: -2.2032
[2:53:16.780495] epoch: 5650 | elbo: 33611.2001171875 | train_rmse: 1.1616 | val_rmse: 1.2366 | val_ll: -2.2034
[2:54:47.926547] epoch: 5700 | elbo: 33564.051523437505 | train_rmse: 1.1744 | val_rmse: 1.2373 | val_ll: -2.2034
[2:56:19.392608] epoch: 5750 | elbo: 33569.29781249999 | train_rmse: 1.1673 | val_rmse: 1.2345 | val_ll: -2.2008
[2:57:50.649371] epoch: 5800 | elbo: 33598.601367187504 | train_rmse: 1.1634 | val_rmse: 1.2279 | val_ll: -2.2024
[2:59:21.318196] epoch: 5850 | elbo: 33557.0107421875 | train_rmse: 1.1725 | val_rmse: 1.238 | val_ll: -2.1998
[3:00:51.552114] epoch: 5900 | elbo: 33538.60296875 | train_rmse: 1.1633 | val_rmse: 1.2356 | val_ll: -2.2029
[3:02:24.247258] epoch: 5950 | elbo: 33565.4609765625 | train_rmse: 1.1576 | val_rmse: 1.2309 | val_ll: -2.2019
[3:03:57.718883] epoch: 6000 | elbo: 33530.005585937506 | train_rmse: 1.1686 | val_rmse: 1.2337 | val_ll: -2.1998
[3:05:31.657600] epoch: 6050 | elbo: 33500.03015625 | train_rmse: 1.1621 | val_rmse: 1.2221 | val_ll: -2.2011
[3:07:03.962159] epoch: 6100 | elbo: 33508.757929687505 | train_rmse: 1.1598 | val_rmse: 1.2275 | val_ll: -2.2
[3:08:36.313834] epoch: 6150 | elbo: 33508.8122265625 | train_rmse: 1.1633 | val_rmse: 1.2267 | val_ll: -2.2004
[3:10:08.378998] epoch: 6200 | elbo: 33463.197734375 | train_rmse: 1.1655 | val_rmse: 1.2288 | val_ll: -2.2002
[3:11:40.096761] epoch: 6250 | elbo: 33490.44796875 | train_rmse: 1.1631 | val_rmse: 1.2309 | val_ll: -2.2014
[3:13:13.206121] epoch: 6300 | elbo: 33467.964375 | train_rmse: 1.1641 | val_rmse: 1.2316 | val_ll: -2.1985
[3:14:46.102533] epoch: 6350 | elbo: 33468.784374999996 | train_rmse: 1.163 | val_rmse: 1.2241 | val_ll: -2.2001
[3:16:17.730434] epoch: 6400 | elbo: 33428.73238281249 | train_rmse: 1.1644 | val_rmse: 1.2259 | val_ll: -2.1981
[3:17:51.601949] epoch: 6450 | elbo: 33438.971484375 | train_rmse: 1.1643 | val_rmse: 1.2257 | val_ll: -2.199
[3:19:24.335677] epoch: 6500 | elbo: 33455.0998046875 | train_rmse: 1.1573 | val_rmse: 1.2264 | val_ll: -2.1987
[3:20:57.608374] epoch: 6550 | elbo: 33406.086640625006 | train_rmse: 1.1528 | val_rmse: 1.2138 | val_ll: -2.1976
[3:22:29.976765] epoch: 6600 | elbo: 33372.969921875 | train_rmse: 1.159 | val_rmse: 1.2229 | val_ll: -2.1982
[3:24:01.617906] epoch: 6650 | elbo: 33379.40753906251 | train_rmse: 1.1481 | val_rmse: 1.2132 | val_ll: -2.1994
[3:25:32.776101] epoch: 6700 | elbo: 33349.9056640625 | train_rmse: 1.1576 | val_rmse: 1.2233 | val_ll: -2.2008
[3:27:03.713256] epoch: 6750 | elbo: 33376.5884765625 | train_rmse: 1.1505 | val_rmse: 1.2191 | val_ll: -2.1994
[3:28:34.957394] epoch: 6800 | elbo: 33356.9571484375 | train_rmse: 1.1483 | val_rmse: 1.2116 | val_ll: -2.1974
[3:30:06.449990] epoch: 6850 | elbo: 33365.846875 | train_rmse: 1.1472 | val_rmse: 1.2134 | val_ll: -2.1979
[3:31:38.285431] epoch: 6900 | elbo: 33583.57796875 | train_rmse: 1.1628 | val_rmse: 1.226 | val_ll: -2.1997
[3:33:09.176157] epoch: 6950 | elbo: 33331.970078125 | train_rmse: 1.1475 | val_rmse: 1.2128 | val_ll: -2.1968
[3:34:41.044094] epoch: 7000 | elbo: 33335.130859375 | train_rmse: 1.1454 | val_rmse: 1.2135 | val_ll: -2.1966
[3:36:13.349185] epoch: 7050 | elbo: 33323.580976562494 | train_rmse: 1.1435 | val_rmse: 1.2056 | val_ll: -2.1981
[3:37:45.161847] epoch: 7100 | elbo: 33383.392851562494 | train_rmse: 1.1579 | val_rmse: 1.2206 | val_ll: -2.1974
[3:39:15.982578] epoch: 7150 | elbo: 33343.668125 | train_rmse: 1.1399 | val_rmse: 1.2076 | val_ll: -2.2004
[3:40:47.147038] epoch: 7200 | elbo: 33251.416953125 | train_rmse: 1.164 | val_rmse: 1.2228 | val_ll: -2.198
[3:42:18.701788] epoch: 7250 | elbo: 33354.365390625004 | train_rmse: 1.14 | val_rmse: 1.2087 | val_ll: -2.196
[3:43:50.282408] epoch: 7300 | elbo: 33278.3715234375 | train_rmse: 1.1455 | val_rmse: 1.2163 | val_ll: -2.1969
[3:45:21.054425] epoch: 7350 | elbo: 33291.733671874994 | train_rmse: 1.1471 | val_rmse: 1.2056 | val_ll: -2.1981
[3:46:51.633881] epoch: 7400 | elbo: 33247.7968359375 | train_rmse: 1.1482 | val_rmse: 1.207 | val_ll: -2.1958
[3:48:22.557579] epoch: 7450 | elbo: 33214.1513671875 | train_rmse: 1.1475 | val_rmse: 1.2153 | val_ll: -2.1957
[3:49:53.509415] epoch: 7500 | elbo: 33242.250507812496 | train_rmse: 1.1275 | val_rmse: 1.1995 | val_ll: -2.1965
[3:51:24.904297] epoch: 7550 | elbo: 33233.969492187505 | train_rmse: 1.1445 | val_rmse: 1.2066 | val_ll: -2.196
[3:52:55.545029] epoch: 7600 | elbo: 33279.6255859375 | train_rmse: 1.1329 | val_rmse: 1.1997 | val_ll: -2.1964
[3:54:26.032644] epoch: 7650 | elbo: 33325.6151171875 | train_rmse: 1.1371 | val_rmse: 1.2008 | val_ll: -2.1962
[3:55:56.784641] epoch: 7700 | elbo: 33282.810625000006 | train_rmse: 1.1388 | val_rmse: 1.2032 | val_ll: -2.197
[3:57:29.001075] epoch: 7750 | elbo: 33160.2041015625 | train_rmse: 1.1367 | val_rmse: 1.2003 | val_ll: -2.1976
[3:59:02.304988] epoch: 7800 | elbo: 33212.834863281256 | train_rmse: 1.1396 | val_rmse: 1.204 | val_ll: -2.1944
[4:00:35.253100] epoch: 7850 | elbo: 33203.053515625 | train_rmse: 1.1354 | val_rmse: 1.206 | val_ll: -2.1964
[4:02:08.390761] epoch: 7900 | elbo: 33170.4694140625 | train_rmse: 1.1311 | val_rmse: 1.2031 | val_ll: -2.1961
[4:03:40.737141] epoch: 7950 | elbo: 33172.477578125 | train_rmse: 1.1258 | val_rmse: 1.1975 | val_ll: -2.1952
[4:05:11.505072] epoch: 8000 | elbo: 33137.8711328125 | train_rmse: 1.1349 | val_rmse: 1.2007 | val_ll: -2.1963
[4:06:42.399549] epoch: 8050 | elbo: 33270.817734375 | train_rmse: 1.1356 | val_rmse: 1.2036 | val_ll: -2.1964
[4:08:13.742750] epoch: 8100 | elbo: 33282.988359375 | train_rmse: 1.1293 | val_rmse: 1.203 | val_ll: -2.196
[4:09:44.946407] epoch: 8150 | elbo: 33191.664296874995 | train_rmse: 1.1232 | val_rmse: 1.1908 | val_ll: -2.1952
[4:11:16.576396] epoch: 8200 | elbo: 33164.63734375 | train_rmse: 1.1346 | val_rmse: 1.195 | val_ll: -2.1972
[4:12:47.874550] epoch: 8250 | elbo: 33161.28121093749 | train_rmse: 1.1358 | val_rmse: 1.2017 | val_ll: -2.1948
[4:14:19.686076] epoch: 8300 | elbo: 33128.34371093749 | train_rmse: 1.1407 | val_rmse: 1.2076 | val_ll: -2.1941
[4:15:51.989748] epoch: 8350 | elbo: 33166.419179687495 | train_rmse: 1.1415 | val_rmse: 1.1951 | val_ll: -2.1933
[4:17:23.955117] epoch: 8400 | elbo: 33268.5479296875 | train_rmse: 1.1288 | val_rmse: 1.189 | val_ll: -2.1919
[4:18:54.913994] epoch: 8450 | elbo: 33124.442812500005 | train_rmse: 1.1237 | val_rmse: 1.1958 | val_ll: -2.1947
[4:20:25.851337] epoch: 8500 | elbo: 33095.9982421875 | train_rmse: 1.1341 | val_rmse: 1.1959 | val_ll: -2.194
[4:21:57.324255] epoch: 8550 | elbo: 33140.94810546875 | train_rmse: 1.1325 | val_rmse: 1.1942 | val_ll: -2.196
[4:23:29.342819] epoch: 8600 | elbo: 33101.076015624996 | train_rmse: 1.1281 | val_rmse: 1.1903 | val_ll: -2.1943
[4:25:01.542352] epoch: 8650 | elbo: 33117.47685546875 | train_rmse: 1.1298 | val_rmse: 1.188 | val_ll: -2.1951
[4:26:33.531774] epoch: 8700 | elbo: 33111.2416796875 | train_rmse: 1.1308 | val_rmse: 1.2013 | val_ll: -2.1963
[4:28:07.242501] epoch: 8750 | elbo: 33148.575390625 | train_rmse: 1.1277 | val_rmse: 1.1918 | val_ll: -2.1946
[4:29:40.119463] epoch: 8800 | elbo: 33144.70859375 | train_rmse: 1.1268 | val_rmse: 1.1902 | val_ll: -2.1924
[4:31:13.173883] epoch: 8850 | elbo: 33091.508945312504 | train_rmse: 1.1246 | val_rmse: 1.1914 | val_ll: -2.1944
[4:32:45.776840] epoch: 8900 | elbo: 33081.9586328125 | train_rmse: 1.125 | val_rmse: 1.1891 | val_ll: -2.1952
[4:34:17.917705] epoch: 8950 | elbo: 33075.35712890625 | train_rmse: 1.1328 | val_rmse: 1.1936 | val_ll: -2.1924
[4:35:50.679908] epoch: 9000 | elbo: 33099.3755078125 | train_rmse: 1.1238 | val_rmse: 1.1939 | val_ll: -2.1943
[4:37:22.460400] epoch: 9050 | elbo: 33049.538808593745 | train_rmse: 1.1187 | val_rmse: 1.1915 | val_ll: -2.1946
[4:38:54.889769] epoch: 9100 | elbo: 33049.80429687501 | train_rmse: 1.1353 | val_rmse: 1.2007 | val_ll: -2.1951
[4:40:28.238598] epoch: 9150 | elbo: 33040.983457031245 | train_rmse: 1.1277 | val_rmse: 1.1902 | val_ll: -2.1918
[4:42:01.103111] epoch: 9200 | elbo: 33034.78308593751 | train_rmse: 1.1282 | val_rmse: 1.1877 | val_ll: -2.1942
[4:43:33.021951] epoch: 9250 | elbo: 33049.42119140625 | train_rmse: 1.125 | val_rmse: 1.1922 | val_ll: -2.1922
[4:45:06.315335] epoch: 9300 | elbo: 33169.8923828125 | train_rmse: 1.1211 | val_rmse: 1.1853 | val_ll: -2.1953
[4:46:37.698919] epoch: 9350 | elbo: 33124.9869921875 | train_rmse: 1.1264 | val_rmse: 1.1988 | val_ll: -2.1959
[4:48:09.157293] epoch: 9400 | elbo: 33001.3130859375 | train_rmse: 1.1166 | val_rmse: 1.1833 | val_ll: -2.1938
[4:49:41.351112] epoch: 9450 | elbo: 33023.076640625004 | train_rmse: 1.1207 | val_rmse: 1.1808 | val_ll: -2.1943
[4:51:13.214708] epoch: 9500 | elbo: 33063.02767578125 | train_rmse: 1.1179 | val_rmse: 1.1846 | val_ll: -2.1962
[4:52:46.002441] epoch: 9550 | elbo: 33053.57798828125 | train_rmse: 1.1268 | val_rmse: 1.1936 | val_ll: -2.1935
[4:54:17.146707] epoch: 9600 | elbo: 33057.3630859375 | train_rmse: 1.1178 | val_rmse: 1.1886 | val_ll: -2.1946
[4:55:47.470319] epoch: 9650 | elbo: 32938.2646484375 | train_rmse: 1.1161 | val_rmse: 1.1743 | val_ll: -2.1945
[4:57:18.177403] epoch: 9700 | elbo: 33384.120546875 | train_rmse: 1.116 | val_rmse: 1.1892 | val_ll: -2.1918
[4:58:50.592286] epoch: 9750 | elbo: 33075.36394531249 | train_rmse: 1.1196 | val_rmse: 1.1883 | val_ll: -2.1915
[5:00:23.734789] epoch: 9800 | elbo: 33032.7050390625 | train_rmse: 1.1222 | val_rmse: 1.1912 | val_ll: -2.1939
[5:01:59.299107] epoch: 9850 | elbo: 33044.597910156255 | train_rmse: 1.1115 | val_rmse: 1.1788 | val_ll: -2.1924
[5:03:32.362451] epoch: 9900 | elbo: 32961.56121093751 | train_rmse: 1.1168 | val_rmse: 1.181 | val_ll: -2.1929
[5:05:05.288973] epoch: 9950 | elbo: 33009.74083984375 | train_rmse: 1.1296 | val_rmse: 1.1929 | val_ll: -2.1927
Training finished in 5:06:36.578269 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-s3/checkpoint_2.pt
File Size is 4.0595598220825195 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:2 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 3.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0123,  0.0116,  0.0155,  ..., -0.0111, -0.0098, -0.0072],
        [-0.0137,  0.0084,  0.0101,  ..., -0.0075, -0.0072, -0.0069],
        [-0.0153,  0.0106,  0.0116,  ..., -0.0085, -0.0045, -0.0040],
        ...,
        [-0.0124,  0.0095,  0.0121,  ..., -0.0109, -0.0024, -0.0049],
        [-0.0137,  0.0078,  0.0090,  ..., -0.0078, -0.0078, -0.0096],
        [-0.0114,  0.0122,  0.0135,  ..., -0.0102, -0.0102, -0.0032]],
       device='cuda:2', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.4396, 0.4410, 0.4419,  ..., 0.4392, 0.4352, 0.4391],
        [0.4402, 0.4408, 0.4422,  ..., 0.4402, 0.4370, 0.4391],
        [0.4406, 0.4397, 0.4411,  ..., 0.4385, 0.4343, 0.4378],
        ...,
        [0.4391, 0.4394, 0.4398,  ..., 0.4397, 0.4363, 0.4381],
        [0.4401, 0.4402, 0.4409,  ..., 0.4393, 0.4363, 0.4382],
        [0.4410, 0.4425, 0.4428,  ..., 0.4404, 0.4368, 0.4396]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-2.3092, -2.3094, -2.3085, -2.3116, -2.3056, -2.3128, -2.3098, -2.3103,
        -2.3103, -2.3089, -2.3090, -2.3102, -2.3092, -2.3080, -2.3110, -2.3092,
        -2.3094, -2.3110, -2.3082, -2.3078, -2.3084, -2.3089, -2.3089, -2.3119,
        -2.3095, -2.3080, -2.3104, -2.3087, -2.3096, -2.3097, -2.3097, -2.3110,
        -2.3090, -2.3088, -2.3075, -2.3087, -2.3104, -2.3076, -2.3115, -2.3067,
        -2.3105, -2.3109, -2.3101, -2.3101, -2.3094, -2.3098, -2.3089, -2.3138,
        -2.3133, -2.3079, -2.3090,  9.6386, -2.3119, -2.3116, -2.3099, -1.7143,
        -2.3104, -2.3120, -2.3090, -2.8308, -2.3129, -2.3070, -2.9946, -2.3077,
        -2.3099, -2.3097, -2.3077, -2.3091, -1.5381, -2.3097, -2.3081, -2.3076,
        -2.3117, -2.3085, -2.3110, -2.3106, -2.3107, -2.3092, -2.3104, -2.3093,
        -2.3083, -2.3130, -3.2271,  3.8767, -2.3103, -2.3079, -2.3102, -2.3091,
        -2.3097, -2.3092, -2.3103, -2.3101, -2.3069, -2.3069, -2.3134, -2.3101,
        -2.3086, -2.3103, -2.3099, -2.3084, -2.8013, -2.3102, -2.3099, -2.3083,
        -2.3083, -2.3102, -2.3086, -2.3123, -2.3105, -2.3098, -2.3069, -2.3112,
        -2.3120, -2.3140, -2.3108, -2.3077, -3.5597, -1.8565, -2.3091, -1.7948,
        -2.3124, -2.3126, -2.3080, -2.3115, -2.3102, -2.3108, -2.3083, -2.3103,
        -1.8587,  8.3716, -2.3074, -2.3102, -2.3086, -2.3105, -2.3103, -2.3077,
        -2.3106, -2.3085, -2.3162, -2.3087, -2.3096, 10.6711, -2.3088, -2.3102,
        -2.3095, -2.3094, -2.3097, -2.3094, -2.3090, -2.3099, -2.3114, -2.3076,
        -2.3111, -2.3101, -2.1245, -2.3080, -2.3093, -2.3109, -2.3100, -2.3127,
        -2.3090, -2.3080, -2.3090, -2.3119, -1.3670, -2.3093, -2.3115, -2.3089,
        -2.3078, -2.3112, -2.3078, -2.3088, -2.3124, -2.3095, -2.3099,  8.2040,
        -2.3107, -2.3082, -2.3080, -2.9272, -2.3114, -2.3108, -2.3102, -2.3100,
        -2.3108, -2.3100, -2.3062, -2.3101, -2.3109, -2.3077, -2.3088, -2.3130,
        -2.3134, -2.6943, -2.3139, -2.3106, -2.3097, -2.3089, -2.3093, -2.3097,
        -2.3093, -2.3092, -2.3091, -3.1504, -2.3078, -2.7993, -2.3109, -2.3092,
        -2.3092, -2.3077,  7.3307, -2.3092, -2.3101, -2.3092, -2.3082, -2.3101,
        -2.3115, -2.3100, -2.3109, -2.3098, -2.3089, -2.3116, -2.3106, -2.3081,
         9.3576, -2.3088,  3.5136, -2.3088, -2.3082, -2.3121, -2.3109, -3.3543,
        -2.3101, -2.3127, -2.3115, -1.6878, -2.3093, -2.3087, -2.3070, -2.3117,
        -2.3085, -2.3102, -2.3109, -2.3098, -2.3073, -2.3075, -2.3108,  4.1822,
        -2.3102, -3.2523, -2.3085, -2.3090, -2.3100, -2.3072, -2.3097, -2.3111,
        -2.3112, -2.3098, -2.3113, -2.3114, -2.3099, -2.3108, -2.3110, -2.3076,
        -2.3110, -2.3117, -2.3136, -2.3085, -2.3146, -2.3080, -2.3098, -2.3105,
        -2.3119, -2.3089, -2.3121, -2.3088, -2.3087, -2.3105, -2.8643, -2.3098,
        -2.3082, -2.3092, -3.0028, -2.3132, -2.3083, -1.7647, -2.3138, -2.3066,
        -2.3088, -2.3066, -2.3113, -3.1440, -2.3092, -2.3074, -2.3106, -2.3073,
        -2.3099, -2.3104, -2.3091, -2.3085, -2.3091, -2.3101, -2.3098, -2.3091,
        -2.3095, -2.3091, -1.2172, -2.3078, -2.3099, -2.3075, -2.3079, -2.3117,
        -2.3111, -2.3090, -2.3097, -2.3116, -2.3087, -2.3235, -2.3108, -2.3108,
        -3.3162, -2.3112, -2.3110, -2.3096, -2.3110, -2.3108, -2.3114, -2.3103,
        -2.3097, -2.3121, -2.3114, -2.3077, -2.3090, -2.3094, -2.3114, -2.3109,
        -2.3091,  6.6344, -2.3090, -2.3093, -2.3083, -2.3093, -2.3101, -2.3094,
        -2.3092, -2.8188, -2.3103, -2.3092, -2.3115, -2.3079, -2.3109, -2.3102,
        -2.3097, -2.3134, -2.3100, -2.3097,  7.7095, -2.3120,  2.8198, -2.3079,
        -2.3079, -2.3116, -2.3095, -2.3092, -2.3092,  3.7589, -2.3117, -2.3074,
        -2.3122, -2.3088, -2.3083, -2.3098, -2.3111, -2.3105, -2.3098, -2.3090,
        -2.3083, -2.3080, -2.3136, -2.3093, -2.3061, -2.3104, -2.3110, -2.3108,
        -2.3102, -2.3088, -2.3107, -2.3084, -2.3098, -2.3102, -2.3070, -2.3103,
        -2.3093, -2.3138, -2.3104, -2.0831, -2.3113, -2.3082, -2.3118, -2.3093,
        -2.3114, -2.3108, -2.3070, -2.3147, -2.3097, -2.3098, -2.3075, -2.3100,
        -2.3173, -2.3103, -2.3104, -2.3103, -2.3087, -2.3100, -2.3079, -2.3098,
        -2.3126, -2.7788, -1.3100, -1.5448, -2.3061, -2.3098, -2.3114, -2.3104,
        -2.3106, -2.3097, -2.3086, -2.3103, -2.3105, -2.3108, -2.3075, -2.3112,
        -2.3077, -2.3079, -2.3093, -2.3085, -2.6508, -2.3111, -2.3090, -2.3093,
        -2.3102, -2.3118, -2.3080, -2.3095, -2.3061, -2.3078, -2.3126, -2.3102,
        -2.3126, -2.3046, -2.3117, -2.3073, -2.3098, -2.3117, -2.3108, -2.3094,
        -1.3050, -2.3091, -2.3138, -2.3097, -2.3083, -2.3059, -2.3103, -2.3101,
        -2.3073, -2.3107, -2.3117, -2.3090, -2.3108, -2.3094, -2.3094, -2.3080,
        -2.3096, -2.3116, -2.3099, -3.2807, -2.3125, -2.3074, -2.3071, -2.3112,
        -2.3104, -2.3073, -2.3075, -2.3112,  3.4357, -2.3093,  9.2110, -2.3129,
        -2.3095, -2.3089, -2.3101, -2.3117, -2.3103, -2.3098, -2.3087, -2.3072,
        -2.3109, -2.3066, -2.3099, -2.3084, -2.3108, -2.3124, -2.3095, -2.3091,
        -2.3072, -2.3104, -2.3127, -2.3107,  0.3675, -2.3111, -2.3080, -2.3073],
       device='cuda:2', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.5109, 0.5114, 0.5109, 0.5104, 0.5123, 0.5100, 0.5101, 0.5111, 0.5102,
        0.5115, 0.5112, 0.5107, 0.5110, 0.5109, 0.5102, 0.5116, 0.5110, 0.5107,
        0.5112, 0.5114, 0.5110, 0.5112, 0.5103, 0.5100, 0.5101, 0.5115, 0.5107,
        0.5119, 0.5114, 0.5112, 0.5112, 0.5104, 0.5119, 0.5113, 0.5118, 0.5115,
        0.5109, 0.5113, 0.5113, 0.5121, 0.5110, 0.5111, 0.5113, 0.5107, 0.5110,
        0.5107, 0.5111, 0.5092, 0.5095, 0.5116, 0.5107, 0.1861, 0.5110, 0.5100,
        0.5115, 0.0820, 0.5099, 0.5099, 0.5116, 0.0984, 0.5106, 0.5113, 0.1297,
        0.5111, 0.5108, 0.5107, 0.5122, 0.5110, 0.0835, 0.5109, 0.5121, 0.5119,
        0.5104, 0.5108, 0.5107, 0.5110, 0.5103, 0.5111, 0.5111, 0.5118, 0.5114,
        0.5096, 0.1218, 0.3251, 0.5109, 0.5117, 0.5113, 0.5108, 0.5108, 0.5110,
        0.5108, 0.5117, 0.5119, 0.5120, 0.5096, 0.5110, 0.5113, 0.5106, 0.5111,
        0.5114, 0.1137, 0.5106, 0.5114, 0.5115, 0.5111, 0.5108, 0.5109, 0.5098,
        0.5113, 0.5109, 0.5121, 0.5107, 0.5097, 0.5099, 0.5102, 0.5109, 0.1103,
        0.0753, 0.5105, 0.0775, 0.5098, 0.5101, 0.5116, 0.5106, 0.5106, 0.5109,
        0.5122, 0.5097, 0.0800, 0.1946, 0.5113, 0.5110, 0.5117, 0.5102, 0.5107,
        0.5116, 0.5109, 0.5109, 0.5108, 0.5104, 0.5111, 0.1612, 0.5110, 0.5116,
        0.5118, 0.5115, 0.5110, 0.5102, 0.5114, 0.5101, 0.5108, 0.5116, 0.5103,
        0.5113, 0.0777, 0.5110, 0.5112, 0.5102, 0.5111, 0.5104, 0.5109, 0.5117,
        0.5110, 0.5089, 0.1919, 0.5110, 0.5100, 0.5111, 0.5125, 0.5098, 0.5105,
        0.5108, 0.5100, 0.5111, 0.5108, 0.2266, 0.5111, 0.5110, 0.5108, 0.2219,
        0.5109, 0.5104, 0.5112, 0.5107, 0.5104, 0.5108, 0.5123, 0.5102, 0.5110,
        0.5117, 0.5115, 0.5096, 0.5097, 0.1793, 0.5094, 0.5109, 0.5107, 0.5114,
        0.5111, 0.5112, 0.5117, 0.5111, 0.5111, 0.2136, 0.5117, 0.1311, 0.5104,
        0.5111, 0.5108, 0.5120, 0.2576, 0.5104, 0.5107, 0.5109, 0.5112, 0.5115,
        0.5106, 0.5108, 0.5108, 0.5106, 0.5107, 0.5100, 0.5107, 0.5112, 0.1882,
        0.5109, 0.0602, 0.5115, 0.5112, 0.5105, 0.5102, 0.1151, 0.5112, 0.5105,
        0.5102, 0.0769, 0.5107, 0.5121, 0.5116, 0.5106, 0.5113, 0.5105, 0.5112,
        0.5103, 0.5115, 0.5117, 0.5103, 0.0622, 0.5106, 0.1381, 0.5108, 0.5113,
        0.5104, 0.5121, 0.5119, 0.5105, 0.5100, 0.5110, 0.5104, 0.5094, 0.5109,
        0.5106, 0.5106, 0.5120, 0.5109, 0.5115, 0.5095, 0.5111, 0.5092, 0.5119,
        0.5114, 0.5108, 0.5107, 0.5111, 0.5102, 0.5115, 0.5117, 0.5111, 0.2532,
        0.5104, 0.5115, 0.5115, 0.1148, 0.5102, 0.5119, 0.3238, 0.5098, 0.5113,
        0.5112, 0.5120, 0.5105, 0.1093, 0.5114, 0.5115, 0.5100, 0.5119, 0.5116,
        0.5109, 0.5116, 0.5109, 0.5117, 0.5117, 0.5117, 0.5111, 0.5102, 0.5108,
        0.0843, 0.5124, 0.5111, 0.5113, 0.5115, 0.5103, 0.5106, 0.5117, 0.5109,
        0.5109, 0.5111, 0.5116, 0.5107, 0.5100, 0.1092, 0.5104, 0.5103, 0.5121,
        0.5104, 0.5104, 0.5102, 0.5106, 0.5109, 0.5103, 0.5103, 0.5115, 0.5115,
        0.5112, 0.5103, 0.5102, 0.5110, 0.2729, 0.5111, 0.5112, 0.5116, 0.5118,
        0.5107, 0.5112, 0.5113, 0.2799, 0.5109, 0.5107, 0.5103, 0.5110, 0.5109,
        0.5115, 0.5111, 0.5092, 0.5106, 0.5110, 0.2464, 0.5102, 0.3816, 0.5114,
        0.5110, 0.5097, 0.5111, 0.5108, 0.5113, 0.0581, 0.5108, 0.5115, 0.5100,
        0.5118, 0.5112, 0.5102, 0.5111, 0.5097, 0.5099, 0.5115, 0.5116, 0.5113,
        0.5096, 0.5115, 0.5121, 0.5118, 0.5112, 0.5105, 0.5107, 0.5114, 0.5106,
        0.5105, 0.5116, 0.5117, 0.5117, 0.5101, 0.5110, 0.5100, 0.5101, 0.0723,
        0.5107, 0.5116, 0.5105, 0.5111, 0.5104, 0.5105, 0.5118, 0.5085, 0.5111,
        0.5109, 0.5111, 0.5112, 0.5085, 0.5109, 0.5114, 0.5111, 0.5116, 0.5103,
        0.5119, 0.5109, 0.5105, 0.2194, 0.0863, 0.0807, 0.5112, 0.5106, 0.5104,
        0.5115, 0.5110, 0.5114, 0.5113, 0.5107, 0.5095, 0.5103, 0.5115, 0.5104,
        0.5111, 0.5112, 0.5117, 0.5115, 0.1641, 0.5101, 0.5103, 0.5111, 0.5107,
        0.5104, 0.5120, 0.5111, 0.5120, 0.5121, 0.5094, 0.5114, 0.5088, 0.5121,
        0.5101, 0.5115, 0.5111, 0.5099, 0.5100, 0.5110, 0.0844, 0.5110, 0.5089,
        0.5112, 0.5116, 0.5120, 0.5107, 0.5108, 0.5117, 0.5116, 0.5099, 0.5118,
        0.5110, 0.5111, 0.5113, 0.5114, 0.5117, 0.5108, 0.5113, 0.1344, 0.5110,
        0.5111, 0.5118, 0.5105, 0.5108, 0.5117, 0.5123, 0.5101, 0.0609, 0.5114,
        0.1959, 0.5102, 0.5112, 0.5105, 0.5104, 0.5108, 0.5108, 0.5111, 0.5114,
        0.5118, 0.5107, 0.5115, 0.5104, 0.5114, 0.5110, 0.5100, 0.5107, 0.5109,
        0.5119, 0.5111, 0.5097, 0.5109, 0.2638, 0.5102, 0.5111, 0.5122],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.0074, -0.0052, -0.0079,  ..., -0.0066, -0.0072, -0.0072],
        [ 0.0051,  0.0071, -0.0136,  ...,  0.0049,  0.0060,  0.0067],
        [-0.0073, -0.0061, -0.0069,  ..., -0.0069, -0.0059, -0.0072],
        ...,
        [-0.0070, -0.0062, -0.0063,  ..., -0.0060, -0.0058, -0.0072],
        [-0.0075, -0.0064, -0.0055,  ..., -0.0064, -0.0063, -0.0064],
        [ 0.0676,  0.0631,  0.0506,  ...,  0.0703,  0.0676,  0.0650]],
       device='cuda:2', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.9996, 0.9997, 0.9995,  ..., 0.9995, 0.9997, 0.9991],
        [0.8442, 0.8416, 0.8454,  ..., 0.8440, 0.8400, 0.8458],
        [0.9995, 0.9995, 1.0000,  ..., 0.9994, 0.9997, 0.9997],
        ...,
        [0.9993, 0.9996, 0.9996,  ..., 0.9997, 0.9998, 0.9990],
        [0.9998, 0.9998, 0.9995,  ..., 0.9998, 0.9995, 0.9995],
        [0.8730, 0.8730, 0.8776,  ..., 0.8748, 0.8738, 0.8763]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.0747,  0.0024, -0.0778, -0.0776, -0.0752, -0.0012, -0.0793, -0.0794,
        -0.0764, -0.0763, -0.0768, -0.1120, -0.0776, -0.0777, -0.0778, -0.0778,
        -0.0760, -0.0791, -0.0770, -0.0797, -0.0794, -0.0787, -0.0754, -0.0773,
        -0.0785, -0.0754, -0.0786, -0.0756, -0.0750, -0.0782, -0.0770, -0.0757,
        -0.0758, -0.0765, -0.0756, -0.0774, -0.0760, -0.0774, -0.1822, -0.0767,
        -0.0760, -0.0762, -0.0759, -0.0755, -0.0750, -0.0774, -0.0786, -0.0763,
        -0.0785, -0.0761, -0.0772, -0.0776, -0.0774, -0.0766, -0.0794, -0.1466,
        -0.0761, -0.0769, -0.0780, -0.0771, -0.0797, -0.0783, -0.0747, -0.0758,
        -0.0762, -0.0763, -0.0786, -0.0779, -0.0773, -0.0767, -0.0767, -0.0785,
        -0.0767, -0.0778, -0.0776, -0.0766, -0.0773, -0.0797, -0.0797, -0.2193,
        -0.0766, -0.0750, -0.0774, -0.0781, -0.0790, -0.0800, -0.0774, -0.0759,
        -0.0768, -0.0779, -0.0580, -0.0755, -0.0772, -0.0768, -0.0759, -0.0753,
        -0.0761, -0.0786,  0.1517, -0.0779, -0.0767, -0.0758, -0.0776, -0.0797,
        -0.0770, -0.0781, -0.0760, -0.0775, -0.0774, -0.0764, -0.0774, -0.0772,
        -0.0781, -0.0769, -0.0758, -0.0789, -0.0771, -0.0757, -0.0784, -0.0801,
        -0.0790, -0.0765, -0.0752, -0.0758, -0.0759, -0.0760, -0.0780, -0.0767,
        -0.0761, -0.0757, -0.0806, -0.0793, -0.0772, -0.0780, -0.0794, -0.0764,
        -0.0751, -0.0791, -0.0781, -0.0773, -0.0756, -0.0773, -0.0771, -0.0765,
        -0.0760, -0.0758, -0.0777, -0.0779,  0.0402, -0.0805, -0.0777, -0.0789,
        -0.0795, -0.0754, -0.0794, -0.0783, -0.0754, -0.0794, -0.0757, -0.0773,
        -0.0768, -0.0765, -0.0752, -0.0770, -0.0777, -0.0796, -0.0745, -0.0771,
        -0.0783, -0.0758, -0.0760, -0.0766, -0.0760, -0.0802, -0.0765, -0.0789,
        -0.0792, -0.0759, -0.0781, -0.0759, -0.0793, -0.0769, -0.0770, -0.0775,
        -0.0765, -0.0779, -0.0759, -0.0787, -0.0749, -0.0755, -0.0752, -0.0779,
        -0.0779, -0.0748, -0.0762, -0.0748, -0.0763, -0.0768, -0.0751, -0.0763,
        -0.0766, -0.0758, -0.0804, -0.0772, -0.0772, -0.0780, -0.0760, -0.0773,
        -0.0773, -0.0803, -0.0786, -0.0762, -0.0766, -0.0760, -0.0757, -0.0761,
        -0.0786, -0.0765, -0.0767, -0.0760, -0.0776, -0.0761, -0.0796, -0.2310,
        -0.0774, -0.0791, -0.0759, -0.0778, -0.0765, -0.0803, -0.0762, -0.0808,
        -0.0759, -0.0778, -0.0811, -0.0781, -0.0041, -0.0763, -0.0781, -0.0780,
        -0.0748, -0.0759, -0.0777, -0.0768, -0.0765, -0.0774, -0.0752, -0.0767,
        -0.0765, -0.0779, -0.0779, -0.0760, -0.0753, -0.0780, -0.0791, -0.0787,
        -0.0773, -0.0767, -0.0778, -0.0773, -0.0742, -0.0787, -0.0778, -0.0778,
        -0.0767, -0.0762, -0.0758, -0.0770, -0.0760, -0.0759, -0.0761, -0.0766,
        -0.0771, -0.0765, -0.0752, -0.0761, -0.0754, -0.0762, -0.0767, -0.0770,
        -0.0760, -0.0770, -0.0756, -0.0773, -0.0788, -0.0765, -0.0769, -0.0771,
        -0.0774, -0.0811, -0.0757, -0.0768, -0.0776, -0.0773, -0.0764, -0.0772,
        -0.0780, -0.0796, -0.0772, -0.0777, -0.0781, -0.0776, -0.0768, -0.0762,
        -0.0760, -0.0766, -0.0777, -0.0756, -0.0762, -0.0771, -0.0745, -0.0760,
        -0.0781, -0.0767, -0.0775, -0.0757, -0.0763, -0.0784, -0.0761, -0.0755,
        -0.0783, -0.0755, -0.0789, -0.0773, -0.0778, -0.0760, -0.0766, -0.0774,
        -0.0764, -0.0769, -0.0764, -0.0762, -0.0792, -0.0757, -0.0764, -0.0758,
        -0.0804, -0.0760, -0.0767, -0.0762, -0.0763, -0.0775, -0.0777, -0.0784,
        -0.0797, -0.0757, -0.0759, -0.0800, -0.0773, -0.0762, -0.0785, -0.0762,
        -0.0754, -0.0780, -0.0776, -0.0753, -0.0753, -0.0763, -0.1194, -0.0759,
        -0.0772, -0.0758, -0.0772, -0.0797, -0.0790, -0.0758, -0.0768, -0.0759,
        -0.0784, -0.0753, -0.0753, -0.0750, -0.0775, -0.0783, -0.0757, -0.0782,
        -0.0760, -0.0765,  1.1482, -0.0789, -0.0771, -0.0774, -0.0768, -0.0763,
        -0.0801, -0.0768, -0.0798, -0.0752, -0.0762, -0.0760, -0.0771, -0.0778,
        -0.0758, -0.0775, -0.0764,  0.1547, -0.0773, -0.0753, -0.0807, -0.0809,
        -0.0743, -0.0780, -0.0777, -0.0756, -0.1655, -0.0836, -0.0749, -0.0775,
        -0.0774, -0.0754, -0.0774, -0.0770, -0.0776, -0.0792, -0.0760, -0.0776,
        -0.0760, -0.0780, -0.0767, -0.0796, -0.0779, -0.0768, -0.0769, -0.0754,
        -0.0766, -0.0762, -0.0784, -0.0764, -0.0806, -0.0770, -0.0763, -0.0756,
        -0.0756, -0.0759, -0.0768, -0.0805, -0.0759, -0.0752, -0.0771, -0.0788,
        -0.0769, -0.0772, -0.0793, -0.0745, -0.0823, -0.0766, -0.0773, -0.0770,
        -0.0751, -0.0781, -0.0770, -0.0758, -0.0798, -0.0774, -0.0757, -0.0754,
        -0.0762, -0.0768, -0.0756, -0.0752, -0.0760, -0.0791, -0.0755, -0.0788,
        -0.0771, -0.0762, -0.0775, -0.0787, -0.0811, -0.0763, -0.0777, -0.0776,
        -0.0758, -0.0802, -0.0777, -0.0764, -0.0780, -0.0757, -0.0756, -0.0757,
        -0.0763, -0.0777, -0.0781, -0.0768, -0.0784, -0.0766, -0.0782, -0.0748,
        -0.0781, -0.0765, -0.0784, -0.0788, -0.0800, -0.0783, -0.0763, -0.0756,
        -0.0775, -0.0751, -0.0745, -0.0787, -0.0777, -0.0766, -0.0789, -0.0767,
        -0.0772, -0.0759, -0.0759, -0.0797, -0.0758, -0.0747, -0.0767, -0.2356],
       device='cuda:2', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.9961, 0.4138, 0.9966, 0.9961, 0.9967, 0.4331, 0.9961, 0.9964, 0.9966,
        0.9961, 0.9966, 0.7992, 0.9958, 0.9962, 0.9965, 0.9961, 0.9965, 0.9965,
        0.9966, 0.9953, 0.9957, 0.9959, 0.9964, 0.9966, 0.9965, 0.9965, 0.9936,
        0.9963, 0.9965, 0.9964, 0.9964, 0.9966, 0.9966, 0.9964, 0.9967, 0.9961,
        0.9967, 0.9961, 0.6597, 0.9964, 0.9962, 0.9957, 0.9965, 0.9964, 0.9963,
        0.9956, 0.9958, 0.9964, 0.9963, 0.9956, 0.9959, 0.9965, 0.9961, 0.9965,
        0.9957, 0.7421, 0.9963, 0.9965, 0.9964, 0.9963, 0.9966, 0.9941, 0.9964,
        0.9964, 0.9959, 0.9965, 0.9965, 0.9964, 0.9956, 0.9965, 0.9966, 0.9963,
        0.9962, 0.9964, 0.9965, 0.9966, 0.9965, 0.9964, 0.9956, 0.5499, 0.9966,
        0.9964, 0.9964, 0.9963, 0.9950, 0.9959, 0.9966, 0.9965, 0.9961, 0.9963,
        0.7214, 0.9959, 0.9966, 0.9964, 0.9965, 0.9965, 0.9963, 0.9963, 0.4037,
        0.9966, 0.9964, 0.9963, 0.9965, 0.9965, 0.9963, 0.9963, 0.9965, 0.9965,
        0.9963, 0.9962, 0.9965, 0.9965, 0.9961, 0.9963, 0.9965, 0.9962, 0.9963,
        0.9964, 0.9964, 0.9964, 0.9963, 0.9963, 0.9960, 0.9964, 0.9964, 0.9962,
        0.9963, 0.9964, 0.9964, 0.9962, 0.9960, 0.9960, 0.9962, 0.9965, 0.9960,
        0.9966, 0.9963, 0.9966, 0.9963, 0.9962, 0.9960, 0.9963, 0.9964, 0.9964,
        0.9966, 0.9960, 0.9963, 0.9965, 0.3089, 0.9956, 0.9962, 0.9963, 0.9963,
        0.9965, 0.9955, 0.9963, 0.9964, 0.9955, 0.9962, 0.9966, 0.9964, 0.9958,
        0.9962, 0.9964, 0.9965, 0.9956, 0.9962, 0.9967, 0.9955, 0.9968, 0.9965,
        0.9964, 0.9964, 0.9951, 0.9966, 0.9953, 0.9955, 0.9964, 0.9967, 0.9964,
        0.9963, 0.9965, 0.9958, 0.9964, 0.9965, 0.9966, 0.9965, 0.9962, 0.9963,
        0.9962, 0.9963, 0.9967, 0.9965, 0.9965, 0.9960, 0.9965, 0.9964, 0.9965,
        0.9963, 0.9962, 0.9964, 0.9961, 0.9963, 0.9965, 0.9959, 0.9964, 0.9963,
        0.9966, 0.9963, 0.9956, 0.9953, 0.9963, 0.9965, 0.9964, 0.9966, 0.9966,
        0.9964, 0.9961, 0.9964, 0.9961, 0.9958, 0.9960, 0.9958, 0.5897, 0.9959,
        0.9949, 0.9962, 0.9959, 0.9962, 0.9960, 0.9961, 0.9927, 0.9963, 0.9965,
        0.9939, 0.9965, 0.5754, 0.9963, 0.9964, 0.9957, 0.9964, 0.9965, 0.9965,
        0.9963, 0.9961, 0.9964, 0.9965, 0.9961, 0.9964, 0.9962, 0.9963, 0.9963,
        0.9964, 0.9963, 0.9963, 0.9961, 0.9965, 0.9964, 0.9965, 0.9959, 0.9965,
        0.9964, 0.9965, 0.9959, 0.9964, 0.9965, 0.9962, 0.9960, 0.9965, 0.9966,
        0.9965, 0.9965, 0.9967, 0.9965, 0.9965, 0.9964, 0.9962, 0.9965, 0.9960,
        0.9961, 0.9965, 0.9962, 0.9966, 0.9963, 0.9960, 0.9964, 0.9961, 0.9966,
        0.9963, 0.9949, 0.9965, 0.9965, 0.9966, 0.9961, 0.9964, 0.9962, 0.9963,
        0.9966, 0.9963, 0.9960, 0.9962, 0.9967, 0.9966, 0.9962, 0.9965, 0.9965,
        0.9962, 0.9962, 0.9966, 0.9961, 0.9963, 0.9963, 0.9960, 0.9963, 0.9960,
        0.9964, 0.9964, 0.9964, 0.9961, 0.9965, 0.9967, 0.9960, 0.9967, 0.9966,
        0.9956, 0.9961, 0.9966, 0.9954, 0.9963, 0.9963, 0.9966, 0.9962, 0.9964,
        0.9966, 0.9964, 0.9962, 0.9962, 0.9965, 0.9963, 0.9962, 0.9964, 0.9963,
        0.9960, 0.9946, 0.9960, 0.9964, 0.9959, 0.9961, 0.9960, 0.9964, 0.9960,
        0.9965, 0.9964, 0.9965, 0.9956, 0.9963, 0.9963, 0.9962, 0.9656, 0.9967,
        0.9957, 0.9961, 0.9966, 0.9955, 0.9959, 0.9964, 0.9960, 0.9958, 0.9965,
        0.9957, 0.9964, 0.9962, 0.9963, 0.9966, 0.9965, 0.9961, 0.9966, 0.9963,
        0.9191, 0.9957, 0.9964, 0.9965, 0.9957, 0.9963, 0.9966, 0.9967, 0.9959,
        0.9966, 0.9963, 0.9964, 0.9964, 0.9965, 0.9962, 0.9965, 0.9964, 0.4293,
        0.9962, 0.9963, 0.9965, 0.9955, 0.9965, 0.9963, 0.9965, 0.9964, 0.7467,
        0.9927, 0.9966, 0.9965, 0.9963, 0.9963, 0.9965, 0.9964, 0.9965, 0.9959,
        0.9961, 0.9966, 0.9959, 0.9963, 0.9960, 0.9949, 0.9962, 0.9959, 0.9965,
        0.9966, 0.9963, 0.9963, 0.9965, 0.9963, 0.9945, 0.9966, 0.9959, 0.9964,
        0.9965, 0.9960, 0.9962, 0.9943, 0.9964, 0.9962, 0.9961, 0.9964, 0.9966,
        0.9965, 0.9963, 0.9961, 0.9959, 0.9966, 0.9954, 0.9963, 0.9963, 0.9961,
        0.9961, 0.9962, 0.9963, 0.9959, 0.9965, 0.9962, 0.9965, 0.9965, 0.9966,
        0.9963, 0.9961, 0.9959, 0.9965, 0.9959, 0.9963, 0.9963, 0.9961, 0.9966,
        0.9957, 0.9961, 0.9963, 0.9962, 0.9963, 0.9965, 0.9963, 0.9965, 0.9965,
        0.9966, 0.9964, 0.9965, 0.9963, 0.9961, 0.9965, 0.9954, 0.9967, 0.9962,
        0.9964, 0.9960, 0.9961, 0.9965, 0.9962, 0.9957, 0.9961, 0.9963, 0.9963,
        0.9962, 0.9964, 0.9964, 0.9962, 0.9957, 0.9963, 0.9966, 0.9965, 0.9965,
        0.9959, 0.9964, 0.9964, 0.9963, 0.9966, 0.9961, 0.9962, 0.5376],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[ 5.6052e-45, -8.9508e-02, -9.8091e-45,  ..., -2.1900e-04,
         -4.6243e-44, -3.9446e-02],
        [-4.2039e-45, -8.6849e-02, -1.1689e-08,  ..., -3.8168e-06,
         -2.5043e-03, -3.7375e-02],
        [-5.1992e-07, -8.8527e-02,  4.5853e-07,  ..., -3.7541e-07,
         -6.4497e-05, -3.7550e-02],
        ...,
        [-2.5947e-07, -7.9709e-02,  3.3631e-44,  ...,  8.4078e-45,
         -1.1210e-44, -1.8674e-02],
        [ 2.4102e-43, -7.5146e-02, -1.7747e-03,  ..., -8.4078e-45,
          3.9398e-31, -2.9374e-02],
        [ 3.0023e-07, -8.4071e-02, -3.1203e-03,  ...,  1.1491e-43,
         -6.9381e-05, -4.3689e-02]], device='cuda:2', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[1.0000, 0.9349, 1.0000,  ..., 1.0000, 1.0000, 0.9764],
        [1.0000, 0.9381, 1.0000,  ..., 1.0000, 0.9994, 0.9763],
        [1.0000, 0.9378, 1.0000,  ..., 1.0000, 1.0000, 0.9805],
        ...,
        [1.0000, 0.9462, 1.0000,  ..., 1.0000, 1.0000, 0.9907],
        [1.0000, 0.9502, 0.9999,  ..., 1.0000, 1.0000, 0.9865],
        [1.0000, 0.9391, 0.9970,  ..., 1.0000, 1.0000, 0.9753]],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-2.1183e-03, -6.5087e-03, -3.8938e-03, -1.0384e-03, -5.3886e-03,
        -2.6105e-03, -2.2460e-03, -4.0792e-03, -1.7718e-03, -2.7372e-03,
        -4.6861e-03, -7.4674e-04, -1.5182e-03, -1.2872e-03, -7.3695e-04,
        -1.8054e-03, -4.4128e-03, -3.0576e-03, -2.9894e-02, -2.2244e-04,
        -4.7333e-03, -3.5561e-03, -3.1575e-03, -4.7982e-02, -5.0420e-03,
        -2.7620e-03, -3.7958e-03, -2.6646e-03, -2.1271e-03, -5.4900e-04,
        -3.7968e-03, -2.1613e-03, -1.5359e-03, -5.1968e-03, -2.6434e-03,
        -1.8988e-03, -5.3410e-03, -2.2698e-03, -3.2519e-03, -3.2552e-03,
        -3.7000e-03, -2.6204e-03, -2.3834e-03, -3.2067e-04, -6.6243e-03,
        -3.4606e-03, -3.9774e-03, -7.1564e-04, -4.5625e-03, -1.5798e-03,
        -2.2959e-03, -2.0549e-03, -8.5222e-04, -1.5412e-03, -3.6603e-03,
        -6.5098e-03, -2.8972e-03, -2.5314e-03, -1.7209e-03,  9.7970e-06,
        -1.3093e-03, -1.5230e-03, -7.8059e-04, -6.5380e-04, -2.1718e-03,
        -8.7716e-04, -3.5986e-03, -8.6674e-04, -4.3737e-03, -1.6838e-03,
        -5.5631e-04, -2.5260e-03, -2.1161e-03, -2.8295e-03, -3.5085e-03,
        -8.4348e-05, -1.8769e-03, -1.6466e-03, -4.2694e-03, -5.8238e-03,
        -6.6857e-04, -2.7872e-03, -1.9627e-03, -4.9024e-03, -1.9789e-03,
        -8.2749e-04, -1.4696e-03, -1.0484e-04, -2.3424e-04, -2.5022e-03,
        -4.0823e-03, -1.9601e-03, -2.4634e-03, -1.8159e-03, -1.4557e-03,
        -2.5879e-03, -1.8050e-03, -7.3712e-03, -2.9910e-03, -3.8840e-03,
        -4.6876e-03, -2.6791e-03, -1.8154e-04, -4.9712e-03, -8.4238e-04,
        -2.6217e-03, -4.6569e-03, -2.3951e-03, -3.0238e-03, -2.4404e-03,
        -8.3640e-03, -2.8233e-02, -1.0271e-03, -4.4867e-03, -9.7803e-04,
        -1.2589e-03, -8.9591e-04, -4.9210e-04, -4.2873e-03, -3.0827e-03,
        -3.6753e-04, -8.2930e-04, -2.9467e-04, -1.4094e-03, -1.9150e-03,
        -4.7703e-03, -1.9514e-04, -1.1268e-03, -5.0850e-04, -6.1564e-04,
        -2.9018e-03, -4.1912e-03, -1.1107e-03, -3.4855e-03, -8.8987e-04,
        -3.1453e-03, -1.0510e-03, -6.9126e-04, -1.9879e-03, -1.2902e-03,
        -6.4711e-04, -6.6409e-03, -1.3773e-03, -1.9427e-03, -8.2429e-04,
        -7.5576e-04, -5.8861e-03, -6.1470e-04, -2.0726e-03, -9.8246e-04,
        -9.7435e-04, -3.2315e-03, -3.6917e-03, -2.2240e-04, -1.1000e-03,
        -5.3862e-04, -6.9001e-04, -5.2783e-03, -2.2186e-03, -9.5672e-04,
        -3.3683e-03, -2.8771e-04,  3.8284e-02, -3.6744e-03, -6.2139e-04,
        -2.1093e-04, -5.1558e-03, -3.7599e-03, -1.7351e-03, -3.5683e-04,
        -2.6081e-03, -1.9986e-03, -4.9116e-03, -2.8487e-03, -4.6253e-03,
        -4.2107e-03, -1.4025e-03, -3.9322e-03, -2.1439e-03, -1.8258e-03,
        -7.6975e-04, -4.3929e-03, -3.2800e-03, -4.2698e-04, -2.6203e-04,
        -3.6343e-03, -3.3836e-03, -3.7483e-03, -1.8301e-03, -3.5266e-03,
        -2.1869e-03, -7.7011e-04, -3.2556e-02, -5.0559e-03, -5.6715e-04,
        -5.7213e-04, -1.0906e-03, -4.7853e-04, -5.8338e-03, -3.0776e-03,
        -2.5408e-04, -1.4643e-03, -1.1820e-03, -3.0624e-03, -1.7471e-03,
        -5.4562e-03, -4.1414e-03, -3.9103e-03, -3.1703e-03, -6.0531e-03,
        -2.0959e-03, -1.2637e-03, -6.9248e-04, -1.4161e-03, -1.9295e-03,
        -2.7860e-03, -4.0353e-03,  4.4940e-02, -2.4875e-03, -6.5279e-03,
        -3.0522e-04, -6.9234e-03,  6.3141e-03, -4.2779e-04, -2.7393e-03,
        -2.7065e-03, -3.3233e-03, -4.9456e-04, -9.8045e-04, -7.1119e-03,
        -1.7565e-03, -2.0134e-03, -4.1851e-03, -1.2155e-04, -3.3164e-03,
        -1.0115e-03, -1.7076e-03, -6.4000e-04, -1.9881e-03, -1.9915e-04,
        -5.8881e-03, -1.1778e-03, -3.1448e-03, -1.3225e-03, -2.7376e-04,
        -1.8707e-03, -9.6604e-04, -2.4228e-03, -1.6411e-03, -1.2130e-03,
        -2.8866e-03, -5.1728e-05, -2.0905e-03, -5.5399e-04, -2.2586e-05,
        -1.3260e-03, -3.7352e-03, -2.2321e-03, -2.7390e-03, -5.6386e-05,
        -1.7798e-03, -2.6857e-03, -5.3327e-04, -3.3606e-03, -2.3827e-03,
        -2.7167e-03, -3.4303e-03, -3.7477e-03, -6.6747e-04, -2.7087e-03,
        -5.0804e-03, -9.0128e-04, -4.8396e-03, -1.6539e-03, -1.8594e-03,
        -1.8782e-03, -2.2481e-03, -4.2992e-03, -2.9040e-03, -1.9092e-03,
        -2.8819e-03, -2.5253e-03, -4.4291e-04, -1.2947e-03, -3.4153e-03,
        -1.8052e-03, -1.8330e-03, -4.8284e-03, -3.8413e-03, -2.1559e-03,
        -3.7877e-03, -3.2816e-03, -3.7330e-03, -1.0054e-03, -1.1364e-03,
        -3.1114e-02, -2.2621e-04, -2.9180e-03, -2.5019e-03, -3.2658e-04,
        -4.0619e-03, -4.1350e-04, -2.2182e-03, -1.4746e-03, -1.9305e-03,
        -4.4950e-03, -9.1882e-04, -2.5132e-03, -4.2354e-03, -6.4182e-03,
        -4.3833e-05, -8.1480e-04, -4.2343e-03, -3.3920e-04, -3.3087e-03,
        -5.0194e-03, -3.9193e-03, -2.6267e-03, -1.9303e-03, -1.4072e-03,
        -1.3438e-04, -3.9193e-03, -4.3780e-04, -1.9140e-03, -2.5075e-03,
        -3.6755e-03, -7.9314e-03, -2.8335e-05, -3.6191e-05, -1.4996e-03,
        -9.6533e-04, -3.6295e-03, -5.7002e-03, -3.4822e-03, -4.3101e-03,
        -3.2822e-03, -7.2089e-04, -4.9753e-03, -5.9981e-04, -4.4198e-03,
        -3.2102e-03, -4.7514e-03, -4.1631e-04, -4.6522e-03, -2.9181e-03,
        -4.4170e-05, -6.1080e-03, -4.9374e-03, -1.0300e-04, -1.5264e-03,
        -2.7509e-04, -4.6754e-03, -3.2154e-03, -3.3718e-03, -2.2799e-03,
        -2.9597e-03, -4.3514e-03, -3.3198e-03, -1.8032e-03, -3.0107e-03,
        -4.6961e-03, -1.9109e-03, -3.4458e-03, -2.0371e-03, -3.2332e-03,
        -3.6770e-03, -1.6936e-03, -2.8510e-03, -2.8162e-03, -2.0269e-03,
        -1.6548e-03, -7.1309e-04, -4.7646e-03, -3.5197e-03, -2.9010e-03,
        -2.5164e-03, -2.3067e-03, -1.4952e-03, -1.5790e-03, -8.9852e-04,
        -2.0683e-03, -2.8870e-03, -1.9033e-03, -2.5411e-03, -3.9043e-03,
        -2.1555e-03, -6.5868e-04, -1.8413e-03, -3.4116e-03, -3.6521e-04,
        -4.0694e-03, -3.6505e-03, -1.8006e-03, -1.2200e-03, -6.4515e-03,
        -1.3491e-03, -3.2115e-03, -2.3048e-03, -2.5943e-03, -2.3524e-03,
        -1.4665e-03, -1.8106e-05, -7.7678e-04, -3.9651e-03, -3.3303e-05,
        -1.2528e-03, -2.3012e-03, -3.9590e-03, -1.5627e-03, -1.0201e-03,
        -1.0494e-03, -4.3903e-04, -4.0573e-03, -1.9029e-03, -1.6616e-03,
        -6.4854e-03, -3.4334e-03, -3.6092e-03, -4.1754e-03, -6.0087e-04,
        -1.4296e-03, -2.5419e-04, -4.5199e-04, -1.7446e-03, -3.3398e-03,
        -1.0001e-04, -5.3768e-04, -3.4970e-03, -5.1276e-04, -1.2788e-03,
        -4.4368e-03, -4.2371e-03, -3.5500e-03, -1.2704e-03, -3.5336e-03,
        -4.4919e-03, -2.8877e-03, -3.4674e-03, -8.5085e-05, -2.1562e-03,
        -3.3951e-03, -1.6831e-03, -8.0366e-04, -1.3248e-03, -3.3809e-03,
        -4.7073e-03, -1.1206e-03, -5.1334e-03, -2.2447e-03, -1.4299e-03,
        -3.6651e-04, -4.8812e-03, -4.3719e-04, -2.8882e-03, -1.4497e-03,
        -2.0491e-03, -3.4230e-03, -2.1330e-03, -5.0699e-04, -1.7733e-03,
        -3.6827e-03, -1.4582e-03, -1.6560e-03, -1.7908e-04, -1.5162e-03,
        -3.0274e-03, -3.9313e-03, -2.3846e-03, -7.7162e-03, -7.6560e-03,
        -1.2027e-03, -2.8509e-04, -4.5067e-03, -2.5235e-03, -7.9567e-04,
        -1.9423e-03, -3.9261e-03, -1.9253e-03, -4.3095e-03, -3.5382e-03,
        -1.2680e-03, -5.3329e-04, -1.5442e-03, -2.5933e-03, -5.5405e-04,
        -3.6164e-03, -5.1532e-03, -1.8175e-03, -2.7510e-03, -3.6788e-03,
        -4.7103e-03, -7.7236e-04, -2.9448e-03, -3.2605e-03, -3.4237e-03,
        -4.3478e-04, -1.6000e-03, -4.3573e-03, -6.1155e-03, -2.1381e-03,
        -4.5117e-03, -1.3495e-03, -4.9526e-04, -3.1201e-03, -3.4146e-03,
        -3.2937e-03, -1.1297e-03, -5.4943e-03, -1.8539e-03, -5.5764e-03,
        -9.9755e-04, -2.3279e-03], device='cuda:2', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([1.0001, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,
        0.9857, 1.0000, 1.0000, 1.0000, 1.0000, 0.9711, 1.0000, 1.0000, 1.0000,
        1.0001, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 0.9997, 1.0000,
        0.9999, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 0.9990, 1.0000, 1.0000,
        0.9993, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9994, 0.9996, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9999, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0001,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9716, 1.0000, 1.0000, 0.9997, 1.0000, 1.0000,
        1.0000, 1.0003, 0.9995, 1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 1.0000, 0.9988, 1.0000,
        0.9329, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9833, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 0.9999, 1.0000, 1.0000,
        1.0002, 1.0000, 1.0000, 1.0002, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9448, 0.9996, 1.0000, 1.0000, 0.9998, 0.9899, 1.0000, 1.0000,
        0.9999, 0.9997, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9997, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0001, 1.0000, 1.0000,
        1.0000, 0.9999, 1.0000, 1.0001, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9739, 1.0000,
        1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0001, 1.0000, 1.0003, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 1.0002,
        0.9994, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999,
        1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000,
        0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997,
        1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0004, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9999, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9999, 0.9983, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9998, 0.9998, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000, 1.0000, 0.9216,
        0.9999, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 0.9999, 1.0002, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 1.0001, 1.0000, 1.0000,
        1.0000, 0.9990, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 0.9990,
        1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[-9.7358e-03,  3.3044e-02, -2.5485e-02,  1.6577e-02, -5.9445e-03,
          3.0769e-03,  5.4410e-03, -1.0649e-02,  8.6212e-03, -5.7997e-04,
          9.7200e-03, -4.3700e-03,  3.2955e-03, -5.0805e-03, -8.2320e-03,
         -1.3274e-02,  3.9572e-03,  7.8993e-03, -1.6257e-02,  1.1761e-02,
          7.3995e-03,  6.3008e-03, -5.7816e-03, -2.1917e-02,  1.0121e-02,
         -1.3106e-02,  1.8632e-02, -7.2784e-03, -3.5547e-03,  1.0527e-03,
          4.8401e-03,  3.3707e-02, -2.1929e-03, -1.6359e-02,  1.0590e-02,
         -2.0332e-02,  2.9928e-03, -7.8572e-03, -7.0993e-03, -1.8375e-02,
         -7.3508e-03, -2.3461e-02, -4.6866e-03,  1.7231e-03,  6.0150e-03,
         -1.3419e-03,  7.7649e-03,  6.4450e-03,  3.5350e-02,  4.7381e-03,
         -2.8696e-03,  2.0879e-02, -1.0139e-02,  7.2232e-03, -5.1539e-03,
         -2.2949e-02,  5.3693e-03, -1.0881e-02, -6.2496e-03,  8.4491e-03,
         -2.1244e-02,  1.3244e-03,  7.5332e-03,  1.4131e-02,  7.9682e-04,
          7.3645e-03, -8.2714e-03, -2.5314e-03, -2.1873e-02,  3.1765e-02,
         -4.8804e-03, -2.4472e-02,  2.2299e-02,  6.1871e-03, -1.7419e-02,
          5.9506e-03,  7.0206e-03, -1.0742e-02,  1.9905e-02, -1.9008e-02,
         -9.4624e-03,  3.4290e-03, -2.0575e-02, -7.0317e-03, -1.8206e-03,
         -2.0653e-02,  2.1290e-03, -1.4344e-02,  3.3918e-03, -2.1440e-03,
         -1.8860e-02, -1.9641e-03, -3.0282e-03, -1.7136e-02,  3.9291e-03,
          1.5897e-02, -2.3567e-02,  8.6886e-03,  2.8193e-03, -1.7935e-03,
          3.5838e-03, -3.8800e-03, -1.8691e-03, -7.2636e-03,  1.2117e-02,
          5.4995e-03, -2.9702e-02, -1.5100e-02, -1.4870e-02,  5.9734e-03,
          4.0719e-03,  2.2055e-02,  2.7724e-03,  1.5866e-02, -1.2415e-02,
          3.9170e-03, -1.0387e-02, -4.5013e-03, -1.0542e-02,  4.1546e-03,
          7.4030e-03, -2.0075e-02,  5.5243e-03,  8.9127e-04, -5.5528e-03,
         -8.6838e-03,  5.7979e-04, -4.8148e-03, -1.0569e-02,  6.6042e-03,
          4.5047e-03, -3.7056e-03, -8.8379e-03,  1.6669e-02, -1.0716e-02,
          1.3307e-02, -5.5699e-03, -2.1069e-03, -1.2610e-02, -2.0722e-02,
         -1.3726e-03,  2.7280e-02, -2.3712e-02, -6.8569e-03, -9.5550e-03,
         -7.0166e-03, -3.6037e-02,  2.0871e-02,  3.6005e-03, -1.7855e-02,
          5.7925e-03, -4.6208e-03,  5.6082e-03,  1.4084e-02, -6.4613e-03,
          4.7176e-03,  2.5638e-03,  3.0731e-03, -4.1387e-03, -6.8294e-03,
          2.2951e-02,  1.9632e-02, -1.2773e-02, -1.2546e-02, -1.1191e-02,
         -1.2668e-02, -4.2750e-03, -1.1302e-02, -8.7087e-03, -4.2122e-03,
         -2.1654e-02, -1.1504e-02, -8.1507e-03,  8.5978e-03, -5.2645e-03,
          9.8671e-03,  4.1211e-03, -1.5438e-02, -1.4726e-02, -7.9520e-03,
         -6.9271e-03,  8.4526e-03, -2.7590e-03, -2.3900e-02, -9.3285e-03,
         -2.3447e-03, -3.2407e-03,  4.7437e-03,  9.5961e-03, -6.7137e-03,
         -1.4155e-02,  4.3349e-03, -1.7242e-02,  1.2230e-02,  1.5125e-02,
         -2.9711e-02,  7.2126e-03,  9.7318e-03,  2.9412e-03,  1.2823e-02,
         -1.7377e-02, -4.8505e-03, -1.3207e-02,  4.2649e-03, -4.5623e-03,
          4.1922e-03,  1.2423e-02, -6.6076e-03,  2.3154e-02, -5.5635e-03,
         -7.1728e-03, -1.2428e-03, -5.7440e-03, -4.9586e-03,  5.5594e-03,
          4.5984e-03, -1.0963e-02, -1.2068e-02, -1.0429e-02,  1.1892e-02,
          6.6652e-04,  8.2442e-05,  6.6607e-03,  2.7221e-02,  6.6386e-03,
          5.8197e-03, -2.9741e-02,  1.1955e-02, -6.4619e-04,  8.2243e-03,
         -1.0927e-02,  1.7632e-02,  8.8429e-03, -1.1930e-02,  7.5890e-03,
         -2.7587e-03,  1.8605e-03, -3.9652e-03, -3.6323e-03,  4.8334e-03,
         -7.8052e-03, -1.0749e-02, -1.2414e-02, -1.8188e-02, -3.2810e-05,
          1.1237e-02,  6.5299e-03,  3.3878e-03, -3.5920e-02,  8.1458e-03,
          5.3817e-03, -1.5797e-02, -1.1172e-02,  8.4930e-03,  1.5536e-02,
          7.7707e-04, -2.6218e-02, -1.0018e-02, -5.8831e-03, -1.9483e-02,
          1.2066e-02,  1.1525e-02, -1.6199e-02,  5.2440e-03,  1.0465e-02,
         -8.6769e-03,  4.6096e-03,  2.4813e-02,  6.4933e-03, -4.8731e-03,
         -4.8448e-03, -1.5116e-02,  1.6647e-02,  1.2577e-02,  2.0959e-02,
         -1.6442e-02,  1.1885e-02,  5.4029e-03, -7.7414e-03, -1.7043e-02,
         -1.1677e-03,  1.5358e-02, -6.0001e-03,  1.9345e-03,  1.3403e-02,
          1.8058e-03,  2.2947e-02,  1.0369e-03, -2.5351e-03,  1.1869e-02,
          4.6377e-03,  3.1175e-03,  2.4040e-02, -1.9897e-02,  1.9616e-02,
          2.1242e-02, -2.3156e-04,  4.9114e-03,  3.1765e-04, -2.1568e-03,
         -1.3640e-02,  1.4862e-03, -2.3982e-02,  8.0717e-03, -1.6823e-02,
         -5.7940e-03,  4.9388e-03, -9.9035e-03, -4.3066e-03, -5.3220e-03,
          8.3757e-03, -1.2719e-03, -1.9322e-03,  2.0610e-03, -5.2693e-03,
          5.6804e-03,  2.0573e-02, -9.6522e-03,  2.9468e-03, -3.0866e-03,
          7.0498e-03,  4.5984e-03, -9.3414e-03,  3.7978e-03, -1.2008e-02,
          3.6009e-03, -5.6653e-03,  8.6586e-03, -5.2770e-03,  1.4292e-02,
         -1.4662e-02, -6.6719e-03,  4.2489e-04, -1.8722e-02,  4.6697e-03,
         -4.0943e-03, -1.2355e-02, -5.7841e-03,  1.6108e-02,  5.8834e-03,
         -2.4147e-02,  2.9542e-03,  9.6795e-03, -7.1999e-03, -1.4275e-03,
         -6.3677e-03, -1.0703e-02,  5.4017e-03,  6.5392e-03,  7.5045e-04,
          2.9139e-03, -6.4808e-03, -1.8375e-02, -6.1639e-03,  1.2249e-02,
          1.9912e-03,  1.4661e-02,  2.6980e-02,  4.1498e-03, -4.6434e-03,
         -1.4824e-03,  3.2232e-02,  2.9898e-03, -1.7606e-02, -6.5047e-03,
         -1.3698e-03,  9.7621e-04, -1.4022e-02,  9.7067e-03,  1.6532e-03,
         -2.3257e-02,  9.1291e-03, -1.5136e-02,  1.8167e-02,  2.2353e-03,
          1.1370e-02,  1.8967e-02,  1.1633e-02, -1.2604e-02,  3.2160e-04,
          1.3261e-02, -5.5081e-04,  1.0882e-02, -1.8358e-02,  1.2518e-02,
          6.3829e-05, -8.4990e-03, -1.2032e-02,  1.4300e-02, -4.1355e-03,
          1.8529e-02, -2.4602e-03, -7.5313e-03,  1.3623e-03,  4.5013e-03,
          1.4576e-02,  1.3978e-02,  4.5528e-05, -6.2549e-03, -3.3501e-03,
          1.3456e-03,  1.7366e-02,  8.6194e-03,  5.4167e-03, -1.6677e-03,
         -7.2368e-03, -2.6845e-04,  2.3956e-02,  3.1590e-03,  3.2946e-04,
          1.1348e-02, -9.5728e-03,  1.7456e-02,  4.1101e-03, -1.7206e-02,
          1.7041e-02, -7.1947e-03, -5.1789e-03,  3.4736e-03,  1.1360e-02,
         -7.7808e-03,  3.3253e-03, -9.9859e-04, -5.0724e-03,  3.3110e-03,
         -1.2051e-02, -1.8740e-02, -2.0499e-02, -4.6010e-03,  4.3809e-03,
         -7.6256e-03, -5.3684e-03, -2.2054e-02, -1.7174e-02, -6.6960e-03,
          4.5842e-03, -4.2019e-03, -2.8270e-03,  8.2985e-03, -6.1501e-03,
         -5.1232e-05, -5.4207e-03,  8.5012e-03, -4.0337e-03, -1.1492e-02,
          1.1574e-02,  7.1391e-03, -6.0209e-03,  8.7758e-03, -4.2017e-03,
         -2.1556e-02,  3.4629e-03,  1.5146e-03, -2.7808e-03, -3.7984e-03,
          7.4385e-04, -3.5643e-03, -3.4608e-03,  1.3158e-02, -1.1477e-03,
          6.2438e-03, -2.8162e-03, -3.0051e-02, -7.3685e-03,  1.7393e-03,
         -5.4014e-03, -3.6246e-03,  4.7081e-03, -1.0188e-02,  1.1342e-02,
         -5.4641e-04, -3.4645e-03, -1.1243e-02,  3.4643e-03,  5.8905e-03,
          3.6375e-03,  8.5945e-03,  5.3828e-03, -9.7318e-03,  3.2512e-03,
         -9.1221e-03,  5.1307e-03,  1.1321e-03,  8.6762e-03, -2.4042e-03,
          1.2902e-02, -1.2409e-03, -7.2556e-04,  1.4254e-02,  4.8999e-03,
          4.5806e-03,  3.1325e-03, -1.3385e-02,  1.1830e-02,  1.2770e-02,
         -5.7909e-03,  1.6711e-03,  2.8369e-03, -1.3920e-02,  1.5913e-03,
         -2.2135e-03, -2.5081e-03,  6.1257e-03,  6.5781e-04,  1.7755e-02,
          1.5372e-02,  2.4198e-02, -1.0335e-02,  4.7767e-05, -1.0211e-02,
         -1.1636e-02, -6.9237e-03]], device='cuda:2', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[1.5328e-01, 3.7942e-01, 4.2730e-01, 5.0577e-01, 3.7025e-01, 5.0867e-01,
         5.1282e-01, 2.3923e-01, 4.0778e-01, 3.8589e-01, 5.3623e-01, 4.1567e-01,
         3.8059e-01, 4.2018e-01, 5.1800e-01, 2.6007e-01, 3.6022e-01, 4.2307e-01,
         5.3612e-04, 4.1672e-01, 2.9744e-01, 1.4764e-01, 1.8348e-01, 4.6459e-04,
         3.6081e-01, 3.6276e-01, 4.4525e-01, 5.3115e-01, 3.0671e-01, 3.3033e-01,
         2.0205e-01, 5.7068e-01, 4.0316e-01, 4.6842e-01, 2.4592e-01, 3.2434e-01,
         3.8518e-01, 3.9215e-01, 3.4688e-01, 4.2561e-01, 3.0967e-01, 4.7012e-01,
         3.5354e-01, 2.0201e-01, 4.4604e-01, 3.4510e-01, 2.8321e-01, 3.7844e-01,
         3.4184e-01, 3.5039e-01, 3.0808e-01, 3.3555e-01, 5.3159e-01, 3.6423e-01,
         3.7877e-01, 5.2820e-01, 3.6787e-01, 1.8712e-01, 3.1596e-01, 4.4592e-01,
         3.3995e-01, 3.8242e-01, 4.4295e-01, 3.9262e-01, 3.8842e-01, 3.4120e-01,
         4.0335e-01, 3.9187e-01, 5.3843e-01, 3.3120e-01, 3.2164e-01, 3.7729e-01,
         4.2875e-01, 3.0047e-01, 3.8182e-01, 3.4978e-01, 3.4953e-01, 2.6765e-01,
         1.9912e-01, 2.8751e-01, 2.5314e-01, 2.8734e-01, 2.0305e-01, 3.5914e-01,
         2.9331e-01, 4.7860e-01, 3.5953e-01, 4.2057e-01, 3.8727e-01, 1.5621e-01,
         3.1984e-01, 3.8808e-01, 1.6623e-01, 5.1653e-01, 3.7636e-01, 4.0145e-01,
         3.3659e-01, 3.3588e-01, 3.6401e-01, 3.7078e-01, 5.1400e-01, 2.9545e-01,
         5.3074e-01, 3.6609e-01, 4.2984e-01, 2.3658e-01, 3.9226e-01, 1.8692e-01,
         3.7161e-01, 2.6896e-01, 5.2033e-01, 4.2156e-04, 1.7520e-01, 2.1679e-01,
         2.0993e-01, 1.6850e-01, 3.7243e-01, 4.7555e-01, 2.1322e-01, 4.3260e-01,
         3.5498e-01, 3.4327e-01, 4.6212e-01, 3.9380e-01, 2.7560e-01, 3.3793e-01,
         3.5191e-01, 3.0256e-01, 3.6515e-01, 2.1445e-01, 3.3423e-01, 4.0806e-01,
         3.3690e-01, 4.3775e-01, 3.0361e-01, 2.7340e-01, 2.5325e-01, 1.7055e-01,
         2.9913e-01, 2.7766e-01, 2.3983e-01, 4.3485e-01, 3.7143e-01, 3.6829e-01,
         2.5804e-01, 3.0963e-01, 3.5789e-01, 3.8113e-01, 3.6976e-01, 3.2193e-01,
         3.6006e-01, 4.7736e-01, 4.3123e-01, 2.8458e-01, 2.6066e-01, 4.1901e-01,
         3.6923e-01, 3.0142e-01, 3.5495e-01, 3.5187e-01, 4.0910e-01, 2.9385e-01,
         6.6246e-05, 3.5796e-01, 4.1208e-01, 3.4439e-01, 3.4931e-01, 2.5157e-01,
         2.0465e-01, 4.0138e-01, 3.2630e-01, 3.7860e-01, 4.4107e-01, 3.2543e-01,
         3.9851e-01, 3.6583e-01, 4.4248e-01, 3.4295e-01, 3.6474e-01, 3.7850e-01,
         3.6017e-01, 3.9163e-01, 3.6939e-01, 3.5186e-01, 4.4118e-01, 5.2141e-01,
         3.3645e-01, 3.6407e-01, 5.8597e-01, 3.4658e-01, 2.9932e-01, 3.4853e-01,
         4.7161e-04, 5.4931e-01, 3.4091e-01, 3.5092e-01, 2.7023e-01, 3.5419e-01,
         3.9826e-01, 3.5010e-01, 4.7129e-01, 3.9045e-01, 3.4802e-01, 4.3014e-01,
         3.1190e-01, 3.6266e-01, 3.2992e-01, 3.3983e-01, 3.4164e-01, 4.4242e-01,
         3.8753e-01, 3.5903e-01, 2.8386e-01, 3.6880e-01, 2.8377e-01, 3.9760e-01,
         3.6535e-01, 9.1071e-05, 3.9591e-01, 4.7776e-01, 5.6533e-01, 4.2753e-01,
         2.4934e-04, 3.5201e-01, 4.2083e-01, 5.8179e-02, 3.7869e-01, 3.0426e-01,
         2.6740e-01, 3.8192e-01, 2.5276e-01, 4.3985e-01, 3.8981e-01, 3.0588e-01,
         4.6192e-01, 2.2760e-01, 1.5610e-01, 2.0474e-01, 4.7994e-01, 3.0459e-01,
         3.0557e-01, 6.1499e-01, 4.0174e-01, 4.1638e-01, 5.1438e-01, 5.5303e-01,
         3.5805e-01, 5.8872e-01, 5.1903e-01, 3.9683e-01, 2.6633e-01, 3.4363e-01,
         3.7454e-01, 3.5965e-01, 3.1131e-01, 4.1374e-01, 1.8362e-01, 4.0987e-01,
         3.3006e-01, 5.6716e-01, 5.1768e-01, 4.3253e-01, 2.0882e-01, 2.2957e-01,
         1.4777e-01, 3.5491e-01, 4.6734e-01, 3.5947e-01, 4.1009e-01, 3.4292e-01,
         3.3978e-01, 3.4683e-01, 4.5449e-01, 2.8344e-01, 3.2447e-01, 3.4094e-01,
         3.6413e-01, 4.0734e-01, 4.9877e-01, 3.3301e-01, 3.2972e-01, 2.6108e-01,
         1.6106e-01, 4.8512e-01, 5.1393e-01, 2.3162e-01, 3.9236e-01, 3.3814e-01,
         3.1575e-01, 3.7498e-01, 4.2402e-01, 4.5803e-01, 4.7642e-01, 3.3478e-01,
         3.3579e-01, 4.1375e-04, 2.8716e-01, 3.3724e-01, 3.5689e-01, 3.2758e-01,
         3.2478e-01, 2.4562e-01, 2.7137e-01, 2.4726e-01, 2.9807e-01, 3.9371e-01,
         4.0034e-01, 3.6306e-01, 3.5947e-01, 3.5515e-01, 4.9800e-01, 3.3127e-01,
         2.9021e-01, 3.3080e-01, 3.8587e-01, 6.3330e-01, 4.4442e-01, 3.8823e-01,
         4.6026e-01, 4.2855e-01, 5.3130e-01, 3.5154e-01, 4.0841e-01, 3.6073e-01,
         3.1293e-01, 3.6135e-01, 5.4697e-01, 4.6362e-01, 3.8359e-01, 3.4186e-01,
         4.9391e-01, 3.9704e-01, 4.0856e-01, 3.9734e-01, 4.9421e-01, 3.5961e-01,
         3.7329e-01, 5.2408e-01, 4.1184e-01, 3.3603e-01, 3.9454e-01, 3.6694e-01,
         3.1199e-01, 3.3930e-01, 3.9762e-01, 4.6059e-01, 3.6939e-01, 3.9055e-01,
         4.0275e-01, 4.1691e-01, 4.8497e-01, 3.2898e-01, 3.9351e-01, 3.3343e-01,
         1.5694e-01, 3.3343e-01, 3.2279e-01, 3.1904e-01, 2.1068e-01, 3.2889e-01,
         3.0266e-01, 4.0845e-01, 4.3174e-01, 2.1733e-01, 3.1329e-01, 3.1634e-01,
         4.3750e-01, 3.0881e-01, 1.4141e-01, 4.6698e-01, 4.4503e-01, 3.9309e-01,
         4.0883e-01, 3.1066e-01, 4.4003e-01, 3.7601e-01, 4.6259e-01, 3.9152e-01,
         4.5493e-01, 3.7941e-01, 3.1611e-01, 2.2873e-01, 1.4799e-01, 3.9809e-01,
         3.3539e-01, 2.2391e-01, 3.1318e-01, 1.9022e-01, 3.2960e-01, 3.9756e-01,
         4.1952e-01, 3.9853e-01, 3.4043e-01, 3.6437e-01, 4.7940e-01, 2.8981e-01,
         3.4686e-01, 2.8621e-01, 2.3026e-01, 4.3637e-01, 5.0303e-01, 4.5660e-01,
         1.6332e-01, 3.4379e-01, 3.6842e-01, 2.6853e-01, 1.6001e-03, 3.0552e-01,
         1.5557e-01, 2.6817e-01, 5.3813e-01, 4.0952e-01, 5.0188e-01, 2.6324e-01,
         3.1768e-01, 5.1004e-01, 3.1630e-01, 2.0366e-01, 2.1758e-01, 2.6350e-01,
         3.3986e-01, 4.8833e-01, 4.4898e-01, 2.1978e-01, 3.8903e-01, 3.0154e-01,
         4.7234e-01, 4.7091e-01, 3.7133e-01, 1.6537e-01, 3.0317e-01, 2.7305e-01,
         4.7650e-01, 2.7456e-01, 3.5796e-01, 4.8350e-01, 3.1593e-01, 2.5717e-01,
         3.5728e-01, 2.8839e-01, 1.0099e-01, 4.1738e-01, 3.9951e-01, 3.7430e-01,
         4.5816e-01, 2.3537e-01, 2.8904e-01, 1.7217e-01, 3.3909e-01, 1.3807e-01,
         5.6217e-01, 3.3124e-01, 3.1366e-01, 2.1850e-01, 2.0950e-01, 1.8102e-01,
         3.1585e-01, 1.8317e-01, 4.2996e-05, 1.6692e-01, 3.2054e-01, 2.5005e-01,
         4.5229e-01, 2.8564e-01, 2.5328e-01, 3.4351e-01, 4.8446e-01, 1.5827e-01,
         4.9703e-01, 3.7654e-01, 2.4682e-01, 5.1492e-01, 4.8519e-01, 3.4211e-01,
         3.0457e-01, 4.1856e-01, 3.1009e-01, 3.1449e-01, 6.7241e-01, 3.4859e-01,
         3.5751e-01, 3.7617e-01, 3.8011e-01, 5.1144e-01, 3.6926e-01, 5.5525e-01,
         3.9055e-01, 3.3733e-01, 4.2278e-01, 3.5801e-01, 3.8826e-01, 4.7540e-01,
         1.8170e-01, 3.9247e-01, 2.8778e-01, 4.1951e-01, 1.5851e-01, 3.9467e-01,
         5.8067e-01, 3.4765e-01, 3.1631e-01, 4.8988e-01, 1.8283e-01, 3.3209e-01,
         3.3776e-01, 4.2555e-01, 3.6439e-01, 3.1732e-01, 4.0533e-01, 4.3377e-01,
         3.2512e-01, 2.2566e-01]], device='cuda:2', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.5115], device='cuda:2', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0301], device='cuda:2', grad_fn=<AddBackward0>)
Using device: cuda:2
===== Training profile tensin-3x512-s3 - 3 =====
[0:00:01.697881] epoch: 0 | elbo: 33174.141835937495 | train_rmse: 1.1154 | val_rmse: 1.182 | val_ll: -2.1937
[0:01:33.963388] epoch: 50 | elbo: 32953.98130859375 | train_rmse: 1.1045 | val_rmse: 1.1776 | val_ll: -2.1916
[0:03:06.657560] epoch: 100 | elbo: 32933.94572265625 | train_rmse: 1.1019 | val_rmse: 1.1692 | val_ll: -2.193
[0:04:37.427750] epoch: 150 | elbo: 32967.67673828125 | train_rmse: 1.1075 | val_rmse: 1.1751 | val_ll: -2.1912
[0:06:09.014963] epoch: 200 | elbo: 33304.6234765625 | train_rmse: 1.1089 | val_rmse: 1.1793 | val_ll: -2.1913
[0:07:40.944880] epoch: 250 | elbo: 32967.209628906254 | train_rmse: 1.1095 | val_rmse: 1.1728 | val_ll: -2.1889
[0:09:12.903114] epoch: 300 | elbo: 32984.03710937501 | train_rmse: 1.1072 | val_rmse: 1.1784 | val_ll: -2.1904
[0:10:46.408722] epoch: 350 | elbo: 33010.1541015625 | train_rmse: 1.1126 | val_rmse: 1.1826 | val_ll: -2.191
[0:12:19.108307] epoch: 400 | elbo: 32977.424374999995 | train_rmse: 1.1108 | val_rmse: 1.1756 | val_ll: -2.1914
[0:13:51.323166] epoch: 450 | elbo: 32916.334492187496 | train_rmse: 1.103 | val_rmse: 1.1632 | val_ll: -2.1905
[0:15:23.867306] epoch: 500 | elbo: 33034.98388671876 | train_rmse: 1.1128 | val_rmse: 1.1874 | val_ll: -2.1901
[0:16:56.518241] epoch: 550 | elbo: 32900.68388671875 | train_rmse: 1.1035 | val_rmse: 1.1656 | val_ll: -2.1903
[0:18:28.631721] epoch: 600 | elbo: 32908.26541015625 | train_rmse: 1.103 | val_rmse: 1.1728 | val_ll: -2.1924
[0:20:01.977817] epoch: 650 | elbo: 32871.66181640625 | train_rmse: 1.1079 | val_rmse: 1.1798 | val_ll: -2.1899
[0:21:35.307585] epoch: 700 | elbo: 32858.615156249994 | train_rmse: 1.1021 | val_rmse: 1.1646 | val_ll: -2.1884
[0:23:07.442274] epoch: 750 | elbo: 32879.00861328126 | train_rmse: 1.105 | val_rmse: 1.1725 | val_ll: -2.1896
[0:24:39.738273] epoch: 800 | elbo: 32891.49451171875 | train_rmse: 1.1012 | val_rmse: 1.1703 | val_ll: -2.1895
[0:26:12.052003] epoch: 850 | elbo: 33002.2134765625 | train_rmse: 1.0999 | val_rmse: 1.1699 | val_ll: -2.1895
[0:27:44.115330] epoch: 900 | elbo: 32957.945898437494 | train_rmse: 1.104 | val_rmse: 1.1664 | val_ll: -2.1892
[0:29:17.237447] epoch: 950 | elbo: 32976.56548828125 | train_rmse: 1.099 | val_rmse: 1.1707 | val_ll: -2.1912
[0:30:49.070850] epoch: 1000 | elbo: 32901.53298828125 | train_rmse: 1.1081 | val_rmse: 1.166 | val_ll: -2.1868
[0:32:23.283323] epoch: 1050 | elbo: 32902.700039062496 | train_rmse: 1.1084 | val_rmse: 1.1767 | val_ll: -2.1907
[0:33:55.520631] epoch: 1100 | elbo: 32866.64365234375 | train_rmse: 1.1018 | val_rmse: 1.1692 | val_ll: -2.1869
[0:35:27.839059] epoch: 1150 | elbo: 32961.95396484375 | train_rmse: 1.1035 | val_rmse: 1.1749 | val_ll: -2.1887
[0:36:59.304441] epoch: 1200 | elbo: 32983.03076171875 | train_rmse: 1.0922 | val_rmse: 1.16 | val_ll: -2.1889
[0:38:30.399488] epoch: 1250 | elbo: 32847.49037109375 | train_rmse: 1.0913 | val_rmse: 1.1599 | val_ll: -2.188
[0:40:01.767773] epoch: 1300 | elbo: 32811.74259765625 | train_rmse: 1.095 | val_rmse: 1.1615 | val_ll: -2.1883
[0:41:33.721020] epoch: 1350 | elbo: 33016.502890625 | train_rmse: 1.0989 | val_rmse: 1.1639 | val_ll: -2.1879
[0:43:05.037449] epoch: 1400 | elbo: 32813.619902343744 | train_rmse: 1.0942 | val_rmse: 1.17 | val_ll: -2.1878
[0:44:35.936875] epoch: 1450 | elbo: 32793.171269531245 | train_rmse: 1.0988 | val_rmse: 1.1653 | val_ll: -2.1869
[0:46:06.629094] epoch: 1500 | elbo: 32811.76974609374 | train_rmse: 1.0928 | val_rmse: 1.1632 | val_ll: -2.1879
[0:47:37.583121] epoch: 1550 | elbo: 32819.307265625 | train_rmse: 1.0929 | val_rmse: 1.1542 | val_ll: -2.1875
[0:49:09.111613] epoch: 1600 | elbo: 32816.967792968746 | train_rmse: 1.0857 | val_rmse: 1.1616 | val_ll: -2.1881
[0:50:40.576515] epoch: 1650 | elbo: 32967.83119140625 | train_rmse: 1.0897 | val_rmse: 1.1554 | val_ll: -2.1873
[0:52:12.975070] epoch: 1700 | elbo: 32802.887539062496 | train_rmse: 1.0834 | val_rmse: 1.1527 | val_ll: -2.1868
[0:53:44.812560] epoch: 1750 | elbo: 32788.71859375 | train_rmse: 1.0971 | val_rmse: 1.1648 | val_ll: -2.1855
[0:55:16.208885] epoch: 1800 | elbo: 32813.839687499996 | train_rmse: 1.0871 | val_rmse: 1.155 | val_ll: -2.1889
[0:56:46.916925] epoch: 1850 | elbo: 32868.11039062501 | train_rmse: 1.0898 | val_rmse: 1.1579 | val_ll: -2.1886
[0:58:18.308789] epoch: 1900 | elbo: 32771.659765625 | train_rmse: 1.0937 | val_rmse: 1.1536 | val_ll: -2.1866
[0:59:49.872666] epoch: 1950 | elbo: 32746.196425781247 | train_rmse: 1.0902 | val_rmse: 1.1541 | val_ll: -2.1861
[1:01:20.429133] epoch: 2000 | elbo: 32767.7856640625 | train_rmse: 1.0967 | val_rmse: 1.1645 | val_ll: -2.1868
[1:02:52.767331] epoch: 2050 | elbo: 32745.138105468755 | train_rmse: 1.0832 | val_rmse: 1.1468 | val_ll: -2.1857
[1:04:24.105997] epoch: 2100 | elbo: 32718.331875000003 | train_rmse: 1.0876 | val_rmse: 1.1536 | val_ll: -2.1864
[1:05:55.683019] epoch: 2150 | elbo: 32734.79125 | train_rmse: 1.0908 | val_rmse: 1.1568 | val_ll: -2.187
[1:07:28.390267] epoch: 2200 | elbo: 32765.358730468746 | train_rmse: 1.0907 | val_rmse: 1.1514 | val_ll: -2.1876
[1:09:02.065610] epoch: 2250 | elbo: 32797.5857421875 | train_rmse: 1.0788 | val_rmse: 1.1548 | val_ll: -2.1868
[1:10:35.332401] epoch: 2300 | elbo: 32773.57861328125 | train_rmse: 1.0825 | val_rmse: 1.147 | val_ll: -2.1877
[1:12:09.073800] epoch: 2350 | elbo: 32902.4604296875 | train_rmse: 1.0837 | val_rmse: 1.1466 | val_ll: -2.188
[1:13:40.479942] epoch: 2400 | elbo: 32728.01349609375 | train_rmse: 1.0946 | val_rmse: 1.16 | val_ll: -2.1873
[1:15:12.344714] epoch: 2450 | elbo: 32742.99783203125 | train_rmse: 1.0851 | val_rmse: 1.148 | val_ll: -2.1853
[1:16:45.737603] epoch: 2500 | elbo: 32743.175058593748 | train_rmse: 1.0867 | val_rmse: 1.1538 | val_ll: -2.1856
[1:18:17.535518] epoch: 2550 | elbo: 32713.865917968746 | train_rmse: 1.0809 | val_rmse: 1.1409 | val_ll: -2.1853
[1:19:50.742853] epoch: 2600 | elbo: 33785.63251953125 | train_rmse: 1.0902 | val_rmse: 1.1554 | val_ll: -2.1871
[1:21:23.955369] epoch: 2650 | elbo: 32785.6410546875 | train_rmse: 1.0871 | val_rmse: 1.1498 | val_ll: -2.1855
[1:22:56.634315] epoch: 2700 | elbo: 32810.14296875 | train_rmse: 1.0757 | val_rmse: 1.1442 | val_ll: -2.1839
[1:24:29.791613] epoch: 2750 | elbo: 32666.64015625 | train_rmse: 1.0698 | val_rmse: 1.1472 | val_ll: -2.1863
[1:26:02.814272] epoch: 2800 | elbo: 32683.652871093753 | train_rmse: 1.0848 | val_rmse: 1.1518 | val_ll: -2.1848
[1:27:35.102450] epoch: 2850 | elbo: 32746.932187500002 | train_rmse: 1.0869 | val_rmse: 1.1567 | val_ll: -2.1871
[1:29:06.190826] epoch: 2900 | elbo: 32670.705039062497 | train_rmse: 1.0867 | val_rmse: 1.1535 | val_ll: -2.184
[1:30:37.194798] epoch: 2950 | elbo: 32779.655078125 | train_rmse: 1.0826 | val_rmse: 1.1469 | val_ll: -2.1858
[1:32:08.912384] epoch: 3000 | elbo: 32810.502324218745 | train_rmse: 1.0834 | val_rmse: 1.1474 | val_ll: -2.1848
[1:33:40.529834] epoch: 3050 | elbo: 32656.868242187506 | train_rmse: 1.0838 | val_rmse: 1.1528 | val_ll: -2.1837
[1:35:12.440084] epoch: 3100 | elbo: 32733.698769531253 | train_rmse: 1.0774 | val_rmse: 1.1429 | val_ll: -2.1847
[1:36:45.724475] epoch: 3150 | elbo: 32935.387265625 | train_rmse: 1.0685 | val_rmse: 1.1443 | val_ll: -2.1832
[1:38:17.612706] epoch: 3200 | elbo: 32636.059902343746 | train_rmse: 1.0763 | val_rmse: 1.1458 | val_ll: -2.1868
[1:39:48.283904] epoch: 3250 | elbo: 32692.513984375 | train_rmse: 1.0729 | val_rmse: 1.1443 | val_ll: -2.1855
[1:41:19.955278] epoch: 3300 | elbo: 32857.25642578125 | train_rmse: 1.0821 | val_rmse: 1.15 | val_ll: -2.1843
[1:42:53.441336] epoch: 3350 | elbo: 32748.011992187494 | train_rmse: 1.0718 | val_rmse: 1.1412 | val_ll: -2.1849
[1:44:26.616845] epoch: 3400 | elbo: 32791.686015625004 | train_rmse: 1.079 | val_rmse: 1.1443 | val_ll: -2.1828
[1:45:59.261528] epoch: 3450 | elbo: 32614.330722656254 | train_rmse: 1.086 | val_rmse: 1.1526 | val_ll: -2.1843
[1:47:30.589435] epoch: 3500 | elbo: 32691.418378906248 | train_rmse: 1.0755 | val_rmse: 1.1353 | val_ll: -2.1852
[1:49:02.095701] epoch: 3550 | elbo: 32655.059531249997 | train_rmse: 1.0668 | val_rmse: 1.139 | val_ll: -2.1823
[1:50:34.815706] epoch: 3600 | elbo: 32644.278203125003 | train_rmse: 1.075 | val_rmse: 1.1382 | val_ll: -2.1832
[1:52:06.522852] epoch: 3650 | elbo: 32830.9619921875 | train_rmse: 1.0765 | val_rmse: 1.1483 | val_ll: -2.1829
[1:53:39.204884] epoch: 3700 | elbo: 32784.882109375 | train_rmse: 1.0788 | val_rmse: 1.1461 | val_ll: -2.1845
[1:55:11.245638] epoch: 3750 | elbo: 32589.005898437503 | train_rmse: 1.076 | val_rmse: 1.1476 | val_ll: -2.184
[1:56:42.406913] epoch: 3800 | elbo: 32615.295312499995 | train_rmse: 1.0744 | val_rmse: 1.1383 | val_ll: -2.1835
[1:58:14.221587] epoch: 3850 | elbo: 32831.03669921875 | train_rmse: 1.0814 | val_rmse: 1.1415 | val_ll: -2.1849
[1:59:46.028394] epoch: 3900 | elbo: 32689.387597656245 | train_rmse: 1.0693 | val_rmse: 1.1315 | val_ll: -2.1856
[2:01:17.509453] epoch: 3950 | elbo: 32632.40236328125 | train_rmse: 1.0651 | val_rmse: 1.131 | val_ll: -2.1841
[2:02:50.459401] epoch: 4000 | elbo: 32806.28572265625 | train_rmse: 1.0805 | val_rmse: 1.1401 | val_ll: -2.1849
[2:04:24.757198] epoch: 4050 | elbo: 32571.198183593748 | train_rmse: 1.0719 | val_rmse: 1.1384 | val_ll: -2.1826
[2:05:56.612017] epoch: 4100 | elbo: 32736.141992187495 | train_rmse: 1.0712 | val_rmse: 1.1378 | val_ll: -2.1839
[2:07:28.044656] epoch: 4150 | elbo: 32579.85068359375 | train_rmse: 1.0783 | val_rmse: 1.1468 | val_ll: -2.1851
[2:09:00.326716] epoch: 4200 | elbo: 32532.925468749996 | train_rmse: 1.0677 | val_rmse: 1.1327 | val_ll: -2.1818
[2:10:32.677027] epoch: 4250 | elbo: 32557.539082031253 | train_rmse: 1.0791 | val_rmse: 1.1379 | val_ll: -2.1869
[2:12:06.157421] epoch: 4300 | elbo: 32599.312539062503 | train_rmse: 1.0707 | val_rmse: 1.1352 | val_ll: -2.1828
[2:13:39.639549] epoch: 4350 | elbo: 32560.991777343752 | train_rmse: 1.0622 | val_rmse: 1.1302 | val_ll: -2.1824
[2:15:13.980248] epoch: 4400 | elbo: 32567.250820312503 | train_rmse: 1.0727 | val_rmse: 1.1383 | val_ll: -2.1848
[2:16:47.021518] epoch: 4450 | elbo: 32584.77517578125 | train_rmse: 1.0748 | val_rmse: 1.1325 | val_ll: -2.1839
[2:18:19.861493] epoch: 4500 | elbo: 32541.298984375004 | train_rmse: 1.0662 | val_rmse: 1.13 | val_ll: -2.1831
[2:19:53.030112] epoch: 4550 | elbo: 32640.46048828125 | train_rmse: 1.0621 | val_rmse: 1.1278 | val_ll: -2.1827
[2:21:26.075296] epoch: 4600 | elbo: 32608.337343750005 | train_rmse: 1.0628 | val_rmse: 1.1332 | val_ll: -2.1827
[2:22:58.834348] epoch: 4650 | elbo: 32527.27005859375 | train_rmse: 1.0661 | val_rmse: 1.1326 | val_ll: -2.1847
[2:24:31.708319] epoch: 4700 | elbo: 32505.14552734375 | train_rmse: 1.0648 | val_rmse: 1.1291 | val_ll: -2.1852
[2:26:05.501686] epoch: 4750 | elbo: 32608.103789062494 | train_rmse: 1.0599 | val_rmse: 1.1313 | val_ll: -2.1835
[2:27:38.771318] epoch: 4800 | elbo: 32569.530175781256 | train_rmse: 1.0748 | val_rmse: 1.1354 | val_ll: -2.1813
[2:29:12.315054] epoch: 4850 | elbo: 32544.00833984375 | train_rmse: 1.072 | val_rmse: 1.1289 | val_ll: -2.181
[2:30:46.123363] epoch: 4900 | elbo: 32529.818417968745 | train_rmse: 1.0565 | val_rmse: 1.1184 | val_ll: -2.1826
[2:32:18.985900] epoch: 4950 | elbo: 32611.849355468752 | train_rmse: 1.0711 | val_rmse: 1.1309 | val_ll: -2.1789
[2:33:50.990798] epoch: 5000 | elbo: 32522.196835937502 | train_rmse: 1.0624 | val_rmse: 1.1244 | val_ll: -2.1844
[2:35:22.196696] epoch: 5050 | elbo: 32481.30173828125 | train_rmse: 1.0609 | val_rmse: 1.1278 | val_ll: -2.1853
[2:36:53.196672] epoch: 5100 | elbo: 32590.5898046875 | train_rmse: 1.0685 | val_rmse: 1.1249 | val_ll: -2.1809
[2:38:25.711473] epoch: 5150 | elbo: 32514.772285156243 | train_rmse: 1.0613 | val_rmse: 1.1314 | val_ll: -2.1802
[2:39:59.418581] epoch: 5200 | elbo: 32711.328671875002 | train_rmse: 1.0697 | val_rmse: 1.1251 | val_ll: -2.1816
[2:41:31.656971] epoch: 5250 | elbo: 32830.19046875 | train_rmse: 1.0661 | val_rmse: 1.1251 | val_ll: -2.1826
[2:43:04.233917] epoch: 5300 | elbo: 32548.6975390625 | train_rmse: 1.0646 | val_rmse: 1.1198 | val_ll: -2.1814
[2:44:36.301121] epoch: 5350 | elbo: 32489.344628906245 | train_rmse: 1.0656 | val_rmse: 1.1252 | val_ll: -2.1795
[2:46:08.001582] epoch: 5400 | elbo: 32679.43935546875 | train_rmse: 1.0541 | val_rmse: 1.1263 | val_ll: -2.1811
[2:47:38.257753] epoch: 5450 | elbo: 32538.441582031246 | train_rmse: 1.0546 | val_rmse: 1.1155 | val_ll: -2.1797
[2:49:09.886097] epoch: 5500 | elbo: 32505.7346484375 | train_rmse: 1.063 | val_rmse: 1.1222 | val_ll: -2.1806
[2:50:43.650908] epoch: 5550 | elbo: 32490.17576171875 | train_rmse: 1.0728 | val_rmse: 1.1301 | val_ll: -2.1795
[2:52:16.459869] epoch: 5600 | elbo: 32533.0038671875 | train_rmse: 1.0573 | val_rmse: 1.12 | val_ll: -2.181
[2:53:47.423930] epoch: 5650 | elbo: 32513.4376953125 | train_rmse: 1.0563 | val_rmse: 1.1167 | val_ll: -2.1796
[2:55:19.118618] epoch: 5700 | elbo: 32470.838789062505 | train_rmse: 1.0578 | val_rmse: 1.1225 | val_ll: -2.1828
[2:56:50.434317] epoch: 5750 | elbo: 32509.036601562497 | train_rmse: 1.0484 | val_rmse: 1.1149 | val_ll: -2.1798
[2:58:21.489625] epoch: 5800 | elbo: 32651.9377734375 | train_rmse: 1.06 | val_rmse: 1.1154 | val_ll: -2.1819
[2:59:52.545564] epoch: 5850 | elbo: 32622.86296875 | train_rmse: 1.0617 | val_rmse: 1.1177 | val_ll: -2.1817
[3:01:24.143232] epoch: 5900 | elbo: 32503.2075390625 | train_rmse: 1.0533 | val_rmse: 1.1158 | val_ll: -2.1817
[3:02:56.186469] epoch: 5950 | elbo: 32465.561289062498 | train_rmse: 1.0557 | val_rmse: 1.1119 | val_ll: -2.1814
[3:04:29.164188] epoch: 6000 | elbo: 32655.21193359375 | train_rmse: 1.0584 | val_rmse: 1.1201 | val_ll: -2.1802
[3:06:00.270663] epoch: 6050 | elbo: 32706.332910156256 | train_rmse: 1.0575 | val_rmse: 1.1295 | val_ll: -2.1789
[3:07:30.787358] epoch: 6100 | elbo: 32471.355527343745 | train_rmse: 1.0603 | val_rmse: 1.1198 | val_ll: -2.18
[3:09:01.690315] epoch: 6150 | elbo: 32798.048769531255 | train_rmse: 1.0598 | val_rmse: 1.12 | val_ll: -2.1807
[3:10:32.290942] epoch: 6200 | elbo: 32819.12603515625 | train_rmse: 1.0509 | val_rmse: 1.1142 | val_ll: -2.1825
[3:12:03.173683] epoch: 6250 | elbo: 32537.981308593746 | train_rmse: 1.0473 | val_rmse: 1.11 | val_ll: -2.1795
[3:13:36.595808] epoch: 6300 | elbo: 32611.940292968753 | train_rmse: 1.0636 | val_rmse: 1.1232 | val_ll: -2.1821
[3:15:09.667830] epoch: 6350 | elbo: 32543.87615234375 | train_rmse: 1.0478 | val_rmse: 1.1128 | val_ll: -2.1824
[3:16:41.111580] epoch: 6400 | elbo: 32459.754042968754 | train_rmse: 1.0499 | val_rmse: 1.1161 | val_ll: -2.1798
[3:18:11.903993] epoch: 6450 | elbo: 32565.334042968752 | train_rmse: 1.0591 | val_rmse: 1.1119 | val_ll: -2.1793
[3:19:42.875277] epoch: 6500 | elbo: 32510.602207031254 | train_rmse: 1.0562 | val_rmse: 1.1142 | val_ll: -2.18
[3:21:14.093489] epoch: 6550 | elbo: 32651.775468750002 | train_rmse: 1.0512 | val_rmse: 1.1072 | val_ll: -2.1805
[3:22:44.933114] epoch: 6600 | elbo: 32556.653691406253 | train_rmse: 1.0551 | val_rmse: 1.1182 | val_ll: -2.1774
[3:24:17.452751] epoch: 6650 | elbo: 32462.385664062498 | train_rmse: 1.0493 | val_rmse: 1.1154 | val_ll: -2.1801
[3:25:48.292920] epoch: 6700 | elbo: 32638.757265624998 | train_rmse: 1.0567 | val_rmse: 1.1176 | val_ll: -2.1791
[3:27:19.508928] epoch: 6750 | elbo: 32422.94255859375 | train_rmse: 1.0588 | val_rmse: 1.1159 | val_ll: -2.1797
[3:28:50.036269] epoch: 6800 | elbo: 32423.514394531252 | train_rmse: 1.0483 | val_rmse: 1.1114 | val_ll: -2.1806
[3:30:20.404165] epoch: 6850 | elbo: 32477.299296875 | train_rmse: 1.0502 | val_rmse: 1.1084 | val_ll: -2.1798
[3:31:52.152206] epoch: 6900 | elbo: 32422.604082031252 | train_rmse: 1.0521 | val_rmse: 1.1153 | val_ll: -2.1821
[3:33:23.306849] epoch: 6950 | elbo: 32488.87796875 | train_rmse: 1.0509 | val_rmse: 1.1078 | val_ll: -2.1797
[3:34:55.334371] epoch: 7000 | elbo: 32466.669472656253 | train_rmse: 1.0521 | val_rmse: 1.1075 | val_ll: -2.178
[3:36:26.183259] epoch: 7050 | elbo: 32484.29755859375 | train_rmse: 1.0635 | val_rmse: 1.1167 | val_ll: -2.1788
[3:37:57.678711] epoch: 7100 | elbo: 32438.23779296875 | train_rmse: 1.044 | val_rmse: 1.1003 | val_ll: -2.1776
[3:39:29.480640] epoch: 7150 | elbo: 32407.28896484375 | train_rmse: 1.0428 | val_rmse: 1.1071 | val_ll: -2.1785
[3:41:01.049117] epoch: 7200 | elbo: 32448.41486328125 | train_rmse: 1.0465 | val_rmse: 1.1021 | val_ll: -2.1783
[3:42:32.086946] epoch: 7250 | elbo: 32900.12419921875 | train_rmse: 1.0427 | val_rmse: 1.1062 | val_ll: -2.1779
[3:44:03.638146] epoch: 7300 | elbo: 32411.959609375004 | train_rmse: 1.0447 | val_rmse: 1.1093 | val_ll: -2.1774
[3:45:35.940632] epoch: 7350 | elbo: 32474.3189453125 | train_rmse: 1.0449 | val_rmse: 1.1027 | val_ll: -2.1761
[3:47:09.415304] epoch: 7400 | elbo: 32987.13986328125 | train_rmse: 1.0456 | val_rmse: 1.1011 | val_ll: -2.1765
[3:48:41.753665] epoch: 7450 | elbo: 32439.589511718754 | train_rmse: 1.0426 | val_rmse: 1.1035 | val_ll: -2.1818
[3:50:14.616008] epoch: 7500 | elbo: 32411.635605468753 | train_rmse: 1.0533 | val_rmse: 1.1118 | val_ll: -2.1757
[3:51:46.186017] epoch: 7550 | elbo: 33090.721953125 | train_rmse: 1.0533 | val_rmse: 1.1083 | val_ll: -2.1778
[3:53:17.178663] epoch: 7600 | elbo: 32520.21552734375 | train_rmse: 1.037 | val_rmse: 1.1054 | val_ll: -2.18
[3:54:48.776223] epoch: 7650 | elbo: 32558.105976562503 | train_rmse: 1.0427 | val_rmse: 1.1017 | val_ll: -2.1788
[3:56:19.739067] epoch: 7700 | elbo: 32429.2073046875 | train_rmse: 1.0422 | val_rmse: 1.0988 | val_ll: -2.1802
[3:57:50.618518] epoch: 7750 | elbo: 32407.091269531247 | train_rmse: 1.0469 | val_rmse: 1.1076 | val_ll: -2.1815
[3:59:23.109970] epoch: 7800 | elbo: 32572.220644531248 | train_rmse: 1.0455 | val_rmse: 1.1005 | val_ll: -2.1796
[4:00:54.889596] epoch: 7850 | elbo: 32419.466562499998 | train_rmse: 1.0491 | val_rmse: 1.1037 | val_ll: -2.1785
[4:02:27.489187] epoch: 7900 | elbo: 32672.383046875 | train_rmse: 1.0411 | val_rmse: 1.0983 | val_ll: -2.1774
[4:04:02.454104] epoch: 7950 | elbo: 32430.81248046875 | train_rmse: 1.0423 | val_rmse: 1.0953 | val_ll: -2.1777
[4:05:37.592814] epoch: 8000 | elbo: 32488.76951171875 | train_rmse: 1.0405 | val_rmse: 1.0944 | val_ll: -2.1778
[4:07:09.032496] epoch: 8050 | elbo: 32699.17685546875 | train_rmse: 1.0456 | val_rmse: 1.1054 | val_ll: -2.1772
[4:08:40.842605] epoch: 8100 | elbo: 32530.10048828125 | train_rmse: 1.047 | val_rmse: 1.0993 | val_ll: -2.1778
[4:10:11.793998] epoch: 8150 | elbo: 32661.846054687496 | train_rmse: 1.0467 | val_rmse: 1.1046 | val_ll: -2.1798
[4:11:43.642724] epoch: 8200 | elbo: 32476.89484375 | train_rmse: 1.0422 | val_rmse: 1.1042 | val_ll: -2.1793
[4:13:14.009488] epoch: 8250 | elbo: 32694.063828124996 | train_rmse: 1.0385 | val_rmse: 1.1056 | val_ll: -2.1776
[4:14:45.750400] epoch: 8300 | elbo: 32615.725507812498 | train_rmse: 1.039 | val_rmse: 1.0976 | val_ll: -2.1788
[4:16:17.070033] epoch: 8350 | elbo: 32385.719648437498 | train_rmse: 1.0437 | val_rmse: 1.1012 | val_ll: -2.1778
[4:17:47.757168] epoch: 8400 | elbo: 32387.51515625 | train_rmse: 1.0409 | val_rmse: 1.1044 | val_ll: -2.1773
[4:19:19.154622] epoch: 8450 | elbo: 32418.184999999998 | train_rmse: 1.0424 | val_rmse: 1.0992 | val_ll: -2.1771
[4:20:50.503628] epoch: 8500 | elbo: 32375.693964843755 | train_rmse: 1.0421 | val_rmse: 1.1007 | val_ll: -2.1793
[4:22:22.807408] epoch: 8550 | elbo: 32985.09314453125 | train_rmse: 1.0428 | val_rmse: 1.1008 | val_ll: -2.1767
[4:23:53.830218] epoch: 8600 | elbo: 32788.082675781254 | train_rmse: 1.0334 | val_rmse: 1.0974 | val_ll: -2.1764
[4:25:25.004398] epoch: 8650 | elbo: 32473.078476562503 | train_rmse: 1.0368 | val_rmse: 1.1024 | val_ll: -2.1762
[4:26:55.728225] epoch: 8700 | elbo: 32387.060136718756 | train_rmse: 1.0393 | val_rmse: 1.0984 | val_ll: -2.1787
[4:28:27.897134] epoch: 8750 | elbo: 32390.175624999996 | train_rmse: 1.0323 | val_rmse: 1.0881 | val_ll: -2.1761
[4:30:01.585170] epoch: 8800 | elbo: 32559.820234374998 | train_rmse: 1.0434 | val_rmse: 1.0954 | val_ll: -2.1781
[4:31:33.978180] epoch: 8850 | elbo: 32663.10556640625 | train_rmse: 1.0266 | val_rmse: 1.0923 | val_ll: -2.1779
[4:33:05.118087] epoch: 8900 | elbo: 32380.38427734375 | train_rmse: 1.0395 | val_rmse: 1.0936 | val_ll: -2.1783
[4:34:36.403900] epoch: 8950 | elbo: 32413.099238281255 | train_rmse: 1.0322 | val_rmse: 1.0909 | val_ll: -2.176
