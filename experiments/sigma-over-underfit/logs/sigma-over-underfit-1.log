Start time: 2023-07-09 21:06:02.024714
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:0 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.03 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0447,  0.0756, -0.2127,  ...,  0.2609,  0.4910,  0.2665],
        [-0.2647, -0.0771,  0.2573,  ..., -0.1631,  0.2988,  0.2101],
        [ 0.0411, -0.3286, -0.2541,  ..., -0.6626, -0.0275,  0.2559],
        ...,
        [-0.4244,  0.2789, -0.2359,  ...,  0.0649, -0.5205, -0.0421],
        [-0.4508, -0.1568,  0.2182,  ..., -0.4388, -0.3859, -0.0195],
        [-0.3507,  0.0489,  0.4109,  ..., -0.0855, -0.2056, -0.2932]],
       device='cuda:0', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6166e-02, -1.3915e-01,  2.9564e-01,  2.9611e-01,  2.2214e-01,
        -9.7888e-02,  5.8897e-01, -1.8524e-01,  2.0443e-01,  5.8446e-02,
        -1.7745e-01, -1.9895e-01, -1.1682e-01, -5.3616e-01,  2.0789e-01,
        -6.4598e-02,  3.3362e-01,  1.3427e-01, -3.1360e-02, -3.8676e-01,
         5.7533e-01,  9.6737e-02, -3.1188e-01, -1.9318e-01, -3.3081e-01,
        -1.8537e-01,  3.6419e-01, -1.3505e-01,  3.0185e-01,  5.7582e-01,
        -3.7531e-01, -4.6969e-01, -6.4841e-02,  4.0309e-01, -4.3244e-01,
        -1.9433e-01,  1.6605e-01, -2.1830e-01,  2.8050e-01, -9.4567e-02,
         1.5417e-01,  1.7902e-01, -2.3644e-01, -2.4925e-01, -2.9956e-01,
        -3.7044e-02,  1.6814e-01, -1.9854e-01, -2.5638e-01,  2.6855e-01,
        -9.5205e-02,  3.6366e-01, -8.0318e-01, -1.6955e-01, -2.2713e-01,
        -2.0035e-02,  1.3097e-01,  2.3065e-01,  3.7070e-01, -1.3243e-01,
        -3.6160e-02, -4.8206e-03, -1.1376e-03,  2.0086e-01, -2.2492e-01,
         3.9010e-01, -2.8781e-01,  4.2783e-02, -3.1186e-01,  2.7993e-01,
         3.9311e-01,  7.0192e-02, -9.7189e-02,  8.0272e-01, -3.1546e-01,
        -5.5958e-01,  2.1423e-02, -1.2455e-01, -3.3646e-01,  2.7934e-01,
        -1.5209e-01,  2.7482e-01, -1.1987e-01,  4.2689e-01,  1.3793e-02,
         2.3633e-01, -3.1819e-01,  5.3045e-02,  1.5081e-01,  3.4590e-01,
        -3.7396e-01, -7.3865e-02, -9.2203e-02,  6.5114e-02,  3.8257e-01,
        -1.5689e-01,  1.7788e-01,  5.1722e-01, -4.2938e-01,  8.0669e-02,
        -6.2920e-03, -1.3110e-01, -2.6427e-02,  4.4446e-01, -5.4677e-01,
        -1.6423e-01,  3.0083e-02,  9.8959e-02, -1.1722e-01,  2.3326e-01,
         1.7362e-01, -4.6287e-01, -5.5670e-01, -9.9926e-02, -3.0622e-01,
         1.4751e-01, -6.1184e-01, -5.0763e-01, -1.6882e-02, -5.2880e-01,
        -7.5939e-01, -1.9816e-01, -1.5566e-01,  3.9489e-01,  3.2330e-01,
        -1.3048e-01,  2.4041e-01, -2.6492e-01, -1.5796e-01,  7.9179e-01,
         1.4048e-01,  5.2655e-01,  2.2442e-01, -9.1619e-02, -1.0366e+00,
         2.6553e-01,  2.0384e-01, -1.7155e-01, -1.6481e-03,  3.2141e-01,
        -5.8581e-01,  6.4256e-01, -3.9943e-01, -1.9391e-01, -2.6091e-01,
         1.1797e-01,  4.4223e-02,  1.6194e-01, -3.5289e-01, -3.2432e-01,
        -2.6665e-01,  3.0188e-01, -8.1698e-02, -1.5214e-01, -6.1498e-01,
        -3.0815e-02, -6.0532e-01,  4.0075e-01, -7.2847e-02,  2.7709e-01,
        -2.6974e-01,  2.5113e-01,  4.1658e-01,  3.3207e-01,  6.8874e-02,
        -5.5198e-02, -4.3679e-01,  5.4634e-01, -1.6460e-01, -1.7971e-01,
         3.5597e-01,  5.3952e-01, -2.0925e-01, -4.3910e-01, -3.2569e-01,
         7.6319e-01, -4.8673e-01,  2.1589e-02,  3.6273e-01, -3.1636e-01,
         9.0259e-02, -1.8414e-02, -2.3747e-01, -5.9817e-02, -1.9344e-01,
        -1.7293e-01, -9.0646e-02, -2.1341e-01, -4.3948e-01, -7.6056e-01,
        -2.0861e-01,  6.6792e-01, -1.3956e-01, -3.2485e-01,  1.1441e-01,
         1.7686e-01, -2.2298e-01,  4.3501e-01, -2.1019e-01, -1.8008e-02,
         3.9982e-01, -1.0706e-01,  3.2638e-01, -5.9527e-01,  1.1342e-01,
        -5.9372e-01, -9.7981e-02, -4.6723e-02,  1.7939e-01, -1.9921e-01,
         6.0286e-01, -5.7475e-01, -2.8755e-01, -2.1078e-02,  3.3662e-01,
        -5.4672e-01, -8.4574e-02,  7.0585e-01, -3.7891e-02, -2.0209e-01,
        -4.6297e-02, -1.0724e-01, -3.7633e-02,  3.6016e-01,  6.9463e-01,
         8.4260e-02,  4.2845e-01, -2.1915e-01,  2.0365e-01,  3.4857e-01,
         2.7607e-01, -4.2154e-01, -7.8276e-01,  3.9964e-02, -1.5784e-02,
        -6.7150e-02,  7.5284e-02,  2.0118e-01,  4.6005e-01,  1.2877e-01,
         1.7722e-01,  2.7403e-01, -4.4400e-01, -3.5898e-01,  3.2615e-01,
        -2.8858e-01,  1.9766e-01,  3.4877e-01,  3.3363e-01, -3.0321e-02,
        -1.4267e-01,  7.5654e-03, -4.5793e-01,  9.4683e-02, -7.8115e-01,
         3.5276e-01, -2.6607e-02, -1.3633e-01,  1.3946e-01, -1.6989e-01,
         9.7561e-02,  1.9092e-01,  1.1244e-01, -1.5142e-02, -2.4859e-01,
         2.6107e-01,  1.3460e-01, -1.9601e-01,  2.2171e-01, -1.1813e-01,
        -2.7271e-02,  2.2744e-01, -6.0848e-02,  1.0853e-01,  2.9351e-01,
         2.4204e-02, -1.1631e-01,  2.2765e-01,  3.5871e-01,  2.9450e-01,
        -5.8593e-01,  3.7996e-02, -4.5191e-01,  2.0098e-01,  1.1031e-01,
         2.5319e-01,  3.8350e-01,  4.1628e-01,  2.2083e-01, -5.2145e-01,
        -1.2084e-04, -2.9774e-01,  5.9605e-01, -1.4718e-01, -2.0328e-01,
        -2.7510e-01,  6.2981e-02,  2.8263e-01,  1.0762e-01, -3.1115e-02,
        -8.6717e-02,  1.2602e-01,  3.4481e-01, -3.1291e-01,  4.4573e-02,
        -5.4093e-02,  1.2785e-02,  5.9368e-01, -9.3458e-02, -3.0025e-02,
        -8.2114e-01,  3.5871e-02, -1.2330e-01, -2.9151e-02, -1.7028e-02,
        -6.4747e-01,  6.7500e-01, -9.3829e-02, -3.5783e-01, -1.1899e-01,
        -1.5495e-01,  1.4848e-01, -1.3111e-02, -4.6007e-01,  7.6984e-02,
        -3.0186e-02,  5.6222e-01, -2.4689e-01, -9.7050e-02, -1.6777e-01,
        -3.7750e-01, -2.4305e-01, -4.1383e-01,  3.1174e-01, -3.7243e-01,
        -7.9324e-02,  1.1726e-01,  6.9679e-01,  1.9514e-01, -5.7582e-02,
        -2.1583e-01,  1.9784e-02,  1.2355e-01,  2.7329e-01,  9.2189e-02,
         1.3324e-01,  1.3457e-01,  1.5220e-01,  5.6737e-01, -4.6108e-01,
        -1.4723e-01,  3.6228e-01,  3.5389e-01, -1.3471e-01, -4.3502e-01,
         5.7319e-02,  6.4020e-01, -3.4931e-01,  5.9252e-01, -3.7241e-01,
         4.8289e-01, -7.1217e-01, -1.7626e-01,  5.8545e-02, -4.4086e-01,
         4.1514e-01,  3.5856e-02,  2.0005e-01, -7.7625e-02,  4.2968e-01,
         4.4622e-01, -2.8619e-01, -5.0256e-01, -6.0599e-01, -1.0697e-01,
        -6.6589e-03, -1.3865e-01, -1.9513e-01, -1.1505e-01, -1.1677e-01,
         1.4042e-01,  1.0355e-01, -1.9033e-01,  2.3525e-02, -3.3168e-01,
         3.7288e-01,  3.2279e-01,  3.5255e-01, -1.1326e-01,  3.3135e-01,
        -1.7131e-01,  3.8987e-01, -1.6718e-01,  2.1797e-02, -6.6744e-01,
        -3.9690e-01, -6.4341e-01,  1.5612e-01,  1.4624e-01, -1.5568e-01,
         1.6977e-02, -1.1612e-01, -2.9578e-01,  3.9115e-01, -1.8719e-01,
         2.2748e-01,  1.3767e-01,  3.3728e-01,  1.0920e-01, -3.8483e-01,
         2.2107e-01,  2.1028e-01,  1.3182e-01,  1.1805e-01,  1.4939e-02,
         4.6891e-01, -3.7607e-01, -1.6891e-01, -1.7116e-01, -6.4544e-02,
        -3.4030e-01,  4.5838e-01,  3.0842e-01,  2.1312e-01, -5.4339e-02,
         2.4640e-01,  9.0786e-02, -6.6366e-01,  3.6862e-02,  3.8134e-01,
        -1.4490e-01, -5.9302e-02, -1.5054e-01,  3.4661e-02, -2.3997e-01,
        -2.0295e-01, -5.0164e-01, -2.1711e-02, -5.0930e-01, -2.2989e-01,
        -1.3395e-01, -7.2049e-02,  5.1076e-01, -1.9724e-01,  1.3791e-01,
         2.4521e-01,  4.4357e-02, -7.0624e-01, -1.9231e-01, -4.9890e-01,
        -3.3431e-01,  9.3956e-02,  3.9203e-02, -3.3491e-01, -7.7078e-01,
        -3.9517e-01, -7.0076e-03, -4.3714e-01, -5.3338e-01, -1.1568e-01,
        -7.7663e-01, -1.9353e-01,  1.6875e-01,  1.5197e-01,  3.1701e-01,
        -2.0656e-01,  2.4972e-01,  6.6190e-02, -2.9960e-02, -8.0230e-01,
        -4.1852e-02,  2.3524e-02,  3.8774e-01, -3.4516e-01,  2.3149e-01,
        -8.1876e-02, -7.5342e-04,  2.5952e-01,  1.2734e-01, -1.9654e-01,
        -2.9427e-02, -4.5514e-02,  5.0196e-01,  4.5032e-01,  4.2538e-01,
         2.0322e-01,  7.1951e-01, -5.9948e-01,  3.0338e-01, -2.7482e-02,
        -9.1157e-02, -2.4218e-01, -2.1302e-02,  2.9174e-01,  3.2023e-02,
         4.5233e-02, -1.0554e-01,  4.9416e-01,  1.0603e-01, -2.3242e-01,
        -1.0399e-01, -2.7018e-01,  2.0762e-01,  3.4917e-01, -7.6422e-02,
         1.4725e-01, -5.7476e-02, -7.6483e-02,  3.3816e-01, -5.8852e-01,
         1.6630e-02,  8.2448e-03], device='cuda:0', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1347,  0.2989, -0.5339,  ..., -0.5265, -0.2761, -0.0241],
        [-0.4257,  0.3315,  0.2079,  ..., -0.1017,  0.1317,  0.5264],
        [-0.6097, -0.1783,  0.2613,  ...,  0.3783, -0.4814, -0.0344],
        ...,
        [ 0.0203, -0.2584, -0.2590,  ..., -0.4645, -0.5785, -0.1913],
        [-0.1921,  0.9825,  0.0440,  ...,  0.2357, -0.6674,  0.3438],
        [ 0.4051, -0.2642,  0.7370,  ...,  0.3095,  0.2294,  0.2429]],
       device='cuda:0', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 5.1383e-01,  2.2676e-01, -1.8156e-01, -2.9724e-01, -1.5685e-01,
         6.0414e-01,  6.1232e-02,  2.7250e-02, -2.3418e-01, -4.5196e-02,
        -2.3660e-01, -6.8167e-01,  5.7750e-02,  3.6466e-01,  3.1874e-01,
         5.3432e-02,  1.8438e-01,  8.8736e-02,  3.0512e-01,  5.2947e-02,
         8.0293e-03,  6.4511e-02,  1.7163e-01,  7.7425e-02,  2.7578e-01,
        -2.7774e-01,  6.8803e-01,  4.6933e-01,  2.0090e-01, -4.3950e-01,
        -2.8161e-01,  3.8266e-01, -3.2890e-02, -6.4451e-01,  6.4740e-01,
        -2.4258e-01, -5.4273e-01, -2.0864e-01, -1.6346e-01,  4.2535e-01,
         6.1291e-02, -1.3541e-01,  3.5634e-01,  2.0267e-01, -3.8041e-01,
         4.7205e-01, -1.3605e-02,  3.2818e-01, -5.6446e-01, -2.6980e-01,
        -3.6690e-01,  1.6149e-01, -2.4639e-01, -4.7106e-02,  4.9669e-02,
        -1.6383e-01, -6.6454e-01, -2.0559e-01,  3.5866e-01, -6.5202e-01,
        -2.5192e-01, -1.2759e-01, -1.0010e-01,  5.5464e-01,  2.6002e-01,
         6.1209e-01,  4.5297e-02, -2.3984e-01,  2.4185e-01, -5.3261e-01,
        -1.5795e-03,  3.7480e-01,  2.2998e-01,  2.5373e-01,  6.2845e-01,
        -4.8846e-01, -2.6556e-01,  1.0543e-01, -1.7961e-01, -4.7442e-02,
         2.0987e-01,  1.4915e-01, -5.4043e-01,  6.3419e-01, -1.4536e-02,
        -1.3952e-01,  1.4541e-03, -1.5868e-01, -4.4381e-02, -7.2536e-03,
         3.0550e-01,  1.7987e-01,  2.4523e-01, -2.6376e-01, -5.4468e-01,
         8.4813e-02, -1.6345e-01, -9.2069e-02,  3.3448e-01,  8.8672e-02,
        -4.2712e-01, -7.8307e-02,  1.5671e-01, -1.4268e-01, -2.6539e-01,
         1.5692e-02, -1.4193e-01,  6.3396e-01, -1.6775e-01, -4.3899e-01,
        -2.9766e-01,  2.0886e-01,  6.2453e-02,  1.4515e-01, -1.7576e-01,
        -5.2782e-01,  2.8647e-01, -3.6118e-01, -1.1295e-01, -1.5333e-01,
         3.8649e-02, -2.3035e-01,  4.8449e-01, -6.5390e-01,  1.7446e-01,
        -2.0998e-01, -2.4014e-01,  3.8094e-01,  5.4638e-01, -9.2301e-02,
         7.2799e-01, -5.4894e-02, -2.6638e-01,  1.8212e-01,  6.3314e-02,
         4.4116e-02, -1.3119e-01,  7.7694e-01,  1.6987e-02, -1.0065e-01,
        -4.2424e-01,  5.3672e-01, -1.0983e-01,  1.8426e-01, -1.5716e-01,
         1.5363e-01,  9.4920e-02, -3.5707e-01, -3.0785e-02, -4.7559e-02,
        -6.1601e-01,  1.6270e-01,  1.1011e-01, -1.8307e-01,  2.0297e-01,
        -3.7307e-01,  1.1780e-01, -8.7353e-01, -6.3581e-01, -9.1177e-02,
         4.5015e-01,  5.8748e-01, -2.0362e-02,  5.2032e-01,  1.2865e-01,
        -2.7387e-01,  3.3855e-01, -8.1991e-02, -3.3930e-02, -2.2129e-01,
        -1.1106e-01,  2.9059e-01,  3.2387e-01, -3.3815e-02, -3.7728e-01,
         5.8272e-01,  6.2464e-01, -2.5074e-01, -4.3780e-01,  5.6216e-01,
         1.2451e-01, -1.2899e-01,  1.2758e-01, -9.5528e-02,  2.0078e-01,
        -2.1147e-01, -1.9996e-01,  2.1658e-01, -4.6028e-01,  1.7152e-01,
        -1.7944e-01,  2.5667e-02,  2.6994e-01, -3.3626e-01, -3.3286e-01,
         2.1351e-01,  3.3059e-01,  8.8407e-02,  1.1184e-01, -9.3410e-01,
        -3.5365e-01, -4.0851e-01,  1.9238e-01,  1.9179e-01,  6.0050e-01,
        -6.1300e-01, -7.7526e-02,  4.3369e-01,  1.9656e-01,  7.5422e-02,
         1.2600e-01,  1.8432e-01,  1.6680e-01,  9.4564e-02,  2.1989e-01,
         6.5338e-02,  3.0670e-01,  2.1004e-01, -3.4678e-02,  3.7545e-01,
        -7.9084e-02, -6.3368e-02,  5.0477e-01,  2.8279e-01, -6.6957e-02,
        -8.2414e-02, -4.5131e-01,  8.7372e-01, -1.2054e-01,  2.9910e-01,
        -7.9745e-04,  1.4130e-01,  2.4211e-01,  1.4722e-01,  2.2276e-02,
         6.4288e-01,  1.9376e-01,  2.3107e-01,  4.2620e-02, -1.0629e-01,
         4.9965e-01,  1.2873e-01, -5.4799e-02,  4.2276e-01, -2.7031e-01,
        -1.9287e-01, -2.8321e-01, -4.5494e-01, -1.1602e-01, -1.0290e-01,
        -1.4740e-01,  4.4534e-01,  5.1917e-01, -1.4431e-01, -7.7550e-01,
         1.6413e-01, -2.1547e-01,  3.4814e-01,  2.9550e-02,  1.2023e-01,
         2.7038e-01, -2.4514e-01, -2.5994e-01, -4.0943e-01,  6.1809e-01,
        -6.4172e-02,  1.4789e-01,  4.9981e-01,  5.8535e-01,  2.1067e-01,
        -7.5197e-02, -2.0989e-01, -6.3907e-02, -4.6315e-01,  2.4584e-01,
        -6.1935e-01,  1.9323e-02,  5.7194e-01, -2.9602e-01, -5.6871e-02,
        -4.8013e-01, -2.9334e-02,  4.9256e-02, -4.2844e-01,  1.4215e-01,
        -1.9628e-01, -5.6353e-01,  2.3770e-01, -1.9504e-01, -6.0946e-02,
         8.6788e-01,  1.5729e-01, -3.3453e-01,  4.2605e-02,  3.4771e-01,
         2.2894e-01,  8.9872e-02,  2.3474e-01,  3.7149e-01, -2.5347e-01,
         1.2892e-01, -3.7038e-01,  6.8718e-02, -2.8476e-01,  3.6732e-01,
        -1.4778e-01,  3.5754e-01,  5.1770e-02, -1.2465e-01,  1.4894e-02,
        -5.7676e-03, -4.7557e-01, -5.1856e-01,  1.0399e-01,  8.1136e-02,
         5.3078e-01, -4.8298e-01,  2.6217e-01,  1.1708e-01, -2.2655e-02,
         3.2414e-01,  3.2335e-01, -3.2487e-01, -3.6452e-01,  4.9279e-01,
         2.7096e-01, -2.5982e-01, -2.0117e-01, -2.4121e-01, -1.1426e-01,
        -7.0331e-02, -3.8127e-01, -3.8204e-01, -1.4516e-01, -3.1634e-01,
         2.4916e-01,  2.1327e-01, -2.6620e-01, -1.7288e-01,  2.1561e-02,
         2.7197e-01, -1.3148e-01,  8.5358e-02, -2.5469e-01,  1.9918e-01,
        -8.2631e-02, -3.2942e-01,  4.1063e-01, -2.2063e-01,  1.4496e-01,
         1.9767e-01, -1.2407e-01, -1.5407e-01, -5.4703e-01,  7.7805e-02,
        -4.6431e-01, -1.4431e-01,  9.7252e-02,  6.0515e-01, -5.5042e-01,
        -2.1915e-01, -2.3620e-01,  6.5295e-02,  8.8855e-01,  1.2050e-01,
        -2.8610e-01,  1.6722e-01,  4.6341e-01,  9.4008e-03, -9.6328e-02,
        -3.3636e-01,  4.1791e-01,  1.4727e-01,  4.5041e-01, -4.3296e-01,
        -3.3456e-02,  6.8088e-02, -1.9341e-01, -2.2595e-01,  1.5026e-01,
        -1.9233e-01, -2.3071e-02, -1.5759e-01,  2.4411e-01, -1.4933e-01,
        -7.8673e-02, -5.1893e-01, -1.8274e-01, -3.6652e-01,  3.9821e-01,
        -1.1308e-03,  1.1077e-01,  1.1272e-01,  1.4247e-01, -2.5590e-01,
        -2.6423e-01, -3.1982e-01,  5.4112e-01,  4.2725e-01,  4.2929e-02,
        -1.4493e-01,  4.0068e-01, -1.5884e-01, -4.5532e-01, -3.6434e-01,
        -2.6256e-01, -1.9314e-01, -3.0976e-01,  2.1947e-01,  1.2527e+00,
         4.6440e-02,  5.4746e-02, -4.3317e-02,  3.7618e-01, -3.7077e-01,
        -2.0447e-01, -1.2383e-01,  2.0692e-01,  1.3606e-01,  1.9516e-01,
         2.2275e-01,  1.7480e-01,  3.2938e-01, -5.1290e-02, -7.7177e-01,
        -2.7293e-01,  3.2536e-01, -1.2914e-01,  8.3979e-02,  3.6308e-02,
        -3.2365e-02, -2.9454e-01,  3.3357e-01, -1.0323e-01,  2.0189e-01,
         2.5102e-02,  4.2577e-01, -3.1884e-01,  4.0641e-01,  8.1616e-03,
         3.8972e-01, -1.5253e-01,  6.5812e-01, -3.0501e-01,  1.1704e-01,
        -3.4881e-01,  1.1053e-02, -3.0909e-01, -7.7253e-02,  3.6673e-01,
        -7.6565e-02,  9.1087e-03, -1.7329e-01,  2.6309e-01,  8.9734e-02,
        -4.5330e-01,  1.5570e-02, -4.8282e-01,  4.0811e-02, -3.2425e-02,
        -3.0552e-01,  9.6641e-02, -1.8799e-02,  1.7777e-02,  7.3005e-01,
         1.4098e-01,  1.0050e-01, -1.0706e-01, -1.7136e-01,  2.0919e-01,
         2.9493e-01, -8.6930e-01, -8.4521e-02, -4.6610e-01,  4.6881e-01,
         2.8357e-01,  2.9123e-01,  5.6292e-01, -7.6437e-01,  9.9124e-02,
        -1.5738e-01,  4.0906e-01,  1.5995e-01, -3.8363e-01,  1.4078e-02,
         2.1916e-01,  2.0725e-01, -1.0849e-01, -4.3832e-01, -4.3885e-01,
        -2.4597e-02,  3.0685e-01, -4.8883e-01, -4.4606e-02, -8.3244e-01,
        -3.0004e-01,  3.6187e-01, -1.8884e-01,  1.1498e-01, -2.2350e-01,
         1.8464e-01, -5.0595e-02,  3.7592e-01, -1.6456e-01,  9.8069e-02,
         2.6785e-01,  2.1655e-01,  2.4135e-01, -4.7972e-01,  1.9272e-01,
         1.7350e-01, -4.1612e-01], device='cuda:0', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1730, -0.2277, -0.1308,  ..., -0.0064,  0.0714,  0.0089],
        [-0.3097, -0.3952,  0.1457,  ...,  0.1606, -0.3123,  0.2392],
        [-0.3884,  0.1280, -0.2651,  ..., -0.3274,  0.0380,  0.5866],
        ...,
        [-0.1138,  0.1471, -0.2607,  ..., -0.1059,  0.1240, -0.2748],
        [ 0.1022, -0.0441,  0.4888,  ...,  0.2420, -0.3309, -0.0214],
        [ 0.3723, -0.2546, -0.5360,  ...,  0.0103,  0.2929, -0.4245]],
       device='cuda:0', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 1.4179e-01,  3.2210e-01, -5.6643e-01,  2.7856e-01,  4.0780e-01,
        -5.2465e-01, -4.4860e-01,  6.3692e-01,  1.2261e-01,  3.3203e-01,
         1.1851e-01,  3.5166e-01,  4.1000e-01, -6.7416e-02,  5.7536e-01,
         9.8060e-02,  4.7483e-02, -9.5358e-02,  5.6268e-02,  4.9899e-02,
         2.9323e-01,  5.6371e-01,  2.5355e-01, -7.4552e-02, -3.9356e-01,
         1.9312e-01, -4.2989e-03, -7.0626e-02, -1.7859e-01,  4.3795e-01,
        -3.8399e-01,  5.2805e-02, -1.2964e-01,  5.6573e-02, -1.4092e-01,
         1.6990e-01,  6.7457e-02, -3.0747e-01,  4.2022e-01,  3.0776e-01,
        -3.1603e-02,  1.7747e-01,  1.3542e-01, -2.8546e-01,  6.4476e-01,
         2.1349e-02, -7.8576e-01,  4.7399e-02, -1.9146e-02,  8.6159e-04,
         4.6737e-01,  3.0309e-01, -8.5541e-02, -2.5931e-02, -1.7506e-01,
        -3.3630e-01,  1.6496e-01, -2.1160e-01,  8.8874e-02,  9.7483e-02,
         1.7767e-01,  8.5455e-01,  3.7553e-01,  1.7289e-01,  3.1729e-01,
        -1.1959e-01,  6.3505e-01, -3.6701e-01, -3.2456e-01,  4.0400e-01,
         3.2255e-01, -6.0352e-01,  2.1656e-02, -3.1416e-01, -3.4468e-01,
        -3.2838e-01,  1.2244e-01,  2.3571e-01,  5.9361e-01,  2.3576e-01,
         2.9457e-01,  2.5872e-01,  5.8149e-01, -4.4340e-01, -5.3872e-01,
        -2.0802e-01,  4.6460e-01, -2.5942e-01, -1.6369e-01, -3.9992e-01,
        -4.5108e-01,  1.5733e-01,  1.2554e-03,  1.2605e-01,  1.7727e-01,
        -1.0899e-01,  4.6134e-01, -1.1725e-01, -1.8230e-01, -3.0823e-01,
         1.0486e-02,  6.9209e-04, -1.7649e-01,  2.0212e-01, -1.1128e-01,
        -2.9200e-01, -4.3355e-01, -2.6220e-02, -3.9349e-01,  7.9136e-02,
         1.3552e-01, -5.1725e-01, -1.9342e-01, -9.7791e-02,  7.8979e-03,
        -3.1055e-01, -3.4341e-01, -1.3593e-01,  1.9896e-01, -1.7927e-01,
        -5.4761e-01, -2.2198e-01,  4.2610e-01,  8.2436e-02, -2.0173e-01,
         2.4040e-02, -1.8343e-01, -2.1978e-01,  3.5884e-01, -7.7458e-01,
         2.3324e-01,  1.3607e-01, -3.9449e-01,  4.1063e-01, -5.3288e-01,
         1.1797e-01, -3.6661e-01,  5.9781e-01, -2.7411e-01,  8.7778e-02,
         1.4884e-01, -3.0281e-01, -6.3868e-01, -1.1967e-01,  2.4771e-01,
         5.4744e-01, -1.4941e-01,  2.3260e-02,  3.9616e-04, -4.2770e-01,
        -4.0471e-01, -2.0044e-01,  3.2872e-01,  4.6651e-02,  2.4371e-01,
         1.9984e-01,  1.5637e-01, -8.4627e-02,  8.0435e-02,  1.6755e-01,
         2.4172e-01, -5.1212e-01,  1.8761e-01, -2.4661e-01,  1.5017e-01,
        -4.1426e-01,  3.1818e-01, -2.4743e-02, -6.2680e-02,  1.5470e-01,
         1.4934e-01,  2.2034e-01,  1.2142e-01,  2.1281e-01,  5.2308e-01,
         1.7412e-01,  2.7777e-01,  1.9550e-01,  5.3673e-01,  1.1650e-01,
        -1.3916e-01, -6.9720e-01,  6.8880e-01,  2.5903e-01, -1.0851e-01,
         1.5277e-01,  3.2483e-01, -2.9587e-02, -2.5160e-01, -8.1416e-02,
         7.9530e-02, -2.6971e-01,  7.0683e-01, -9.2652e-02, -2.7599e-01,
        -2.8310e-01, -3.0244e-01,  2.5042e-01, -2.0662e-01, -1.3467e-01,
        -3.8732e-01, -3.3861e-01, -2.8108e-01, -3.0866e-01, -6.7359e-02,
         3.5845e-01, -1.1492e-01, -2.6174e-01,  7.9780e-01,  6.6983e-02,
        -3.5975e-01,  4.4830e-01,  2.3262e-01, -3.8860e-01,  3.2931e-01,
        -5.0148e-01,  3.9907e-01,  6.3266e-01,  1.0864e-01,  3.5626e-02,
         4.6151e-01, -4.1601e-01,  2.0126e-01,  2.8901e-01, -2.9050e-01,
         1.5636e-01,  1.5314e-01, -2.6690e-01,  2.0631e-01, -1.2344e-01,
         5.0111e-02, -2.8826e-01, -1.3203e-01, -1.0933e-01, -1.2740e-01,
        -1.6363e-01, -3.2539e-02, -4.1435e-02,  2.0555e-01,  2.7631e-01,
        -1.3128e-01,  1.9418e-01,  2.1531e-01,  3.0464e-01,  1.2553e-01,
        -4.3558e-03,  3.5148e-03,  4.5660e-02, -2.6346e-01, -1.2613e-01,
        -1.6006e-01,  2.5396e-01,  2.3544e-03,  2.7709e-01,  2.7365e-01,
         1.5975e-01, -2.0429e-01,  2.6308e-01,  1.1232e-01, -1.7520e-01,
        -2.9343e-02,  1.9986e-01,  3.7351e-01,  1.8321e-03,  2.8144e-01,
         1.6961e-01,  1.1166e-01, -4.7342e-01,  2.5215e-01,  1.2465e-01,
        -1.7574e-01, -2.3487e-01, -3.2847e-02, -3.3511e-01, -1.6397e-01,
         9.2866e-02,  6.8842e-01, -1.2720e-01,  3.8358e-03, -1.8727e-01,
         1.1597e-01,  1.0454e-01, -2.6154e-01,  2.1201e-01, -4.5223e-01,
         2.0397e-01, -6.3676e-01, -1.0293e-01, -2.7385e-01, -3.9719e-01,
         1.6923e-01,  8.8046e-02,  1.1130e-01, -3.8459e-01, -3.7879e-01,
         5.1172e-01,  4.1529e-01, -3.8174e-01, -1.2938e-01,  3.2678e-01,
         4.4141e-01,  5.7247e-01,  5.7335e-01,  2.2937e-01,  1.7478e-01,
         3.0146e-01,  2.0555e-01, -8.7688e-01,  2.6807e-01,  3.9275e-01,
         3.8590e-02, -3.1944e-01, -5.3004e-02,  1.4634e-02,  1.4402e-01,
        -2.0849e-01,  1.3237e-01,  1.5445e-01, -2.2459e-02,  2.4120e-01,
         1.2028e-01, -1.7279e-01,  4.2801e-01, -7.6152e-01, -2.1103e-01,
         6.2899e-01, -1.6014e-01,  5.1742e-02,  4.7674e-01,  5.3797e-02,
        -1.6155e-01,  1.0281e-01, -5.7464e-01,  2.1592e-01, -1.2658e-01,
         4.8350e-02,  3.5443e-02,  1.1428e-01, -4.5882e-01, -9.1370e-02,
        -8.0386e-02, -5.6888e-02,  7.8916e-02,  1.6889e-02,  9.4458e-02,
        -1.0846e-02,  1.4602e-01,  2.9974e-01, -1.2817e-01,  1.4236e-01,
        -5.1285e-01, -4.5173e-01,  3.7114e-01, -6.4035e-02,  3.9342e-01,
         3.0148e-01, -8.0483e-02, -7.5345e-02, -3.7438e-01, -1.1560e-01,
        -7.6414e-01,  1.1941e-01,  3.2946e-01,  1.6418e-01,  2.5149e-01,
         2.8158e-01, -3.1064e-01, -2.1117e-01, -3.6134e-01, -1.7946e-01,
        -3.8067e-01, -1.8442e-02,  5.4178e-01,  1.0826e-01, -4.2489e-01,
         9.8786e-02, -1.9886e-01, -1.9944e-01, -2.6189e-01,  3.2915e-01,
         2.7132e-01, -2.7107e-01,  1.9721e-01, -3.4363e-02, -3.4736e-01,
         7.6883e-01,  7.6609e-01,  1.3300e-01,  1.0317e-01, -9.9651e-02,
        -1.5886e-01,  3.3379e-01,  1.1817e-01,  6.1088e-01, -9.8981e-02,
         3.2033e-02, -3.2494e-01,  6.3448e-01, -1.8965e-02, -1.0558e-01,
         1.8887e-01,  2.4920e-02, -4.7867e-01,  1.6645e-03,  1.6404e-01,
        -2.1174e-01,  5.4358e-01, -1.9561e-01, -6.2061e-01, -5.3127e-01,
         1.8306e-01,  5.1041e-01,  2.6210e-01,  4.7968e-01,  4.0350e-01,
         1.7007e-01, -6.5120e-02,  3.2445e-02, -2.4497e-01,  3.8961e-02,
         3.3875e-01, -2.3760e-01,  4.1712e-01,  1.6046e-01, -2.4134e-01,
         1.4197e-01, -1.9834e-01,  4.0619e-01,  2.6777e-01,  2.4601e-01,
         9.6592e-02,  2.2297e-02,  4.6209e-01,  3.8926e-01,  1.6503e-01,
         1.6308e-01, -2.7902e-01, -1.7507e-01, -1.0870e-01,  4.1737e-02,
         1.2515e-01, -1.6289e-01,  2.8835e-01, -5.0282e-01,  2.9335e-01,
         5.7595e-01, -2.0366e-01,  4.6764e-02, -2.6312e-01,  4.7355e-01,
        -3.3565e-01,  2.4745e-01, -8.0281e-02,  3.2135e-02, -6.0093e-01,
         4.9367e-01, -3.1144e-01,  2.6010e-01,  6.3402e-02, -1.1066e-02,
        -2.8310e-01, -5.9138e-01,  1.9894e-02, -5.6878e-01,  4.3710e-01,
        -1.8511e-01,  1.4252e-01, -4.7774e-01, -1.0077e-01, -2.5265e-01,
         1.0201e-01, -2.4135e-01, -3.3036e-01,  4.7532e-02,  1.9244e-01,
        -1.9187e-01,  3.5796e-02,  2.8442e-01, -2.9295e-01,  2.3329e-02,
         2.8451e-01, -1.1799e-01,  2.6453e-01,  1.4019e-01, -7.1528e-02,
         1.0160e+00,  2.1072e-01,  1.9396e-01, -1.2166e-01, -6.4238e-02,
        -1.6593e-01,  2.4005e-01, -2.9599e-01, -1.6909e-01,  9.1479e-03,
        -5.0724e-02, -3.1212e-01,  4.8560e-01,  3.9701e-01,  1.2635e-01,
        -5.3069e-01, -8.0266e-02,  3.7003e-01, -3.5217e-01,  8.7458e-02,
         5.4936e-01, -4.9864e-01,  1.0496e-01,  1.4389e-01, -7.2095e-01,
         1.7354e-01, -1.0492e-01], device='cuda:0', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 0.7570,  0.0071,  0.1389,  0.6667,  0.1712,  0.3561,  0.6226, -0.3891,
          0.5041, -0.1839, -0.3202, -0.1969,  0.1615,  0.0672,  0.6710, -0.3528,
         -0.3141, -0.2882, -0.8871,  0.1790, -0.3063,  0.4413, -1.0605, -0.2951,
         -0.3255, -0.0646,  0.2149, -0.5881, -0.2235, -0.0753,  0.5737, -0.5544,
         -0.0257, -0.1990, -0.4254, -0.1341,  0.0489, -0.3904, -0.2764, -0.3566,
          0.1979,  0.6079, -0.1551, -0.4622,  0.5872, -0.5354, -0.3222, -0.0805,
         -0.3098, -0.0448,  0.7059,  0.6188,  0.2833,  0.2381, -0.2853, -0.1509,
         -0.4050, -0.2043, -0.2229, -0.4423,  0.1936, -0.2756, -0.6272, -0.0563,
          0.0129,  0.0949,  0.0776,  0.0665,  0.0221, -0.1633, -0.2864,  0.3297,
          0.3653,  0.2385, -0.1601,  0.0213,  0.0414,  0.4344, -0.1426,  0.3209,
         -0.1330, -0.0715,  0.1667, -0.1738, -0.5722, -0.5953, -0.0185, -0.6191,
          0.4194,  0.3419,  0.0859,  0.2053,  0.2619, -0.2133, -0.4736,  0.0040,
         -0.3127, -0.3731, -0.2702,  0.1135,  0.2918,  0.2723, -0.2393, -0.0356,
          0.3593, -0.0179, -0.0283,  0.2146,  0.2183,  0.4363, -0.0966,  0.5011,
          0.4738, -0.2301,  0.3377,  0.5182,  0.2462, -0.2835, -0.2046, -0.0658,
          0.4032,  0.2224, -0.2755,  0.8335,  0.2519, -0.2333,  0.0282,  0.0173,
         -0.2267,  0.3252, -0.0499,  0.0456, -0.2739,  0.3119, -0.1075, -0.3342,
          0.2032, -0.3366, -0.2031,  0.1704, -0.1464, -0.3186,  0.1348, -0.2662,
         -0.4975,  0.1851,  0.2307,  0.0125, -0.1342,  0.5029, -0.3441, -0.0833,
         -0.5997,  0.1109,  0.1963,  0.0987, -0.6184,  0.0578,  0.1665,  0.3195,
          0.2226,  0.3615, -0.7867, -0.3353, -0.0744,  0.2562, -0.1247,  0.2649,
         -0.3539, -0.0216,  0.0795,  0.0297, -0.0140,  0.0654,  0.0853, -0.0177,
          0.1761, -0.1112,  0.1291, -0.4507, -0.3451, -0.2394,  0.4435,  0.4105,
         -0.4136, -0.0931, -0.5896,  0.0572, -0.3367, -0.3010,  0.0908, -0.1063,
         -0.5687,  0.1835, -0.1890, -0.0747,  0.2271,  0.1515, -0.0937,  0.0105,
         -0.5979,  0.5121,  0.2226,  0.2394, -0.1796, -0.5596, -0.1476, -0.3604,
          0.0613, -0.6463, -0.2884,  0.0498,  0.0476, -0.1238,  0.0033, -0.1810,
          0.2991, -0.7201, -0.0066,  0.6760,  0.0838, -0.2200,  0.5599, -0.4012,
          0.0079,  0.4337,  0.0768,  0.5233,  0.0051, -0.1023, -0.2747,  0.1599,
         -0.0071,  0.3136,  0.1999, -0.0581,  0.1474, -0.4202,  0.3128, -0.0721,
          0.1800, -0.6434,  0.2795,  0.0963,  0.1675, -0.2505,  0.5946,  0.0881,
         -0.3420, -0.4583,  0.2318, -0.1727,  0.3149, -0.2287, -0.0798, -0.1796,
          0.2559,  0.4442, -0.1899, -0.2371, -0.5489, -0.4815,  0.0435, -0.3169,
          0.4664,  0.0213,  0.0719, -0.0919,  0.0049,  0.0717, -0.2498, -0.1512,
          0.0769, -0.3439,  0.0776, -0.3771,  0.3978, -0.3865,  0.0013,  0.3145,
         -0.2653,  0.3279, -0.5856, -0.6437, -0.3371,  0.2609, -0.0104, -0.0156,
          0.1098,  0.2341,  0.1565,  0.2076,  0.0886, -0.3759, -0.1986,  0.2679,
          0.2184,  0.0187, -0.2609, -0.0485,  0.2321,  0.2120, -0.3796, -0.1598,
         -0.2731, -0.2402, -0.7589, -0.0833,  0.2278,  0.1961,  0.3413, -0.0987,
          0.4173, -0.0623, -0.5167, -0.5240, -0.3195, -0.2283,  0.4674, -0.0255,
         -0.5547,  0.4798, -0.4382, -0.0420, -0.1176,  0.0931, -0.0501,  0.4722,
          0.6665, -0.0861,  0.6480, -0.0445, -0.3785, -0.0294,  0.3172, -0.1142,
          0.0714, -0.1573,  0.0901, -0.3999,  0.2734,  0.3352,  0.1221,  0.1154,
         -0.2944,  0.4117, -0.2558, -0.0271, -0.0529, -0.1143, -0.3698,  0.3019,
         -0.3937, -0.0406,  0.4142, -0.0227,  0.1544,  0.1405,  0.6259, -0.3281,
         -0.1419,  0.6239,  0.0778, -0.2243, -0.2499,  0.0725, -0.4267, -0.4464,
          0.5441, -0.0022, -0.2501,  0.1621, -0.2522,  0.2555, -0.1617, -0.1284,
         -0.0315,  0.1421, -0.1258, -0.0346,  0.0374,  0.2211, -0.6120,  0.1248,
         -0.3892, -0.1977,  0.4000,  0.2652,  0.1389, -0.1081,  0.6099,  0.2467,
         -0.0990,  0.2355,  0.4636, -0.2268,  0.4451, -0.2121, -0.5002, -0.0408,
          0.2536,  0.3768, -0.6709,  0.2756, -0.2226, -0.4895, -0.5475,  0.0174,
          0.4232,  0.3590,  0.6767,  0.3510, -0.0845, -0.0845, -0.4329, -0.2155,
         -0.0831,  0.1408,  0.3465,  0.3161,  0.3256,  0.1817, -0.4837,  0.2695,
         -0.1407, -0.2695,  0.2053,  0.8556, -0.1175, -0.1025,  0.1077,  0.5011,
         -0.3269,  0.2269, -0.1758, -0.2570,  0.0990, -0.4238, -0.2143, -0.4339,
          0.5493,  0.0367, -0.0246,  0.2124,  0.3997, -0.2968, -0.6228,  0.4566,
          0.2689, -0.6811,  0.2121,  0.0886,  0.2624,  0.4336,  0.1599,  0.4324,
          0.5957, -0.5103,  0.8678, -0.3761, -0.3902, -0.2045, -0.1148,  0.1110,
         -0.3288, -0.1500,  0.7500, -0.5143,  0.4097, -0.2637,  0.2661, -0.3321,
          0.1945,  0.0906, -0.0694,  0.2230, -0.0518,  0.0263,  0.7312,  0.0204,
         -0.3837, -0.0236, -0.0973,  0.4279, -0.1067, -0.4604,  0.3151,  0.2388,
          0.0919,  0.3406, -0.2363, -0.3606,  0.5200,  0.1322, -0.2743,  0.1592,
         -0.4984,  0.0436, -0.2330, -0.0258,  0.0537, -0.4252,  0.3755, -0.1834,
         -0.2058, -0.5455,  0.1601, -0.1842, -0.2745, -0.3696, -0.3067,  0.3493]],
       device='cuda:0', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.1671], device='cuda:0', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:0', grad_fn=<AddBackward0>)
Using device: cuda:0
===== Training profile tensin-3x512-s003 - 1 =====
[0:00:04.683674] epoch: 0 | elbo: 40205175398.4 | train_rmse: 66.8983 | val_rmse: 69.1621 | val_ll: -30.9518
[0:03:59.203286] epoch: 50 | elbo: 1809336570.8800004 | train_rmse: 16.3065 | val_rmse: 19.6252 | val_ll: -5.9635
[0:08:01.911139] epoch: 100 | elbo: 994756677.1200001 | train_rmse: 11.206 | val_rmse: 16.4826 | val_ll: -5.4194
[0:11:57.613918] epoch: 150 | elbo: 666798759.6800001 | train_rmse: 8.4345 | val_rmse: 14.9944 | val_ll: -5.2179
[0:15:53.039178] epoch: 200 | elbo: 484204867.84 | train_rmse: 6.5782 | val_rmse: 14.1475 | val_ll: -5.1864
[0:19:57.404469] epoch: 250 | elbo: 371446455.36 | train_rmse: 5.1883 | val_rmse: 13.5066 | val_ll: -5.1509
[0:22:44.830532] epoch: 300 | elbo: 293316760.96 | train_rmse: 4.1393 | val_rmse: 13.0476 | val_ll: -5.233
[0:24:15.814713] epoch: 350 | elbo: 241402227.03999996 | train_rmse: 3.3254 | val_rmse: 12.6847 | val_ll: -5.3125
[0:25:48.298792] epoch: 400 | elbo: 198368562.23999995 | train_rmse: 2.6234 | val_rmse: 12.3241 | val_ll: -5.4329
[0:27:19.435006] epoch: 450 | elbo: 167847764.0 | train_rmse: 2.1077 | val_rmse: 12.0373 | val_ll: -5.5559
[0:28:52.138872] epoch: 500 | elbo: 143480067.04 | train_rmse: 1.6879 | val_rmse: 11.7434 | val_ll: -5.677
[0:30:22.936439] epoch: 550 | elbo: 124420114.08 | train_rmse: 1.4035 | val_rmse: 11.4774 | val_ll: -5.8113
[0:31:53.613066] epoch: 600 | elbo: 108792427.44000001 | train_rmse: 1.1457 | val_rmse: 11.1807 | val_ll: -6.0037
[0:33:24.609083] epoch: 650 | elbo: 95448897.91999999 | train_rmse: 0.959 | val_rmse: 10.8923 | val_ll: -6.1776
[0:34:55.866186] epoch: 700 | elbo: 84425770.88000001 | train_rmse: 0.8541 | val_rmse: 10.6015 | val_ll: -6.3632
[0:36:27.081188] epoch: 750 | elbo: 74699777.91999999 | train_rmse: 0.744 | val_rmse: 10.3245 | val_ll: -6.6257
[0:37:59.069056] epoch: 800 | elbo: 65613852.519999996 | train_rmse: 0.7638 | val_rmse: 10.0732 | val_ll: -6.812
[0:39:29.930730] epoch: 850 | elbo: 58018309.11999999 | train_rmse: 0.6297 | val_rmse: 9.7866 | val_ll: -7.0597
[0:40:59.055368] epoch: 900 | elbo: 52285481.24 | train_rmse: 0.5948 | val_rmse: 9.515 | val_ll: -7.3862
[0:42:30.486169] epoch: 950 | elbo: 46605121.39999999 | train_rmse: 0.53 | val_rmse: 9.2851 | val_ll: -7.7687
[0:44:03.020051] epoch: 1000 | elbo: 41240686.760000005 | train_rmse: 0.5798 | val_rmse: 9.0468 | val_ll: -8.0517
[0:45:36.625559] epoch: 1050 | elbo: 37293834.39999999 | train_rmse: 0.4846 | val_rmse: 8.8414 | val_ll: -8.4208
[0:47:08.213382] epoch: 1100 | elbo: 33329332.46 | train_rmse: 0.4503 | val_rmse: 8.6108 | val_ll: -8.9037
[0:48:39.765488] epoch: 1150 | elbo: 29685056.480000008 | train_rmse: 0.4364 | val_rmse: 8.4123 | val_ll: -9.3265
[0:50:11.044441] epoch: 1200 | elbo: 26971829.920000006 | train_rmse: 0.4765 | val_rmse: 8.2219 | val_ll: -9.8522
[0:51:42.640436] epoch: 1250 | elbo: 24399548.339999996 | train_rmse: 0.438 | val_rmse: 8.0274 | val_ll: -10.2941
[0:53:13.452486] epoch: 1300 | elbo: 21855795.46 | train_rmse: 0.3744 | val_rmse: 7.8626 | val_ll: -10.7662
[0:54:44.245560] epoch: 1350 | elbo: 19856354.740000002 | train_rmse: 0.3544 | val_rmse: 7.7057 | val_ll: -11.4491
[0:56:15.177032] epoch: 1400 | elbo: 18225230.42 | train_rmse: 0.3572 | val_rmse: 7.5549 | val_ll: -12.1152
[0:57:46.390556] epoch: 1450 | elbo: 16514270.440000001 | train_rmse: 0.3263 | val_rmse: 7.4131 | val_ll: -12.8551
[0:59:16.835018] epoch: 1500 | elbo: 15168923.65 | train_rmse: 0.3151 | val_rmse: 7.2685 | val_ll: -13.606
[1:00:47.662488] epoch: 1550 | elbo: 14084650.959999997 | train_rmse: 0.3381 | val_rmse: 7.1387 | val_ll: -14.3478
[1:02:19.190233] epoch: 1600 | elbo: 12815491.110000001 | train_rmse: 0.2752 | val_rmse: 7.0126 | val_ll: -15.2979
[1:03:50.405984] epoch: 1650 | elbo: 11764492.68 | train_rmse: 0.2998 | val_rmse: 6.9053 | val_ll: -16.3897
[1:05:22.478814] epoch: 1700 | elbo: 10857436.190000001 | train_rmse: 0.2671 | val_rmse: 6.7955 | val_ll: -17.3432
[1:06:54.112253] epoch: 1750 | elbo: 10109973.139999999 | train_rmse: 0.2631 | val_rmse: 6.6914 | val_ll: -18.1866
[1:08:26.244517] epoch: 1800 | elbo: 9471301.740000002 | train_rmse: 0.2537 | val_rmse: 6.5991 | val_ll: -19.2483
[1:09:57.516562] epoch: 1850 | elbo: 8878583.72 | train_rmse: 0.2335 | val_rmse: 6.4969 | val_ll: -20.6888
[1:11:29.471593] epoch: 1900 | elbo: 8346281.010000001 | train_rmse: 0.2562 | val_rmse: 6.4106 | val_ll: -21.8998
[1:12:59.858473] epoch: 1950 | elbo: 7755156.734999999 | train_rmse: 0.2201 | val_rmse: 6.3233 | val_ll: -23.0583
[1:14:30.111193] epoch: 2000 | elbo: 7329176.545 | train_rmse: 0.2037 | val_rmse: 6.2466 | val_ll: -24.7982
[1:16:00.908563] epoch: 2050 | elbo: 6969221.875 | train_rmse: 0.2251 | val_rmse: 6.1751 | val_ll: -26.1787
[1:17:32.385919] epoch: 2100 | elbo: 6644066.425000002 | train_rmse: 0.2081 | val_rmse: 6.1033 | val_ll: -27.97
[1:19:03.953623] epoch: 2150 | elbo: 6296423.85 | train_rmse: 0.2038 | val_rmse: 6.0372 | val_ll: -29.3837
[1:20:34.752201] epoch: 2200 | elbo: 5973070.529999999 | train_rmse: 0.1976 | val_rmse: 5.9781 | val_ll: -31.3477
[1:22:04.574684] epoch: 2250 | elbo: 5725175.574999999 | train_rmse: 0.1887 | val_rmse: 5.9206 | val_ll: -33.3092
[1:23:35.896926] epoch: 2300 | elbo: 5449164.525 | train_rmse: 0.1753 | val_rmse: 5.8606 | val_ll: -35.8481
[1:25:07.408794] epoch: 2350 | elbo: 5239259.05 | train_rmse: 0.1799 | val_rmse: 5.8107 | val_ll: -37.2163
[1:26:39.069351] epoch: 2400 | elbo: 5063003.625000001 | train_rmse: 0.1726 | val_rmse: 5.7649 | val_ll: -40.3807
[1:28:11.593280] epoch: 2450 | elbo: 4866413.3149999995 | train_rmse: 0.1717 | val_rmse: 5.7111 | val_ll: -42.3485
[1:29:42.770341] epoch: 2500 | elbo: 4725714.88 | train_rmse: 0.1566 | val_rmse: 5.674 | val_ll: -45.3823
[1:31:13.964993] epoch: 2550 | elbo: 4540773.76 | train_rmse: 0.1498 | val_rmse: 5.6338 | val_ll: -47.855
[1:32:45.526637] epoch: 2600 | elbo: 4427641.5600000005 | train_rmse: 0.1646 | val_rmse: 5.5982 | val_ll: -51.231
[1:34:15.765895] epoch: 2650 | elbo: 4279825.5725 | train_rmse: 0.14 | val_rmse: 5.5563 | val_ll: -54.359
[1:35:47.284757] epoch: 2700 | elbo: 4190680.1399999997 | train_rmse: 0.1403 | val_rmse: 5.5184 | val_ll: -58.6694
[1:37:17.324278] epoch: 2750 | elbo: 4094245.6149999998 | train_rmse: 0.1524 | val_rmse: 5.4829 | val_ll: -61.4681
[1:38:46.895912] epoch: 2800 | elbo: 4000650.3074999996 | train_rmse: 0.1389 | val_rmse: 5.4522 | val_ll: -65.8958
[1:40:16.307591] epoch: 2850 | elbo: 3937415.1225 | train_rmse: 0.1391 | val_rmse: 5.4212 | val_ll: -68.889
[1:41:45.734137] epoch: 2900 | elbo: 3842784.4050000003 | train_rmse: 0.1284 | val_rmse: 5.3916 | val_ll: -73.7917
[1:43:14.836279] epoch: 2950 | elbo: 3818635.5325 | train_rmse: 0.1601 | val_rmse: 5.3642 | val_ll: -76.2119
[1:44:45.076903] epoch: 3000 | elbo: 3713783.345 | train_rmse: 0.1317 | val_rmse: 5.343 | val_ll: -81.6495
[1:46:14.800815] epoch: 3050 | elbo: 3660571.4549999996 | train_rmse: 0.1255 | val_rmse: 5.3163 | val_ll: -85.7907
[1:47:44.186294] epoch: 3100 | elbo: 3603582.7674999996 | train_rmse: 0.1291 | val_rmse: 5.2894 | val_ll: -91.1612
[1:49:14.230995] epoch: 3150 | elbo: 3573725.4450000003 | train_rmse: 0.1556 | val_rmse: 5.2707 | val_ll: -95.6344
[1:50:45.899368] epoch: 3200 | elbo: 3512274.8225000007 | train_rmse: 0.114 | val_rmse: 5.2478 | val_ll: -100.732
[1:52:15.726728] epoch: 3250 | elbo: 3470043.4574999996 | train_rmse: 0.124 | val_rmse: 5.2274 | val_ll: -106.5443
[1:53:48.233599] epoch: 3300 | elbo: 3428197.2424999997 | train_rmse: 0.1163 | val_rmse: 5.2021 | val_ll: -110.7457
[1:55:21.587338] epoch: 3350 | elbo: 3404753.7775 | train_rmse: 0.1139 | val_rmse: 5.1902 | val_ll: -117.5555
[1:56:51.923381] epoch: 3400 | elbo: 3367755.7925000004 | train_rmse: 0.1099 | val_rmse: 5.167 | val_ll: -121.5946
[1:58:21.515856] epoch: 3450 | elbo: 3348238.6925000004 | train_rmse: 0.1091 | val_rmse: 5.1491 | val_ll: -127.0316
[1:59:50.862732] epoch: 3500 | elbo: 3327896.6425 | train_rmse: 0.1095 | val_rmse: 5.1305 | val_ll: -133.1297
[2:01:20.222050] epoch: 3550 | elbo: 3296058.9825 | train_rmse: 0.1134 | val_rmse: 5.116 | val_ll: -138.6121
[2:02:50.055081] epoch: 3600 | elbo: 3266558.7775000003 | train_rmse: 0.1087 | val_rmse: 5.0974 | val_ll: -144.2454
[2:04:20.310782] epoch: 3650 | elbo: 3259466.3425000003 | train_rmse: 0.1135 | val_rmse: 5.0829 | val_ll: -148.2311
[2:05:50.355776] epoch: 3700 | elbo: 3232091.88 | train_rmse: 0.1019 | val_rmse: 5.0657 | val_ll: -157.7354
[2:07:19.331781] epoch: 3750 | elbo: 3222870.5624999995 | train_rmse: 0.1181 | val_rmse: 5.049 | val_ll: -162.1335
[2:08:50.526007] epoch: 3800 | elbo: 3189301.2175000003 | train_rmse: 0.0974 | val_rmse: 5.0372 | val_ll: -168.2632
[2:10:20.710664] epoch: 3850 | elbo: 3185546.4025 | train_rmse: 0.1147 | val_rmse: 5.0247 | val_ll: -169.7161
[2:11:49.751826] epoch: 3900 | elbo: 3167454.5975 | train_rmse: 0.0978 | val_rmse: 5.0102 | val_ll: -181.1797
[2:13:19.586276] epoch: 3950 | elbo: 3151245.165 | train_rmse: 0.0988 | val_rmse: 4.9986 | val_ll: -183.7416
[2:14:51.113359] epoch: 4000 | elbo: 3144517.7975000003 | train_rmse: 0.1022 | val_rmse: 4.988 | val_ll: -188.0415
[2:16:23.110995] epoch: 4050 | elbo: 3133528.4225000003 | train_rmse: 0.0978 | val_rmse: 4.975 | val_ll: -192.2037
[2:17:54.207192] epoch: 4100 | elbo: 3120914.1850000005 | train_rmse: 0.0981 | val_rmse: 4.9619 | val_ll: -197.8657
[2:19:24.713445] epoch: 4150 | elbo: 3112304.365 | train_rmse: 0.0961 | val_rmse: 4.9492 | val_ll: -197.7391
[2:20:54.168284] epoch: 4200 | elbo: 3109024.1649999996 | train_rmse: 0.103 | val_rmse: 4.9368 | val_ll: -202.9637
[2:22:24.176584] epoch: 4250 | elbo: 3086456.5300000003 | train_rmse: 0.0949 | val_rmse: 4.9284 | val_ll: -209.4018
[2:23:53.689717] epoch: 4300 | elbo: 3078179.6599999997 | train_rmse: 0.101 | val_rmse: 4.9146 | val_ll: -213.2097
[2:25:24.065379] epoch: 4350 | elbo: 3073657.6574999997 | train_rmse: 0.103 | val_rmse: 4.9027 | val_ll: -215.2504
[2:26:53.876165] epoch: 4400 | elbo: 3058215.0325000007 | train_rmse: 0.0955 | val_rmse: 4.8897 | val_ll: -217.3675
[2:28:22.979830] epoch: 4450 | elbo: 3056455.6425 | train_rmse: 0.1005 | val_rmse: 4.8804 | val_ll: -221.7407
[2:29:53.354176] epoch: 4500 | elbo: 3054633.1875 | train_rmse: 0.1019 | val_rmse: 4.8689 | val_ll: -225.4568
[2:31:23.807348] epoch: 4550 | elbo: 3037942.4274999998 | train_rmse: 0.0888 | val_rmse: 4.8598 | val_ll: -225.8099
[2:32:53.771028] epoch: 4600 | elbo: 3035689.6475 | train_rmse: 0.097 | val_rmse: 4.8483 | val_ll: -227.9086
[2:34:24.363998] epoch: 4650 | elbo: 3022867.41 | train_rmse: 0.0912 | val_rmse: 4.8381 | val_ll: -233.9676
[2:35:54.127745] epoch: 4700 | elbo: 3016509.0375 | train_rmse: 0.0999 | val_rmse: 4.8286 | val_ll: -235.1756
[2:37:24.364722] epoch: 4750 | elbo: 3010952.745 | train_rmse: 0.0994 | val_rmse: 4.8154 | val_ll: -235.3697
[2:38:54.640434] epoch: 4800 | elbo: 3006532.63 | train_rmse: 0.0963 | val_rmse: 4.807 | val_ll: -237.8825
[2:40:24.464583] epoch: 4850 | elbo: 3001653.9675 | train_rmse: 0.093 | val_rmse: 4.7953 | val_ll: -236.1337
[2:41:53.601018] epoch: 4900 | elbo: 2983493.16 | train_rmse: 0.0864 | val_rmse: 4.785 | val_ll: -239.2499
[2:43:23.897752] epoch: 4950 | elbo: 2985823.1300000004 | train_rmse: 0.0861 | val_rmse: 4.774 | val_ll: -241.9996
[2:44:55.588943] epoch: 5000 | elbo: 2981280.09 | train_rmse: 0.0962 | val_rmse: 4.7672 | val_ll: -238.7757
[2:46:28.985463] epoch: 5050 | elbo: 2971055.255 | train_rmse: 0.0866 | val_rmse: 4.7585 | val_ll: -238.8653
[2:48:00.654454] epoch: 5100 | elbo: 2968841.2149999994 | train_rmse: 0.0838 | val_rmse: 4.7477 | val_ll: -239.4883
[2:49:29.918709] epoch: 5150 | elbo: 2956551.5775000006 | train_rmse: 0.0859 | val_rmse: 4.739 | val_ll: -243.4045
[2:50:59.409017] epoch: 5200 | elbo: 2945730.1875 | train_rmse: 0.0806 | val_rmse: 4.728 | val_ll: -244.1596
[2:52:28.953898] epoch: 5250 | elbo: 2951592.1525 | train_rmse: 0.0888 | val_rmse: 4.7166 | val_ll: -238.8168
[2:53:57.926058] epoch: 5300 | elbo: 2947043.8600000003 | train_rmse: 0.0919 | val_rmse: 4.7137 | val_ll: -241.1072
[2:55:27.097230] epoch: 5350 | elbo: 2940932.945 | train_rmse: 0.0997 | val_rmse: 4.6986 | val_ll: -238.5918
[2:56:56.565210] epoch: 5400 | elbo: 2931656.415 | train_rmse: 0.0844 | val_rmse: 4.6922 | val_ll: -240.7462
[2:58:26.430533] epoch: 5450 | elbo: 2930210.9750000006 | train_rmse: 0.0855 | val_rmse: 4.6846 | val_ll: -242.1821
[2:59:55.921581] epoch: 5500 | elbo: 2919638.62 | train_rmse: 0.0813 | val_rmse: 4.6747 | val_ll: -243.7424
[3:01:27.022525] epoch: 5550 | elbo: 2910699.065 | train_rmse: 0.0836 | val_rmse: 4.664 | val_ll: -242.4334
[3:02:57.303507] epoch: 5600 | elbo: 2913192.2875 | train_rmse: 0.0902 | val_rmse: 4.654 | val_ll: -237.3947
[3:04:26.721766] epoch: 5650 | elbo: 2903112.5575 | train_rmse: 0.0879 | val_rmse: 4.6446 | val_ll: -239.2635
[3:05:56.252594] epoch: 5700 | elbo: 2901850.69 | train_rmse: 0.0885 | val_rmse: 4.6385 | val_ll: -240.1166
[3:07:25.207041] epoch: 5750 | elbo: 2890484.43 | train_rmse: 0.0865 | val_rmse: 4.6255 | val_ll: -238.9088
[3:08:54.452670] epoch: 5800 | elbo: 2880035.4725 | train_rmse: 0.0894 | val_rmse: 4.6183 | val_ll: -239.6345
[3:10:24.483232] epoch: 5850 | elbo: 2891688.0475000003 | train_rmse: 0.0956 | val_rmse: 4.6095 | val_ll: -243.6444
[3:11:53.237511] epoch: 5900 | elbo: 2877330.0300000003 | train_rmse: 0.1006 | val_rmse: 4.601 | val_ll: -237.4775
[3:13:22.874350] epoch: 5950 | elbo: 2865988.4175 | train_rmse: 0.0885 | val_rmse: 4.5944 | val_ll: -237.965
[3:14:52.386381] epoch: 6000 | elbo: 2862558.355 | train_rmse: 0.0819 | val_rmse: 4.5823 | val_ll: -237.0143
[3:16:22.775342] epoch: 6050 | elbo: 2852797.1049999995 | train_rmse: 0.0794 | val_rmse: 4.5754 | val_ll: -233.5285
[3:17:55.492023] epoch: 6100 | elbo: 2853590.51 | train_rmse: 0.0835 | val_rmse: 4.5656 | val_ll: -237.9034
[3:19:29.811222] epoch: 6150 | elbo: 2835741.115 | train_rmse: 0.0713 | val_rmse: 4.5568 | val_ll: -232.4906
[3:21:02.013291] epoch: 6200 | elbo: 2833722.675 | train_rmse: 0.0796 | val_rmse: 4.5498 | val_ll: -233.5509
[3:22:34.454331] epoch: 6250 | elbo: 2833485.4400000004 | train_rmse: 0.0783 | val_rmse: 4.5426 | val_ll: -235.4315
[3:24:06.813050] epoch: 6300 | elbo: 2823617.9800000004 | train_rmse: 0.0806 | val_rmse: 4.5326 | val_ll: -232.5964
[3:25:40.271142] epoch: 6350 | elbo: 2826488.7725 | train_rmse: 0.0918 | val_rmse: 4.5253 | val_ll: -231.4379
[3:27:10.433524] epoch: 6400 | elbo: 2815497.925 | train_rmse: 0.0823 | val_rmse: 4.5172 | val_ll: -230.4853
[3:28:41.405865] epoch: 6450 | elbo: 2813614.9375 | train_rmse: 0.0863 | val_rmse: 4.5076 | val_ll: -232.4983
[3:30:12.655114] epoch: 6500 | elbo: 2808701.6225000005 | train_rmse: 0.0912 | val_rmse: 4.5019 | val_ll: -233.1609
[3:31:43.090695] epoch: 6550 | elbo: 2795691.7799999993 | train_rmse: 0.0761 | val_rmse: 4.4923 | val_ll: -230.2206
[3:33:15.072402] epoch: 6600 | elbo: 2791290.0975 | train_rmse: 0.0838 | val_rmse: 4.4844 | val_ll: -226.1615
[3:34:46.590115] epoch: 6650 | elbo: 2795690.775 | train_rmse: 0.0887 | val_rmse: 4.4759 | val_ll: -229.7286
[3:36:18.523930] epoch: 6700 | elbo: 2800953.1425000005 | train_rmse: 0.1114 | val_rmse: 4.4705 | val_ll: -230.7996
[3:37:50.266639] epoch: 6750 | elbo: 2772847.85 | train_rmse: 0.0699 | val_rmse: 4.4635 | val_ll: -228.4703
[3:39:21.906242] epoch: 6800 | elbo: 2767848.0425 | train_rmse: 0.076 | val_rmse: 4.4552 | val_ll: -228.6077
[3:40:52.419625] epoch: 6850 | elbo: 2760807.5825 | train_rmse: 0.0759 | val_rmse: 4.4431 | val_ll: -229.3081
[3:42:23.103818] epoch: 6900 | elbo: 2766178.0949999997 | train_rmse: 0.0884 | val_rmse: 4.4368 | val_ll: -225.6216
[3:43:53.855788] epoch: 6950 | elbo: 2747741.3899999997 | train_rmse: 0.0729 | val_rmse: 4.4261 | val_ll: -228.0917
[3:45:25.060322] epoch: 7000 | elbo: 2749130.2275 | train_rmse: 0.0826 | val_rmse: 4.4184 | val_ll: -223.571
[3:46:55.308298] epoch: 7050 | elbo: 2738335.1199999996 | train_rmse: 0.0789 | val_rmse: 4.4143 | val_ll: -225.3578
[3:48:28.257768] epoch: 7100 | elbo: 2736340.9374999995 | train_rmse: 0.0781 | val_rmse: 4.4048 | val_ll: -223.9163
[3:50:00.124665] epoch: 7150 | elbo: 2730140.4724999997 | train_rmse: 0.0817 | val_rmse: 4.3949 | val_ll: -222.3625
[3:51:31.746616] epoch: 7200 | elbo: 2725539.78 | train_rmse: 0.0806 | val_rmse: 4.3887 | val_ll: -220.8591
[3:53:02.383711] epoch: 7250 | elbo: 2717251.7075 | train_rmse: 0.0761 | val_rmse: 4.3816 | val_ll: -219.4786
[3:54:34.027898] epoch: 7300 | elbo: 2712464.0149999997 | train_rmse: 0.0761 | val_rmse: 4.3752 | val_ll: -217.951
[3:56:04.277475] epoch: 7350 | elbo: 2706078.82 | train_rmse: 0.0795 | val_rmse: 4.3683 | val_ll: -221.8122
[3:57:36.584365] epoch: 7400 | elbo: 2710239.0175 | train_rmse: 0.0855 | val_rmse: 4.3584 | val_ll: -218.9383
[3:59:08.061466] epoch: 7450 | elbo: 2713329.88 | train_rmse: 0.0958 | val_rmse: 4.3516 | val_ll: -219.9194
[4:00:38.039729] epoch: 7500 | elbo: 2692947.2525000004 | train_rmse: 0.0759 | val_rmse: 4.3459 | val_ll: -215.4228
[4:02:07.387244] epoch: 7550 | elbo: 2688261.7150000003 | train_rmse: 0.0832 | val_rmse: 4.3357 | val_ll: -218.958
[4:03:36.768444] epoch: 7600 | elbo: 2690442.0375 | train_rmse: 0.0819 | val_rmse: 4.3298 | val_ll: -217.1802
[4:05:06.223770] epoch: 7650 | elbo: 2671970.3575 | train_rmse: 0.0781 | val_rmse: 4.3233 | val_ll: -217.2652
[4:06:35.818280] epoch: 7700 | elbo: 2670047.1975 | train_rmse: 0.0734 | val_rmse: 4.3134 | val_ll: -215.5683
[4:08:06.457915] epoch: 7750 | elbo: 2672900.4575000005 | train_rmse: 0.0756 | val_rmse: 4.3067 | val_ll: -215.2718
[4:09:36.715532] epoch: 7800 | elbo: 2659062.8150000004 | train_rmse: 0.0799 | val_rmse: 4.301 | val_ll: -213.9627
[4:11:05.888049] epoch: 7850 | elbo: 2652381.5750000007 | train_rmse: 0.0728 | val_rmse: 4.2933 | val_ll: -213.6739
[4:12:35.308338] epoch: 7900 | elbo: 2648174.1274999995 | train_rmse: 0.0753 | val_rmse: 4.2863 | val_ll: -214.1613
[4:14:05.402808] epoch: 7950 | elbo: 2640916.8875 | train_rmse: 0.0814 | val_rmse: 4.2808 | val_ll: -216.798
[4:15:35.286629] epoch: 8000 | elbo: 2641823.6849999996 | train_rmse: 0.0753 | val_rmse: 4.2725 | val_ll: -213.6341
[4:17:05.248754] epoch: 8050 | elbo: 2625807.9549999996 | train_rmse: 0.0657 | val_rmse: 4.2653 | val_ll: -218.287
[4:18:36.123754] epoch: 8100 | elbo: 2624160.6550000003 | train_rmse: 0.0755 | val_rmse: 4.2618 | val_ll: -212.2882
[4:20:04.723160] epoch: 8150 | elbo: 2620118.335 | train_rmse: 0.086 | val_rmse: 4.2534 | val_ll: -210.2704
[4:21:35.031223] epoch: 8200 | elbo: 2609725.7075 | train_rmse: 0.0711 | val_rmse: 4.2474 | val_ll: -214.3898
[4:23:03.775743] epoch: 8250 | elbo: 2606845.7425 | train_rmse: 0.0728 | val_rmse: 4.2412 | val_ll: -211.5259
[4:24:34.139594] epoch: 8300 | elbo: 2602627.335 | train_rmse: 0.0783 | val_rmse: 4.2323 | val_ll: -212.0868
[4:26:03.171761] epoch: 8350 | elbo: 2594867.3525 | train_rmse: 0.0762 | val_rmse: 4.2274 | val_ll: -208.1825
[4:27:32.747646] epoch: 8400 | elbo: 2594413.2074999996 | train_rmse: 0.0795 | val_rmse: 4.2218 | val_ll: -207.3334
[4:29:03.387187] epoch: 8450 | elbo: 2582609.8075 | train_rmse: 0.0655 | val_rmse: 4.2134 | val_ll: -207.2977
[4:30:32.616210] epoch: 8500 | elbo: 2587391.7299999995 | train_rmse: 0.0787 | val_rmse: 4.2111 | val_ll: -206.9511
[4:32:01.901296] epoch: 8550 | elbo: 2574047.0249999994 | train_rmse: 0.0758 | val_rmse: 4.2023 | val_ll: -208.3438
[4:33:31.224293] epoch: 8600 | elbo: 2566696.7225 | train_rmse: 0.0712 | val_rmse: 4.1971 | val_ll: -205.2167
[4:35:00.416340] epoch: 8650 | elbo: 2572694.695 | train_rmse: 0.0898 | val_rmse: 4.1872 | val_ll: -208.3223
[4:36:30.539317] epoch: 8700 | elbo: 2570422.16 | train_rmse: 0.1108 | val_rmse: 4.1839 | val_ll: -206.6957
[4:37:59.601120] epoch: 8750 | elbo: 2551651.265 | train_rmse: 0.0673 | val_rmse: 4.174 | val_ll: -202.2858
[4:39:29.300771] epoch: 8800 | elbo: 2552942.8899999997 | train_rmse: 0.0771 | val_rmse: 4.1681 | val_ll: -200.9276
[4:40:58.654707] epoch: 8850 | elbo: 2536368.9 | train_rmse: 0.0678 | val_rmse: 4.1637 | val_ll: -203.6764
[4:42:28.238535] epoch: 8900 | elbo: 2544842.1150000007 | train_rmse: 0.0919 | val_rmse: 4.1564 | val_ll: -203.1946
[4:44:00.654909] epoch: 8950 | elbo: 2528167.0774999997 | train_rmse: 0.0706 | val_rmse: 4.1502 | val_ll: -204.7901
[4:45:30.304462] epoch: 9000 | elbo: 2525205.284999999 | train_rmse: 0.0704 | val_rmse: 4.1451 | val_ll: -202.6036
[4:46:59.996651] epoch: 9050 | elbo: 2533187.8675 | train_rmse: 0.0823 | val_rmse: 4.1388 | val_ll: -199.5613
[4:48:29.687291] epoch: 9100 | elbo: 2511645.0325 | train_rmse: 0.0684 | val_rmse: 4.1316 | val_ll: -203.0714
[4:49:58.897945] epoch: 9150 | elbo: 2507918.0200000005 | train_rmse: 0.067 | val_rmse: 4.1265 | val_ll: -200.5435
[4:51:28.525519] epoch: 9200 | elbo: 2502019.6325 | train_rmse: 0.07 | val_rmse: 4.1226 | val_ll: -202.4187
[4:52:57.288676] epoch: 9250 | elbo: 2500637.3525 | train_rmse: 0.071 | val_rmse: 4.1135 | val_ll: -202.4733
[4:54:26.299343] epoch: 9300 | elbo: 2494197.1775 | train_rmse: 0.0761 | val_rmse: 4.1092 | val_ll: -198.6762
[4:55:55.348373] epoch: 9350 | elbo: 2484111.62 | train_rmse: 0.067 | val_rmse: 4.1027 | val_ll: -197.8448
[4:57:24.850342] epoch: 9400 | elbo: 2485965.3950000005 | train_rmse: 0.0775 | val_rmse: 4.0949 | val_ll: -200.203
[4:58:53.932847] epoch: 9450 | elbo: 2474019.92 | train_rmse: 0.0628 | val_rmse: 4.0908 | val_ll: -198.2065
[5:00:24.575136] epoch: 9500 | elbo: 2471072.3600000003 | train_rmse: 0.069 | val_rmse: 4.0859 | val_ll: -198.212
[5:01:55.253378] epoch: 9550 | elbo: 2466221.515 | train_rmse: 0.0648 | val_rmse: 4.0794 | val_ll: -196.8962
[5:03:26.804754] epoch: 9600 | elbo: 2459609.4850000003 | train_rmse: 0.0699 | val_rmse: 4.0746 | val_ll: -200.5081
[5:04:58.099073] epoch: 9650 | elbo: 2454947.6075 | train_rmse: 0.0699 | val_rmse: 4.0667 | val_ll: -199.4346
[5:06:29.552811] epoch: 9700 | elbo: 2461266.6025 | train_rmse: 0.0773 | val_rmse: 4.0593 | val_ll: -198.9056
[5:08:01.539632] epoch: 9750 | elbo: 2446586.8175000004 | train_rmse: 0.0698 | val_rmse: 4.0538 | val_ll: -196.1918
[5:09:34.247545] epoch: 9800 | elbo: 2437835.3550000004 | train_rmse: 0.0647 | val_rmse: 4.0471 | val_ll: -195.1768
[5:11:05.344120] epoch: 9850 | elbo: 2432041.8649999993 | train_rmse: 0.0679 | val_rmse: 4.0414 | val_ll: -195.1772
[5:12:35.779487] epoch: 9900 | elbo: 2430644.4524999997 | train_rmse: 0.0655 | val_rmse: 4.0352 | val_ll: -193.7654
[5:14:06.453592] epoch: 9950 | elbo: 2425249.9175 | train_rmse: 0.0737 | val_rmse: 4.0311 | val_ll: -195.812
Training finished in 5:15:35.552771 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-s003/checkpoint_1.pt
File Size is 4.0595598220825195 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:0 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.03 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0244,  0.0088, -0.1413,  ...,  0.2210,  0.3271,  0.1992],
        [-0.2062, -0.0405,  0.1719,  ..., -0.1829,  0.1949,  0.1738],
        [ 0.0738, -0.2494, -0.2019,  ..., -0.5314, -0.0232,  0.1700],
        ...,
        [-0.3709,  0.1648, -0.1391,  ...,  0.0097, -0.3795, -0.0426],
        [-0.3528, -0.1597,  0.1633,  ..., -0.3160, -0.2945, -0.0551],
        [-0.2400, -0.0042,  0.3008,  ..., -0.0354, -0.1077, -0.1935]],
       device='cuda:0', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0002, 0.0003, 0.0002,  ..., 0.0003, 0.0003, 0.0002],
        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],
        ...,
        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],
        [0.0003, 0.0002, 0.0003,  ..., 0.0003, 0.0003, 0.0003]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-9.7709e-02, -2.6672e-01,  1.5345e-01,  1.4253e-01,  1.1557e-01,
        -2.8010e-01,  5.5939e-01, -3.2902e-01,  9.6433e-02, -2.2346e-02,
        -2.6844e-01, -1.8475e-01, -2.2040e-01, -5.7842e-01,  1.0819e-01,
        -1.8070e-01,  2.5324e-01, -3.0385e-03, -5.0179e-02, -4.2960e-01,
         4.7095e-01,  8.6490e-03, -3.9424e-01, -2.9088e-01, -4.3004e-01,
        -2.7984e-01,  3.4507e-01, -2.2053e-01,  2.0953e-01,  4.6730e-01,
        -4.4756e-01, -5.6982e-01, -1.8446e-01,  2.4396e-01, -4.0628e-01,
        -2.4188e-01, -3.7280e-02, -2.7181e-01,  1.4579e-01, -1.8456e-01,
         1.0106e-01,  3.4954e-02, -2.9837e-01, -3.4086e-01, -4.3061e-01,
        -1.2466e-01,  5.3255e-02, -2.4115e-01, -2.4192e-01,  1.7852e-01,
        -2.0550e-01,  3.1131e-01, -8.9492e-01, -2.4441e-01, -2.9216e-01,
        -1.2891e-01, -3.8750e-02,  1.8371e-01,  2.9258e-01, -2.3709e-01,
        -1.4059e-01, -5.4504e-02, -1.9482e-01,  1.2741e-01, -2.9756e-01,
         2.5357e-01, -3.5721e-01, -2.6214e-02, -4.5586e-01,  1.8361e-01,
         2.0893e-01, -5.2953e-02, -2.5860e-01,  6.6991e-01, -4.4901e-01,
        -6.4156e-01, -8.6646e-02, -1.8574e-01, -4.3811e-01,  1.5608e-01,
        -2.2016e-01,  1.7222e-01, -2.0326e-01,  3.3972e-01, -1.1864e-01,
         1.3330e-01, -3.5129e-01, -5.0392e-02,  3.6990e-02,  2.3698e-01,
        -4.1008e-01, -1.3466e-01, -2.0696e-01, -7.4474e-04,  2.4620e-01,
        -2.6334e-01,  3.6803e-02,  4.0238e-01, -4.4650e-01,  5.3665e-03,
        -1.5767e-01, -2.0263e-01, -1.3810e-01,  2.7986e-01, -6.7340e-01,
        -2.5437e-01, -3.6031e-02,  1.9721e-02, -1.7391e-01,  1.0792e-01,
         5.7027e-02, -5.1999e-01, -6.8788e-01, -2.1932e-01, -3.4358e-01,
         5.5048e-02, -6.6889e-01, -5.9489e-01, -8.3671e-02, -5.4743e-01,
        -8.4683e-01, -2.9885e-01, -2.2093e-01,  2.4891e-01,  1.6081e-01,
        -1.7768e-01,  1.3246e-01, -3.1378e-01, -1.8644e-01,  6.5577e-01,
         8.5816e-02,  3.6373e-01,  1.5914e-01, -2.4764e-01, -1.0557e+00,
         9.6862e-02,  9.3364e-02, -2.4382e-01, -4.7256e-02,  2.5501e-01,
        -6.4190e-01,  4.9793e-01, -4.7670e-01, -2.4814e-01, -3.4900e-01,
         8.3222e-03, -3.7750e-02,  8.6853e-02, -4.1333e-01, -5.3248e-01,
        -3.6341e-01,  1.8787e-01, -1.8240e-01, -1.9331e-01, -7.2920e-01,
        -1.7533e-01, -6.3265e-01,  3.0250e-01, -2.0616e-01,  2.0267e-01,
        -4.2832e-01,  2.0619e-01,  2.5442e-01,  1.6079e-01,  1.8722e-02,
        -1.7178e-01, -4.8587e-01,  3.5102e-01, -2.2019e-01, -3.2702e-01,
         1.8262e-01,  3.8016e-01, -2.1736e-01, -5.7345e-01, -4.5623e-01,
         6.5625e-01, -5.8091e-01, -6.6779e-02,  2.7049e-01, -3.8601e-01,
        -4.9320e-02, -1.1942e-01, -3.7342e-01, -1.4000e-01, -3.3398e-01,
        -2.7584e-01, -1.9147e-01, -3.3340e-01, -4.9972e-01, -7.3996e-01,
        -2.3052e-01,  5.6434e-01, -2.7800e-01, -4.0135e-01,  7.7104e-03,
         2.5969e-02, -3.4306e-01,  2.9553e-01, -2.7380e-01, -1.8354e-01,
         2.7721e-01, -2.4816e-01,  2.4300e-01, -6.9561e-01, -7.1980e-02,
        -6.5525e-01, -1.5886e-01, -1.4849e-01,  1.0582e-01, -2.9545e-01,
         4.5556e-01, -6.4020e-01, -3.8259e-01, -1.0296e-01,  2.4253e-01,
        -5.8037e-01, -1.9886e-01,  5.6362e-01, -1.6810e-01, -2.7618e-01,
        -1.4412e-01, -2.0591e-01, -1.1770e-01,  2.6085e-01,  5.8536e-01,
        -5.1969e-02,  3.8596e-01, -2.6817e-01,  1.7432e-01,  3.0274e-01,
         1.4801e-01, -5.1962e-01, -9.2954e-01, -1.1142e-01, -1.9347e-01,
        -1.3556e-01, -4.2036e-03,  6.3351e-02,  3.3036e-01,  1.8450e-02,
         9.3874e-02,  1.6143e-01, -5.2936e-01, -4.7092e-01,  1.6167e-01,
        -3.6486e-01,  1.3581e-01,  2.5504e-01,  2.4138e-01, -1.0951e-01,
        -1.8129e-01, -1.5051e-01, -5.5080e-01, -1.2370e-01, -8.7217e-01,
         2.9428e-01, -1.5245e-01, -2.8318e-01,  6.8125e-02, -2.1872e-01,
        -2.2173e-02,  7.0967e-02, -1.5314e-02, -9.0318e-02, -3.7740e-01,
         1.6319e-01,  5.7605e-03, -3.1586e-01,  9.9234e-02, -1.8381e-01,
        -1.1083e-01,  1.1522e-01, -1.2430e-01, -4.4314e-02,  1.7483e-01,
        -1.0799e-01, -2.0805e-01,  1.8298e-01,  2.3184e-01,  1.9829e-01,
        -5.9162e-01, -8.2497e-03, -5.1936e-01,  8.0828e-02,  8.1080e-03,
         8.3245e-02,  2.2915e-01,  3.0006e-01,  7.3167e-02, -6.0392e-01,
        -7.9707e-02, -3.6447e-01,  4.9268e-01, -2.3831e-01, -2.7786e-01,
        -3.3915e-01, -9.7769e-02,  1.2651e-01,  8.2531e-02, -1.2215e-01,
        -2.0622e-01,  1.1142e-02,  2.3221e-01, -3.6046e-01, -6.3028e-02,
        -1.8018e-01, -3.9879e-02,  4.7285e-01, -1.5969e-01, -8.6508e-02,
        -8.8183e-01, -7.1640e-02, -1.8732e-01, -1.0183e-01, -1.0860e-01,
        -8.1097e-01,  5.7224e-01, -1.7814e-01, -4.0841e-01, -2.1705e-01,
        -3.1515e-01,  4.2869e-02, -1.3764e-01, -5.3670e-01, -3.0081e-02,
        -1.8646e-01,  4.4212e-01, -3.5347e-01, -8.6111e-02, -2.6367e-01,
        -5.1177e-01, -3.4932e-01, -5.4682e-01,  1.9445e-01, -4.5228e-01,
        -2.2374e-01, -9.1194e-03,  4.9787e-01, -5.7840e-03, -1.0998e-01,
        -3.0195e-01, -1.1593e-01, -2.1502e-02,  1.6043e-01,  3.8231e-03,
        -3.9243e-02,  8.3922e-02,  1.0871e-01,  3.6315e-01, -5.6975e-01,
        -2.1455e-01,  2.1558e-01,  1.6301e-01, -2.1567e-01, -5.2765e-01,
        -2.7456e-02,  5.1682e-01, -5.2726e-01,  4.3682e-01, -4.8216e-01,
         3.4020e-01, -8.2110e-01, -2.6902e-01, -7.9271e-02, -5.7768e-01,
         3.8045e-01, -7.3595e-02,  5.5112e-02, -1.8720e-01,  3.1020e-01,
         3.5632e-01, -3.9697e-01, -6.2266e-01, -6.2274e-01, -1.5908e-01,
        -5.7423e-02, -2.8079e-01, -3.1253e-01, -2.1000e-01, -2.5251e-01,
         3.9712e-02, -2.5397e-02, -2.6221e-01, -1.1386e-01, -4.7823e-01,
         2.7023e-01,  2.1007e-01,  1.9275e-01, -2.5338e-01,  1.7075e-01,
        -2.5325e-01,  3.4010e-01, -3.0758e-01, -6.0337e-02, -8.3504e-01,
        -4.7544e-01, -7.6776e-01,  6.4830e-02,  7.4105e-02, -2.7299e-01,
        -7.2963e-02, -2.7300e-01, -3.9873e-01,  2.3444e-01, -3.0157e-01,
         7.5952e-02,  5.0620e-02,  2.5762e-01, -5.1742e-02, -4.2123e-01,
         9.0582e-02,  1.1818e-01,  9.1469e-02,  5.7985e-02, -7.7741e-02,
         3.5162e-01, -4.8407e-01, -3.4627e-01, -2.6755e-01, -2.3138e-01,
        -4.2704e-01,  3.2659e-01,  1.4140e-01,  6.4355e-02, -1.8892e-01,
         1.2039e-01, -1.8968e-02, -8.2068e-01, -1.5709e-01,  2.6018e-01,
        -2.2766e-01, -9.5763e-02, -2.8282e-01, -7.7267e-02, -2.9426e-01,
        -2.6449e-01, -6.5826e-01, -1.2914e-01, -5.3377e-01, -3.1275e-01,
        -2.6100e-01, -1.8916e-01,  3.4464e-01, -2.8250e-01,  4.7393e-02,
         1.5404e-01, -6.3633e-02, -8.5188e-01, -2.6626e-01, -5.8059e-01,
        -3.6618e-01,  2.0536e-02, -5.9610e-02, -4.1016e-01, -8.8846e-01,
        -4.8367e-01, -4.3151e-02, -4.4903e-01, -5.7292e-01, -1.9343e-01,
        -8.0839e-01, -3.0113e-01,  1.0343e-01,  4.8441e-02,  1.8627e-01,
        -2.7962e-01,  1.4674e-01, -8.4520e-03, -1.0785e-01, -8.1580e-01,
        -1.2083e-01, -1.5775e-01,  2.1016e-01, -4.2222e-01,  1.3088e-01,
        -1.5554e-01, -5.1760e-02,  7.6348e-02, -5.0002e-03, -3.0449e-01,
        -2.3632e-01, -1.4270e-01,  3.6307e-01,  3.5681e-01,  3.2596e-01,
         8.1257e-02,  6.0661e-01, -6.2174e-01,  1.9148e-01, -7.7460e-02,
        -2.2229e-01, -2.9223e-01, -1.4687e-01,  1.6746e-01, -3.3684e-02,
        -4.3402e-02, -2.2476e-01,  3.2835e-01, -1.1730e-02, -2.9089e-01,
        -2.2034e-01, -3.4392e-01,  8.6288e-02,  1.9403e-01, -1.5515e-01,
         4.4742e-02, -1.3984e-01, -1.6424e-01,  2.2279e-01, -6.1985e-01,
        -6.2644e-02, -1.2430e-01], device='cuda:0', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0002, 0.0003, 0.0005, 0.0002, 0.0003, 0.0002, 0.0003,
        0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002,
        0.0003, 0.0002, 0.0002, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0005, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003,
        0.0003, 0.0002, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004,
        0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003,
        0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0002, 0.0003, 0.0004, 0.0005, 0.0003, 0.0003, 0.0003, 0.0005,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003,
        0.0003, 0.0004, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0004, 0.0005,
        0.0002, 0.0003, 0.0004, 0.0003, 0.0002, 0.0004, 0.0002, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0004, 0.0003, 0.0004, 0.0002, 0.0003, 0.0003, 0.0004, 0.0002,
        0.0002, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0004, 0.0004, 0.0002, 0.0004, 0.0003, 0.0002, 0.0004,
        0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004,
        0.0004, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0002,
        0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0004, 0.0003, 0.0002, 0.0004,
        0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002,
        0.0003, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0004, 0.0004, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004,
        0.0004, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003, 0.0003,
        0.0004, 0.0003, 0.0004, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002,
        0.0002, 0.0004, 0.0002, 0.0004, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003,
        0.0003, 0.0004, 0.0003, 0.0004, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,
        0.0002, 0.0002, 0.0002, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0005, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004,
        0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0004, 0.0003,
        0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0002, 0.0003,
        0.0003, 0.0005, 0.0003, 0.0003, 0.0010, 0.0002, 0.0003, 0.0003, 0.0003,
        0.0002, 0.0002, 0.0003, 0.0004, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0004, 0.0002, 0.0003,
        0.0002, 0.0004, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0012, 0.0004,
        0.0017, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0002, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0007, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003,
        0.0004, 0.0002, 0.0003, 0.0004, 0.0005, 0.0004, 0.0002, 0.0003, 0.0004,
        0.0003, 0.0004, 0.0003, 0.0003, 0.0003, 0.0002, 0.0004, 0.0003, 0.0003,
        0.0003, 0.0007, 0.0003, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0003,
        0.0002, 0.0004, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0005, 0.0003, 0.0003,
        0.0003, 0.0002, 0.0003, 0.0003, 0.0002, 0.0004, 0.0003, 0.0003],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1785,  0.2789, -0.5506,  ..., -0.4283, -0.2951, -0.0037],
        [-0.3984,  0.3074,  0.1563,  ..., -0.1344,  0.1284,  0.5014],
        [-0.6217, -0.2656,  0.2992,  ...,  0.4767, -0.5040, -0.0567],
        ...,
        [ 0.0310, -0.1979, -0.2152,  ..., -0.4528, -0.5738, -0.1629],
        [-0.1889,  0.9040,  0.0415,  ...,  0.1836, -0.7067,  0.3300],
        [ 0.3484, -0.2191,  0.7588,  ...,  0.3013,  0.2306,  0.2273]],
       device='cuda:0', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0035, 0.0080, 0.0022,  ..., 0.0045, 0.0024, 0.0053],
        [0.0023, 0.0039, 0.0014,  ..., 0.0032, 0.0018, 0.0032],
        [0.0105, 0.0171, 0.0050,  ..., 0.0192, 0.0088, 0.0082],
        ...,
        [0.0029, 0.0049, 0.0017,  ..., 0.0050, 0.0019, 0.0035],
        [0.0020, 0.0031, 0.0011,  ..., 0.0018, 0.0022, 0.0028],
        [0.0019, 0.0037, 0.0013,  ..., 0.0024, 0.0020, 0.0023]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 4.8306e-01,  1.7817e-01, -2.1407e-01, -2.9212e-01, -1.8061e-01,
         5.7227e-01,  2.3782e-02,  3.2522e-02, -2.4489e-01, -6.2778e-02,
        -2.7027e-01, -6.8614e-01,  5.5797e-02,  3.3173e-01,  2.7871e-01,
         1.7296e-02,  1.5177e-01,  5.6365e-02,  2.6018e-01,  3.3779e-02,
        -2.3716e-03,  5.9066e-02,  1.2706e-01,  6.6232e-02,  2.4343e-01,
        -3.0932e-01,  6.7922e-01,  4.6620e-01,  2.4352e-01, -4.2174e-01,
        -2.5631e-01,  3.9718e-01, -5.1719e-02, -6.7164e-01,  6.3417e-01,
        -2.6640e-01, -5.3861e-01, -2.0576e-01, -1.9268e-01,  4.4423e-01,
         2.2401e-02, -1.1460e-01,  3.3500e-01,  1.2327e-01, -3.6631e-01,
         4.4159e-01, -4.2124e-02,  2.7792e-01, -5.7860e-01, -2.9750e-01,
        -4.2590e-01,  1.4494e-01, -2.7881e-01, -8.1586e-02, -8.7679e-03,
        -1.8574e-01, -7.2309e-01, -1.6928e-01,  3.1049e-01, -6.8971e-01,
        -3.6433e-01, -1.2358e-01, -9.3874e-02,  5.2791e-01,  2.3780e-01,
         5.6983e-01,  1.8215e-02, -2.5247e-01,  2.4886e-01, -5.0601e-01,
         4.1553e-03,  3.5310e-01,  2.2287e-01,  2.5566e-01,  6.2394e-01,
        -5.2093e-01, -2.4610e-01,  6.1986e-02, -1.6869e-01, -9.2217e-02,
         1.5793e-01,  1.6758e-01, -5.8956e-01,  6.0045e-01, -3.1089e-02,
        -1.3451e-01,  1.0117e-02, -1.8615e-01, -5.4941e-02, -8.5445e-02,
         2.8437e-01,  1.5159e-01,  2.0235e-01, -2.5421e-01, -5.6556e-01,
         3.6198e-02, -1.6977e-01, -9.5291e-02,  3.3649e-01,  8.7215e-02,
        -3.9636e-01, -8.1477e-02,  1.5331e-01, -1.2500e-01, -3.1359e-01,
         3.3712e-02, -1.7580e-01,  6.4316e-01, -1.7818e-01, -4.4666e-01,
        -2.8268e-01,  1.6672e-01,  2.2586e-02,  1.2964e-01, -2.0733e-01,
        -5.3096e-01,  2.9296e-01, -3.8547e-01, -1.4032e-01, -1.5128e-01,
         1.1418e-02, -2.3656e-01,  4.9469e-01, -6.8273e-01,  1.8423e-01,
        -2.3721e-01, -2.3325e-01,  3.5928e-01,  5.2766e-01, -1.1666e-01,
         7.5790e-01, -7.2969e-02, -2.7653e-01,  2.0487e-01,  1.5382e-02,
         6.7789e-02, -1.4296e-01,  7.4118e-01, -8.4653e-03, -1.1047e-01,
        -4.1768e-01,  5.3359e-01, -1.3037e-01,  1.8607e-01, -2.0313e-01,
         1.2735e-01,  6.8050e-02, -3.8641e-01, -7.9295e-02, -8.8401e-02,
        -6.2533e-01,  1.1571e-01,  7.1598e-02, -2.0294e-01,  2.0841e-01,
        -3.6428e-01, -1.2320e-01, -8.4356e-01, -6.3232e-01, -1.0498e-01,
         4.2789e-01,  5.4178e-01, -7.5361e-02,  5.2265e-01,  8.4039e-02,
        -2.6937e-01,  3.3212e-01, -9.4599e-02, -3.4131e-02, -2.2504e-01,
        -1.3264e-01,  2.7440e-01,  2.8242e-01, -5.2183e-02, -3.4659e-01,
         5.7585e-01,  6.1910e-01, -2.7070e-01, -4.5024e-01,  5.5037e-01,
         8.5526e-02, -2.3384e-01,  1.1789e-01, -1.2587e-01,  1.9894e-01,
        -2.3517e-01, -2.3369e-01,  1.9143e-01, -4.5501e-01,  1.3966e-01,
        -1.9227e-01,  3.9775e-02,  2.1560e-01, -3.3690e-01, -3.8437e-01,
         1.7647e-01,  3.2752e-01,  9.3719e-02,  1.2895e-01, -9.3952e-01,
        -3.8214e-01, -4.3267e-01,  1.9816e-01,  1.6576e-01,  5.5276e-01,
        -6.1787e-01, -6.4238e-02,  4.1464e-01,  1.5574e-01,  6.5090e-02,
         1.1775e-01,  1.1705e-01,  1.1174e-01, -9.8249e-02,  1.9385e-01,
         4.2154e-02,  2.8664e-01,  1.9533e-01, -5.9119e-02,  3.5323e-01,
        -6.1269e-02, -8.5089e-02,  4.6307e-01,  2.7760e-01, -9.6143e-02,
        -7.4827e-02, -4.5589e-01,  8.0687e-01, -1.2860e-01,  2.8639e-01,
        -2.0439e-02,  1.4679e-01,  2.2997e-01,  1.1598e-01,  4.0634e-02,
         6.0511e-01,  1.6979e-01,  2.2652e-01,  1.4899e-02, -1.1324e-01,
         4.9865e-01,  1.0883e-01, -1.2512e-01,  3.9757e-01, -3.0077e-01,
        -2.0194e-01, -3.1410e-01, -4.8397e-01, -1.5339e-01, -1.1883e-01,
        -1.3114e-01,  4.3743e-01,  5.0149e-01, -1.4896e-01, -8.2309e-01,
         1.3179e-01, -2.1141e-01,  3.1561e-01,  1.8145e-03,  1.0392e-01,
         2.9074e-01, -2.2170e-01, -3.0791e-01, -4.2069e-01,  5.8855e-01,
        -8.9967e-02,  8.1194e-02,  4.6586e-01,  5.5955e-01,  1.8574e-01,
        -1.0527e-01, -2.3358e-01, -9.4016e-02, -4.8465e-01,  2.2229e-01,
        -6.2900e-01,  5.2304e-03,  5.0439e-01, -2.9019e-01, -9.2053e-02,
        -5.2383e-01, -3.3552e-02,  4.3284e-02, -4.3537e-01,  1.4919e-01,
        -2.0695e-01, -5.6656e-01,  2.2140e-01, -2.1030e-01, -1.1001e-01,
         8.5679e-01,  1.1387e-01, -3.5059e-01, -1.3430e-01,  3.5372e-01,
         2.0641e-01,  6.2591e-02,  1.7398e-01,  3.3149e-01, -2.5503e-01,
         8.1854e-02, -4.1848e-01, -3.8363e-02, -3.1164e-01,  3.7103e-01,
        -1.4613e-01,  3.7279e-01,  3.8274e-02, -1.6675e-01, -7.0739e-03,
        -6.1171e-02, -4.8125e-01, -5.6152e-01,  1.0805e-01,  7.6530e-02,
         5.2409e-01, -4.8720e-01,  2.2968e-01,  8.6017e-02, -9.9089e-03,
         3.3068e-01,  2.9863e-01, -3.3764e-01, -3.9067e-01,  4.5896e-01,
         2.5548e-01, -2.7692e-01, -1.9906e-01, -2.6608e-01, -1.5738e-01,
        -1.0761e-01, -3.7992e-01, -4.4012e-01, -1.4690e-01, -3.1457e-01,
         2.1568e-01,  2.2613e-01, -2.8068e-01, -1.7176e-01, -1.1444e-01,
         2.3916e-01, -1.6279e-01,  9.4988e-02, -2.7666e-01,  2.0556e-01,
        -7.6074e-02, -3.6980e-01,  3.7804e-01, -2.4630e-01,  1.3576e-01,
         1.6534e-01, -1.3109e-01, -2.0494e-01, -5.3798e-01,  7.6692e-02,
        -4.6616e-01, -1.4679e-01,  1.0859e-01,  6.0278e-01, -5.8286e-01,
        -2.5979e-01, -2.7589e-01,  4.8136e-02,  8.7222e-01, -3.5569e-02,
        -3.3385e-01,  1.2776e-01,  4.5560e-01,  1.4252e-02, -1.2123e-01,
        -3.5344e-01,  4.0702e-01,  1.4908e-01,  4.5900e-01, -4.5274e-01,
        -4.6956e-02,  4.7538e-02, -2.1533e-01, -2.4453e-01,  1.2743e-01,
        -1.8984e-01, -2.6452e-02, -2.1470e-01,  2.0226e-01, -1.5740e-01,
        -7.1930e-02, -5.0863e-01, -2.0890e-01, -4.0327e-01,  3.6499e-01,
        -2.6165e-04,  6.0603e-02,  6.9569e-02,  1.3398e-01, -2.3802e-01,
        -2.9838e-01, -3.2634e-01,  5.3177e-01,  4.6295e-01,  3.2145e-02,
        -1.7110e-01,  3.6059e-01, -1.4404e-01, -4.7977e-01, -4.0452e-01,
        -2.6013e-01, -2.1419e-01, -3.1320e-01,  2.0940e-01,  1.1994e+00,
         4.2735e-02,  4.2101e-02, -8.9060e-02,  3.8788e-01, -3.6061e-01,
        -3.6121e-01, -1.2262e-01,  2.2240e-01,  1.3988e-01,  1.7171e-01,
         1.7589e-02,  1.5245e-01,  3.2679e-01, -6.4240e-02, -7.7391e-01,
        -2.8837e-01,  3.2101e-01, -1.2887e-01,  5.3393e-02,  2.5353e-02,
        -6.0654e-02, -3.1816e-01,  2.7098e-01, -1.3659e-01,  1.7188e-01,
         5.9826e-02,  2.0921e-01, -3.2964e-01,  3.7444e-01, -4.3947e-02,
         3.4526e-01, -1.6506e-01,  6.4708e-01, -3.2409e-01,  1.1355e-01,
        -3.7349e-01, -3.4691e-02, -3.2723e-01, -6.4022e-02,  3.4477e-01,
        -7.5624e-02, -1.7574e-02, -1.6774e-01,  2.4028e-01,  4.9508e-02,
        -4.6333e-01, -1.6326e-02, -5.8205e-01, -6.9666e-03, -4.1566e-02,
        -3.3514e-01,  5.8114e-02, -4.6286e-02, -4.8341e-04,  7.0344e-01,
         1.2674e-01,  5.3124e-02, -1.2335e-01, -1.9522e-01,  1.7934e-01,
         2.7061e-01, -8.5578e-01, -6.8343e-02, -5.0685e-01,  4.5522e-01,
         2.4988e-01,  2.6867e-01,  5.5378e-01, -7.9803e-01,  7.7997e-02,
        -2.0203e-01,  4.1295e-01,  1.1794e-01, -3.8049e-01,  1.6412e-02,
         1.7891e-01,  1.6681e-01, -1.3894e-01, -4.7101e-01, -4.6546e-01,
        -4.7259e-02,  3.2267e-01, -4.9633e-01, -7.2095e-02, -7.9168e-01,
        -2.9296e-01,  3.4422e-01, -1.9746e-01,  7.6544e-02, -2.5928e-01,
         1.8074e-01, -6.8216e-02,  3.4519e-01, -1.8312e-01,  6.7683e-02,
         2.6935e-01,  1.8959e-01,  2.3142e-01, -4.6679e-01,  1.6267e-01,
         1.8326e-01, -4.3748e-01], device='cuda:0', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0015, 0.0010, 0.0039, 0.0018, 0.0015, 0.0011, 0.0015, 0.0017, 0.0010,
        0.0014, 0.0022, 0.0012, 0.0010, 0.0013, 0.0019, 0.0013, 0.0009, 0.0010,
        0.0010, 0.0054, 0.0008, 0.0012, 0.0017, 0.0010, 0.0010, 0.0010, 0.0007,
        0.0014, 0.0019, 0.0018, 0.0032, 0.0010, 0.0059, 0.0012, 0.0023, 0.0023,
        0.0011, 0.0016, 0.0011, 0.0025, 0.0013, 0.0013, 0.0013, 0.0199, 0.0023,
        0.0012, 0.0009, 0.0380, 0.0020, 0.0027, 0.0095, 0.0008, 0.0024, 0.0025,
        0.0016, 0.0008, 0.0108, 0.0025, 0.0026, 0.0349, 0.1076, 0.0011, 0.0009,
        0.0227, 0.0011, 0.0023, 0.0026, 0.0010, 0.0012, 0.0027, 0.0024, 0.0022,
        0.0021, 0.0015, 0.0011, 0.0044, 0.0008, 0.0011, 0.0021, 0.0014, 0.0010,
        0.0054, 0.0031, 0.0016, 0.0014, 0.0015, 0.0010, 0.0014, 0.0019, 0.0263,
        0.0010, 0.0014, 0.0008, 0.0024, 0.0033, 0.0045, 0.0017, 0.0007, 0.0011,
        0.0020, 0.0028, 0.0016, 0.0008, 0.0015, 0.0016, 0.0021, 0.0012, 0.0009,
        0.0010, 0.0019, 0.0025, 0.0010, 0.0015, 0.0010, 0.0023, 0.0026, 0.0011,
        0.0028, 0.0039, 0.0016, 0.0025, 0.0014, 0.0023, 0.0045, 0.0026, 0.0021,
        0.0009, 0.0015, 0.0013, 0.0010, 0.0012, 0.0049, 0.0047, 0.0014, 0.0413,
        0.0011, 0.0017, 0.0011, 0.0019, 0.0011, 0.0010, 0.0013, 0.0010, 0.0027,
        0.0024, 0.0026, 0.0009, 0.0015, 0.0009, 0.0010, 0.0019, 0.0016, 0.0108,
        0.0034, 0.0013, 0.0012, 0.2578, 0.0054, 0.0014, 0.0012, 0.0014, 0.0017,
        0.0016, 0.0010, 0.0019, 0.0052, 0.0011, 0.0018, 0.0008, 0.0018, 0.0021,
        0.0008, 0.0019, 0.0021, 0.0011, 0.0019, 0.0009, 0.0025, 0.0020, 0.0009,
        0.0023, 0.1437, 0.0016, 0.0316, 0.0013, 0.0013, 0.0010, 0.0010, 0.0020,
        0.0021, 0.0016, 0.0014, 0.0014, 0.0040, 0.0010, 0.0018, 0.0015, 0.0021,
        0.0009, 0.0019, 0.0066, 0.0013, 0.0011, 0.0008, 0.0038, 0.0019, 0.0014,
        0.0019, 0.0018, 0.0008, 0.0020, 0.0013, 0.0160, 0.2233, 0.0012, 0.0019,
        0.0016, 0.0014, 0.0062, 0.0016, 0.0012, 0.0011, 0.0016, 0.0012, 0.0014,
        0.0019, 0.0039, 0.0037, 0.0012, 0.0011, 0.0013, 0.0008, 0.0012, 0.0030,
        0.0031, 0.0025, 0.0009, 0.0022, 0.0008, 0.0012, 0.0016, 0.0040, 0.0351,
        0.0025, 0.0014, 0.0033, 0.0010, 0.0008, 0.0017, 0.0011, 0.0018, 0.0012,
        0.0010, 0.0039, 0.0393, 0.0010, 0.0039, 0.0010, 0.0011, 0.0038, 0.0015,
        0.0019, 0.0066, 0.0039, 0.0010, 0.0013, 0.0221, 0.0008, 0.0009, 0.0010,
        0.0039, 0.0008, 0.0017, 0.0107, 0.0011, 0.0009, 0.0019, 0.0042, 0.0017,
        0.0009, 0.0017, 0.0011, 0.0009, 0.0019, 0.0050, 0.0045, 0.0012, 0.0012,
        0.0010, 0.0015, 0.0018, 0.0023, 0.0016, 0.1816, 0.0010, 0.0018, 0.0016,
        0.0009, 0.0012, 0.0017, 0.0011, 0.0020, 0.1237, 0.0013, 0.0012, 0.0016,
        0.0016, 0.0015, 0.0018, 0.0017, 0.0011, 0.0011, 0.0014, 0.0013, 0.0081,
        0.0011, 0.0021, 0.0019, 0.0012, 0.0011, 0.0014, 0.0011, 0.0014, 0.0016,
        0.0008, 0.0012, 0.0021, 0.0057, 0.0023, 0.0045, 0.0020, 0.0010, 0.0018,
        0.0015, 0.0015, 0.0017, 0.0011, 0.0012, 0.0020, 0.1117, 0.0009, 0.0028,
        0.0019, 0.0028, 0.0016, 0.0013, 0.0020, 0.0009, 0.0036, 0.0008, 0.0020,
        0.0020, 0.0018, 0.0039, 0.0018, 0.0022, 0.0014, 0.0011, 0.0010, 0.0016,
        0.0022, 0.0010, 0.0009, 0.0027, 0.1588, 0.0009, 0.0025, 0.0011, 0.0013,
        0.0018, 0.0011, 0.0007, 0.0015, 0.0014, 0.0015, 0.0027, 0.0013, 0.0014,
        0.0004, 0.0008, 0.0011, 0.0019, 0.0209, 0.0016, 0.0016, 0.0013, 0.0016,
        0.0008, 0.0015, 0.0022, 0.0023, 0.0364, 0.0028, 0.0026, 0.0027, 0.0007,
        0.0018, 0.0010, 0.0019, 0.0009, 0.0010, 0.0021, 0.0018, 0.0010, 0.0010,
        0.0018, 0.0100, 0.0011, 0.0013, 0.0010, 0.0017, 0.0012, 0.0043, 0.0015,
        0.0071, 0.1736, 0.0020, 0.0011, 0.0012, 0.0015, 0.2485, 0.0009, 0.0013,
        0.0016, 0.0021, 0.0009, 0.0018, 0.0024, 0.0012, 0.0011, 0.0036, 0.0022,
        0.0119, 0.0032, 0.0016, 0.0014, 0.2320, 0.0011, 0.0015, 0.0032, 0.0016,
        0.0057, 0.0016, 0.0010, 0.0013, 0.0010, 0.0021, 0.0008, 0.0011, 0.0029,
        0.0016, 0.0039, 0.0016, 0.0010, 0.0054, 0.0010, 0.0008, 0.1276, 0.0051,
        0.0014, 0.0022, 0.0009, 0.0010, 0.0010, 0.0023, 0.0026, 0.0017, 0.0013,
        0.0016, 0.0023, 0.0018, 0.0015, 0.0027, 0.0016, 0.0013, 0.0021, 0.0014,
        0.0043, 0.0008, 0.0015, 0.0016, 0.0028, 0.0013, 0.0051, 0.0012, 0.0013,
        0.0014, 0.0010, 0.0007, 0.0017, 0.0010, 0.0015, 0.0018, 0.0011, 0.0013,
        0.0013, 0.0010, 0.0010, 0.0016, 0.0011, 0.0010, 0.0012, 0.0031, 0.0017,
        0.0017, 0.0035, 0.0011, 0.0009, 0.0028, 0.0012, 0.0009, 0.0009],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-1.6215e-01, -1.9679e-01, -1.2658e-01,  ..., -3.5523e-03,
          3.8833e-02, -2.8624e-03],
        [-1.5693e-01, -3.5565e-01, -1.6951e-02,  ..., -3.1277e-01,
         -2.6739e-01, -4.2784e-01],
        [-1.5250e-01, -2.2026e-01, -2.1334e-02,  ..., -2.5777e-01,
         -1.9180e-01, -1.6079e-01],
        ...,
        [-1.5331e-01,  1.1296e-01, -2.6415e-01,  ..., -1.4421e-01,
          1.6665e-01, -2.4855e-01],
        [ 1.0140e-01, -6.0144e-02,  4.6212e-01,  ...,  2.4765e-01,
         -2.9743e-01, -1.2598e-02],
        [ 3.6668e-01, -2.2725e-01, -5.4217e-01,  ...,  9.6206e-05,
          2.5094e-01, -4.2031e-01]], device='cuda:0', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[4.9287e-04, 3.2508e-04, 1.0967e-03,  ..., 3.1553e-04, 3.7379e-04,
         2.1720e-04],
        [9.7707e-01, 9.4684e-01, 9.9602e-01,  ..., 9.3000e-01, 9.6365e-01,
         8.9069e-01],
        [9.8714e-01, 9.5658e-01, 1.0001e+00,  ..., 9.5852e-01, 9.7238e-01,
         8.9958e-01],
        ...,
        [3.7841e-03, 3.6639e-03, 1.1777e-02,  ..., 3.3776e-03, 3.0917e-03,
         2.8208e-03],
        [1.3324e-03, 9.1748e-04, 3.1512e-03,  ..., 8.7627e-04, 1.0201e-03,
         6.0184e-04],
        [1.1558e-03, 7.7013e-04, 2.7874e-03,  ..., 7.8002e-04, 8.9527e-04,
         5.3145e-04]], device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 1.5846e-01, -3.6783e-01, -3.5703e-01,  2.3801e-01, -1.8287e-01,
        -5.1418e-01, -4.2303e-01,  6.3794e-01,  6.8816e-02,  3.3079e-01,
        -2.6358e-01,  3.2462e-01,  4.2508e-01, -1.7441e-01,  5.0507e-01,
         9.7192e-02,  1.0977e-02, -9.6508e-02,  6.2244e-02,  1.2690e-02,
         2.4598e-01,  5.7585e-01,  2.3211e-01, -1.0563e-01, -4.0446e-01,
         1.7440e-01, -1.1882e-01, -2.1169e-01, -1.8700e-01,  3.9106e-01,
        -3.7856e-01, -7.9295e-02, -1.6629e-01,  1.1434e-02, -1.4441e-01,
         1.3030e-01,  7.0308e-02, -3.1776e-01,  4.2273e-01,  2.4049e-01,
        -1.6996e-02,  1.8460e-01,  9.1170e-02, -2.9394e-01,  6.3632e-01,
         9.4333e-03, -7.8079e-01, -1.0649e-01, -1.3355e-02, -1.3730e-02,
         4.6899e-01,  3.2319e-01, -8.3046e-02, -3.4966e-02, -1.9849e-01,
        -3.1361e-01,  1.4742e-01, -2.0901e-01,  7.4949e-02,  4.9004e-02,
         1.6119e-01,  8.5941e-01,  3.1963e-01, -2.1719e-01, -2.7188e-01,
        -1.6163e-01,  3.8252e-01, -3.2876e-01, -2.8395e-01,  3.8760e-01,
         3.1520e-01, -5.8861e-01,  6.1271e-03, -3.2148e-01, -3.4951e-01,
        -3.0528e-01, -2.8280e-01,  2.0261e-01,  5.6575e-01,  2.4130e-01,
         2.7711e-01, -2.6389e-01,  5.7538e-01, -4.4786e-01, -5.3783e-01,
        -2.1849e-01,  2.8709e-01, -2.6642e-01, -1.6737e-01, -3.6309e-01,
        -4.4208e-01,  1.7243e-01,  9.1655e-03,  4.1416e-02,  1.5959e-01,
        -2.6156e-01,  4.5157e-01, -1.1964e-01, -1.5460e-01, -2.9453e-01,
        -2.8786e-02,  4.1944e-04, -2.2863e-01, -2.3942e-01, -8.2199e-02,
        -3.0604e-01, -2.8770e-01, -7.5470e-02, -4.0329e-01,  9.5800e-02,
        -2.5587e-01, -5.1132e-01, -1.7196e-01, -1.4796e-01,  2.2532e-02,
        -3.0241e-01, -3.3817e-01, -1.9545e-01,  1.8050e-01, -2.7067e-01,
        -5.5327e-01, -2.3824e-01,  3.9008e-01,  9.2138e-02, -1.8541e-01,
        -1.5860e-02, -1.8177e-01, -2.6006e-01,  3.2785e-01, -7.6045e-01,
         5.8722e-02, -2.4760e-01, -3.9406e-01,  3.4173e-01, -4.0611e-01,
         1.0528e-01, -3.6516e-01,  5.8729e-01, -2.6902e-01,  9.8967e-02,
         1.3079e-01, -3.2537e-01, -6.5887e-01, -1.2948e-01,  1.8794e-01,
         5.5403e-01, -1.7111e-01, -1.5268e-01, -1.4180e-02, -4.1412e-01,
        -4.2279e-01, -2.7145e-01,  2.9061e-01, -3.5683e-02,  2.2959e-01,
         2.0202e-02,  8.9505e-02, -2.3174e-01,  7.6668e-02,  1.6486e-01,
         2.3895e-01, -5.0149e-01,  1.8281e-01, -2.4883e-01,  1.1687e-01,
        -4.0923e-01,  2.9516e-01,  1.2138e-03, -1.4739e-01,  6.2006e-02,
         1.2941e-01, -2.0239e-01, -2.0961e-01, -2.0318e-01,  4.8978e-01,
        -3.3071e-01,  2.7428e-01,  1.5870e-01,  5.2722e-01,  3.6026e-02,
        -1.6409e-01, -7.0195e-01,  6.8785e-01,  2.5152e-01, -1.2678e-01,
        -3.6100e-01,  3.0910e-01, -2.0155e-01, -2.9434e-01, -9.4403e-02,
         8.5342e-02, -2.9662e-01,  6.9493e-01, -3.0174e-01, -3.0657e-01,
        -3.2835e-01, -2.9918e-01,  2.4088e-01, -2.8917e-01, -2.7540e-01,
        -4.1360e-01, -3.3828e-01, -2.6743e-01, -3.3987e-01, -8.7294e-02,
         3.3507e-01, -1.4796e-01, -2.7372e-01,  5.7054e-01,  2.6509e-02,
        -3.8002e-01,  3.4992e-01, -4.4177e-02, -3.4350e-01, -2.8195e-01,
        -5.0248e-01,  4.0462e-01,  6.3151e-01, -4.0638e-01, -6.7686e-03,
        -2.6065e-01, -4.2208e-01,  2.3048e-01,  2.7921e-01, -2.7348e-01,
         1.6374e-01,  1.1064e-01, -2.5024e-01, -3.4959e-01, -2.4883e-01,
        -8.8562e-02, -3.1068e-01, -2.8023e-01, -9.0202e-02, -1.7435e-01,
        -2.5947e-01, -2.3045e-02, -3.5193e-02,  1.7325e-01,  7.6959e-02,
        -1.4353e-01,  1.7495e-01,  2.2901e-01, -6.1632e-02,  9.9343e-02,
        -3.3285e-01, -3.7887e-02, -2.8384e-01, -3.8453e-01, -1.2605e-01,
        -1.5706e-01,  2.4148e-01,  7.4934e-03,  2.5350e-01,  2.2535e-01,
         1.2496e-01, -2.0516e-01,  2.4298e-01,  9.8800e-02, -3.9630e-01,
        -8.0094e-02,  1.4173e-01, -3.1725e-01, -4.9468e-03,  2.9207e-01,
         1.2876e-01, -2.6679e-01, -3.4464e-01, -3.1944e-01, -2.6566e-01,
        -1.9139e-01, -2.4702e-01, -2.6798e-01, -3.3689e-01, -2.1022e-01,
         8.1901e-02,  6.6964e-01, -1.5496e-01, -3.3386e-01, -1.6036e-01,
         7.7338e-02,  1.0056e-01, -2.6388e-01,  1.9334e-01, -4.7203e-01,
         2.1555e-01, -3.3713e-01, -2.7924e-01, -2.7135e-01, -4.0301e-01,
         1.5230e-01,  2.9298e-02, -2.6101e-01, -4.5651e-01, -3.1767e-01,
         4.1515e-01,  4.2702e-01, -4.1675e-01, -2.3556e-01, -1.9035e-01,
         4.6204e-01,  6.0097e-01,  5.6345e-01,  2.0851e-01,  1.6724e-01,
         2.9645e-01,  1.2656e-01, -7.4095e-01,  2.8414e-01,  3.9684e-01,
        -1.1573e-01, -3.5782e-01, -4.2691e-02, -2.3361e-03,  1.1464e-01,
        -2.3958e-01,  8.9338e-02,  1.5166e-01, -3.7575e-02, -9.9780e-02,
         6.2765e-02, -1.6037e-01,  4.1547e-01, -3.0798e-01, -2.5050e-01,
         6.3644e-01, -2.8569e-01, -4.4960e-01,  4.5717e-01,  3.5211e-03,
        -1.7819e-01, -2.7467e-01, -6.3773e-01, -2.7993e-01, -1.6186e-01,
        -1.4958e-01, -2.2234e-02, -2.5984e-01, -3.4519e-01, -9.2132e-02,
        -7.5982e-02, -5.7796e-02,  4.2381e-02, -2.5573e-04,  8.0105e-02,
        -8.2145e-02,  1.2456e-01, -2.6670e-01, -2.0614e-01, -1.6841e-01,
        -3.9205e-01, -4.3330e-01,  3.3117e-01, -2.4899e-01,  4.2722e-01,
        -1.9439e-01, -9.1902e-02, -9.5804e-02, -3.6708e-01, -1.1744e-01,
        -7.7950e-01,  4.0218e-02,  3.0687e-01,  1.5068e-01,  2.3503e-01,
        -8.6238e-02, -3.3340e-01, -2.4087e-01, -3.4119e-01, -4.1495e-01,
        -3.7860e-01, -2.5959e-02,  4.3974e-01,  1.1730e-01, -4.2924e-01,
        -6.0538e-02, -3.3456e-01, -2.1266e-01, -2.8101e-01, -3.4720e-01,
        -3.5186e-01, -2.4181e-01,  1.9941e-01, -2.3478e-02, -3.5415e-01,
         7.6365e-01,  7.7148e-01,  1.5078e-01,  1.1196e-01, -2.3461e-01,
        -2.3135e-01,  3.1321e-01,  8.0802e-02,  6.0760e-01, -1.4675e-01,
         3.4140e-02, -3.4499e-01,  6.1002e-01, -1.5107e-02, -2.3803e-01,
         1.7045e-01,  3.9631e-02, -4.7239e-01,  1.1184e-02,  1.4599e-01,
        -2.2489e-01,  5.4565e-01, -1.4893e-01, -6.1106e-01, -5.1749e-01,
         1.7083e-01,  5.0490e-01, -1.8602e-01,  4.8072e-01,  4.0835e-01,
        -2.3501e-01, -8.2266e-02, -5.6952e-03, -2.5463e-01,  6.0547e-02,
         3.3476e-01, -2.5142e-01,  3.3577e-01,  1.8367e-01, -2.8185e-01,
         1.1703e-01, -2.2574e-01,  3.6461e-01,  2.4291e-01,  2.4034e-01,
         8.8268e-02,  8.0691e-03,  4.0186e-01,  4.0289e-01,  1.3739e-01,
        -3.6338e-01, -2.9768e-01, -1.6752e-01, -1.4436e-01,  4.4970e-02,
         1.4246e-01, -2.1698e-01, -2.8142e-01, -4.9756e-01,  2.8387e-01,
         5.2781e-01, -2.0728e-01,  6.6667e-02, -3.0296e-01,  4.6984e-01,
        -2.9953e-01, -1.4246e-01, -7.3479e-02,  4.8888e-03, -5.6605e-01,
         5.0775e-01, -3.0561e-01,  2.5509e-01,  7.6687e-02, -2.1281e-02,
        -2.9075e-01, -5.8655e-01, -1.5876e-01, -5.7872e-01,  4.3677e-01,
        -2.6698e-01,  1.3285e-01, -4.8778e-01, -1.4400e-01, -2.6903e-01,
         1.1332e-01, -2.8856e-01, -3.3921e-01,  4.5871e-02,  1.5848e-01,
        -2.9094e-01, -2.4441e-01, -2.5668e-01, -3.3707e-01, -3.0549e-01,
         2.6412e-01, -2.8545e-01,  2.5240e-01,  1.3337e-01, -2.5049e-01,
         8.2262e-01,  1.9493e-01,  2.0623e-01, -2.7848e-01, -6.2014e-02,
        -1.8815e-01,  1.7065e-01, -2.7952e-01, -1.7623e-01, -3.9981e-02,
        -6.4413e-02, -3.1204e-01, -2.0545e-01,  3.6547e-01, -2.9286e-01,
        -3.3336e-01, -1.4755e-01,  3.8593e-01, -3.5596e-01, -1.9228e-01,
         5.4086e-01, -4.6615e-01,  7.5002e-02,  1.1605e-01, -7.4137e-01,
         1.6146e-01, -8.2921e-02], device='cuda:0', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([4.6865e-04, 9.7293e-01, 9.8352e-01, 3.0338e-02, 9.8495e-01, 1.7913e-02,
        3.1497e-03, 1.3598e-03, 4.9868e-03, 4.3688e-03, 9.6908e-01, 3.5534e-02,
        5.9612e-03, 9.6338e-01, 5.3099e-03, 1.2601e-03, 5.2838e-03, 4.8752e-03,
        6.9851e-04, 8.8555e-03, 7.4263e-02, 9.6699e-04, 6.6065e-04, 4.3296e-03,
        1.2819e-03, 1.1606e-01, 5.1387e-01, 5.8496e-01, 1.8690e-03, 1.3709e-02,
        9.7991e-04, 3.2686e-01, 4.5212e-02, 4.7160e-02, 8.6128e-04, 2.9147e-02,
        3.8686e-02, 2.3229e-03, 2.0691e-03, 7.2735e-03, 4.4792e-03, 8.1030e-03,
        1.3497e-02, 8.3192e-04, 4.4022e-03, 2.3783e-03, 1.4789e-03, 6.7404e-01,
        2.4061e-03, 9.9815e-03, 1.6149e-03, 1.6718e-03, 1.3270e-01, 1.2270e-02,
        2.7726e-03, 9.8314e-01, 2.1726e-03, 6.1211e-03, 2.1857e-03, 5.7954e-03,
        1.3140e-02, 9.1705e-03, 1.0511e-02, 9.8875e-01, 9.8487e-01, 2.1900e-01,
        5.6113e-01, 9.8528e-01, 9.8534e-01, 2.9708e-03, 1.9975e-03, 4.3938e-03,
        5.2270e-03, 1.7384e-03, 3.9781e-03, 9.8413e-01, 9.8527e-01, 4.0264e-03,
        4.9523e-03, 1.2592e-03, 1.1584e-02, 9.8656e-01, 2.9711e-03, 8.2152e-03,
        8.7972e-04, 2.5415e-03, 5.2519e-01, 3.2652e-03, 9.3786e-03, 1.2708e-03,
        1.3445e-02, 2.9637e-03, 2.7884e-03, 2.7938e-01, 1.1345e-03, 9.8764e-01,
        2.5429e-03, 1.6635e-03, 1.6654e-03, 1.6451e-02, 4.8906e-02, 3.6849e-03,
        6.2428e-01, 9.8852e-01, 8.0464e-03, 5.2979e-01, 9.8657e-01, 7.4838e-03,
        4.4766e-03, 9.4419e-04, 9.7628e-01, 1.7626e-03, 1.1312e-03, 5.8924e-02,
        2.3261e-03, 6.9512e-04, 1.1490e-02, 3.2619e-01, 2.0090e-03, 9.8530e-01,
        2.2145e-03, 3.4554e-03, 2.6997e-02, 1.2147e-03, 1.4114e-03, 1.2326e-02,
        1.7671e-01, 9.9057e-01, 7.6508e-03, 1.3356e-03, 6.8525e-01, 9.8382e-01,
        1.6767e-03, 1.5558e-01, 9.6512e-01, 2.0718e-03, 3.2861e-01, 9.8705e-04,
        2.5468e-03, 3.1601e-03, 3.4958e-03, 2.4804e-03, 5.8338e-02, 2.6886e-03,
        3.3797e-03, 2.1617e-03, 3.4788e-03, 9.8159e-01, 5.5149e-03, 8.7032e-04,
        3.9549e-03, 9.8848e-01, 3.9276e-03, 3.9897e-01, 6.4168e-03, 4.9559e-01,
        5.5820e-03, 9.8617e-01, 3.2588e-03, 2.3433e-03, 7.6033e-03, 1.3414e-03,
        4.4838e-04, 2.4085e-03, 9.7228e-02, 4.4779e-03, 1.1762e-02, 1.6703e-03,
        5.6040e-03, 4.0979e-01, 1.3089e-02, 9.8991e-01, 9.8724e-01, 9.8787e-01,
        1.5165e-02, 9.7948e-01, 3.9124e-03, 1.1453e-02, 8.9125e-03, 1.7151e-02,
        2.8159e-03, 1.8903e-03, 2.3445e-03, 1.8358e-03, 6.3910e-03, 9.7559e-01,
        1.2556e-03, 9.8734e-01, 1.1700e-01, 2.8324e-03, 5.2237e-03, 1.8221e-02,
        1.5824e-03, 9.6242e-01, 2.1044e-02, 3.0640e-02, 3.4018e-03, 8.7338e-03,
        9.8063e-01, 9.8429e-01, 3.9390e-03, 1.6209e-03, 3.0694e-03, 9.7158e-01,
        3.0909e-03, 3.2407e-03, 2.8116e-02, 3.6080e-03, 5.4723e-01, 3.0330e-03,
        4.0207e-03, 3.0364e-01, 9.8160e-01, 9.8461e-01, 9.8320e-01, 1.7419e-02,
        4.3808e-03, 6.9682e-04, 9.6840e-01, 1.7925e-02, 9.8215e-01, 4.1288e-02,
        6.6609e-04, 5.3544e-03, 9.8530e-01, 1.6675e-03, 4.9498e-02, 1.7285e-03,
        9.7583e-01, 9.7727e-01, 3.0979e-01, 4.8857e-01, 9.8648e-01, 1.3417e-03,
        1.1300e-02, 9.8978e-01, 2.8980e-03, 8.4642e-04, 1.1190e-02, 8.3461e-01,
        6.3736e-03, 9.2102e-03, 1.5260e-03, 9.7823e-01, 1.6918e-01, 9.8089e-01,
        4.5787e-03, 9.8355e-01, 9.7219e-01, 1.8300e-03, 2.1443e-03, 3.7235e-03,
        9.1127e-03, 4.4497e-03, 9.2033e-02, 2.0513e-01, 1.6207e-03, 3.8061e-03,
        2.6251e-03, 9.6934e-01, 9.6397e-03, 1.3104e-02, 9.8192e-01, 1.5993e-03,
        7.3796e-04, 7.9429e-02, 9.7192e-01, 9.8413e-01, 9.7898e-01, 9.8623e-01,
        3.3651e-03, 2.8658e-03, 9.8770e-01, 1.2750e-03, 5.5685e-01, 4.8427e-03,
        4.8277e-03, 1.9538e-03, 9.7794e-01, 2.7453e-03, 4.2453e-03, 2.7805e-03,
        7.9448e-04, 1.4699e-02, 1.2923e-01, 3.0281e-03, 9.7893e-01, 9.8376e-01,
        4.4930e-03, 3.1829e-01, 7.2213e-03, 3.5572e-01, 9.8295e-01, 1.4140e-01,
        8.4769e-01, 1.3240e-02, 1.7469e-03, 9.7098e-01, 5.3965e-01, 9.8689e-01,
        2.1536e-03, 4.5146e-03, 1.3610e-03, 3.1801e-03, 1.6000e-03, 7.6211e-03,
        2.9492e-03, 4.4848e-01, 2.3241e-03, 3.5176e-03, 5.2240e-01, 2.2620e-02,
        8.7564e-04, 6.9978e-03, 2.8281e-03, 3.2716e-02, 2.6460e-02, 7.2187e-03,
        2.0742e-02, 9.8758e-01, 5.9197e-02, 1.4024e-03, 2.4998e-03, 9.8836e-01,
        1.8039e-01, 8.7549e-03, 9.7942e-01, 9.4827e-01, 2.5410e-03, 1.0661e-01,
        2.8589e-03, 9.8481e-01, 9.7920e-03, 9.8322e-01, 1.3404e-02, 9.7640e-01,
        3.2115e-01, 9.8292e-01, 9.8364e-01, 2.0350e-03, 8.2322e-03, 6.0274e-03,
        7.1980e-02, 4.3153e-02, 5.2603e-03, 2.4599e-01, 8.2342e-03, 9.8631e-01,
        9.3842e-01, 9.8008e-01, 9.7000e-01, 3.2221e-03, 4.6235e-03, 9.8887e-01,
        1.0233e-03, 9.8896e-01, 1.1865e-02, 3.0177e-03, 8.3064e-04, 1.9462e-03,
        3.6528e-02, 1.3076e-02, 4.6348e-02, 1.9161e-03, 2.3494e-03, 9.7865e-01,
        7.9379e-03, 1.0450e-02, 7.0007e-04, 9.6771e-01, 8.5653e-03, 1.6665e-02,
        1.5703e-01, 2.4273e-03, 1.9830e-02, 7.7463e-01, 9.7728e-01, 1.5601e-01,
        8.8160e-01, 9.7889e-01, 9.7459e-01, 3.2657e-03, 5.5573e-04, 1.6756e-02,
        1.2937e-03, 2.3610e-03, 1.3185e-03, 2.4781e-03, 8.2772e-03, 9.8037e-01,
        2.5084e-01, 5.0866e-03, 6.1327e-02, 5.1131e-03, 1.8486e-01, 2.0585e-03,
        1.3290e-02, 2.0453e-03, 2.1219e-03, 9.8291e-01, 1.2967e-02, 3.1212e-03,
        6.7526e-04, 5.2704e-03, 3.0385e-03, 1.7063e-03, 9.0354e-04, 1.5217e-02,
        1.1197e-03, 2.1129e-03, 3.3428e-03, 8.1286e-03, 9.8989e-01, 1.1486e-01,
        1.1518e-03, 9.7883e-01, 7.4150e-03, 4.5813e-02, 2.6957e-03, 1.9722e-03,
        1.5891e-03, 6.4817e-02, 1.6257e-01, 1.4762e-03, 9.8363e-01, 3.0233e-03,
        1.0924e-02, 1.4888e-02, 8.9553e-02, 8.2019e-03, 4.7933e-02, 3.3909e-03,
        1.6414e-02, 2.4075e-03, 1.2947e-02, 9.7459e-01, 1.4105e-02, 1.8994e-03,
        4.7213e-03, 1.8025e-03, 8.0998e-04, 7.0225e-01, 9.8155e-01, 5.0057e-03,
        1.0210e-02, 5.0090e-03, 6.4931e-04, 7.9383e-04, 2.6637e-02, 5.1489e-04,
        8.4479e-01, 9.8747e-01, 1.8817e-03, 2.7609e-03, 3.8363e-01, 8.0657e-04,
        1.0555e-03, 7.4602e-04, 3.9821e-04, 1.0292e-03, 1.6555e-03, 2.8969e-03,
        9.6590e-01, 9.2866e-03, 3.0667e-03, 8.4975e-01, 2.4681e-03, 9.5819e-04,
        6.5064e-03, 9.5939e-03, 2.9954e-03, 3.9615e-02, 9.7737e-01, 3.6843e-02,
        4.6232e-02, 9.5141e-01, 9.8839e-01, 9.8476e-01, 6.4651e-03, 9.8134e-01,
        3.1933e-03, 9.8330e-01, 1.7728e-02, 6.4463e-03, 9.7930e-01, 2.4166e-01,
        4.3896e-03, 2.6942e-03, 9.8143e-01, 2.2198e-03, 8.8419e-02, 1.8079e-02,
        6.8293e-04, 1.0380e-01, 8.6740e-03, 1.4609e-02, 7.7479e-04, 9.8752e-01,
        2.0986e-02, 9.8413e-01, 9.8877e-01, 8.4863e-03, 1.5813e-03, 7.5445e-03,
        9.6126e-01, 1.3642e-03, 2.9711e-01, 5.9541e-03, 1.0152e-02, 4.2272e-03,
        1.2969e-03, 1.1308e-03], device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 6.4577e-01, -7.2909e-04, -7.5008e-05,  1.5535e-01, -1.3633e-05,
          5.1733e-02,  2.9633e-01, -2.5736e-01,  1.8025e-01, -8.5231e-02,
          3.9563e-04, -1.5872e-02,  7.6184e-02,  3.0145e-04,  3.0102e-01,
         -2.6216e-01, -1.2391e-01, -1.3320e-01, -5.3477e-01,  6.0374e-02,
         -4.9247e-03,  3.3752e-01, -6.9490e-01, -1.1279e-01, -2.5166e-01,
         -6.4815e-04, -3.1950e-04, -2.6287e-04, -1.7253e-01, -2.6698e-02,
          3.9352e-01,  9.4704e-04, -1.0811e-02, -8.9158e-03, -3.5629e-01,
         -1.9075e-02,  8.7210e-03, -1.9483e-01, -1.6883e-01, -1.2853e-01,
          9.9327e-02,  2.7228e-01, -3.3355e-02, -3.7703e-01,  2.3468e-01,
         -2.4246e-01, -2.4157e-01, -1.5443e-04, -1.7213e-01, -4.1955e-02,
          3.2983e-01,  2.9564e-01,  6.9370e-04,  5.2478e-02, -1.7364e-01,
          2.7245e-05, -2.2680e-01, -8.4753e-02, -1.5430e-01, -1.6179e-01,
          3.7057e-02, -8.3284e-02, -2.0694e-01,  1.7662e-04,  1.2489e-04,
          1.0150e-04, -1.2364e-04, -2.1195e-05,  6.4125e-04, -1.2922e-01,
         -1.7880e-01,  1.7565e-01,  1.7851e-01,  1.7965e-01, -9.8576e-02,
          1.5701e-04, -1.3111e-04,  1.8804e-01, -8.1187e-02,  2.5425e-01,
         -3.5807e-02,  1.3240e-04,  1.1328e-01, -5.3370e-02, -3.8178e-01,
         -2.1107e-01,  7.9965e-05, -2.3361e-01,  1.1871e-01,  2.7863e-01,
          2.9370e-02,  1.4885e-01,  1.5086e-01,  4.2759e-06, -3.2789e-01,
         -9.8939e-05, -1.7194e-01, -2.3381e-01, -2.1493e-01,  3.2060e-02,
          2.3892e-02,  1.5239e-01,  7.7344e-05, -5.5939e-05,  1.7474e-01,
          2.5184e-04, -9.3200e-05,  7.7465e-02,  1.1355e-01,  3.4526e-01,
         -7.4946e-04,  2.7660e-01,  3.1421e-01, -6.5638e-03,  1.9253e-01,
          4.3726e-01,  7.4531e-02,  5.1240e-04, -1.6031e-01,  3.1369e-05,
          2.2524e-01,  1.1389e-01, -3.1394e-02,  4.4987e-01,  2.2412e-01,
         -6.3608e-02,  3.1361e-04, -1.8775e-04, -8.3964e-02,  2.4037e-01,
         -3.7495e-05,  1.5360e-04, -1.9176e-01,  7.7662e-04,  2.1794e-04,
         -1.7784e-01,  4.1274e-04, -3.0840e-01, -1.4177e-01,  1.1764e-01,
         -9.4285e-02, -1.8158e-01,  1.1358e-02, -1.4328e-01, -2.0709e-01,
          1.5358e-01,  1.2953e-01,  5.7996e-05, -6.8562e-02,  3.7688e-01,
         -1.5501e-01, -3.4642e-04, -2.4697e-01, -2.8034e-05,  6.6731e-02,
          4.0028e-04, -1.8395e-01,  1.3575e-04,  1.1704e-01,  1.9018e-01,
          7.8589e-02,  2.6667e-01, -6.7015e-01, -1.9444e-01, -1.0966e-03,
          1.1289e-01, -3.3628e-02,  1.9842e-01, -1.4599e-01, -3.1838e-04,
          3.0442e-02,  6.3366e-05,  1.2186e-04,  1.2329e-04,  3.9932e-02,
          1.9596e-04,  1.1179e-01, -3.7421e-02,  5.1019e-02, -1.7623e-01,
         -1.9742e-01, -1.7006e-01,  2.0029e-01,  2.0838e-01, -1.3456e-01,
          7.7044e-04, -3.3161e-01,  6.4080e-05, -1.9554e-03, -1.8825e-01,
          6.6737e-02, -2.3483e-02, -2.9315e-01,  6.7352e-04, -2.2963e-02,
         -1.0491e-02,  1.1106e-01,  5.8537e-02, -2.1541e-04, -8.5351e-06,
         -2.2968e-01,  2.8151e-01,  1.2610e-01,  3.7340e-04, -1.1578e-01,
         -2.2536e-01, -1.4509e-02, -1.4329e-01,  9.2597e-05, -2.2423e-01,
         -1.4010e-01,  2.3172e-04,  5.0961e-05, -1.0050e-04,  1.8998e-04,
         -4.0996e-02,  1.2654e-01, -4.9824e-01,  4.0911e-04,  1.2101e-01,
         -4.7305e-04, -1.2780e-02,  4.6920e-01, -1.3467e-01,  1.2437e-04,
          2.6244e-01,  6.3485e-03,  2.9353e-01, -2.7161e-04,  2.6840e-04,
         -1.7762e-04,  2.5676e-04,  9.4515e-05,  2.3906e-01,  6.5780e-02,
         -2.4458e-05,  1.1753e-01, -3.6341e-01,  1.2642e-01,  5.2437e-05,
          7.5632e-02, -2.6591e-01,  2.2036e-01,  5.7902e-05,  1.7631e-03,
          4.9922e-04,  1.8497e-01,  2.2951e-04,  2.9888e-05, -2.5049e-01,
          1.5960e-01, -1.0242e-01,  1.0859e-01, -1.0780e-01, -4.4446e-04,
         -3.6076e-04,  1.9181e-01,  1.9421e-01, -1.3181e-01,  4.4832e-05,
         -2.9478e-01, -1.3147e-01, -1.7217e-04, -2.1371e-01,  4.1649e-01,
          2.0119e-03, -2.0815e-05,  1.9517e-05,  5.3316e-04, -1.0348e-04,
         -1.2894e-01, -1.1155e-01,  1.6583e-04, -2.5591e-01,  1.4695e-04,
         -1.6568e-01,  1.4769e-01, -2.2369e-01, -6.9960e-04,  1.6983e-01,
         -1.2006e-01,  1.5086e-01, -4.2219e-01, -1.3550e-01, -3.3170e-03,
          1.5110e-01,  4.7047e-05,  1.8422e-04,  7.8506e-02,  2.6751e-04,
          6.5094e-02, -2.0329e-05,  2.9573e-05, -1.6755e-03, -2.1963e-04,
          1.0287e-01,  1.8494e-01, -3.8642e-04, -2.2488e-04, -3.0823e-04,
          1.5651e-01,  1.0663e-01, -2.7114e-01, -1.1144e-01, -2.0656e-01,
         -6.6806e-02, -3.4022e-01, -1.8994e-04,  1.4482e-01,  1.1539e-01,
          3.7731e-04, -2.1958e-02,  3.5321e-01, -5.5009e-02, -2.2224e-01,
         -6.0726e-02, -5.5711e-02, -1.0642e-01,  8.6131e-02,  9.1096e-05,
         -3.3334e-02,  2.8770e-01, -2.1020e-01, -1.2750e-05, -1.3160e-04,
          5.0758e-02, -4.9829e-04, -1.2584e-03,  2.6460e-01, -2.0529e-04,
          2.7977e-01,  2.0071e-04, -1.3510e-01, -1.6438e-04,  8.3940e-02,
          2.4177e-04,  1.6211e-04,  7.7586e-05, -4.5417e-05, -2.2126e-01,
          1.0483e-01,  1.1763e-01,  2.2434e-03,  5.9190e-03, -1.0625e-01,
          6.4401e-04, -7.7094e-02,  1.0506e-04, -6.3340e-05,  2.1121e-05,
          2.1362e-04,  1.4822e-01, -1.1931e-01,  7.5787e-05,  3.3011e-01,
          4.0499e-05,  4.3445e-02,  1.1106e-01,  4.1763e-01, -2.0603e-01,
         -1.5504e-02,  1.8960e-01,  6.2128e-03, -1.7789e-01, -1.6120e-01,
          1.3706e-04, -1.3601e-01, -1.0242e-01,  4.5232e-01, -6.3769e-04,
         -9.7641e-02,  6.7449e-02, -8.2269e-04,  1.5141e-01, -3.6116e-02,
          1.5571e-06,  9.1909e-04,  5.2132e-04, -6.0929e-05, -9.7767e-04,
         -4.7253e-04,  1.5909e-01, -5.4573e-01,  2.7477e-02, -2.6909e-01,
         -1.4311e-01,  2.8029e-01,  1.5778e-01,  6.1870e-02,  1.4332e-04,
          3.8910e-04,  9.8437e-02, -1.9109e-03,  1.1264e-01, -1.1595e-04,
         -1.7929e-01,  1.1008e-01, -1.6685e-01, -2.4467e-01,  1.5129e-04,
          5.5036e-02,  1.8888e-01, -4.9070e-01,  1.0770e-01, -1.2642e-01,
         -2.5248e-01, -3.6730e-01,  2.9933e-02,  2.9801e-01,  1.9837e-01,
          3.2370e-01,  9.6730e-02, -3.1517e-05, -6.5278e-04, -3.0289e-01,
          8.5886e-05, -6.3515e-02,  8.9225e-03,  1.7848e-01,  1.8654e-01,
          2.2092e-01,  3.4184e-03, -6.8336e-02,  2.1826e-01, -1.8636e-04,
         -1.4797e-01,  9.6047e-02,  2.0714e-01, -5.6592e-04, -5.8109e-02,
          4.0969e-03,  2.1169e-01, -5.5524e-02,  1.5000e-01, -3.6503e-02,
         -2.0887e-04,  3.3607e-02, -2.4436e-01, -9.7484e-02, -2.4364e-01,
          4.0951e-01,  1.3492e-04,  5.8873e-04,  1.0672e-01,  1.1390e-01,
         -1.3709e-01, -5.1214e-01,  3.9138e-01,  2.5224e-02, -5.8860e-01,
         -5.4918e-04, -2.7341e-05,  1.7951e-01,  2.3471e-01,  2.4999e-04,
          3.8599e-01,  3.7293e-01, -4.2288e-01,  7.6399e-01, -3.1065e-01,
         -2.3598e-01, -1.3779e-01, -2.8052e-05,  4.3045e-02, -1.5004e-01,
         -1.4417e-04,  2.9571e-01, -3.6173e-01,  1.3266e-01, -9.6298e-02,
          1.5399e-01, -4.7406e-02,  4.0435e-04,  7.3378e-03, -3.7510e-03,
          1.4158e-05,  2.6991e-04, -5.4402e-04,  2.0556e-01, -1.3219e-05,
         -1.7302e-01,  2.0884e-04, -2.3762e-02,  1.3350e-01,  3.5928e-04,
          3.3962e-04,  1.3947e-01,  1.5184e-01, -4.2890e-04,  2.0897e-01,
         -2.9896e-03, -7.6849e-02,  4.4809e-01,  9.2182e-04, -1.1103e-01,
          4.0160e-02, -4.0298e-01, -3.5550e-04, -5.9915e-02,  1.8748e-04,
         -1.3221e-05, -1.6467e-01,  2.3606e-01, -1.0433e-01,  4.5409e-04,
         -2.7623e-01,  2.8250e-04, -8.2951e-02, -7.1539e-02, -1.5337e-01,
         -2.3966e-01,  2.7622e-01]], device='cuda:0', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[2.9239e-05, 2.1233e-03, 4.2355e-04, 1.2095e-03, 2.5490e-04, 2.7724e-04,
         1.8728e-04, 5.0179e-05, 1.9621e-04, 6.1661e-05, 9.9962e-04, 1.5020e-04,
         8.9123e-05, 1.7650e-04, 3.7115e-04, 3.9920e-05, 1.4511e-04, 1.1294e-04,
         5.3675e-05, 1.0456e-04, 3.1343e-04, 3.2161e-05, 6.1862e-05, 9.2287e-05,
         3.1895e-05, 1.3006e-04, 4.6755e-04, 1.5497e-03, 3.7282e-05, 5.6486e-05,
         4.8857e-05, 2.6165e-03, 1.7513e-04, 2.1789e-04, 2.8451e-05, 1.1924e-04,
         9.6529e-05, 7.9720e-05, 4.4515e-05, 1.8581e-04, 6.1996e-05, 3.6282e-04,
         8.9347e-05, 3.1249e-05, 1.9311e-04, 9.2329e-05, 4.7695e-05, 4.6265e-04,
         6.8235e-05, 7.3807e-05, 9.8497e-05, 9.7249e-05, 3.4877e-04, 1.2277e-04,
         7.8134e-05, 4.1344e-04, 7.8344e-05, 8.8429e-05, 3.8012e-05, 2.2232e-04,
         9.6854e-05, 1.4032e-04, 5.2906e-04, 2.1328e-04, 4.3358e-04, 1.1801e-04,
         2.2689e-04, 1.6227e-04, 4.5266e-04, 5.7676e-05, 4.9766e-05, 1.5084e-04,
         1.6852e-04, 3.4074e-05, 6.0309e-05, 5.2161e-04, 4.8224e-04, 1.5708e-04,
         6.1515e-05, 3.2460e-05, 7.3199e-05, 4.3532e-04, 4.4774e-05, 7.5492e-05,
         3.7556e-05, 1.2104e-04, 2.2115e-04, 1.7801e-04, 2.1688e-04, 4.0039e-05,
         7.1911e-05, 6.9571e-05, 5.1728e-05, 7.8084e-04, 5.1651e-05, 2.6041e-04,
         6.7947e-05, 5.0262e-05, 4.5550e-05, 9.6090e-05, 3.9829e-04, 1.0371e-04,
         5.8295e-04, 2.4981e-04, 2.8410e-04, 2.1135e-04, 1.8861e-04, 9.5355e-05,
         8.5783e-05, 3.7738e-05, 5.0876e-04, 7.3181e-05, 4.4529e-05, 1.7193e-04,
         6.1711e-05, 2.8545e-05, 1.5767e-04, 3.3238e-04, 3.8959e-05, 3.5798e-04,
         6.0857e-05, 5.5572e-05, 2.8441e-04, 9.5611e-05, 3.2332e-05, 1.4338e-04,
         1.5406e-04, 1.3679e-04, 1.0784e-04, 3.5832e-05, 2.0183e-04, 2.6274e-04,
         3.9165e-05, 3.3063e-04, 3.7861e-04, 5.2344e-05, 5.5396e-04, 2.9881e-05,
         5.6081e-05, 4.7739e-05, 4.5576e-05, 7.0709e-05, 2.8795e-04, 5.2661e-05,
         1.4555e-04, 3.8953e-05, 6.2325e-05, 1.5835e-04, 6.0332e-05, 4.0584e-05,
         1.1277e-04, 2.4346e-04, 1.6385e-04, 1.7370e-04, 7.4402e-05, 4.7699e-04,
         2.3289e-04, 1.4119e-04, 5.2008e-05, 7.0962e-05, 1.1293e-04, 4.3951e-05,
         2.7065e-05, 7.4540e-05, 1.4697e-04, 8.4554e-05, 6.6408e-05, 4.1785e-05,
         1.7102e-04, 1.8368e-04, 6.6144e-05, 1.5266e-04, 1.8551e-04, 1.8044e-04,
         1.3263e-04, 1.2876e-03, 6.6164e-05, 6.4639e-05, 6.8040e-05, 6.9438e-04,
         8.3613e-05, 3.6883e-05, 9.3183e-05, 5.7978e-05, 1.7964e-04, 1.4313e-03,
         6.6450e-05, 1.4998e-04, 4.0984e-04, 8.4383e-05, 5.1861e-05, 8.1313e-05,
         7.1763e-05, 1.1154e-03, 1.0231e-04, 8.2153e-05, 5.5493e-05, 1.0706e-04,
         2.1200e-04, 3.8337e-04, 2.1559e-04, 7.6337e-05, 5.2529e-05, 6.1466e-04,
         3.8928e-05, 1.4184e-04, 1.0457e-04, 9.8294e-05, 1.5304e-04, 1.5949e-04,
         1.0497e-04, 1.8593e-04, 1.3447e-04, 2.3930e-04, 4.5936e-04, 1.1892e-04,
         8.7030e-05, 4.1351e-05, 4.2439e-03, 7.7830e-04, 4.8080e-04, 2.0878e-04,
         2.7354e-05, 1.4908e-04, 3.9411e-04, 6.4119e-05, 1.1063e-04, 6.9347e-05,
         1.7575e-03, 2.4993e-04, 4.9564e-04, 3.2527e-04, 2.7453e-04, 3.8286e-05,
         1.5157e-04, 1.3535e-04, 4.0578e-05, 2.8631e-05, 3.0224e-04, 1.3207e-04,
         8.3928e-05, 5.6068e-04, 3.6099e-05, 2.1560e-04, 8.9481e-04, 7.7427e-04,
         2.0152e-04, 5.1822e-04, 1.1196e-03, 6.9406e-05, 4.3197e-05, 6.4282e-05,
         2.3508e-04, 9.0835e-05, 8.1304e-05, 2.2057e-04, 3.3403e-05, 1.5008e-04,
         4.8590e-05, 1.9071e-03, 6.0691e-04, 4.4362e-04, 9.2546e-04, 4.1670e-05,
         2.9990e-05, 1.2416e-04, 6.7490e-04, 2.5747e-04, 1.0606e-03, 3.5505e-04,
         7.3025e-05, 3.8340e-05, 3.4023e-04, 3.5091e-05, 2.2074e-04, 1.4756e-04,
         1.3725e-04, 6.9427e-05, 1.0554e-03, 7.5740e-05, 9.4078e-05, 6.9818e-05,
         4.2624e-05, 5.8809e-04, 6.6257e-04, 6.0708e-05, 1.3193e-03, 4.1469e-04,
         4.7562e-05, 3.3167e-04, 9.2427e-05, 3.8414e-04, 3.7157e-04, 4.2487e-04,
         4.1095e-04, 2.3567e-04, 3.3987e-05, 3.6766e-03, 5.4352e-04, 2.4011e-04,
         4.0363e-05, 7.8700e-05, 4.5415e-05, 4.3715e-05, 4.0532e-05, 9.6801e-05,
         2.1112e-04, 2.5972e-04, 4.4662e-05, 5.7602e-05, 1.1829e-03, 1.0372e-04,
         3.0952e-05, 5.6147e-05, 1.3793e-04, 6.4802e-04, 3.6608e-04, 1.2873e-04,
         4.3097e-04, 1.3859e-04, 8.3234e-04, 5.4176e-05, 9.7266e-05, 2.3594e-04,
         1.1414e-04, 8.5996e-05, 3.7171e-04, 1.2079e-02, 1.3432e-04, 8.1104e-05,
         1.7374e-04, 4.0237e-04, 3.2648e-04, 5.4558e-04, 2.9932e-04, 1.8846e-04,
         1.4490e-04, 3.7771e-04, 1.8135e-04, 7.0473e-05, 1.9291e-04, 1.0511e-04,
         1.3577e-04, 1.1111e-04, 1.0031e-04, 7.9493e-04, 1.1594e-04, 3.8054e-04,
         2.2822e-04, 2.5181e-04, 8.2207e-04, 7.5002e-05, 9.8555e-05, 2.2076e-04,
         3.5899e-05, 1.8577e-04, 9.5321e-05, 4.2852e-05, 4.4890e-05, 5.2382e-05,
         1.7323e-04, 6.1656e-04, 8.5294e-05, 4.5084e-05, 5.6020e-05, 2.2219e-04,
         2.4054e-04, 2.8529e-04, 3.1529e-05, 3.4988e-03, 1.5689e-04, 2.0913e-04,
         3.2586e-04, 6.4042e-05, 1.5321e-04, 2.5588e-04, 1.2832e-03, 2.5482e-04,
         2.4545e-04, 1.4497e-03, 1.6131e-03, 7.2712e-05, 2.6860e-05, 8.7453e-05,
         3.8715e-05, 3.8004e-05, 4.5800e-05, 5.2005e-05, 8.1971e-05, 2.1694e-04,
         1.1740e-03, 8.7269e-05, 1.1865e-04, 1.0255e-04, 7.5483e-04, 4.4529e-05,
         3.7886e-04, 3.6804e-05, 8.5188e-05, 2.5093e-04, 1.6114e-04, 1.2311e-04,
         3.9015e-05, 1.0435e-04, 5.7993e-05, 6.7731e-05, 4.1766e-05, 8.6170e-05,
         3.5684e-05, 6.9673e-05, 2.5605e-04, 1.7863e-04, 1.6923e-04, 1.6902e-04,
         4.3995e-05, 4.0771e-04, 7.7349e-05, 1.2125e-04, 7.4205e-05, 5.7237e-05,
         5.0325e-05, 1.8132e-04, 2.9037e-03, 3.3945e-05, 2.3780e-04, 5.7347e-05,
         2.1999e-04, 9.2770e-04, 1.0252e-04, 9.3194e-05, 8.9849e-05, 1.5592e-04,
         2.3577e-04, 4.9479e-05, 9.9548e-05, 1.7361e-03, 8.1593e-05, 7.9621e-05,
         7.7026e-05, 6.3092e-05, 4.2917e-05, 2.1244e-04, 5.4619e-04, 8.1348e-05,
         2.8433e-04, 1.3395e-04, 3.1135e-05, 2.9328e-05, 1.5862e-04, 2.7871e-05,
         5.4232e-04, 1.5167e-04, 4.6348e-05, 1.1926e-04, 2.6273e-04, 2.7852e-05,
         5.0086e-05, 3.0622e-05, 2.4540e-05, 2.8834e-05, 5.6529e-05, 6.4228e-05,
         2.4338e-04, 6.0627e-05, 6.7908e-05, 3.6664e-04, 1.6126e-04, 3.6652e-05,
         1.9483e-04, 1.4543e-04, 6.3305e-05, 5.0999e-04, 4.6737e-04, 6.7142e-05,
         8.8517e-05, 7.2285e-04, 2.0200e-04, 3.2562e-04, 3.5761e-04, 7.8356e-04,
         1.0502e-04, 4.5129e-04, 8.6165e-05, 1.8677e-04, 3.1343e-04, 9.6244e-04,
         1.1353e-04, 5.9844e-05, 3.4020e-04, 6.8580e-05, 3.3626e-04, 3.4895e-04,
         2.6205e-05, 1.6161e-04, 1.9558e-04, 1.1168e-04, 3.4090e-05, 2.2969e-04,
         3.1329e-04, 5.3694e-04, 1.4805e-04, 2.9646e-04, 5.3301e-05, 1.2338e-04,
         4.4686e-04, 6.7263e-05, 4.1970e-04, 8.0735e-05, 1.5368e-04, 1.2234e-04,
         3.7674e-05, 3.1678e-05]], device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.1573], device='cuda:0', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0003], device='cuda:0', grad_fn=<AddBackward0>)
Using device: cuda:0
===== Training profile tensin-3x512-s003 - 2 =====
[0:00:01.703141] epoch: 0 | elbo: 4321765.64 | train_rmse: 0.6675 | val_rmse: 4.0917 | val_ll: -204.6276
[0:01:31.681327] epoch: 50 | elbo: 2396593.7049999996 | train_rmse: 0.0413 | val_rmse: 4.0197 | val_ll: -192.3891
[0:03:01.902611] epoch: 100 | elbo: 2398208.3975 | train_rmse: 0.0469 | val_rmse: 4.0174 | val_ll: -193.0922
[0:04:32.318091] epoch: 150 | elbo: 2396090.355 | train_rmse: 0.0523 | val_rmse: 4.0146 | val_ll: -192.8327
[0:06:02.666187] epoch: 200 | elbo: 2399816.7825 | train_rmse: 0.0616 | val_rmse: 4.0102 | val_ll: -193.774
[0:07:33.837430] epoch: 250 | elbo: 2397936.335 | train_rmse: 0.066 | val_rmse: 4.0054 | val_ll: -189.8745
[0:09:04.029141] epoch: 300 | elbo: 2393262.6399999997 | train_rmse: 0.0788 | val_rmse: 3.9985 | val_ll: -189.9739
[0:10:34.168895] epoch: 350 | elbo: 2388710.1825 | train_rmse: 0.0659 | val_rmse: 3.9931 | val_ll: -190.9264
[0:12:06.321590] epoch: 400 | elbo: 2387735.615 | train_rmse: 0.0738 | val_rmse: 3.9846 | val_ll: -191.9015
[0:13:38.937469] epoch: 450 | elbo: 2380536.7375 | train_rmse: 0.0701 | val_rmse: 3.9771 | val_ll: -190.0943
[0:15:10.419199] epoch: 500 | elbo: 2372979.5174999996 | train_rmse: 0.0647 | val_rmse: 3.9748 | val_ll: -191.0734
[0:16:39.691511] epoch: 550 | elbo: 2367477.3974999995 | train_rmse: 0.0691 | val_rmse: 3.967 | val_ll: -189.3198
[0:18:08.910661] epoch: 600 | elbo: 2371303.0799999996 | train_rmse: 0.0714 | val_rmse: 3.9633 | val_ll: -190.2049
[0:19:40.177692] epoch: 650 | elbo: 2357740.9574999996 | train_rmse: 0.0634 | val_rmse: 3.957 | val_ll: -187.2893
[0:21:09.354927] epoch: 700 | elbo: 2356646.3449999997 | train_rmse: 0.064 | val_rmse: 3.9514 | val_ll: -188.8545
[0:22:39.679924] epoch: 750 | elbo: 2349802.7525 | train_rmse: 0.0656 | val_rmse: 3.9452 | val_ll: -187.5741
[0:24:08.833976] epoch: 800 | elbo: 2341866.37 | train_rmse: 0.071 | val_rmse: 3.9416 | val_ll: -186.3548
[0:25:37.822701] epoch: 850 | elbo: 2338468.8 | train_rmse: 0.072 | val_rmse: 3.9333 | val_ll: -188.0902
[0:27:07.644064] epoch: 900 | elbo: 2328472.4775000005 | train_rmse: 0.062 | val_rmse: 3.9272 | val_ll: -190.2457
[0:28:36.479893] epoch: 950 | elbo: 2329572.6125 | train_rmse: 0.0665 | val_rmse: 3.9246 | val_ll: -186.2863
[0:30:05.571501] epoch: 1000 | elbo: 2325783.3525 | train_rmse: 0.0661 | val_rmse: 3.917 | val_ll: -190.734
[0:31:34.557592] epoch: 1050 | elbo: 2331593.0175 | train_rmse: 0.0867 | val_rmse: 3.9103 | val_ll: -187.1128
[0:33:03.515347] epoch: 1100 | elbo: 2311365.097499999 | train_rmse: 0.0651 | val_rmse: 3.9049 | val_ll: -185.9111
[0:34:32.658090] epoch: 1150 | elbo: 2319745.8025 | train_rmse: 0.0733 | val_rmse: 3.9012 | val_ll: -186.675
[0:36:02.177383] epoch: 1200 | elbo: 2310783.9524999997 | train_rmse: 0.07 | val_rmse: 3.8956 | val_ll: -186.9243
[0:37:32.062242] epoch: 1250 | elbo: 2298974.58 | train_rmse: 0.0645 | val_rmse: 3.8874 | val_ll: -186.783
[0:39:02.752965] epoch: 1300 | elbo: 2296911.5925000003 | train_rmse: 0.0648 | val_rmse: 3.8818 | val_ll: -184.3908
[0:40:33.382226] epoch: 1350 | elbo: 2293259.3899999997 | train_rmse: 0.0753 | val_rmse: 3.8773 | val_ll: -183.4527
[0:42:03.158009] epoch: 1400 | elbo: 2282943.355 | train_rmse: 0.0637 | val_rmse: 3.8716 | val_ll: -183.6693
[0:43:32.702034] epoch: 1450 | elbo: 2279388.6825 | train_rmse: 0.0657 | val_rmse: 3.866 | val_ll: -184.4181
[0:45:03.226497] epoch: 1500 | elbo: 2276931.8725 | train_rmse: 0.0647 | val_rmse: 3.8589 | val_ll: -182.7473
[0:46:34.530736] epoch: 1550 | elbo: 2271212.8924999996 | train_rmse: 0.0629 | val_rmse: 3.8537 | val_ll: -184.1878
[0:48:03.983424] epoch: 1600 | elbo: 2263384.225 | train_rmse: 0.0601 | val_rmse: 3.8491 | val_ll: -178.2394
[0:49:33.526750] epoch: 1650 | elbo: 2258913.5275 | train_rmse: 0.0607 | val_rmse: 3.8451 | val_ll: -183.9794
[0:51:03.104632] epoch: 1700 | elbo: 2262417.1324999994 | train_rmse: 0.077 | val_rmse: 3.8401 | val_ll: -178.8381
[0:52:34.369542] epoch: 1750 | elbo: 2259495.2375 | train_rmse: 0.0655 | val_rmse: 3.8337 | val_ll: -179.3622
[0:54:06.702952] epoch: 1800 | elbo: 2246011.7749999994 | train_rmse: 0.0646 | val_rmse: 3.8286 | val_ll: -182.1371
[0:55:39.248965] epoch: 1850 | elbo: 2248143.6225 | train_rmse: 0.0765 | val_rmse: 3.8219 | val_ll: -182.0809
[0:57:09.077362] epoch: 1900 | elbo: 2234837.5125 | train_rmse: 0.0666 | val_rmse: 3.8166 | val_ll: -180.2901
[0:58:38.773033] epoch: 1950 | elbo: 2231720.125 | train_rmse: 0.0626 | val_rmse: 3.8087 | val_ll: -179.1425
[1:00:09.121137] epoch: 2000 | elbo: 2228195.92 | train_rmse: 0.0615 | val_rmse: 3.8042 | val_ll: -178.3834
[1:01:40.546523] epoch: 2050 | elbo: 2224380.3025 | train_rmse: 0.0612 | val_rmse: 3.7984 | val_ll: -178.075
[1:03:11.770572] epoch: 2100 | elbo: 2216768.6700000004 | train_rmse: 0.0618 | val_rmse: 3.7925 | val_ll: -178.1753
[1:04:42.788251] epoch: 2150 | elbo: 2215304.8475 | train_rmse: 0.0608 | val_rmse: 3.7865 | val_ll: -176.8313
[1:06:13.895962] epoch: 2200 | elbo: 2214350.17 | train_rmse: 0.0671 | val_rmse: 3.7831 | val_ll: -176.1885
[1:07:44.479037] epoch: 2250 | elbo: 2206015.2175000003 | train_rmse: 0.0593 | val_rmse: 3.7783 | val_ll: -176.6326
[1:09:14.502041] epoch: 2300 | elbo: 2209138.165 | train_rmse: 0.0882 | val_rmse: 3.7743 | val_ll: -174.8944
[1:10:44.678373] epoch: 2350 | elbo: 2196466.0375 | train_rmse: 0.0583 | val_rmse: 3.7683 | val_ll: -177.0616
[1:12:13.890413] epoch: 2400 | elbo: 2191104.6950000003 | train_rmse: 0.0564 | val_rmse: 3.7628 | val_ll: -176.5622
[1:13:42.944956] epoch: 2450 | elbo: 2196047.9924999997 | train_rmse: 0.0626 | val_rmse: 3.7551 | val_ll: -175.2294
[1:15:12.178569] epoch: 2500 | elbo: 2190924.9475000002 | train_rmse: 0.0734 | val_rmse: 3.752 | val_ll: -172.6188
[1:16:41.126382] epoch: 2550 | elbo: 2180508.005 | train_rmse: 0.0568 | val_rmse: 3.7473 | val_ll: -175.5305
[1:18:10.414877] epoch: 2600 | elbo: 2175028.1375 | train_rmse: 0.0645 | val_rmse: 3.745 | val_ll: -172.7032
[1:19:39.930621] epoch: 2650 | elbo: 2170694.2675000005 | train_rmse: 0.0599 | val_rmse: 3.7381 | val_ll: -175.1859
[1:21:10.024053] epoch: 2700 | elbo: 2170250.505 | train_rmse: 0.0621 | val_rmse: 3.7309 | val_ll: -172.2164
[1:22:39.679913] epoch: 2750 | elbo: 2161606.0125 | train_rmse: 0.0575 | val_rmse: 3.7281 | val_ll: -172.8105
[1:24:08.954038] epoch: 2800 | elbo: 2157981.8625 | train_rmse: 0.0594 | val_rmse: 3.7213 | val_ll: -170.0981
[1:25:38.494437] epoch: 2850 | elbo: 2154428.6275 | train_rmse: 0.0595 | val_rmse: 3.7156 | val_ll: -174.0653
[1:27:09.067735] epoch: 2900 | elbo: 2149834.1100000003 | train_rmse: 0.0671 | val_rmse: 3.7094 | val_ll: -171.2291
[1:28:38.212468] epoch: 2950 | elbo: 2149692.2875 | train_rmse: 0.061 | val_rmse: 3.7058 | val_ll: -170.7557
[1:30:07.637438] epoch: 3000 | elbo: 2141745.8525 | train_rmse: 0.0587 | val_rmse: 3.7005 | val_ll: -169.8658
[1:31:36.549642] epoch: 3050 | elbo: 2135793.3549999995 | train_rmse: 0.0529 | val_rmse: 3.6939 | val_ll: -168.9619
[1:33:06.463598] epoch: 3100 | elbo: 2141076.7975000003 | train_rmse: 0.0621 | val_rmse: 3.6879 | val_ll: -170.954
[1:34:37.105595] epoch: 3150 | elbo: 2128812.09 | train_rmse: 0.0554 | val_rmse: 3.6842 | val_ll: -169.2343
[1:36:05.894037] epoch: 3200 | elbo: 2126680.8775 | train_rmse: 0.0632 | val_rmse: 3.6789 | val_ll: -166.4581
[1:37:34.548692] epoch: 3250 | elbo: 2124177.93 | train_rmse: 0.0701 | val_rmse: 3.6701 | val_ll: -167.0892
[1:39:03.777218] epoch: 3300 | elbo: 2115477.4775 | train_rmse: 0.0552 | val_rmse: 3.6675 | val_ll: -167.6144
[1:40:33.337548] epoch: 3350 | elbo: 2113545.465 | train_rmse: 0.0617 | val_rmse: 3.6606 | val_ll: -166.9114
[1:42:03.372201] epoch: 3400 | elbo: 2111024.52625 | train_rmse: 0.0638 | val_rmse: 3.6576 | val_ll: -167.2336
[1:43:32.332442] epoch: 3450 | elbo: 2106264.255 | train_rmse: 0.0603 | val_rmse: 3.6517 | val_ll: -167.6053
[1:45:02.381602] epoch: 3500 | elbo: 2099939.5587500003 | train_rmse: 0.056 | val_rmse: 3.6464 | val_ll: -165.1659
[1:46:31.786297] epoch: 3550 | elbo: 2094066.95625 | train_rmse: 0.0564 | val_rmse: 3.6427 | val_ll: -163.6779
[1:48:01.423691] epoch: 3600 | elbo: 2093489.8075000003 | train_rmse: 0.0579 | val_rmse: 3.6363 | val_ll: -165.0814
[1:49:31.563648] epoch: 3650 | elbo: 2087695.87375 | train_rmse: 0.0587 | val_rmse: 3.6317 | val_ll: -165.3844
[1:51:00.872340] epoch: 3700 | elbo: 2088830.0662500001 | train_rmse: 0.0598 | val_rmse: 3.6266 | val_ll: -164.5495
[1:52:30.075080] epoch: 3750 | elbo: 2079965.8512499998 | train_rmse: 0.0598 | val_rmse: 3.6232 | val_ll: -165.125
[1:53:59.209113] epoch: 3800 | elbo: 2074298.6012500003 | train_rmse: 0.0595 | val_rmse: 3.6186 | val_ll: -162.9398
[1:55:28.674561] epoch: 3850 | elbo: 2078464.5900000003 | train_rmse: 0.0652 | val_rmse: 3.6105 | val_ll: -163.1187
[1:56:58.464280] epoch: 3900 | elbo: 2069535.3375 | train_rmse: 0.0693 | val_rmse: 3.6079 | val_ll: -161.5567
[1:58:27.517699] epoch: 3950 | elbo: 2069566.08125 | train_rmse: 0.0627 | val_rmse: 3.6023 | val_ll: -162.4057
[1:59:55.984347] epoch: 4000 | elbo: 2059222.98125 | train_rmse: 0.0512 | val_rmse: 3.5973 | val_ll: -161.5598
[2:01:25.117821] epoch: 4050 | elbo: 2054656.8975000002 | train_rmse: 0.0567 | val_rmse: 3.5933 | val_ll: -159.5379
[2:02:55.009545] epoch: 4100 | elbo: 2051158.5224999997 | train_rmse: 0.0556 | val_rmse: 3.5866 | val_ll: -162.8285
[2:04:23.987696] epoch: 4150 | elbo: 2050857.65625 | train_rmse: 0.0586 | val_rmse: 3.5846 | val_ll: -158.0905
[2:05:53.558225] epoch: 4200 | elbo: 2045282.5850000002 | train_rmse: 0.0547 | val_rmse: 3.5787 | val_ll: -159.8714
[2:07:24.778367] epoch: 4250 | elbo: 2044375.9900000002 | train_rmse: 0.0599 | val_rmse: 3.5733 | val_ll: -159.3289
[2:08:54.962043] epoch: 4300 | elbo: 2036645.6949999998 | train_rmse: 0.0545 | val_rmse: 3.5682 | val_ll: -157.6894
[2:10:24.412981] epoch: 4350 | elbo: 2032013.07625 | train_rmse: 0.0546 | val_rmse: 3.5636 | val_ll: -158.1771
[2:11:53.305479] epoch: 4400 | elbo: 2027375.7175 | train_rmse: 0.0569 | val_rmse: 3.5588 | val_ll: -159.6059
[2:13:22.994076] epoch: 4450 | elbo: 2025586.4112500001 | train_rmse: 0.0537 | val_rmse: 3.5541 | val_ll: -157.8117
[2:14:53.316684] epoch: 4500 | elbo: 2023338.94375 | train_rmse: 0.0569 | val_rmse: 3.5488 | val_ll: -157.5718
[2:16:25.337632] epoch: 4550 | elbo: 2016712.99375 | train_rmse: 0.0527 | val_rmse: 3.5441 | val_ll: -157.4685
[2:17:55.912066] epoch: 4600 | elbo: 2015164.8262500004 | train_rmse: 0.0502 | val_rmse: 3.5402 | val_ll: -156.7767
[2:19:26.664996] epoch: 4650 | elbo: 2016356.7274999998 | train_rmse: 0.0571 | val_rmse: 3.5341 | val_ll: -154.3253
[2:20:57.686625] epoch: 4700 | elbo: 2006551.01125 | train_rmse: 0.052 | val_rmse: 3.5305 | val_ll: -155.2887
[2:22:28.825274] epoch: 4750 | elbo: 2006595.3362499997 | train_rmse: 0.0532 | val_rmse: 3.5235 | val_ll: -155.1193
[2:23:59.168789] epoch: 4800 | elbo: 1998249.2737500002 | train_rmse: 0.0498 | val_rmse: 3.5203 | val_ll: -155.7551
[2:25:30.651630] epoch: 4850 | elbo: 1995446.2287500002 | train_rmse: 0.053 | val_rmse: 3.5177 | val_ll: -153.8638
[2:27:01.687926] epoch: 4900 | elbo: 1991653.5900000003 | train_rmse: 0.0492 | val_rmse: 3.5117 | val_ll: -154.7616
[2:28:32.461415] epoch: 4950 | elbo: 1989113.8712500003 | train_rmse: 0.0516 | val_rmse: 3.508 | val_ll: -152.4405
[2:30:02.731704] epoch: 5000 | elbo: 1986796.0637500002 | train_rmse: 0.0536 | val_rmse: 3.502 | val_ll: -152.0941
[2:31:32.250031] epoch: 5050 | elbo: 1993592.3137500002 | train_rmse: 0.0611 | val_rmse: 3.4988 | val_ll: -153.2098
[2:33:03.969809] epoch: 5100 | elbo: 1976032.04875 | train_rmse: 0.0546 | val_rmse: 3.4929 | val_ll: -152.3366
[2:34:35.772788] epoch: 5150 | elbo: 1975567.31 | train_rmse: 0.0512 | val_rmse: 3.4908 | val_ll: -151.6366
[2:36:06.287876] epoch: 5200 | elbo: 1977759.6800000002 | train_rmse: 0.0633 | val_rmse: 3.4876 | val_ll: -152.8017
[2:37:37.019152] epoch: 5250 | elbo: 1981853.32375 | train_rmse: 0.0701 | val_rmse: 3.4837 | val_ll: -150.6799
[2:39:07.830918] epoch: 5300 | elbo: 1964121.35625 | train_rmse: 0.0497 | val_rmse: 3.4807 | val_ll: -152.3269
[2:40:40.286785] epoch: 5350 | elbo: 1960859.2762499996 | train_rmse: 0.0521 | val_rmse: 3.4753 | val_ll: -149.6275
[2:42:12.801509] epoch: 5400 | elbo: 1958066.9600000002 | train_rmse: 0.0558 | val_rmse: 3.4716 | val_ll: -150.7508
[2:43:44.399046] epoch: 5450 | elbo: 1952427.51125 | train_rmse: 0.0526 | val_rmse: 3.4675 | val_ll: -148.9048
[2:45:16.736468] epoch: 5500 | elbo: 1953686.7874999996 | train_rmse: 0.0519 | val_rmse: 3.4624 | val_ll: -149.535
[2:46:49.176152] epoch: 5550 | elbo: 1947271.0625 | train_rmse: 0.0509 | val_rmse: 3.459 | val_ll: -150.8974
[2:48:19.546394] epoch: 5600 | elbo: 1945009.51375 | train_rmse: 0.0529 | val_rmse: 3.4553 | val_ll: -148.3352
[2:49:50.661380] epoch: 5650 | elbo: 1945490.73375 | train_rmse: 0.068 | val_rmse: 3.4489 | val_ll: -148.093
[2:51:21.400724] epoch: 5700 | elbo: 1941294.55 | train_rmse: 0.0648 | val_rmse: 3.4465 | val_ll: -149.4383
[2:52:51.780933] epoch: 5750 | elbo: 1931918.19875 | train_rmse: 0.0492 | val_rmse: 3.4425 | val_ll: -149.5781
[2:54:22.441977] epoch: 5800 | elbo: 1931466.0350000001 | train_rmse: 0.0522 | val_rmse: 3.4365 | val_ll: -150.7396
[2:55:52.979015] epoch: 5850 | elbo: 1926429.4125 | train_rmse: 0.0535 | val_rmse: 3.4324 | val_ll: -151.3111
[2:57:23.355628] epoch: 5900 | elbo: 1922331.1375 | train_rmse: 0.0484 | val_rmse: 3.4298 | val_ll: -148.4697
[2:58:52.270120] epoch: 5950 | elbo: 1920707.5662500001 | train_rmse: 0.0511 | val_rmse: 3.4245 | val_ll: -149.5602
[3:00:21.486040] epoch: 6000 | elbo: 1917450.4837500001 | train_rmse: 0.0554 | val_rmse: 3.4212 | val_ll: -147.0056
[3:01:51.219833] epoch: 6050 | elbo: 1912594.9475000002 | train_rmse: 0.0522 | val_rmse: 3.4181 | val_ll: -148.6522
[3:03:21.834835] epoch: 6100 | elbo: 1912045.7987499996 | train_rmse: 0.0569 | val_rmse: 3.4146 | val_ll: -148.1885
[3:04:52.382257] epoch: 6150 | elbo: 1907922.0362500001 | train_rmse: 0.0523 | val_rmse: 3.4093 | val_ll: -148.1249
[3:06:22.122652] epoch: 6200 | elbo: 1904483.3287499999 | train_rmse: 0.0498 | val_rmse: 3.4061 | val_ll: -147.3483
[3:07:52.586077] epoch: 6250 | elbo: 1900416.98125 | train_rmse: 0.0528 | val_rmse: 3.401 | val_ll: -146.5942
[3:09:21.446673] epoch: 6300 | elbo: 1898697.0225000002 | train_rmse: 0.0551 | val_rmse: 3.3978 | val_ll: -147.5242
[3:10:51.101726] epoch: 6350 | elbo: 1894321.4299999997 | train_rmse: 0.0525 | val_rmse: 3.3935 | val_ll: -147.4954
[3:12:20.867216] epoch: 6400 | elbo: 1888948.9562499996 | train_rmse: 0.0487 | val_rmse: 3.3913 | val_ll: -146.889
[3:13:50.304676] epoch: 6450 | elbo: 1887353.4425000001 | train_rmse: 0.0564 | val_rmse: 3.3866 | val_ll: -144.1807
[3:15:20.907233] epoch: 6500 | elbo: 1883323.2600000002 | train_rmse: 0.0485 | val_rmse: 3.3817 | val_ll: -146.0823
[3:16:51.919123] epoch: 6550 | elbo: 1882793.8399999999 | train_rmse: 0.0535 | val_rmse: 3.3803 | val_ll: -147.0154
[3:18:21.505612] epoch: 6600 | elbo: 1877509.9300000002 | train_rmse: 0.0461 | val_rmse: 3.3748 | val_ll: -145.455
[3:19:50.577801] epoch: 6650 | elbo: 1877159.4325000003 | train_rmse: 0.0549 | val_rmse: 3.3711 | val_ll: -146.3005
[3:21:21.049856] epoch: 6700 | elbo: 1873838.9787500002 | train_rmse: 0.0529 | val_rmse: 3.3662 | val_ll: -145.8043
[3:22:50.390512] epoch: 6750 | elbo: 1868280.845 | train_rmse: 0.0518 | val_rmse: 3.3634 | val_ll: -145.4478
[3:24:19.538254] epoch: 6800 | elbo: 1868397.10875 | train_rmse: 0.05 | val_rmse: 3.3594 | val_ll: -145.6421
[3:25:48.722222] epoch: 6850 | elbo: 1864792.2624999997 | train_rmse: 0.0588 | val_rmse: 3.3562 | val_ll: -144.241
[3:27:18.059119] epoch: 6900 | elbo: 1863066.7125000004 | train_rmse: 0.0544 | val_rmse: 3.3507 | val_ll: -142.2518
[3:28:47.237859] epoch: 6950 | elbo: 1859664.47 | train_rmse: 0.0535 | val_rmse: 3.3452 | val_ll: -142.8746
[3:30:17.574653] epoch: 7000 | elbo: 1854845.795 | train_rmse: 0.0549 | val_rmse: 3.3426 | val_ll: -142.1148
[3:31:47.413390] epoch: 7050 | elbo: 1849649.3900000001 | train_rmse: 0.0506 | val_rmse: 3.3368 | val_ll: -142.6164
[3:33:17.726141] epoch: 7100 | elbo: 1846820.02875 | train_rmse: 0.0525 | val_rmse: 3.3345 | val_ll: -142.1099
[3:34:47.178970] epoch: 7150 | elbo: 1845082.3524999998 | train_rmse: 0.0485 | val_rmse: 3.3317 | val_ll: -141.8703
[3:36:16.304255] epoch: 7200 | elbo: 1844747.9150000003 | train_rmse: 0.0519 | val_rmse: 3.3269 | val_ll: -142.1483
[3:37:45.521830] epoch: 7250 | elbo: 1837876.5362500001 | train_rmse: 0.0487 | val_rmse: 3.3232 | val_ll: -140.5751
[3:39:15.290833] epoch: 7300 | elbo: 1837772.63625 | train_rmse: 0.0501 | val_rmse: 3.3194 | val_ll: -144.1277
[3:40:45.634790] epoch: 7350 | elbo: 1833247.4449999998 | train_rmse: 0.0515 | val_rmse: 3.3159 | val_ll: -141.4961
[3:42:15.045852] epoch: 7400 | elbo: 1827686.90625 | train_rmse: 0.0546 | val_rmse: 3.3106 | val_ll: -142.2142
[3:43:43.838297] epoch: 7450 | elbo: 1827823.045 | train_rmse: 0.0531 | val_rmse: 3.3069 | val_ll: -139.9412
[3:45:13.392773] epoch: 7500 | elbo: 1823903.57875 | train_rmse: 0.051 | val_rmse: 3.303 | val_ll: -141.3078
[3:46:43.043773] epoch: 7550 | elbo: 1819320.9774999998 | train_rmse: 0.0467 | val_rmse: 3.2996 | val_ll: -139.7191
[3:48:12.416425] epoch: 7600 | elbo: 1816269.8324999996 | train_rmse: 0.0445 | val_rmse: 3.2955 | val_ll: -139.074
[3:49:41.756604] epoch: 7650 | elbo: 1815094.5162499999 | train_rmse: 0.0535 | val_rmse: 3.2902 | val_ll: -140.8496
[3:51:11.477412] epoch: 7700 | elbo: 1811826.5287499998 | train_rmse: 0.0523 | val_rmse: 3.2875 | val_ll: -140.586
[3:52:40.856496] epoch: 7750 | elbo: 1809094.3787499997 | train_rmse: 0.0534 | val_rmse: 3.2855 | val_ll: -137.3474
[3:54:09.481130] epoch: 7800 | elbo: 1807095.3125 | train_rmse: 0.0476 | val_rmse: 3.2801 | val_ll: -140.4018
[3:55:38.605266] epoch: 7850 | elbo: 1802526.97 | train_rmse: 0.0508 | val_rmse: 3.2781 | val_ll: -140.6339
[3:57:08.741731] epoch: 7900 | elbo: 1801190.62625 | train_rmse: 0.0547 | val_rmse: 3.2715 | val_ll: -137.7742
[3:58:38.260307] epoch: 7950 | elbo: 1797936.36 | train_rmse: 0.0493 | val_rmse: 3.2695 | val_ll: -136.7194
[4:00:09.162640] epoch: 8000 | elbo: 1794635.6024999998 | train_rmse: 0.0528 | val_rmse: 3.2648 | val_ll: -135.7785
[4:01:38.762505] epoch: 8050 | elbo: 1793090.8575000004 | train_rmse: 0.051 | val_rmse: 3.2618 | val_ll: -138.4227
[4:03:07.575972] epoch: 8100 | elbo: 1786908.83 | train_rmse: 0.045 | val_rmse: 3.2584 | val_ll: -136.6806
[4:04:37.206609] epoch: 8150 | elbo: 1783755.9637499999 | train_rmse: 0.0464 | val_rmse: 3.2541 | val_ll: -137.8665
[4:06:06.359356] epoch: 8200 | elbo: 1783272.4175 | train_rmse: 0.0513 | val_rmse: 3.2496 | val_ll: -138.3021
[4:07:36.060489] epoch: 8250 | elbo: 1779986.36375 | train_rmse: 0.0467 | val_rmse: 3.2485 | val_ll: -138.1579
[4:09:06.614255] epoch: 8300 | elbo: 1776320.4337500003 | train_rmse: 0.0479 | val_rmse: 3.2439 | val_ll: -138.788
[4:10:36.745826] epoch: 8350 | elbo: 1775231.1512499999 | train_rmse: 0.0455 | val_rmse: 3.2413 | val_ll: -137.7549
[4:12:06.516652] epoch: 8400 | elbo: 1770908.5037500001 | train_rmse: 0.0474 | val_rmse: 3.2371 | val_ll: -137.8484
[4:13:36.100939] epoch: 8450 | elbo: 1770628.045 | train_rmse: 0.0495 | val_rmse: 3.233 | val_ll: -137.5666
[4:15:05.660311] epoch: 8500 | elbo: 1767087.1887500002 | train_rmse: 0.0604 | val_rmse: 3.2296 | val_ll: -136.6685
[4:16:34.660815] epoch: 8550 | elbo: 1762104.2850000001 | train_rmse: 0.0471 | val_rmse: 3.2255 | val_ll: -137.3539
[4:18:06.469739] epoch: 8600 | elbo: 1762388.7012500004 | train_rmse: 0.0504 | val_rmse: 3.2229 | val_ll: -138.0963
[4:19:39.889141] epoch: 8650 | elbo: 1757684.4212500001 | train_rmse: 0.0452 | val_rmse: 3.2179 | val_ll: -135.0866
[4:21:13.669787] epoch: 8700 | elbo: 1754684.65 | train_rmse: 0.0503 | val_rmse: 3.2159 | val_ll: -138.4344
[4:22:44.207492] epoch: 8750 | elbo: 1752733.9924999997 | train_rmse: 0.0484 | val_rmse: 3.2124 | val_ll: -135.0096
[4:24:14.545960] epoch: 8800 | elbo: 1751580.3387499999 | train_rmse: 0.047 | val_rmse: 3.2061 | val_ll: -135.6019
[4:25:44.214203] epoch: 8850 | elbo: 1747518.16875 | train_rmse: 0.0499 | val_rmse: 3.2056 | val_ll: -134.887
[4:27:13.735420] epoch: 8900 | elbo: 1744303.54875 | train_rmse: 0.0454 | val_rmse: 3.2011 | val_ll: -135.4058
[4:28:43.208388] epoch: 8950 | elbo: 1741761.8275000001 | train_rmse: 0.0548 | val_rmse: 3.1988 | val_ll: -133.4716
[4:30:13.451180] epoch: 9000 | elbo: 1742085.58125 | train_rmse: 0.0476 | val_rmse: 3.1931 | val_ll: -136.8648
[4:31:43.058274] epoch: 9050 | elbo: 1738972.98 | train_rmse: 0.054 | val_rmse: 3.1903 | val_ll: -136.2612
[4:33:12.375727] epoch: 9100 | elbo: 1733173.6062500004 | train_rmse: 0.0463 | val_rmse: 3.1846 | val_ll: -135.1398
[4:34:42.749133] epoch: 9150 | elbo: 1731234.985 | train_rmse: 0.0502 | val_rmse: 3.1829 | val_ll: -134.9693
[4:36:12.175079] epoch: 9200 | elbo: 1726856.61375 | train_rmse: 0.044 | val_rmse: 3.1795 | val_ll: -135.9164
[4:37:41.547533] epoch: 9250 | elbo: 1725023.1212499999 | train_rmse: 0.0445 | val_rmse: 3.1761 | val_ll: -136.0739
[4:39:11.187360] epoch: 9300 | elbo: 1722648.88125 | train_rmse: 0.0458 | val_rmse: 3.172 | val_ll: -133.3694
[4:40:42.049722] epoch: 9350 | elbo: 1719857.1324999998 | train_rmse: 0.0469 | val_rmse: 3.1676 | val_ll: -133.4273
[4:42:12.941335] epoch: 9400 | elbo: 1716615.6662500002 | train_rmse: 0.0462 | val_rmse: 3.165 | val_ll: -133.7073
[4:43:44.931641] epoch: 9450 | elbo: 1717373.1849999998 | train_rmse: 0.0502 | val_rmse: 3.1608 | val_ll: -131.7615
[4:45:15.403081] epoch: 9500 | elbo: 1714052.9599999997 | train_rmse: 0.0529 | val_rmse: 3.1603 | val_ll: -132.5018
[4:46:46.361523] epoch: 9550 | elbo: 1708469.1462500002 | train_rmse: 0.045 | val_rmse: 3.1547 | val_ll: -133.6796
[4:48:17.278814] epoch: 9600 | elbo: 1705536.4337499999 | train_rmse: 0.0485 | val_rmse: 3.1519 | val_ll: -131.1714
[4:49:46.178668] epoch: 9650 | elbo: 1704889.5050000001 | train_rmse: 0.052 | val_rmse: 3.147 | val_ll: -132.6956
[4:51:15.670884] epoch: 9700 | elbo: 1702352.5875 | train_rmse: 0.0473 | val_rmse: 3.1448 | val_ll: -134.9101
[4:52:45.080114] epoch: 9750 | elbo: 1700010.9125 | train_rmse: 0.0474 | val_rmse: 3.1409 | val_ll: -133.0061
[4:54:14.970311] epoch: 9800 | elbo: 1699019.4162500002 | train_rmse: 0.0519 | val_rmse: 3.137 | val_ll: -131.07
[4:55:45.044673] epoch: 9850 | elbo: 1695495.855 | train_rmse: 0.0468 | val_rmse: 3.1341 | val_ll: -131.6304
[4:57:14.475785] epoch: 9900 | elbo: 1689819.79875 | train_rmse: 0.0418 | val_rmse: 3.1291 | val_ll: -131.5693
[4:58:44.569442] epoch: 9950 | elbo: 1688054.3074999999 | train_rmse: 0.0426 | val_rmse: 3.126 | val_ll: -132.3298
Training finished in 5:00:11.970346 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-s003/checkpoint_2.pt
File Size is 4.0595598220825195 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:0 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.03 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0252, -0.0232, -0.1024,  ...,  0.1582,  0.2151,  0.1448],
        [-0.1255, -0.0120,  0.1110,  ..., -0.0951,  0.1480,  0.0971],
        [ 0.0633, -0.2030, -0.1599,  ..., -0.4622, -0.0494,  0.1566],
        ...,
        [-0.2877,  0.1223, -0.1131,  ..., -0.0084, -0.3249, -0.0109],
        [-0.2884, -0.1230,  0.1274,  ..., -0.2425, -0.2292, -0.0647],
        [-0.1811, -0.0100,  0.2404,  ..., -0.0333, -0.0731, -0.1592]],
       device='cuda:0', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0003, 0.0004, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        [0.0004, 0.0005, 0.0004,  ..., 0.0005, 0.0004, 0.0004],
        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        ...,
        [0.0005, 0.0005, 0.0005,  ..., 0.0005, 0.0005, 0.0005],
        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],
        [0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-1.3720e-01, -3.0815e-01,  1.0714e-01,  9.2133e-02,  3.9282e-02,
        -3.2078e-01,  5.1839e-01, -3.3143e-01,  2.3366e-02, -6.5018e-02,
        -3.0902e-01, -1.7642e-01, -2.5059e-01, -6.3372e-01,  7.1781e-02,
        -2.0508e-01,  1.8971e-01, -7.1772e-02, -6.9949e-02, -4.7431e-01,
         3.9368e-01, -1.6638e-02, -4.2452e-01, -3.0716e-01, -4.6189e-01,
        -3.3537e-01,  3.1787e-01, -2.4349e-01,  1.9217e-01,  3.5563e-01,
        -4.4895e-01, -5.7223e-01, -1.7282e-01,  1.8763e-01, -4.1073e-01,
        -2.8635e-01, -9.5614e-02, -2.9385e-01,  7.2892e-02, -2.0082e-01,
         3.6067e-02, -3.7055e-02, -3.4943e-01, -3.7667e-01, -4.4516e-01,
        -1.7666e-01, -4.0264e-02, -2.9758e-01, -2.5704e-01,  1.4263e-01,
        -2.5436e-01,  2.5797e-01, -9.0694e-01, -2.9870e-01, -2.9668e-01,
        -1.6149e-01, -9.1092e-02,  9.3704e-02,  2.6591e-01, -2.4786e-01,
        -1.6836e-01, -1.1093e-01, -2.5573e-01,  8.8090e-02, -3.5521e-01,
         2.3210e-01, -4.1106e-01, -6.1173e-02, -4.5636e-01,  1.0597e-01,
         1.3485e-01, -5.4131e-02, -2.9244e-01,  6.1132e-01, -4.9602e-01,
        -6.5440e-01, -1.5549e-01, -1.8448e-01, -4.3917e-01,  9.8623e-02,
        -2.7218e-01,  6.0451e-02, -2.6812e-01,  2.3974e-01, -1.9743e-01,
         3.3504e-02, -3.4938e-01, -8.5560e-02, -6.5877e-03,  1.8485e-01,
        -4.2635e-01, -1.6601e-01, -2.2460e-01, -9.8844e-03,  8.2034e-02,
        -3.0703e-01, -5.2680e-03,  3.4495e-01, -4.4019e-01, -8.0584e-02,
        -2.1183e-01, -3.9806e-01, -2.0283e-01,  2.3246e-01, -7.0813e-01,
        -3.3804e-01, -5.4692e-02, -1.8637e-02, -1.8679e-01,  7.3608e-02,
         1.9062e-02, -5.5101e-01, -7.1334e-01, -2.3039e-01, -3.7196e-01,
         5.3360e-03, -7.1759e-01, -6.3085e-01, -1.1151e-01, -5.9190e-01,
        -9.3849e-01, -3.3928e-01, -2.3381e-01,  1.8709e-01,  8.5320e-02,
        -2.2469e-01,  5.4760e-02, -3.4679e-01, -2.2626e-01,  5.9497e-01,
        -4.1552e-03,  2.8175e-01,  9.9894e-02, -3.4059e-01, -1.0731e+00,
         2.9645e-02, -1.6670e-03, -2.9911e-01, -8.6234e-02,  2.0076e-01,
        -6.5913e-01,  4.3556e-01, -4.9737e-01, -2.4851e-01, -3.6285e-01,
        -6.8360e-02, -6.4087e-02,  5.5997e-02, -4.1964e-01, -5.4660e-01,
        -3.9309e-01,  1.4361e-01, -2.0277e-01, -2.2712e-01, -8.2923e-01,
        -2.1323e-01, -6.5602e-01,  2.6119e-01, -2.2377e-01,  1.0796e-01,
        -4.7792e-01,  1.7065e-01,  2.1346e-01,  7.7835e-02, -2.0990e-02,
        -2.1688e-01, -5.4818e-01,  3.2064e-01, -2.5363e-01, -3.7100e-01,
         1.3311e-01,  3.0396e-01, -2.1631e-01, -5.9986e-01, -5.0643e-01,
         5.8812e-01, -5.8514e-01, -1.3517e-01,  2.2096e-01, -4.3081e-01,
        -1.5665e-01, -1.9541e-01, -4.3049e-01, -2.0014e-01, -3.9985e-01,
        -3.5045e-01, -2.2919e-01, -3.5534e-01, -5.1729e-01, -7.6164e-01,
        -2.4903e-01,  5.4663e-01, -2.6473e-01, -4.2357e-01, -2.7928e-02,
        -6.3415e-02, -4.0970e-01,  2.2531e-01, -3.0937e-01, -2.3123e-01,
         2.3123e-01, -3.3003e-01,  1.3781e-01, -7.3011e-01, -1.2051e-01,
        -6.6254e-01, -2.2192e-01, -1.7940e-01,  9.1053e-02, -3.1763e-01,
         3.7974e-01, -6.8353e-01, -4.3284e-01, -1.3042e-01,  1.6057e-01,
        -5.7453e-01, -2.0792e-01,  4.9451e-01, -1.8772e-01, -3.2700e-01,
        -1.8229e-01, -2.3361e-01, -1.6337e-01,  2.0402e-01,  5.2820e-01,
        -8.0559e-02,  3.2174e-01, -2.9847e-01,  7.5321e-02,  2.3371e-01,
         1.1183e-01, -5.3808e-01, -9.3939e-01, -1.7519e-01, -2.2732e-01,
        -1.5922e-01, -3.0419e-03,  1.8753e-02,  3.1097e-01, -7.6056e-02,
         5.2540e-02,  1.3906e-01, -5.5640e-01, -4.6445e-01,  9.5952e-02,
        -3.6033e-01,  8.3528e-02,  2.4936e-01,  1.9203e-01, -1.1051e-01,
        -2.2778e-01, -2.2034e-01, -5.9889e-01, -1.9331e-01, -9.1024e-01,
         1.9785e-01, -1.7419e-01, -3.7554e-01, -2.8370e-03, -2.6813e-01,
        -5.2941e-02,  2.4595e-02, -5.7546e-02, -1.2515e-01, -4.1885e-01,
         1.2255e-01, -6.0912e-02, -3.1107e-01,  4.2062e-02, -2.3064e-01,
        -1.7517e-01,  4.4195e-02, -1.5897e-01, -5.1924e-02,  9.9965e-02,
        -1.5318e-01, -2.8361e-01,  1.1596e-01,  1.8086e-01,  1.2466e-01,
        -6.4609e-01, -5.4421e-02, -5.1603e-01,  2.7749e-03, -1.9780e-02,
         2.8190e-02,  1.7459e-01,  2.2477e-01,  2.0146e-03, -6.2867e-01,
        -1.2705e-01, -4.1195e-01,  3.9162e-01, -2.7716e-01, -3.2105e-01,
        -3.7638e-01, -1.7090e-01,  3.5213e-02,  3.2518e-02, -1.3944e-01,
        -3.0357e-01, -3.3465e-02,  1.6961e-01, -4.0397e-01, -1.1260e-01,
        -2.1486e-01, -6.0897e-02,  4.2342e-01, -1.8574e-01, -1.3497e-01,
        -9.0783e-01, -1.0791e-01, -2.2350e-01, -1.2335e-01, -1.7280e-01,
        -8.2192e-01,  5.2479e-01, -2.0621e-01, -4.6435e-01, -2.2019e-01,
        -3.5983e-01, -2.3270e-02, -1.6503e-01, -5.5243e-01, -4.8937e-02,
        -2.5718e-01,  3.6267e-01, -3.8789e-01, -1.3182e-01, -3.0002e-01,
        -5.2319e-01, -3.9513e-01, -5.4533e-01,  1.6522e-01, -4.9072e-01,
        -2.5666e-01, -4.1116e-02,  4.1252e-01, -2.7223e-02, -1.8641e-01,
        -3.0618e-01, -1.4182e-01, -1.1411e-01,  1.3527e-01, -8.4259e-02,
        -6.4225e-02,  8.8937e-03,  6.4680e-02,  3.1263e-01, -6.3159e-01,
        -2.3757e-01,  1.6777e-01,  1.2519e-01, -1.8502e-01, -5.6911e-01,
        -7.0991e-02,  4.7359e-01, -5.6359e-01,  3.5927e-01, -5.2441e-01,
         2.6203e-01, -8.9529e-01, -2.6670e-01, -1.0800e-01, -7.9797e-01,
         3.4974e-01, -1.3370e-01,  1.1218e-03, -1.9345e-01,  2.0524e-01,
         3.0865e-01, -4.5709e-01, -6.2280e-01, -6.3362e-01, -1.7492e-01,
        -9.8326e-02, -3.7224e-01, -3.3306e-01, -2.3324e-01, -2.7750e-01,
        -9.0328e-03, -6.7444e-02, -2.8548e-01, -1.8114e-01, -4.8012e-01,
         2.1815e-01,  1.6554e-01,  1.1607e-01, -2.8569e-01,  9.9363e-02,
        -2.5019e-01,  3.1543e-01, -3.4233e-01, -7.7308e-02, -9.6247e-01,
        -5.0554e-01, -9.5288e-01,  5.3045e-02,  1.8130e-02, -3.1204e-01,
        -1.2435e-01, -3.3014e-01, -4.6188e-01,  1.5583e-01, -3.0838e-01,
         2.7850e-02, -1.7399e-02,  1.9961e-01, -1.1011e-01, -4.5046e-01,
         3.3491e-02,  2.8233e-02,  4.9653e-02, -7.6739e-03, -1.3788e-01,
         3.1179e-01, -5.0538e-01, -4.2881e-01, -3.0783e-01, -2.5819e-01,
        -4.7637e-01,  2.6428e-01,  1.1870e-01, -1.2922e-02, -2.0574e-01,
         6.9624e-02, -8.2659e-02, -8.5111e-01, -2.3143e-01,  2.2383e-01,
        -2.2470e-01, -1.3153e-01, -3.2437e-01, -1.1377e-01, -3.2781e-01,
        -2.7168e-01, -7.1311e-01, -1.7382e-01, -5.5897e-01, -3.4928e-01,
        -2.8322e-01, -2.6044e-01,  2.9447e-01, -3.2511e-01,  1.0843e-02,
         1.0185e-01, -1.3363e-01, -8.3878e-01, -3.0829e-01, -5.5848e-01,
        -3.9776e-01, -1.9935e-02, -1.2427e-01, -4.3146e-01, -8.9629e-01,
        -4.7257e-01, -6.2611e-02, -4.7988e-01, -5.8331e-01, -1.8730e-01,
        -8.1468e-01, -3.0597e-01,  3.1840e-02,  1.3376e-04,  9.0617e-02,
        -4.4087e-01,  8.2801e-02, -5.0737e-02, -1.7443e-01, -8.0534e-01,
        -1.7156e-01, -1.8879e-01,  1.2215e-01, -4.3684e-01,  1.1028e-01,
        -1.7238e-01, -8.2996e-02,  5.1284e-05, -1.0125e-01, -3.1578e-01,
        -3.1689e-01, -1.9417e-01,  3.1226e-01,  3.2943e-01,  2.3859e-01,
         3.0324e-02,  5.5731e-01, -6.2945e-01,  1.2133e-01, -1.1823e-01,
        -2.3254e-01, -3.1908e-01, -2.2110e-01,  1.1945e-01, -7.3342e-02,
        -8.2989e-02, -2.2886e-01,  2.7667e-01, -5.9622e-02, -3.3652e-01,
        -2.2423e-01, -4.3626e-01,  3.4952e-02,  1.2073e-01, -1.9552e-01,
        -1.4515e-02, -1.6808e-01, -2.0988e-01,  1.5107e-01, -5.9306e-01,
        -1.1061e-01, -1.5193e-01], device='cuda:0', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0004, 0.0005, 0.0003, 0.0003, 0.0004, 0.0004, 0.0003, 0.0005, 0.0004,
        0.0004, 0.0004, 0.0004, 0.0004, 0.0009, 0.0003, 0.0004, 0.0003, 0.0005,
        0.0004, 0.0005, 0.0003, 0.0004, 0.0005, 0.0004, 0.0005, 0.0004, 0.0003,
        0.0005, 0.0003, 0.0003, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004,
        0.0004, 0.0005, 0.0003, 0.0004, 0.0004, 0.0004, 0.0005, 0.0005, 0.0005,
        0.0004, 0.0004, 0.0005, 0.0005, 0.0004, 0.0005, 0.0003, 0.0006, 0.0004,
        0.0005, 0.0003, 0.0004, 0.0003, 0.0003, 0.0004, 0.0004, 0.0004, 0.0005,
        0.0004, 0.0006, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,
        0.0005, 0.0003, 0.0004, 0.0005, 0.0004, 0.0004, 0.0005, 0.0004, 0.0008,
        0.0004, 0.0004, 0.0003, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004,
        0.0006, 0.0004, 0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0003, 0.0005,
        0.0004, 0.0004, 0.0288, 0.0004, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004,
        0.0005, 0.0003, 0.0004, 0.0006, 0.0010, 0.0004, 0.0005, 0.0003, 0.0007,
        0.0004, 0.0004, 0.0003, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,
        0.0004, 0.0006, 0.0004, 0.0003, 0.0004, 0.0003, 0.0004, 0.0007, 0.0007,
        0.0003, 0.0004, 0.0007, 0.0004, 0.0003, 0.0007, 0.0003, 0.0004, 0.0004,
        0.0005, 0.0004, 0.0003, 0.0004, 0.0005, 0.0005, 0.0005, 0.0004, 0.0004,
        0.0004, 0.0006, 0.0004, 0.0006, 0.0003, 0.0005, 0.0004, 0.0006, 0.0003,
        0.0003, 0.0004, 0.0004, 0.0005, 0.0006, 0.0004, 0.0004, 0.0006, 0.0004,
        0.0004, 0.0006, 0.0005, 0.0006, 0.0003, 0.0006, 0.0004, 0.0003, 0.0015,
        0.0006, 0.0005, 0.0006, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004, 0.0007,
        0.0006, 0.0004, 0.0003, 0.0005, 0.0005, 0.0004, 0.0004, 0.0006, 0.0003,
        0.0006, 0.0005, 0.0003, 0.0005, 0.0004, 0.0006, 0.0004, 0.0005, 0.0004,
        0.0004, 0.0003, 0.0004, 0.0003, 0.0004, 0.0005, 0.0004, 0.0004, 0.0006,
        0.0004, 0.0003, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0003, 0.0003,
        0.0005, 0.0003, 0.0005, 0.0003, 0.0003, 0.0004, 0.0005, 0.0005, 0.0004,
        0.0005, 0.0003, 0.0003, 0.0003, 0.0003, 0.0004, 0.0004, 0.0003, 0.0005,
        0.0005, 0.0004, 0.0004, 0.0003, 0.0003, 0.0004, 0.0002, 0.0005, 0.0004,
        0.0005, 0.0005, 0.0005, 0.0003, 0.0004, 0.0005, 0.0004, 0.0005, 0.0004,
        0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005,
        0.0004, 0.0004, 0.0004, 0.0003, 0.0004, 0.0005, 0.0004, 0.0004, 0.0003,
        0.0004, 0.0005, 0.0003, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004,
        0.0004, 0.0006, 0.0005, 0.0005, 0.0003, 0.0004, 0.0004, 0.0005, 0.0004,
        0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0003, 0.0004, 0.0004, 0.0004,
        0.0003, 0.0003, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005, 0.0004, 0.0004,
        0.0002, 0.0003, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0006,
        0.0003, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005,
        0.0003, 0.0005, 0.0004, 0.0003, 0.0003, 0.0004, 0.0006, 0.0005, 0.0004,
        0.0004, 0.0003, 0.0004, 0.0004, 0.0004, 0.0004, 0.0003, 0.0006, 0.0004,
        0.0004, 0.0003, 0.0003, 0.0005, 0.0004, 0.0003, 0.0004, 0.0003, 0.0005,
        0.0004, 0.0009, 0.0005, 0.0004, 0.0554, 0.0002, 0.0004, 0.0004, 0.0004,
        0.0003, 0.0003, 0.0005, 0.0005, 0.0007, 0.0005, 0.0004, 0.0005, 0.0004,
        0.0004, 0.0004, 0.0003, 0.0004, 0.0004, 0.0004, 0.0006, 0.0003, 0.0004,
        0.0003, 0.0005, 0.0004, 0.0005, 0.0003, 0.0005, 0.0003, 0.0668, 0.0005,
        0.0653, 0.0003, 0.0004, 0.0006, 0.0004, 0.0004, 0.0005, 0.0003, 0.0005,
        0.0003, 0.0004, 0.0003, 0.0005, 0.0005, 0.0004, 0.0004, 0.0003, 0.0003,
        0.0006, 0.0003, 0.0004, 0.0005, 0.0004, 0.0004, 0.0006, 0.0003, 0.0004,
        0.0004, 0.0004, 0.0004, 0.0004, 0.0003, 0.0004, 0.0003, 0.0005, 0.0004,
        0.0004, 0.0004, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0004,
        0.0005, 0.0003, 0.0005, 0.0004, 0.0004, 0.0004, 0.0006, 0.0008, 0.0005,
        0.0005, 0.0003, 0.0004, 0.0006, 0.0007, 0.0005, 0.0002, 0.0005, 0.0007,
        0.0004, 0.0006, 0.0003, 0.0003, 0.0004, 0.0003, 0.0320, 0.0004, 0.0004,
        0.0005, 0.0255, 0.0005, 0.0004, 0.0004, 0.0005, 0.0003, 0.0004, 0.0004,
        0.0004, 0.0004, 0.0004, 0.0005, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003,
        0.0003, 0.0005, 0.0003, 0.0004, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004,
        0.0004, 0.0004, 0.0003, 0.0004, 0.0004, 0.0004, 0.0296, 0.0004, 0.0004,
        0.0004, 0.0003, 0.0004, 0.0004, 0.0003, 0.0006, 0.0004, 0.0004],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.2141,  0.3007, -0.5350,  ..., -0.4061, -0.3156, -0.0361],
        [-0.3820,  0.3451,  0.1852,  ..., -0.0924,  0.1657,  0.4971],
        [-0.5837, -0.2089,  0.2199,  ...,  0.5772, -0.5498, -0.0387],
        ...,
        [ 0.0437, -0.1629, -0.2434,  ..., -0.5713, -0.5948, -0.1954],
        [-0.1776,  0.8635,  0.0571,  ...,  0.1784, -0.7255,  0.3978],
        [ 0.3748, -0.2630,  0.7529,  ...,  0.3125,  0.2768,  0.2389]],
       device='cuda:0', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0091, 0.0303, 0.0035,  ..., 0.0069, 0.0044, 0.0102],
        [0.0050, 0.0129, 0.0027,  ..., 0.0073, 0.0038, 0.0071],
        [0.0667, 0.2029, 0.0274,  ..., 0.1249, 0.0602, 0.0542],
        ...,
        [0.0075, 0.0164, 0.0028,  ..., 0.0114, 0.0037, 0.0074],
        [0.0046, 0.0092, 0.0022,  ..., 0.0039, 0.0047, 0.0058],
        [0.0044, 0.0113, 0.0022,  ..., 0.0054, 0.0039, 0.0048]],
       device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 0.4482,  0.1609, -0.2811, -0.2823, -0.1916,  0.5890,  0.0040,  0.0304,
        -0.2381, -0.0491, -0.3077, -0.6853,  0.0550,  0.3425,  0.2630,  0.0190,
         0.1443,  0.0600,  0.2581, -0.0478,  0.0058,  0.0705,  0.0844,  0.0744,
         0.2504, -0.2970,  0.6837,  0.4601,  0.2457, -0.4048, -0.4131,  0.4265,
        -0.2459, -0.6754,  0.5950, -0.2732, -0.5281, -0.1843, -0.2090,  0.4675,
         0.0185, -0.0800,  0.3604, -0.2774, -0.3645,  0.4223, -0.0367, -0.1588,
        -0.5771, -0.3127, -0.7463,  0.1597, -0.3253, -0.1114, -0.0042, -0.1875,
        -1.0649, -0.1641,  0.2623, -0.9975, -0.7118, -0.1033, -0.0662,  0.0262,
         0.2297,  0.5608, -0.0223, -0.2548,  0.2444, -0.4887,  0.0036,  0.3230,
         0.2172,  0.2640,  0.6345, -0.8017, -0.2281,  0.0515, -0.1819, -0.1068,
         0.1500, -0.1993, -0.6346,  0.5866, -0.0539, -0.1191,  0.0254, -0.1878,
        -0.0835, -0.4971,  0.2983,  0.1546,  0.2156, -0.2895, -0.5750, -0.0137,
        -0.1481, -0.0837,  0.3491,  0.0729, -0.3975, -0.0877,  0.1802, -0.1093,
        -0.3383,  0.0200, -0.2022,  0.6629, -0.1861, -0.4325, -0.2927,  0.1485,
        -0.0052,  0.1325, -0.2596, -0.5425,  0.2828, -0.3903, -0.2149, -0.1774,
         0.0055, -0.2367,  0.5279, -0.8121,  0.1913, -0.2729, -0.2106,  0.3942,
         0.5017, -0.1256,  0.7975, -0.4086, -0.5059,  0.2369, -0.3556,  0.1167,
        -0.1478,  0.7441, -0.0325, -0.0953, -0.4237,  0.5347, -0.1082,  0.1866,
        -0.2417,  0.1025,  0.0751, -0.4235, -0.0755, -0.1549, -0.6194,  0.0851,
        -0.3283, -0.2274,  0.2302, -0.3522, -0.3925, -1.0972, -0.6337, -0.1116,
         0.4245,  0.4726, -0.1154,  0.5455,  0.0577, -0.4716,  0.3534, -0.1049,
        -0.0259, -0.2189, -0.1519,  0.2817,  0.2691, -0.0795, -0.3157,  0.5537,
         0.6300, -0.3123, -0.4671,  0.5761,  0.0190, -0.5864,  0.0926, -0.5154,
         0.1968, -0.2579, -0.2434,  0.2017, -0.4348,  0.0963, -0.2096,  0.0535,
         0.1903, -0.4591, -0.3793,  0.1509,  0.3287,  0.0825,  0.1385, -0.9163,
        -0.6549, -0.4265,  0.2124,  0.1609,  0.3159, -0.6113, -0.0391,  0.4243,
         0.1639,  0.0778,  0.1404,  0.0872, -0.2964, -0.4231,  0.1969,  0.0127,
         0.2541,  0.2090, -0.3356,  0.3354, -0.0493, -0.0757,  0.4640,  0.2725,
        -0.1100, -0.0959, -0.5513,  0.4105, -0.1194,  0.2974, -0.0413,  0.1580,
         0.2278,  0.0178,  0.0296,  0.5797,  0.1883,  0.2327,  0.0146, -0.0976,
         0.4749, -0.1002, -0.5133,  0.3354, -0.3071, -0.4175, -0.3202, -0.4894,
        -0.2033, -0.1056, -0.1181,  0.4460,  0.4920, -0.3156, -1.1093,  0.1358,
        -0.2547,  0.3131,  0.0027,  0.0822,  0.2988, -0.2263, -0.5873, -0.6659,
         0.5852, -0.0909, -0.3265,  0.4661,  0.5643,  0.1889, -0.2912, -0.2085,
        -0.1144, -0.8239,  0.2223, -0.6395, -0.0015,  0.0950, -0.2965, -0.1091,
        -0.5253, -0.0236,  0.0515, -0.4290, -0.1993, -0.3796, -0.5643,  0.2049,
        -0.2060, -0.1205,  0.8469,  0.0909, -0.3473, -0.4774,  0.3712,  0.1580,
         0.0669,  0.1646,  0.2909, -0.2618,  0.0767, -0.4751, -0.4220, -0.3177,
         0.3818, -0.1394,  0.4030,  0.0305, -0.2156, -0.0308, -0.0822, -0.4765,
        -0.5872,  0.1105, -0.3136,  0.5201, -0.4617,  0.2061,  0.0909, -0.0096,
         0.3432,  0.3037, -0.3210, -0.4117,  0.4466,  0.2766, -0.2988, -0.4497,
        -0.2924, -0.4653, -0.1024, -0.3797, -0.5086, -0.1565, -0.2972,  0.2196,
         0.2444, -0.2746, -0.1754, -0.4778,  0.2595, -0.1952,  0.1170, -0.2924,
         0.2314, -0.0817, -0.4144,  0.3656, -0.3203,  0.1509,  0.1558, -0.1221,
        -0.2365, -0.7673,  0.1050, -0.4792, -0.1424,  0.1285,  0.5893, -0.6142,
        -0.2847, -0.2658,  0.0704,  0.8495, -0.3673, -0.3352,  0.0827,  0.4576,
         0.0153, -0.1468, -0.3471,  0.4127,  0.1461,  0.4598, -0.4548, -0.0402,
         0.0486, -0.2393, -0.2341,  0.1378, -0.1874, -0.0233, -0.5861,  0.1805,
        -0.1766, -0.0512, -0.5010, -0.2169, -0.4202,  0.3644, -0.0072, -0.3558,
         0.0483,  0.0837, -0.3113, -0.2801, -0.3015,  0.5496,  0.4973,  0.0440,
        -0.1716,  0.3261, -0.1375, -0.5006, -0.4175, -0.2937, -0.5944, -0.3123,
         0.2384,  1.2095,  0.0480,  0.0450, -0.3649,  0.4156, -0.6771, -0.6942,
        -0.1368,  0.2542,  0.1710,  0.1924, -0.3045,  0.1296,  0.3309, -0.0710,
        -0.7594, -0.2663,  0.3374, -0.1333,  0.0478,  0.0218, -0.2794, -0.3257,
        -0.1626, -0.2113,  0.1554,  0.0757, -0.2046, -0.3328,  0.3583, -0.1597,
         0.2934, -0.4394,  0.6386, -0.3183,  0.1336, -0.3757, -0.0600, -0.3172,
        -0.0275,  0.0706, -0.0958, -0.1523, -0.1654,  0.2331, -0.2674, -0.4579,
        -0.0064, -0.8818, -0.2326, -0.0306, -0.4193,  0.0374, -0.0593, -0.0021,
         0.6542,  0.1152,  0.0183, -0.1071, -0.2244,  0.1544,  0.2144, -0.8410,
        -0.0790, -0.5211,  0.4704,  0.2268,  0.2880,  0.2275, -0.8053,  0.0437,
        -0.2047,  0.3867,  0.1103, -0.4029,  0.0387,  0.1515,  0.1304, -0.1615,
        -0.4602, -0.4803, -0.0432,  0.3262, -0.4932, -0.0521, -0.7617, -0.2759,
         0.3478, -0.1988,  0.0717, -0.2661,  0.2158, -0.0525,  0.3306, -0.2057,
         0.0575, -0.0462,  0.1803,  0.2436, -0.5046,  0.1523,  0.1996, -0.4368],
       device='cuda:0', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([2.0212e-03, 1.6006e-03, 1.5912e-02, 4.0638e-03, 2.4551e-03, 1.2584e-03,
        2.1347e-03, 2.9192e-03, 1.2981e-03, 1.8932e-03, 5.5685e-03, 1.5215e-03,
        1.4078e-03, 2.0028e-03, 3.1815e-03, 1.8249e-03, 1.1749e-03, 1.3980e-03,
        1.4253e-03, 3.4957e-02, 1.1790e-03, 1.7391e-03, 2.5228e-03, 1.4891e-03,
        9.6070e-04, 1.1742e-03, 1.0517e-03, 1.8768e-03, 4.4026e-03, 3.1613e-03,
        2.2444e-01, 1.4401e-03, 2.6887e-01, 1.7398e-03, 3.6109e-03, 6.8211e-03,
        1.5659e-03, 2.0488e-03, 1.3990e-03, 5.4838e-03, 2.0563e-03, 1.8367e-03,
        1.8247e-03, 3.9570e-01, 3.4714e-03, 1.7494e-03, 1.3283e-03, 3.9194e-01,
        3.6350e-03, 1.4621e-02, 3.3914e-01, 1.1275e-03, 6.9509e-03, 5.9157e-03,
        2.1872e-03, 1.0108e-03, 4.3384e-01, 7.3085e-03, 8.9468e-03, 3.7492e-01,
        4.0676e-01, 1.5250e-03, 1.2800e-03, 4.1869e-01, 1.5207e-03, 4.4167e-03,
        5.5843e-03, 1.2563e-03, 1.7175e-03, 6.5916e-03, 5.5992e-03, 4.7315e-03,
        4.7054e-03, 2.2976e-03, 1.5538e-03, 3.1519e-01, 1.1015e-03, 1.5316e-03,
        5.3111e-03, 2.1763e-03, 1.3560e-03, 3.4298e-01, 4.5362e-02, 2.5774e-03,
        2.0648e-03, 1.9630e-03, 1.2032e-03, 1.9343e-03, 3.1085e-03, 4.0013e-01,
        1.5073e-03, 2.1225e-03, 1.2432e-03, 1.8060e-02, 9.0760e-03, 7.9147e-02,
        2.5956e-03, 7.9667e-04, 1.3950e-03, 3.2085e-03, 1.4105e-02, 2.7257e-03,
        1.1435e-03, 2.1504e-03, 2.5743e-03, 4.3750e-03, 1.8422e-03, 1.0071e-03,
        1.1927e-03, 3.6126e-03, 4.0054e-03, 1.4510e-03, 2.3146e-03, 1.2984e-03,
        2.3587e-02, 7.9423e-03, 1.5622e-03, 7.1870e-03, 4.9926e-02, 2.6960e-03,
        4.4290e-03, 2.0166e-03, 5.0347e-03, 9.3465e-02, 9.1138e-03, 2.3787e-03,
        1.3232e-03, 2.0279e-03, 2.0275e-03, 1.5347e-03, 1.5237e-03, 3.4291e-01,
        3.1231e-01, 1.7728e-03, 4.0296e-01, 1.6434e-03, 2.5663e-03, 1.3078e-03,
        3.5376e-03, 1.4814e-03, 1.4713e-03, 1.8108e-03, 1.2047e-03, 6.8992e-03,
        5.0874e-03, 6.8293e-03, 1.3436e-03, 2.3364e-03, 1.3156e-03, 1.4837e-03,
        4.4992e-03, 2.2075e-03, 4.0735e-01, 3.3019e-02, 1.8287e-03, 1.6921e-03,
        3.8109e-01, 3.2732e-01, 2.1127e-03, 1.9109e-03, 2.0521e-03, 2.4106e-03,
        2.3011e-03, 1.0853e-03, 3.7034e-03, 2.6101e-01, 1.4625e-03, 3.1527e-03,
        1.0851e-03, 3.1998e-03, 3.8321e-03, 8.5355e-04, 2.7123e-03, 3.8795e-03,
        1.5327e-03, 3.1957e-03, 1.2659e-03, 4.1215e-03, 4.7103e-03, 9.6936e-04,
        2.4677e-03, 4.2625e-01, 2.7032e-03, 4.0371e-01, 1.6867e-03, 1.9312e-03,
        1.4609e-03, 1.4713e-03, 3.3703e-03, 3.5091e-03, 2.3308e-03, 1.6556e-03,
        2.0249e-03, 1.2053e-01, 1.3919e-03, 3.5928e-03, 2.3405e-03, 3.9264e-03,
        1.1492e-03, 3.8344e-03, 3.4586e-01, 1.8004e-03, 1.5670e-03, 1.1296e-03,
        2.3059e-01, 3.4038e-03, 2.0544e-03, 3.0987e-03, 2.7892e-03, 1.1248e-03,
        3.5260e-03, 1.8068e-03, 3.9355e-01, 4.2282e-01, 1.2942e-03, 3.0382e-03,
        2.4615e-03, 2.2108e-03, 3.3850e-01, 2.4527e-03, 1.7254e-03, 1.5774e-03,
        2.7082e-03, 1.7049e-03, 2.1172e-03, 3.1493e-03, 3.9962e-02, 3.0764e-01,
        1.4064e-03, 1.5274e-03, 1.9531e-03, 8.7870e-04, 1.6227e-03, 8.8148e-03,
        1.0753e-02, 5.9147e-03, 1.0516e-03, 5.5242e-03, 1.0873e-03, 1.9743e-03,
        3.1688e-03, 2.2867e-01, 3.9338e-01, 6.5443e-03, 2.6165e-03, 2.2555e-01,
        1.4319e-03, 1.0557e-03, 2.7500e-03, 1.5150e-03, 2.8062e-03, 1.5635e-03,
        1.2265e-03, 1.7685e-01, 3.9636e-01, 1.3375e-03, 7.1348e-02, 1.2349e-03,
        1.5240e-03, 2.1591e-02, 2.4895e-03, 2.8304e-03, 3.3958e-01, 2.6268e-01,
        1.3405e-03, 2.0162e-03, 3.8524e-01, 1.1444e-03, 1.1869e-03, 1.1620e-03,
        2.1260e-01, 1.0365e-03, 3.1624e-03, 3.7701e-01, 1.6068e-03, 1.2917e-03,
        2.9595e-03, 3.4157e-01, 3.1386e-03, 1.0708e-03, 2.6331e-03, 1.4359e-03,
        8.4010e-04, 2.8938e-03, 3.5392e-01, 1.7907e-01, 1.7699e-03, 1.7877e-03,
        1.3123e-03, 2.3088e-03, 2.9228e-03, 4.3778e-03, 2.8733e-03, 4.1036e-01,
        1.4293e-03, 2.5774e-03, 2.8215e-03, 1.0806e-03, 1.7191e-03, 3.2487e-03,
        1.4071e-03, 1.7030e-03, 4.2872e-01, 1.8054e-03, 1.6884e-03, 2.3676e-03,
        2.1624e-03, 2.3784e-03, 3.0219e-03, 2.4783e-03, 1.4307e-03, 1.4015e-03,
        2.3124e-03, 1.7123e-03, 3.5316e-01, 1.7071e-03, 3.9657e-03, 3.5012e-03,
        1.6766e-03, 1.5966e-03, 1.8649e-03, 1.3327e-03, 2.2377e-03, 2.5215e-03,
        1.1003e-03, 1.7069e-03, 3.8401e-03, 3.3773e-01, 3.9350e-03, 3.3533e-01,
        3.5078e-03, 1.4303e-03, 2.6010e-03, 2.4463e-03, 1.8244e-03, 2.7508e-03,
        9.6781e-04, 1.6763e-03, 4.0435e-03, 3.8051e-01, 1.1665e-03, 9.0830e-03,
        2.9455e-03, 6.0751e-03, 2.0078e-03, 2.0029e-03, 3.3312e-03, 1.1725e-03,
        8.1503e-03, 1.1301e-03, 3.4334e-03, 3.3511e-03, 3.1569e-03, 3.0487e-01,
        3.0483e-03, 3.3947e-03, 1.9408e-03, 1.5541e-03, 1.4719e-03, 2.2512e-03,
        3.6534e-03, 1.3644e-03, 9.9742e-04, 2.2681e-02, 3.7134e-01, 1.3429e-03,
        8.7323e-03, 1.5063e-03, 1.8052e-03, 3.2146e-03, 1.5342e-03, 8.6735e-04,
        2.4241e-03, 2.3777e-03, 2.2874e-03, 4.8244e-03, 1.9681e-03, 2.0104e-03,
        3.6951e-04, 1.0100e-03, 1.4473e-03, 3.4760e-03, 3.8203e-01, 2.7376e-03,
        2.3876e-03, 1.8940e-03, 2.5949e-03, 1.1085e-03, 2.3077e-03, 4.1542e-03,
        4.4946e-03, 3.9803e-01, 4.1400e-03, 2.4786e-02, 4.8622e-02, 1.0167e-03,
        2.7779e-03, 1.3162e-03, 4.5698e-03, 1.1924e-03, 1.3161e-03, 3.2314e-03,
        3.6443e-03, 1.2977e-03, 1.4052e-03, 3.7274e-03, 3.8805e-01, 1.4562e-03,
        1.8310e-03, 1.3737e-03, 3.2323e-03, 1.5404e-03, 3.1830e-01, 2.5506e-03,
        3.7458e-01, 4.2215e-01, 1.3728e-02, 1.2641e-03, 1.6960e-03, 2.2629e-03,
        4.4274e-01, 1.2244e-03, 1.7502e-03, 2.5611e-03, 3.9302e-03, 1.3289e-03,
        2.8093e-03, 3.7588e-03, 1.2245e-03, 1.5947e-03, 2.2252e-01, 4.2494e-03,
        3.6919e-01, 5.4951e-03, 2.4809e-03, 2.2049e-03, 4.6550e-01, 1.6345e-03,
        2.2939e-03, 8.8567e-03, 2.4915e-03, 3.3003e-01, 2.1585e-03, 1.2679e-03,
        1.6536e-03, 1.3175e-03, 3.5611e-03, 1.0506e-03, 1.6495e-03, 2.5270e-01,
        2.8635e-03, 3.5157e-02, 2.2568e-03, 1.3486e-03, 3.2602e-01, 1.3091e-03,
        1.0519e-03, 3.8383e-01, 2.3570e-01, 1.8063e-03, 9.4222e-02, 1.3773e-03,
        1.4235e-03, 1.3031e-03, 2.6392e-03, 6.7557e-03, 2.5590e-03, 1.6406e-03,
        2.3618e-03, 4.5368e-03, 3.4817e-03, 2.2582e-03, 8.1497e-03, 2.5310e-03,
        2.0841e-03, 3.8784e-03, 2.1635e-03, 3.0510e-01, 1.1940e-03, 2.0034e-03,
        2.5068e-03, 5.7947e-03, 1.7844e-03, 2.0164e-02, 1.6974e-03, 1.8092e-03,
        2.0290e-03, 1.2527e-03, 8.4390e-04, 2.6270e-03, 1.3909e-03, 2.2889e-03,
        2.7748e-03, 1.5718e-03, 1.2650e-03, 1.9614e-03, 1.2903e-03, 1.3838e-03,
        2.6059e-03, 1.6668e-03, 1.2663e-03, 1.7801e-03, 8.8145e-03, 2.7286e-03,
        3.0067e-03, 3.0187e-01, 1.6161e-03, 9.9462e-04, 2.3310e-02, 1.6916e-03,
        1.4193e-03, 1.2452e-03], device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1436, -0.1891, -0.1247,  ...,  0.0045,  0.0270, -0.0155],
        [-0.0759, -0.1675, -0.0062,  ..., -0.1532, -0.1041, -0.2910],
        [-0.0603, -0.1140, -0.0015,  ..., -0.1020, -0.0869, -0.2138],
        ...,
        [-0.1451,  0.0730, -0.2041,  ..., -0.1438,  0.2133, -0.2315],
        [ 0.1054, -0.0579,  0.4204,  ...,  0.2493, -0.2776,  0.0014],
        [ 0.3576, -0.2212, -0.5243,  ..., -0.0079,  0.2387, -0.4212]],
       device='cuda:0', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[6.6593e-04, 4.4265e-04, 2.5906e-03,  ..., 4.4665e-04, 5.3996e-04,
         2.9451e-04],
        [9.8964e-01, 9.7879e-01, 9.9953e-01,  ..., 9.8112e-01, 9.9073e-01,
         9.5848e-01],
        [9.9436e-01, 9.8874e-01, 9.9955e-01,  ..., 9.8951e-01, 9.9324e-01,
         9.7548e-01],
        ...,
        [2.6095e-02, 2.7784e-02, 8.0644e-02,  ..., 2.3971e-02, 2.6565e-02,
         2.0989e-02],
        [1.8729e-03, 1.2927e-03, 7.8892e-03,  ..., 1.3092e-03, 1.5278e-03,
         8.5482e-04],
        [1.6568e-03, 1.1137e-03, 6.6459e-03,  ..., 1.1639e-03, 1.3508e-03,
         7.5253e-04]], device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 1.6177e-01, -2.5892e-01, -1.9021e-01, -2.7383e-01, -1.9174e-01,
        -2.0947e-01, -3.9223e-01,  6.3384e-01,  2.2397e-02,  2.8580e-01,
        -2.1222e-01, -1.8885e-01, -6.2034e-02, -1.8926e-01,  4.5659e-01,
         9.9446e-02, -4.4819e-02, -8.1228e-02,  6.4938e-02, -1.9772e-01,
        -1.9391e-01,  5.7952e-01,  2.2811e-01, -1.4189e-01, -4.0685e-01,
        -1.8748e-01, -1.9305e-01, -2.3576e-01, -1.8410e-01, -1.6209e-01,
        -3.8008e-01, -2.7278e-01, -2.0045e-01, -1.9911e-01, -1.4370e-01,
        -1.8590e-01, -1.8627e-01, -3.3967e-01,  4.2672e-01,  2.1266e-02,
        -2.6461e-02,  2.0924e-01, -1.9433e-01, -2.9192e-01,  6.5674e-01,
         1.4146e-02, -7.7624e-01, -1.9762e-01, -1.0043e-02, -1.9735e-01,
         4.7597e-01,  3.3814e-01, -1.9526e-01, -1.9832e-01, -2.1183e-01,
        -1.9514e-01,  1.5532e-01, -2.4637e-01,  7.0448e-02, -1.0846e-01,
        -1.8714e-01, -9.9272e-02, -2.8000e-01, -1.8543e-01, -1.9562e-01,
        -1.8840e-01, -1.8811e-01, -1.9075e-01, -1.9294e-01,  3.8068e-01,
         3.0507e-01, -5.7474e-01,  1.8207e-02, -3.1627e-01, -3.4826e-01,
        -1.9544e-01, -1.9219e-01,  1.8838e-01,  5.4971e-01,  2.4223e-01,
        -1.7827e-01, -1.9338e-01,  5.7621e-01, -2.4744e-01, -5.3141e-01,
        -2.1279e-01, -1.8665e-01, -2.5013e-01, -2.4344e-01, -3.5114e-01,
        -2.1360e-01,  1.6800e-01,  2.1200e-03, -2.0841e-01,  1.5547e-01,
        -1.8758e-01,  4.4551e-01, -1.2021e-01, -1.4808e-01, -1.9981e-01,
        -2.0271e-01, -9.0292e-03, -1.9580e-01, -1.9421e-01, -3.6548e-02,
        -1.9090e-01, -1.8910e-01, -2.1378e-01, -4.0007e-01,  9.5334e-02,
        -1.9711e-01, -5.1456e-01, -1.6541e-01, -1.9197e-01,  2.6788e-02,
        -2.9807e-01, -2.2562e-01, -1.9076e-01,  1.8059e-01, -1.9388e-01,
        -5.5615e-01, -2.4560e-01, -1.9558e-01,  1.0130e-01, -1.8318e-01,
        -1.9690e-01, -1.8792e-01, -1.8739e-01,  6.5130e-02, -7.5931e-01,
        -1.8601e-01, -1.8899e-01, -3.9089e-01, -1.9350e-01, -1.9890e-01,
         9.9375e-02, -1.9694e-01,  5.8614e-01, -2.6286e-01,  9.7310e-02,
         1.2865e-01, -3.3682e-01, -1.9967e-01, -1.3606e-01,  1.5846e-01,
         5.5669e-01, -1.9265e-01, -1.8868e-01, -1.3470e-01, -4.1330e-01,
        -4.3046e-01, -1.8805e-01,  2.8128e-01, -1.8576e-01, -1.2310e-01,
        -1.9457e-01,  3.3742e-02, -1.8916e-01,  7.3058e-02,  1.6022e-01,
        -1.5674e-01, -4.9703e-01,  1.8465e-01, -2.4709e-01, -1.8807e-01,
        -4.1250e-01, -1.7141e-01,  8.8387e-03, -2.0725e-01, -1.8874e-01,
        -1.8274e-01, -1.8583e-01, -1.8927e-01, -1.8635e-01, -1.7619e-01,
        -2.2628e-01,  2.6502e-01, -1.7606e-01, -1.1508e-01, -2.7887e-01,
        -1.6474e-01, -6.9805e-01,  6.8758e-01,  2.4434e-01, -2.4142e-01,
        -2.2992e-01,  3.0603e-01, -1.8559e-01, -2.0061e-01, -1.0243e-01,
         5.4598e-04, -2.0346e-01,  6.8731e-01, -2.2093e-01, -1.9400e-01,
        -1.9555e-01, -3.0583e-01, -1.9579e-01, -1.8697e-01, -1.9449e-01,
        -4.2895e-01, -3.3409e-01, -2.5668e-01, -1.9878e-01, -9.3630e-02,
         3.2571e-01, -1.9115e-01, -2.7678e-01, -1.8196e-01,  1.9874e-02,
        -3.8695e-01, -1.8569e-01, -1.8775e-01, -1.8934e-01, -1.9539e-01,
        -2.0995e-01,  4.2660e-01,  6.3321e-01, -3.1081e-01, -2.5451e-01,
        -1.9920e-01, -2.0123e-01,  2.3446e-01,  2.1713e-01, -1.9417e-01,
         1.5647e-01, -1.9018e-01, -2.3882e-01, -2.4815e-01, -1.8998e-01,
        -1.9378e-01, -1.9068e-01, -1.8801e-01, -8.5840e-02, -2.0642e-01,
        -1.8860e-01, -1.9411e-02, -3.3858e-02, -2.2208e-01, -1.8368e-01,
        -2.3400e-01, -2.8512e-01,  2.2636e-01, -1.8703e-01, -2.1510e-01,
        -2.0847e-01, -5.1539e-02, -1.9816e-01, -2.1719e-01, -1.2352e-01,
        -1.5254e-01,  2.4907e-01, -2.2875e-01,  2.3437e-01, -1.8905e-01,
        -1.8424e-01, -2.0049e-01,  2.3404e-01,  1.0255e-01, -2.4614e-01,
        -1.0179e-01, -2.4322e-01, -2.1264e-01,  5.3747e-03,  2.9770e-01,
        -1.8660e-01, -2.0533e-01, -1.8928e-01, -2.2024e-01, -1.9140e-01,
        -1.8939e-01, -2.4795e-01, -1.8902e-01, -3.3379e-01, -1.9034e-01,
         7.1815e-02,  6.5536e-01, -1.5826e-01, -2.1699e-01, -1.5717e-01,
         5.2165e-02,  1.0306e-01, -2.6193e-01, -2.4691e-01, -2.1142e-01,
         2.3415e-01, -2.3015e-01, -1.9161e-01, -2.6823e-01, -1.9316e-01,
        -1.6810e-01, -1.9042e-01, -1.9002e-01, -1.9770e-01, -1.9162e-01,
        -1.6372e-01,  4.3212e-01, -2.9984e-01, -1.9872e-01, -1.8993e-01,
         4.7352e-01,  6.1422e-01,  5.6792e-01,  2.1221e-01,  1.6999e-01,
        -1.5061e-01,  8.4075e-02, -1.9199e-01,  2.9075e-01,  3.8728e-01,
        -2.2283e-01, -2.0093e-01, -3.8506e-02, -1.8481e-01,  1.1372e-01,
        -2.2963e-01, -2.1022e-01,  3.3439e-03, -2.1496e-01, -1.8959e-01,
        -2.3488e-01, -1.4887e-01,  4.2967e-01, -1.9415e-01, -1.9031e-01,
        -1.3485e-01, -1.9465e-01, -4.1448e-01,  4.5563e-01, -1.8844e-01,
        -1.6456e-01, -1.8670e-01, -3.8206e-01, -1.9504e-01, -2.2338e-01,
        -1.8584e-01, -1.8703e-01, -1.9222e-01, -1.8810e-01, -8.7631e-02,
        -2.4861e-01, -5.7018e-02, -1.9237e-01, -1.8702e-01, -2.1948e-02,
        -2.1476e-01, -1.9318e-01, -1.9300e-01, -1.8984e-01, -1.9188e-01,
        -2.0509e-01, -4.1436e-01,  3.1636e-01, -1.8807e-01,  4.3971e-01,
        -1.9070e-01, -1.9793e-01, -9.0971e-02, -3.5707e-01, -1.1338e-01,
        -2.0091e-01, -2.7365e-01, -1.8283e-01,  1.5533e-01,  2.2463e-01,
        -1.9226e-01, -3.1540e-01, -2.4453e-01, -3.3441e-01, -2.9215e-01,
        -2.8568e-01, -2.0804e-01, -1.9170e-01,  1.1504e-01, -2.0118e-01,
        -1.8847e-01, -2.2769e-01, -1.9619e-01, -1.8409e-01, -2.3525e-01,
        -2.4102e-01, -2.3711e-01,  2.0152e-01, -1.9389e-01, -3.5862e-01,
         7.6877e-01,  7.6836e-01,  1.5938e-01, -1.8191e-01, -1.8450e-01,
        -2.2687e-01,  2.2854e-01, -1.8967e-01,  6.0621e-01, -2.1247e-01,
         3.5907e-02, -2.4421e-01,  6.0465e-01, -8.4934e-03, -1.9018e-01,
        -1.9447e-01,  4.2114e-02, -4.7020e-01,  1.2866e-03,  1.5021e-01,
        -2.3199e-01,  5.4949e-01, -1.9838e-01, -6.0961e-01, -5.0848e-01,
         1.6944e-01, -1.8017e-01, -1.8465e-01, -1.8627e-01,  4.1272e-01,
        -1.9537e-01, -2.1845e-01, -1.8840e-01, -2.5387e-01,  6.3312e-02,
         3.2830e-01, -1.9539e-01, -2.8887e-01,  1.9355e-01, -1.9082e-01,
         1.2588e-01, -2.4125e-01, -2.8189e-01, -1.8443e-01, -1.0727e-01,
        -1.9090e-01,  1.1165e-02, -1.9545e-01,  4.1171e-01, -1.9529e-01,
        -2.4292e-01, -2.0945e-01, -1.6369e-01, -1.6314e-01,  4.9507e-02,
         1.4924e-01, -1.8874e-01, -1.9877e-01, -5.0781e-01, -2.1610e-01,
         4.9078e-01, -2.0821e-01,  7.3691e-02, -2.0425e-01,  4.7181e-01,
        -1.9505e-01, -1.8583e-01, -7.4057e-02, -1.4672e-02, -1.9000e-01,
         5.1408e-01, -3.0103e-01,  2.5615e-01,  8.0392e-02, -2.1012e-02,
        -2.8793e-01, -6.0194e-01, -1.8758e-01, -2.5686e-01,  4.3137e-01,
        -1.9011e-01,  1.3006e-01, -4.8645e-01, -1.8682e-01, -2.6290e-01,
         1.2146e-01, -2.1999e-01, -1.9755e-01, -1.9083e-01, -1.8660e-01,
        -1.9944e-01, -1.8032e-01, -1.8654e-01, -3.8313e-01, -2.0920e-01,
         2.5334e-01, -1.9469e-01, -1.8434e-01, -1.1695e-01, -1.8887e-01,
        -2.1539e-01,  1.8668e-01,  2.0568e-01, -1.8945e-01, -5.7499e-02,
        -2.0192e-01, -2.2530e-01, -2.7387e-01, -1.9098e-01, -2.2043e-01,
        -1.9220e-01, -3.0735e-01, -1.8642e-01, -2.1030e-01, -1.9787e-01,
        -1.9557e-01, -2.7086e-01,  3.9257e-01, -3.3902e-01, -1.9350e-01,
         5.3872e-01, -1.9817e-01,  6.6212e-02, -2.0002e-01, -7.4034e-01,
         1.6142e-01, -7.6631e-02], device='cuda:0', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([4.8577e-04, 9.8474e-01, 9.9092e-01, 9.8330e-01, 9.9083e-01, 9.8832e-01,
        5.4197e-03, 1.7697e-03, 5.6452e-02, 1.8906e-01, 9.8814e-01, 9.8913e-01,
        9.8822e-01, 9.9025e-01, 1.0495e-02, 1.4691e-03, 2.6301e-01, 1.4208e-02,
        8.8260e-04, 9.9034e-01, 9.8986e-01, 1.0687e-03, 8.7699e-04, 1.9591e-02,
        1.4366e-03, 9.9113e-01, 9.9067e-01, 9.8623e-01, 2.3121e-03, 9.9115e-01,
        1.1675e-03, 9.8374e-01, 9.9031e-01, 9.8980e-01, 9.0908e-04, 9.8998e-01,
        9.9087e-01, 3.6543e-03, 2.7707e-03, 6.4282e-01, 8.6349e-02, 1.0342e-01,
        9.8939e-01, 9.0216e-04, 9.2066e-03, 3.6279e-03, 1.7844e-03, 9.8966e-01,
        3.5914e-03, 9.8945e-01, 2.6167e-03, 2.4385e-03, 9.8857e-01, 9.8987e-01,
        3.5828e-03, 9.8949e-01, 3.3689e-03, 7.0376e-01, 3.0484e-03, 6.9926e-01,
        9.9018e-01, 9.8998e-01, 9.8184e-01, 9.9092e-01, 9.8963e-01, 9.9078e-01,
        9.9103e-01, 9.8916e-01, 9.8819e-01, 3.6332e-03, 2.6199e-03, 6.0234e-03,
        7.4533e-03, 2.0262e-03, 1.1170e-02, 9.9103e-01, 9.9026e-01, 8.9064e-03,
        1.1386e-01, 1.4159e-03, 9.8989e-01, 9.8951e-01, 4.7140e-03, 9.9047e-01,
        1.0227e-03, 4.4652e-03, 9.8882e-01, 7.5950e-03, 9.8909e-01, 1.4355e-03,
        9.9089e-01, 4.4837e-03, 4.5242e-03, 9.8870e-01, 1.4503e-03, 9.8961e-01,
        4.2871e-03, 2.1779e-03, 2.1554e-03, 9.9137e-01, 9.8921e-01, 6.9483e-03,
        9.8955e-01, 9.8875e-01, 8.2629e-02, 9.8991e-01, 9.8933e-01, 9.8832e-01,
        1.1960e-02, 1.0685e-03, 9.8882e-01, 2.6388e-03, 1.4046e-03, 9.9024e-01,
        3.5424e-03, 7.2789e-04, 9.8903e-01, 9.9061e-01, 2.4675e-03, 9.8956e-01,
        3.0485e-03, 8.3067e-03, 9.9043e-01, 1.7791e-03, 1.5998e-03, 9.8922e-01,
        9.9085e-01, 9.9176e-01, 9.6063e-01, 1.6115e-03, 9.8984e-01, 9.8995e-01,
        1.9098e-03, 9.9112e-01, 9.8930e-01, 2.9567e-03, 9.9008e-01, 1.0309e-03,
        4.0983e-03, 5.8632e-03, 9.9675e-03, 4.3441e-03, 9.9120e-01, 4.7733e-03,
        7.3755e-03, 2.7678e-03, 6.5276e-03, 9.9000e-01, 9.5501e-01, 9.6556e-04,
        7.3173e-03, 9.9139e-01, 5.9546e-03, 9.9058e-01, 9.8836e-01, 9.9045e-01,
        4.7995e-02, 9.9010e-01, 4.5178e-03, 3.6164e-03, 9.8890e-01, 1.7467e-03,
        4.5989e-04, 3.5021e-03, 9.8926e-01, 3.1110e-02, 9.8953e-01, 1.9786e-03,
        1.4221e-02, 9.8997e-01, 9.9146e-01, 9.9122e-01, 9.8957e-01, 9.9107e-01,
        9.8946e-01, 9.8730e-01, 8.7651e-03, 9.9101e-01, 9.8961e-01, 9.8439e-01,
        4.3475e-03, 2.3065e-03, 3.8645e-03, 2.7935e-03, 9.7777e-01, 9.8704e-01,
        1.6819e-03, 9.8962e-01, 9.8936e-01, 3.5735e-03, 5.4374e-01, 9.9105e-01,
        2.1757e-03, 9.8826e-01, 9.9025e-01, 9.8919e-01, 7.8678e-03, 9.8932e-01,
        9.9163e-01, 9.8890e-01, 8.1434e-03, 2.2366e-03, 4.4841e-03, 9.8862e-01,
        5.3189e-03, 7.2089e-03, 9.9110e-01, 6.4523e-03, 9.9039e-01, 5.6748e-03,
        7.8640e-03, 9.9015e-01, 9.9216e-01, 9.9135e-01, 9.8988e-01, 9.8988e-01,
        1.3289e-02, 8.0845e-04, 9.7860e-01, 9.8652e-01, 9.8944e-01, 9.8866e-01,
        7.0504e-04, 2.7771e-01, 9.8964e-01, 2.2478e-03, 9.8828e-01, 2.4654e-03,
        9.8441e-01, 9.9122e-01, 9.9089e-01, 9.9042e-01, 9.8995e-01, 1.5548e-03,
        9.9046e-01, 9.9019e-01, 3.9930e-03, 8.8483e-04, 9.8811e-01, 9.9056e-01,
        9.8545e-01, 9.7446e-01, 1.7925e-03, 9.9028e-01, 9.8875e-01, 9.8779e-01,
        3.1667e-02, 9.8958e-01, 9.8849e-01, 2.6083e-03, 2.8232e-03, 8.6490e-03,
        9.8901e-01, 2.8157e-02, 9.8940e-01, 9.9087e-01, 1.7813e-03, 1.1059e-02,
        4.2054e-03, 9.8525e-01, 1.3725e-02, 9.8770e-01, 9.8721e-01, 1.9508e-03,
        7.6066e-04, 9.9103e-01, 9.9000e-01, 9.9075e-01, 9.8853e-01, 9.8978e-01,
        9.2402e-03, 4.3961e-03, 9.8979e-01, 1.4561e-03, 9.9057e-01, 1.4195e-02,
        8.5194e-03, 2.8194e-03, 9.8867e-01, 4.7909e-03, 1.4899e-02, 5.0512e-03,
        8.9575e-04, 9.8533e-01, 9.8962e-01, 4.4665e-03, 9.8628e-01, 9.8906e-01,
        1.9262e-02, 9.9064e-01, 9.9051e-01, 9.8998e-01, 9.9103e-01, 9.8957e-01,
        9.8886e-01, 9.8549e-01, 1.9257e-03, 9.8025e-01, 9.9050e-01, 9.8838e-01,
        2.9932e-03, 1.8139e-02, 1.6357e-03, 5.0224e-03, 1.9334e-03, 9.9117e-01,
        4.5631e-03, 9.8954e-01, 3.2915e-03, 7.5362e-03, 9.8807e-01, 9.9021e-01,
        9.4323e-04, 9.9030e-01, 4.6076e-03, 9.8691e-01, 9.8953e-01, 7.7276e-01,
        9.8946e-01, 9.9024e-01, 9.8619e-01, 1.8480e-03, 3.5844e-03, 9.9040e-01,
        9.9076e-01, 9.9037e-01, 9.8867e-01, 9.6427e-01, 5.1446e-03, 9.9134e-01,
        5.1679e-03, 9.9118e-01, 9.8267e-01, 9.9037e-01, 9.8890e-01, 9.9106e-01,
        9.9067e-01, 9.9072e-01, 9.9029e-01, 2.9490e-03, 9.8626e-01, 6.1151e-02,
        9.9009e-01, 9.8912e-01, 5.7973e-01, 9.8886e-01, 9.8972e-01, 9.9009e-01,
        9.8981e-01, 9.8952e-01, 9.9097e-01, 6.7374e-03, 1.7816e-01, 9.9015e-01,
        1.1584e-03, 9.8927e-01, 9.8991e-01, 4.4337e-03, 1.0252e-03, 2.8008e-03,
        9.8915e-01, 9.8421e-01, 9.9008e-01, 2.4906e-03, 3.0748e-03, 9.9004e-01,
        9.7460e-01, 9.8687e-01, 7.4372e-04, 9.8090e-01, 9.8940e-01, 9.8856e-01,
        9.9170e-01, 3.8414e-03, 9.9138e-01, 9.9008e-01, 9.8772e-01, 9.8972e-01,
        9.9111e-01, 9.8695e-01, 9.8599e-01, 4.2668e-03, 5.7417e-04, 9.9021e-01,
        1.5482e-03, 2.6281e-03, 1.6401e-03, 3.4763e-03, 9.9095e-01, 9.9080e-01,
        9.8625e-01, 3.0035e-01, 9.9073e-01, 1.1374e-02, 9.8882e-01, 2.6860e-03,
        9.8684e-01, 2.6926e-03, 3.1296e-03, 9.8999e-01, 9.8859e-01, 6.5779e-03,
        7.6886e-04, 2.8166e-01, 6.7500e-03, 2.3979e-03, 1.0573e-03, 9.8973e-01,
        1.2921e-03, 3.3775e-03, 5.6803e-03, 9.8570e-01, 9.9185e-01, 9.9004e-01,
        1.3585e-03, 9.8915e-01, 9.8919e-01, 9.8994e-01, 4.8282e-03, 3.1175e-03,
        2.1672e-03, 9.9026e-01, 9.7983e-01, 1.6526e-03, 9.9050e-01, 4.4023e-03,
        9.8746e-01, 9.8187e-01, 9.9055e-01, 9.9000e-01, 9.8910e-01, 7.5184e-03,
        9.9021e-01, 3.4685e-03, 9.8990e-01, 9.8715e-01, 9.8840e-01, 2.7802e-03,
        1.8144e-02, 2.2643e-03, 8.9773e-04, 9.8907e-01, 9.8982e-01, 7.6600e-02,
        9.8731e-01, 9.7239e-03, 7.0910e-04, 8.4571e-04, 9.8961e-01, 5.2999e-04,
        9.8712e-01, 9.8869e-01, 2.4023e-03, 4.0250e-03, 9.9117e-01, 8.4655e-04,
        1.3491e-03, 8.0545e-04, 4.0973e-04, 1.1219e-03, 2.3792e-03, 5.0688e-03,
        9.9062e-01, 9.9030e-01, 5.4287e-03, 9.9089e-01, 3.6826e-03, 1.1543e-03,
        4.4637e-01, 9.8786e-01, 4.4599e-03, 9.8937e-01, 9.8926e-01, 9.9009e-01,
        9.9066e-01, 9.8940e-01, 9.9069e-01, 9.9027e-01, 2.4936e-01, 9.8930e-01,
        6.4168e-03, 9.9096e-01, 9.9115e-01, 9.7122e-01, 9.8961e-01, 9.8639e-01,
        1.2414e-02, 3.3839e-03, 9.9051e-01, 3.0947e-03, 9.8809e-01, 9.8880e-01,
        7.0288e-04, 9.8948e-01, 9.8119e-01, 9.9045e-01, 8.3074e-04, 9.9010e-01,
        9.8918e-01, 9.8963e-01, 9.8826e-01, 9.7391e-01, 2.0509e-03, 4.2030e-01,
        9.8891e-01, 1.9733e-03, 9.8905e-01, 1.2800e-02, 9.9133e-01, 2.7909e-02,
        1.4119e-03, 1.2408e-03], device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 6.2297e-01, -6.5681e-05,  6.2522e-05,  1.1680e-04,  4.3229e-05,
         -9.6232e-06,  1.7830e-01, -1.9614e-01,  1.0707e-02, -4.6559e-05,
          2.2263e-04, -7.1331e-05,  7.2231e-06,  1.0371e-04,  1.7720e-01,
         -2.2106e-01,  2.1625e-04, -4.8213e-02, -4.1989e-01,  6.4770e-05,
         -1.3717e-04,  3.0391e-01, -5.3492e-01, -2.5571e-02, -2.2364e-01,
         -1.2322e-04,  1.0338e-04, -8.0335e-05, -1.3948e-01,  6.5461e-05,
          3.2980e-01, -8.1443e-06, -2.6025e-04,  2.2516e-05, -3.3587e-01,
         -3.6403e-05, -4.9199e-05, -1.2896e-01, -1.2498e-01,  2.3872e-04,
          9.2564e-04,  1.3363e-02, -2.5394e-05, -3.4510e-01,  1.1127e-01,
         -1.5486e-01, -1.9744e-01,  8.3235e-05, -1.1523e-01, -4.3053e-05,
          2.0833e-01,  2.0892e-01,  8.6383e-05, -1.0126e-04, -1.3628e-01,
         -4.6439e-05, -1.5035e-01, -2.1137e-04, -1.0987e-01,  8.1020e-06,
          1.1908e-04,  1.4001e-04,  9.2192e-05,  1.8447e-05, -1.4283e-04,
          8.8695e-05,  1.5930e-04,  1.2354e-04,  8.3801e-05, -1.0618e-01,
         -1.3752e-01,  1.4094e-01,  1.3236e-01,  1.5429e-01, -3.7720e-02,
          9.8320e-05, -1.4463e-04,  9.2634e-02, -4.5606e-04,  2.2413e-01,
          8.4699e-05,  5.3230e-05,  7.2761e-02,  1.0058e-05, -3.2174e-01,
         -1.3176e-01,  8.3000e-05, -1.0695e-01,  1.0739e-04,  2.4363e-01,
         -1.2283e-05,  1.0087e-01,  1.0027e-01, -4.8408e-05, -2.5447e-01,
          3.3380e-05, -1.0779e-01, -1.7876e-01, -1.6953e-01, -3.1313e-05,
          6.2492e-05,  8.0432e-02,  9.8858e-05, -1.0780e-04,  1.3571e-02,
          1.3542e-04,  2.1685e-04,  3.6277e-05,  4.4278e-02,  3.0168e-01,
          3.3761e-05,  1.9104e-01,  2.5241e-01,  2.7803e-05,  1.3041e-01,
          4.1617e-01, -6.5434e-05, -8.4468e-05, -1.3027e-01,  8.7285e-05,
          1.6192e-01,  4.8257e-02, -6.9687e-06,  3.1485e-01,  1.9682e-01,
         -1.1716e-04, -4.8394e-07, -9.0442e-05,  3.8998e-05,  2.0027e-01,
         -2.1758e-05, -1.8428e-04, -1.6586e-01,  3.0052e-05, -4.5945e-05,
         -1.2403e-01, -2.8998e-06, -2.9355e-01, -8.9777e-02,  6.3444e-02,
         -3.2942e-02, -1.0483e-01,  1.5649e-04, -8.1919e-02, -1.0212e-01,
          1.1988e-01,  7.1324e-02, -1.2171e-04,  4.5925e-05,  3.3295e-01,
         -8.5293e-02,  1.2103e-04, -1.6820e-01, -5.1492e-05, -8.2118e-05,
          2.1870e-05, -2.0622e-02, -3.0946e-05,  8.5749e-02,  1.2536e-01,
         -1.1773e-04,  2.0354e-01, -6.5424e-01, -1.3870e-01,  1.0300e-04,
          1.5000e-02,  4.1611e-05,  1.6483e-01, -6.8156e-02, -2.8074e-04,
          6.5767e-05,  7.7205e-05,  6.5979e-05, -2.8894e-06,  5.5111e-05,
         -3.0458e-05,  5.2487e-02,  2.5816e-05,  1.3750e-04,  1.6272e-05,
         -1.3144e-01, -1.3895e-01,  1.2287e-01,  1.3789e-01, -2.3245e-04,
         -4.4871e-05, -2.4529e-01,  4.6871e-05,  7.4257e-05, -1.4845e-01,
          9.0518e-05, -1.6130e-04, -2.1444e-01,  2.4627e-05,  2.2464e-05,
         -1.6265e-04,  5.0825e-02, -4.4237e-06,  1.4281e-04, -1.8214e-04,
         -1.2352e-01,  2.0601e-01,  8.5275e-02,  1.7716e-04, -6.9041e-02,
         -1.0676e-01,  9.5362e-05, -8.1502e-02, -9.1176e-06, -1.2677e-01,
         -7.4958e-02, -2.6122e-05,  1.5831e-04, -6.7374e-05, -5.5852e-05,
          2.7716e-05,  4.5334e-02, -4.2221e-01, -1.3394e-04, -2.8415e-04,
         -2.2898e-05,  5.7904e-05,  4.3897e-01, -1.3877e-04,  6.4148e-05,
          1.9763e-01,  1.2215e-04,  2.0910e-01, -4.1873e-05,  1.0866e-04,
          5.8392e-05,  1.1554e-04,  2.3321e-04,  2.0726e-01, -1.6647e-04,
          2.4240e-05,  8.7072e-02, -3.4443e-01, -1.3079e-04, -7.5165e-05,
          1.5977e-04,  5.7777e-05,  1.8533e-01, -1.8562e-04,  1.2871e-04,
          2.0952e-04,  2.6135e-02,  8.2671e-05,  9.7023e-05, -1.7854e-01,
          1.2283e-01, -4.4962e-02, -1.9296e-04, -1.6550e-02,  3.8967e-05,
         -1.2500e-04,  1.7185e-01,  7.4670e-02, -8.3568e-02, -3.8480e-05,
         -2.5696e-01, -1.6719e-04,  1.4708e-04, -1.7395e-01,  4.0083e-01,
          2.0712e-05,  2.1915e-04,  9.1060e-05, -1.2129e-04, -1.3270e-04,
         -4.9360e-02, -7.3006e-02, -1.9811e-05, -2.2143e-01,  4.5384e-05,
         -6.4138e-02,  8.6084e-02, -1.5758e-01, -6.8521e-05,  1.0092e-01,
         -3.7662e-02,  8.5365e-02, -3.6836e-01,  2.5437e-04, -9.9603e-07,
          1.0445e-01,  7.9775e-05, -4.8025e-05,  1.7079e-02,  1.6575e-04,
         -2.2149e-05,  8.8916e-05, -1.8689e-05, -2.7278e-05,  5.1402e-05,
         -3.3204e-05,  1.6727e-01, -2.8277e-04,  2.0716e-05, -1.0019e-04,
          1.1133e-01,  2.6143e-02, -2.2648e-01, -6.9690e-02, -1.6804e-01,
          6.2543e-06, -2.4063e-01, -1.3851e-04,  1.0094e-01,  5.7283e-02,
         -9.6949e-05,  2.6592e-05,  3.2563e-01, -1.8698e-04, -1.3898e-01,
         -3.4266e-05, -9.0239e-06,  2.4238e-05,  1.1884e-04,  6.1904e-05,
          8.0459e-05,  2.1754e-01, -1.5202e-01,  9.9560e-05,  3.1214e-04,
          5.9391e-05,  4.6561e-05,  4.7799e-04,  1.3775e-01,  8.0115e-05,
          1.6935e-01, -6.3787e-05,  1.0710e-05,  5.8000e-05,  8.2044e-05,
         -6.4074e-05, -2.0331e-04, -9.9000e-05, -2.0991e-05, -1.5510e-01,
          1.7067e-05,  8.1903e-03, -5.2487e-05, -1.7068e-04, -6.7217e-05,
          1.9363e-04,  6.0738e-05,  5.5229e-05, -1.1846e-04, -2.2516e-06,
          4.5802e-05,  7.1316e-02, -3.0915e-04, -3.7117e-05,  2.8873e-01,
         -1.3312e-04, -1.0634e-04,  7.6441e-02,  3.3789e-01, -1.4512e-01,
          8.8050e-05, -1.6984e-04, -1.5785e-04, -1.3837e-01, -1.2397e-01,
          1.2423e-04,  2.9292e-04,  1.9213e-04,  4.1863e-01, -2.8734e-04,
         -1.7206e-04, -2.8945e-05,  4.2939e-05,  9.6389e-02,  8.9110e-05,
          4.6619e-05, -4.9196e-05, -8.2995e-05, -2.2747e-05,  5.3548e-05,
         -4.6471e-05,  1.2424e-01, -5.2596e-01, -5.2765e-06, -2.2319e-01,
         -1.3014e-01,  2.2736e-01,  1.1294e-01,  4.4462e-05,  1.8474e-05,
          8.3677e-05,  1.7960e-04, -3.9213e-05,  5.5603e-02,  1.1122e-05,
         -1.3906e-01, -4.4273e-05, -1.2646e-01, -1.7062e-01, -1.5571e-04,
         -1.2082e-04,  9.6071e-02, -4.2427e-01,  2.5969e-04, -5.8283e-02,
         -1.8156e-01, -3.1136e-01,  9.1255e-05,  2.5670e-01,  1.2896e-01,
          1.9925e-01,  1.5282e-04, -7.3685e-05, -2.3961e-05, -2.5331e-01,
         -2.0461e-04, -6.3145e-05,  1.3373e-04,  1.0224e-01,  1.1903e-01,
          1.6232e-01, -3.1368e-04,  2.9422e-04,  1.9154e-01, -1.7447e-05,
         -1.0145e-01, -1.2389e-05, -7.4310e-05, -2.7038e-05,  1.5442e-04,
          9.4948e-05,  1.0152e-01, -5.0653e-05,  1.0614e-01, -8.2083e-05,
          1.7950e-04, -1.3770e-04, -1.6916e-01, -2.5837e-02, -1.9341e-01,
          3.6478e-01,  5.5271e-05, -9.9670e-05,  2.3583e-03, -2.0874e-04,
         -7.3730e-02, -4.6265e-01,  3.6547e-01, -7.1534e-05, -5.6942e-01,
          2.1873e-04,  2.2308e-05,  1.4118e-01,  1.6874e-01, -1.4784e-04,
          3.6265e-01,  2.8695e-01, -3.8812e-01,  7.3783e-01, -2.8086e-01,
         -1.6647e-01, -8.2728e-02,  2.3286e-05, -2.4785e-05, -8.8362e-02,
         -2.2362e-06,  2.0585e-01, -2.9719e-01,  1.3974e-05, -1.5019e-04,
          1.0343e-01,  3.7927e-06, -1.1097e-04, -4.5294e-05,  5.0072e-06,
         -3.3618e-05,  3.0946e-05, -2.3600e-05,  1.0683e-03,  1.0369e-04,
         -8.7393e-02, -7.3283e-07,  1.5033e-04, -1.5194e-05,  2.6386e-05,
         -6.9381e-06,  5.0262e-02,  1.2312e-01, -7.0142e-06,  1.4952e-01,
         -7.2779e-05, -1.3055e-04,  4.3065e-01, -1.3229e-04,  1.1904e-04,
         -4.8998e-05, -3.7114e-01,  2.0379e-07, -8.4985e-05,  1.5681e-04,
          6.1211e-05,  1.4775e-04,  1.7733e-01, -1.0033e-04, -4.9723e-05,
         -1.9210e-01,  5.8933e-05, -3.9039e-02, -1.4310e-04, -2.0388e-02,
         -2.1853e-01,  2.4939e-01]], device='cuda:0', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[3.5027e-05, 2.1365e-04, 8.5608e-05, 2.7732e-04, 8.1849e-05, 9.5990e-05,
         2.3506e-04, 6.0090e-05, 2.6986e-04, 1.0715e-04, 1.1412e-04, 7.8305e-05,
         1.0501e-04, 7.8070e-05, 4.0690e-04, 4.5621e-05, 2.2219e-04, 1.4749e-04,
         6.4566e-05, 7.8526e-05, 8.5727e-05, 3.8393e-05, 7.9879e-05, 1.1853e-04,
         3.8764e-05, 7.6453e-05, 8.6308e-05, 1.5572e-04, 4.3327e-05, 6.3804e-05,
         5.7114e-05, 2.5459e-04, 8.3074e-05, 8.5720e-05, 3.3815e-05, 7.3277e-05,
         7.4418e-05, 1.0706e-04, 5.2419e-05, 2.6039e-04, 9.7482e-05, 4.7532e-04,
         7.5857e-05, 3.5330e-05, 2.2221e-04, 1.0639e-04, 5.5682e-05, 8.6111e-05,
         8.1243e-05, 7.2378e-05, 1.2044e-04, 1.1750e-04, 8.6150e-05, 8.1125e-05,
         1.0007e-04, 8.5315e-05, 1.0397e-04, 1.5996e-04, 4.8871e-05, 2.9631e-04,
         7.3068e-05, 9.3635e-05, 2.5199e-04, 7.8296e-05, 8.7468e-05, 7.5443e-05,
         7.7543e-05, 7.8084e-05, 8.5309e-05, 7.2213e-05, 6.2570e-05, 1.9776e-04,
         1.9528e-04, 3.9957e-05, 8.8117e-05, 9.2038e-05, 8.7445e-05, 1.9813e-04,
         9.6876e-05, 3.7914e-05, 7.0156e-05, 8.7670e-05, 5.6814e-05, 7.8843e-05,
         4.3585e-05, 1.5883e-04, 7.5397e-05, 2.0816e-04, 1.2437e-04, 4.5908e-05,
         6.8818e-05, 8.7208e-05, 7.3696e-05, 1.0534e-04, 6.1812e-05, 7.8177e-05,
         9.4645e-05, 6.2431e-05, 5.7147e-05, 7.0552e-05, 9.3205e-05, 1.2367e-04,
         9.1871e-05, 8.2521e-05, 3.2163e-04, 7.8179e-05, 7.8519e-05, 9.0112e-05,
         1.1006e-04, 4.6809e-05, 9.0106e-05, 9.6354e-05, 5.3245e-05, 7.8911e-05,
         7.8370e-05, 3.3858e-05, 8.2321e-05, 8.2158e-05, 4.7095e-05, 8.4918e-05,
         7.6821e-05, 6.8817e-05, 8.7573e-05, 1.2036e-04, 3.9564e-05, 7.6102e-05,
         7.6903e-05, 7.8077e-05, 1.2460e-04, 4.4967e-05, 7.1594e-05, 8.0732e-05,
         4.4967e-05, 8.4279e-05, 8.6611e-05, 6.3675e-05, 9.4630e-05, 3.4199e-05,
         7.1607e-05, 6.0736e-05, 5.6708e-05, 8.6066e-05, 8.6730e-05, 6.7270e-05,
         1.9006e-04, 4.8082e-05, 8.1008e-05, 7.8374e-05, 1.0556e-04, 4.7556e-05,
         1.3616e-04, 8.2379e-05, 1.8522e-04, 7.5419e-05, 7.9064e-05, 8.4483e-05,
         2.7370e-04, 7.6154e-05, 6.6891e-05, 8.7491e-05, 9.4349e-05, 5.8535e-05,
         3.1232e-05, 9.6269e-05, 7.5759e-05, 1.1890e-04, 6.5952e-05, 4.8475e-05,
         2.2565e-04, 7.4066e-05, 7.3031e-05, 7.6040e-05, 7.7986e-05, 7.8048e-05,
         7.5312e-05, 1.3991e-04, 9.0544e-05, 6.7770e-05, 6.6105e-05, 2.5593e-04,
         1.1161e-04, 4.5378e-05, 1.1647e-04, 7.3142e-05, 1.7416e-04, 1.5000e-04,
         7.4416e-05, 7.5872e-05, 9.1212e-05, 9.6728e-05, 1.3112e-04, 7.1839e-05,
         8.6257e-05, 1.2543e-04, 7.5469e-05, 7.2640e-05, 7.4698e-05, 9.6317e-05,
         7.7655e-05, 8.5063e-05, 2.7187e-04, 9.2009e-05, 6.0156e-05, 8.9757e-05,
         5.3112e-05, 1.8193e-04, 7.3980e-05, 1.2704e-04, 7.5053e-05, 2.0140e-04,
         1.4004e-04, 7.6760e-05, 7.6468e-05, 7.9395e-05, 8.9741e-05, 7.6502e-05,
         1.2235e-04, 4.7752e-05, 4.5270e-04, 1.9823e-04, 8.9495e-05, 7.8566e-05,
         3.1590e-05, 2.0602e-04, 8.6557e-05, 7.8130e-05, 7.6556e-05, 8.4573e-05,
         1.8392e-04, 7.9907e-05, 8.6537e-05, 7.7610e-05, 7.9908e-05, 4.5986e-05,
         8.9586e-05, 7.5858e-05, 4.9378e-05, 3.2830e-05, 1.3704e-04, 7.2431e-05,
         1.0210e-04, 4.1429e-04, 4.2154e-05, 7.7098e-05, 1.1889e-04, 9.9931e-05,
         2.7894e-04, 9.1955e-05, 1.2158e-04, 8.7965e-05, 5.4033e-05, 8.7104e-05,
         1.3426e-04, 1.4330e-04, 7.6178e-05, 7.7491e-05, 3.7963e-05, 2.0641e-04,
         6.1524e-05, 1.9004e-04, 6.0563e-04, 1.6080e-04, 1.1470e-04, 4.8051e-05,
         3.4998e-05, 7.3639e-05, 1.0010e-04, 7.8406e-05, 1.2706e-04, 8.5112e-05,
         1.0173e-04, 4.6454e-05, 8.4156e-05, 4.1789e-05, 7.6664e-05, 1.8543e-04,
         1.5103e-04, 8.6690e-05, 1.2450e-04, 9.7763e-05, 1.3495e-04, 9.2050e-05,
         4.8903e-05, 1.6602e-04, 1.0662e-04, 7.1565e-05, 1.4506e-04, 8.5259e-05,
         6.0412e-05, 8.1989e-05, 8.2255e-05, 8.3218e-05, 8.5443e-05, 8.7679e-05,
         8.2542e-05, 1.3837e-04, 3.9302e-05, 3.8422e-04, 9.0587e-05, 7.9122e-05,
         4.9002e-05, 1.1018e-04, 5.4876e-05, 5.5457e-05, 4.6595e-05, 7.4892e-05,
         2.5753e-04, 7.7621e-05, 5.1386e-05, 8.2298e-05, 1.2788e-04, 7.2577e-05,
         3.6292e-05, 6.3730e-05, 1.6338e-04, 1.4066e-04, 1.0003e-04, 1.7003e-04,
         1.1036e-04, 7.8805e-05, 1.4244e-04, 6.6992e-05, 1.1953e-04, 8.3885e-05,
         7.7678e-05, 7.7564e-05, 8.5128e-05, 1.8968e-03, 1.7314e-04, 7.3591e-05,
         2.2528e-04, 8.2603e-05, 2.2756e-04, 9.1537e-05, 1.1181e-04, 7.6024e-05,
         7.3557e-05, 8.4781e-05, 7.8635e-05, 9.4226e-05, 1.3870e-04, 1.5457e-04,
         7.7660e-05, 7.7042e-05, 1.9400e-04, 1.1043e-04, 8.9588e-05, 8.5544e-05,
         7.6002e-05, 8.0408e-05, 1.0572e-04, 9.6369e-05, 1.2721e-04, 8.1670e-05,
         4.3534e-05, 7.8464e-05, 7.7657e-05, 5.1634e-05, 5.7338e-05, 6.2481e-05,
         7.8906e-05, 2.3475e-04, 7.4738e-05, 5.4980e-05, 6.6689e-05, 7.8092e-05,
         1.9672e-04, 1.3907e-04, 3.7284e-05, 3.4110e-04, 1.0604e-04, 9.2264e-05,
         8.4684e-05, 7.9766e-05, 7.3785e-05, 7.8521e-05, 1.4288e-04, 8.3731e-05,
         7.6125e-05, 1.5550e-04, 1.6881e-04, 9.0146e-05, 3.2237e-05, 7.6228e-05,
         4.7452e-05, 4.3786e-05, 5.7260e-05, 6.1719e-05, 7.8038e-05, 7.6520e-05,
         1.3472e-04, 1.7106e-04, 7.7254e-05, 1.3705e-04, 1.1476e-04, 5.6343e-05,
         1.4189e-04, 4.3935e-05, 1.1405e-04, 8.0417e-05, 8.4330e-05, 1.6350e-04,
         4.7779e-05, 1.6701e-04, 7.5218e-05, 7.7909e-05, 4.9178e-05, 7.2346e-05,
         4.1699e-05, 9.1855e-05, 2.8181e-04, 1.2167e-04, 7.8164e-05, 7.4584e-05,
         5.2587e-05, 8.3520e-05, 8.0359e-05, 7.7906e-05, 9.3548e-05, 7.4141e-05,
         6.1901e-05, 8.4940e-05, 3.3517e-04, 3.8449e-05, 8.0523e-05, 6.9400e-05,
         1.1767e-04, 2.7794e-04, 7.2084e-05, 6.7384e-05, 7.3195e-05, 2.1088e-04,
         8.8537e-05, 6.2716e-05, 8.5376e-05, 1.7957e-04, 7.4574e-05, 9.4639e-05,
         1.0328e-04, 6.8109e-05, 4.9980e-05, 7.6738e-05, 9.5638e-05, 1.2835e-04,
         1.2456e-04, 1.5833e-04, 3.7954e-05, 3.6240e-05, 8.0910e-05, 3.1937e-05,
         8.4258e-05, 7.5028e-05, 5.6586e-05, 1.5051e-04, 7.4512e-05, 3.3840e-05,
         6.0025e-05, 3.6657e-05, 3.0020e-05, 3.4226e-05, 7.0352e-05, 8.6402e-05,
         7.8747e-05, 7.3060e-05, 8.8114e-05, 8.1595e-05, 1.8085e-04, 4.3688e-05,
         2.7993e-04, 9.7617e-05, 7.3965e-05, 1.1386e-04, 8.7955e-05, 7.2918e-05,
         7.4879e-05, 9.7147e-05, 7.3679e-05, 7.9990e-05, 4.2448e-04, 1.0713e-04,
         1.2710e-04, 8.9629e-05, 7.4401e-05, 1.8736e-04, 8.1544e-05, 1.1587e-04,
         1.3917e-04, 7.4377e-05, 8.1928e-05, 8.3507e-05, 9.0389e-05, 1.2140e-04,
         3.1396e-05, 7.3539e-05, 1.5518e-04, 8.0592e-05, 3.9070e-05, 8.0489e-05,
         1.0640e-04, 9.1684e-05, 7.8439e-05, 2.4869e-04, 6.3782e-05, 1.6631e-04,
         8.3629e-05, 8.6576e-05, 8.8100e-05, 9.3623e-05, 8.9831e-05, 1.6489e-04,
         4.4198e-05, 3.7931e-05]], device='cuda:0', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.1563], device='cuda:0', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0003], device='cuda:0', grad_fn=<AddBackward0>)
Using device: cuda:0
===== Training profile tensin-3x512-s003 - 3 =====
[0:00:01.718878] epoch: 0 | elbo: 2429817.7 | train_rmse: 0.422 | val_rmse: 3.1428 | val_ll: -135.4767
[0:01:33.474147] epoch: 50 | elbo: 1683212.7925 | train_rmse: 0.0302 | val_rmse: 3.1208 | val_ll: -131.1264
[0:03:04.802468] epoch: 100 | elbo: 1678034.20125 | train_rmse: 0.0357 | val_rmse: 3.1192 | val_ll: -129.9054
[0:04:35.096207] epoch: 150 | elbo: 1677645.13375 | train_rmse: 0.044 | val_rmse: 3.1161 | val_ll: -129.1107
[0:06:05.131787] epoch: 200 | elbo: 1677929.8225000002 | train_rmse: 0.0423 | val_rmse: 3.1134 | val_ll: -129.7526
[0:07:35.783331] epoch: 250 | elbo: 1674885.58 | train_rmse: 0.0419 | val_rmse: 3.1099 | val_ll: -130.9266
[0:09:07.381821] epoch: 300 | elbo: 1673454.0287499998 | train_rmse: 0.0488 | val_rmse: 3.1082 | val_ll: -130.3038
[0:10:37.856445] epoch: 350 | elbo: 1671727.3450000002 | train_rmse: 0.0424 | val_rmse: 3.1033 | val_ll: -129.8827
[0:12:08.649812] epoch: 400 | elbo: 1668946.18125 | train_rmse: 0.0468 | val_rmse: 3.0986 | val_ll: -130.0737
[0:13:38.289843] epoch: 450 | elbo: 1666397.5562500001 | train_rmse: 0.0466 | val_rmse: 3.0963 | val_ll: -132.0024
[0:15:07.745727] epoch: 500 | elbo: 1662819.7475 | train_rmse: 0.0491 | val_rmse: 3.0902 | val_ll: -128.5405
[0:16:40.596469] epoch: 550 | elbo: 1661117.1675 | train_rmse: 0.0442 | val_rmse: 3.0858 | val_ll: -132.9851
[0:18:12.187697] epoch: 600 | elbo: 1658201.65375 | train_rmse: 0.0509 | val_rmse: 3.0827 | val_ll: -129.0631
[0:19:41.607721] epoch: 650 | elbo: 1654402.8187499999 | train_rmse: 0.0452 | val_rmse: 3.0794 | val_ll: -131.0676
[0:21:10.064755] epoch: 700 | elbo: 1658499.5387499998 | train_rmse: 0.0512 | val_rmse: 3.0763 | val_ll: -130.8029
[0:22:39.374877] epoch: 750 | elbo: 1650504.5474999999 | train_rmse: 0.0478 | val_rmse: 3.0734 | val_ll: -131.0299
[0:24:09.452253] epoch: 800 | elbo: 1647206.09125 | train_rmse: 0.0443 | val_rmse: 3.068 | val_ll: -130.7209
[0:25:38.849211] epoch: 850 | elbo: 1643775.9475000002 | train_rmse: 0.0474 | val_rmse: 3.0657 | val_ll: -132.4261
[0:27:07.737529] epoch: 900 | elbo: 1644124.6212500003 | train_rmse: 0.0497 | val_rmse: 3.0622 | val_ll: -129.0776
[0:28:37.507761] epoch: 950 | elbo: 1635548.02 | train_rmse: 0.0423 | val_rmse: 3.0596 | val_ll: -129.6528
[0:30:07.216821] epoch: 1000 | elbo: 1638926.70625 | train_rmse: 0.0435 | val_rmse: 3.0555 | val_ll: -127.8042
[0:31:36.554852] epoch: 1050 | elbo: 1636898.5862500002 | train_rmse: 0.0512 | val_rmse: 3.0524 | val_ll: -130.0706
[0:33:06.941205] epoch: 1100 | elbo: 1631311.17375 | train_rmse: 0.0437 | val_rmse: 3.048 | val_ll: -127.0718
[0:34:37.474678] epoch: 1150 | elbo: 1630821.575 | train_rmse: 0.0454 | val_rmse: 3.0454 | val_ll: -128.3751
[0:36:07.181820] epoch: 1200 | elbo: 1626032.1337500003 | train_rmse: 0.0399 | val_rmse: 3.0416 | val_ll: -129.0397
[0:37:36.597071] epoch: 1250 | elbo: 1623791.3350000002 | train_rmse: 0.0478 | val_rmse: 3.0372 | val_ll: -128.6775
[0:39:06.118485] epoch: 1300 | elbo: 1624022.6762500003 | train_rmse: 0.0436 | val_rmse: 3.0333 | val_ll: -128.3224
[0:40:35.118977] epoch: 1350 | elbo: 1619881.1424999996 | train_rmse: 0.0454 | val_rmse: 3.0327 | val_ll: -126.9083
[0:42:04.528791] epoch: 1400 | elbo: 1620601.695 | train_rmse: 0.0472 | val_rmse: 3.0291 | val_ll: -129.0564
[0:43:33.787291] epoch: 1450 | elbo: 1613594.8775 | train_rmse: 0.0398 | val_rmse: 3.0253 | val_ll: -126.2047
[0:45:03.844028] epoch: 1500 | elbo: 1611764.8612499996 | train_rmse: 0.0479 | val_rmse: 3.0206 | val_ll: -126.3869
[0:46:33.756677] epoch: 1550 | elbo: 1607566.1849999998 | train_rmse: 0.0434 | val_rmse: 3.0174 | val_ll: -126.9715
[0:48:03.335867] epoch: 1600 | elbo: 1612375.8599999999 | train_rmse: 0.0526 | val_rmse: 3.0137 | val_ll: -126.0364
[0:49:33.207273] epoch: 1650 | elbo: 1612151.88625 | train_rmse: 0.0458 | val_rmse: 3.0109 | val_ll: -128.2719
[0:51:03.326800] epoch: 1700 | elbo: 1603149.07375 | train_rmse: 0.05 | val_rmse: 3.0069 | val_ll: -128.8926
[0:52:33.180204] epoch: 1750 | elbo: 1602328.20875 | train_rmse: 0.0438 | val_rmse: 3.0034 | val_ll: -127.2338
[0:54:02.987289] epoch: 1800 | elbo: 1600454.8737500003 | train_rmse: 0.0582 | val_rmse: 3.0007 | val_ll: -129.1334
[0:55:33.178276] epoch: 1850 | elbo: 1596487.33875 | train_rmse: 0.0465 | val_rmse: 2.9978 | val_ll: -126.2052
[0:57:02.744293] epoch: 1900 | elbo: 1592655.73625 | train_rmse: 0.0475 | val_rmse: 2.9932 | val_ll: -126.6095
[0:58:33.353864] epoch: 1950 | elbo: 1593576.24 | train_rmse: 0.0438 | val_rmse: 2.9918 | val_ll: -124.8859
[1:00:02.708085] epoch: 2000 | elbo: 1587322.9175 | train_rmse: 0.046 | val_rmse: 2.9869 | val_ll: -128.0658
[1:01:32.373612] epoch: 2050 | elbo: 1585720.9300000002 | train_rmse: 0.0426 | val_rmse: 2.9837 | val_ll: -126.1498
[1:03:02.419206] epoch: 2100 | elbo: 1585985.34 | train_rmse: 0.0432 | val_rmse: 2.9806 | val_ll: -126.2485
[1:04:33.156460] epoch: 2150 | elbo: 1582836.685 | train_rmse: 0.0443 | val_rmse: 2.9759 | val_ll: -126.276
[1:06:03.150818] epoch: 2200 | elbo: 1577796.98625 | train_rmse: 0.0426 | val_rmse: 2.9744 | val_ll: -125.6455
[1:07:33.521771] epoch: 2250 | elbo: 1574910.94625 | train_rmse: 0.0415 | val_rmse: 2.97 | val_ll: -126.9427
[1:09:03.824172] epoch: 2300 | elbo: 1572024.96125 | train_rmse: 0.0413 | val_rmse: 2.966 | val_ll: -125.5691
[1:10:34.212016] epoch: 2350 | elbo: 1572271.4100000001 | train_rmse: 0.0466 | val_rmse: 2.9639 | val_ll: -126.9275
[1:12:05.450201] epoch: 2400 | elbo: 1567214.9537499999 | train_rmse: 0.0401 | val_rmse: 2.9615 | val_ll: -123.8361
[1:13:35.694932] epoch: 2450 | elbo: 1567804.6375 | train_rmse: 0.0471 | val_rmse: 2.9565 | val_ll: -124.1894
[1:15:07.457365] epoch: 2500 | elbo: 1565134.9025 | train_rmse: 0.0397 | val_rmse: 2.9531 | val_ll: -126.6605
[1:16:38.280694] epoch: 2550 | elbo: 1563118.2162499998 | train_rmse: 0.0433 | val_rmse: 2.9498 | val_ll: -125.0684
[1:18:08.834960] epoch: 2600 | elbo: 1560263.8 | train_rmse: 0.0418 | val_rmse: 2.9471 | val_ll: -126.7941
[1:19:40.151329] epoch: 2650 | elbo: 1555774.9562499998 | train_rmse: 0.0416 | val_rmse: 2.9424 | val_ll: -126.6469
[1:21:10.867534] epoch: 2700 | elbo: 1555524.5875 | train_rmse: 0.0435 | val_rmse: 2.9396 | val_ll: -125.0932
[1:22:42.496582] epoch: 2750 | elbo: 1551510.5949999997 | train_rmse: 0.0415 | val_rmse: 2.9357 | val_ll: -123.676
[1:24:15.397134] epoch: 2800 | elbo: 1549286.11375 | train_rmse: 0.041 | val_rmse: 2.932 | val_ll: -124.9613
[1:25:46.305001] epoch: 2850 | elbo: 1549659.57875 | train_rmse: 0.0464 | val_rmse: 2.9303 | val_ll: -123.0391
[1:27:16.573899] epoch: 2900 | elbo: 1545901.375 | train_rmse: 0.0477 | val_rmse: 2.9269 | val_ll: -126.1491
[1:28:47.569685] epoch: 2950 | elbo: 1541670.44125 | train_rmse: 0.0396 | val_rmse: 2.921 | val_ll: -122.9373
[1:30:17.905998] epoch: 3000 | elbo: 1542597.7887499998 | train_rmse: 0.0465 | val_rmse: 2.9188 | val_ll: -124.3177
[1:31:49.425314] epoch: 3050 | elbo: 1537814.31125 | train_rmse: 0.0426 | val_rmse: 2.9144 | val_ll: -123.185
[1:33:20.445752] epoch: 3100 | elbo: 1534137.14375 | train_rmse: 0.0413 | val_rmse: 2.912 | val_ll: -123.898
[1:34:50.493483] epoch: 3150 | elbo: 1531618.15125 | train_rmse: 0.039 | val_rmse: 2.9081 | val_ll: -125.0023
[1:36:20.785054] epoch: 3200 | elbo: 1528201.98125 | train_rmse: 0.0397 | val_rmse: 2.9028 | val_ll: -122.0768
[1:37:50.503708] epoch: 3250 | elbo: 1528362.3800000001 | train_rmse: 0.0401 | val_rmse: 2.9004 | val_ll: -122.5601
[1:39:19.451405] epoch: 3300 | elbo: 1528290.26625 | train_rmse: 0.0413 | val_rmse: 2.897 | val_ll: -121.7403
[1:40:49.561113] epoch: 3350 | elbo: 1522319.1749999998 | train_rmse: 0.0425 | val_rmse: 2.8939 | val_ll: -122.8912
[1:42:18.079800] epoch: 3400 | elbo: 1520667.3050000002 | train_rmse: 0.0467 | val_rmse: 2.89 | val_ll: -118.6813
[1:43:48.374377] epoch: 3450 | elbo: 1516838.48125 | train_rmse: 0.0385 | val_rmse: 2.8846 | val_ll: -121.4256
[1:45:17.232164] epoch: 3500 | elbo: 1515010.1099999999 | train_rmse: 0.0391 | val_rmse: 2.8821 | val_ll: -121.5879
[1:46:46.648755] epoch: 3550 | elbo: 1513824.73 | train_rmse: 0.044 | val_rmse: 2.8782 | val_ll: -121.231
[1:48:16.635172] epoch: 3600 | elbo: 1511330.9899999998 | train_rmse: 0.0418 | val_rmse: 2.8749 | val_ll: -122.3074
[1:49:47.131799] epoch: 3650 | elbo: 1508894.4625 | train_rmse: 0.0393 | val_rmse: 2.873 | val_ll: -120.0281
[1:51:17.163702] epoch: 3700 | elbo: 1508446.1925000001 | train_rmse: 0.0422 | val_rmse: 2.8685 | val_ll: -119.553
[1:52:47.720995] epoch: 3750 | elbo: 1504546.4537499999 | train_rmse: 0.0386 | val_rmse: 2.864 | val_ll: -118.7435
[1:54:16.131951] epoch: 3800 | elbo: 1502036.9637499996 | train_rmse: 0.0417 | val_rmse: 2.8614 | val_ll: -119.7553
[1:55:45.790497] epoch: 3850 | elbo: 1498333.17875 | train_rmse: 0.043 | val_rmse: 2.8569 | val_ll: -118.6795
[1:57:15.159390] epoch: 3900 | elbo: 1499123.38875 | train_rmse: 0.0507 | val_rmse: 2.8544 | val_ll: -119.5378
[1:58:44.755908] epoch: 3950 | elbo: 1495290.21875 | train_rmse: 0.0405 | val_rmse: 2.8493 | val_ll: -119.4416
[2:00:14.296864] epoch: 4000 | elbo: 1489087.2137499999 | train_rmse: 0.0382 | val_rmse: 2.8459 | val_ll: -119.345
[2:01:44.273525] epoch: 4050 | elbo: 1491229.0225 | train_rmse: 0.0403 | val_rmse: 2.8418 | val_ll: -119.3314
[2:03:16.860420] epoch: 4100 | elbo: 1486928.1912499997 | train_rmse: 0.0449 | val_rmse: 2.8377 | val_ll: -120.0382
[2:04:45.994000] epoch: 4150 | elbo: 1484698.61 | train_rmse: 0.0388 | val_rmse: 2.8348 | val_ll: -118.0592
[2:06:15.692706] epoch: 4200 | elbo: 1481889.8587500001 | train_rmse: 0.0445 | val_rmse: 2.8312 | val_ll: -117.0959
[2:07:45.170553] epoch: 4250 | elbo: 1480384.32625 | train_rmse: 0.0392 | val_rmse: 2.8263 | val_ll: -118.0362
[2:09:14.948259] epoch: 4300 | elbo: 1477680.8787499997 | train_rmse: 0.0449 | val_rmse: 2.8225 | val_ll: -119.414
[2:10:44.662545] epoch: 4350 | elbo: 1473937.5387499998 | train_rmse: 0.0371 | val_rmse: 2.8183 | val_ll: -118.0088
[2:12:13.685284] epoch: 4400 | elbo: 1473127.1812500001 | train_rmse: 0.0388 | val_rmse: 2.8156 | val_ll: -117.2563
[2:13:44.125940] epoch: 4450 | elbo: 1470457.24125 | train_rmse: 0.0379 | val_rmse: 2.8115 | val_ll: -116.9157
[2:15:14.610670] epoch: 4500 | elbo: 1467565.04625 | train_rmse: 0.0413 | val_rmse: 2.8087 | val_ll: -116.3808
[2:16:44.034534] epoch: 4550 | elbo: 1464581.6475000002 | train_rmse: 0.0387 | val_rmse: 2.804 | val_ll: -116.9288
[2:18:14.275860] epoch: 4600 | elbo: 1464453.31375 | train_rmse: 0.0394 | val_rmse: 2.8 | val_ll: -117.3263
[2:19:45.385957] epoch: 4650 | elbo: 1459350.1575 | train_rmse: 0.0405 | val_rmse: 2.7953 | val_ll: -119.1651
[2:21:15.270871] epoch: 4700 | elbo: 1459246.1774999998 | train_rmse: 0.0403 | val_rmse: 2.7923 | val_ll: -116.5078
[2:22:44.883618] epoch: 4750 | elbo: 1455753.065 | train_rmse: 0.0394 | val_rmse: 2.7873 | val_ll: -117.6091
[2:24:14.670098] epoch: 4800 | elbo: 1459589.855 | train_rmse: 0.0367 | val_rmse: 2.7842 | val_ll: -117.9205
[2:25:45.184382] epoch: 4850 | elbo: 1451983.6425 | train_rmse: 0.0416 | val_rmse: 2.7807 | val_ll: -116.0933
[2:27:16.771695] epoch: 4900 | elbo: 1448178.1575 | train_rmse: 0.0374 | val_rmse: 2.7763 | val_ll: -115.0345
[2:28:48.895449] epoch: 4950 | elbo: 1447139.65125 | train_rmse: 0.0411 | val_rmse: 2.7717 | val_ll: -115.5618
[2:30:19.892928] epoch: 5000 | elbo: 1443770.5949999997 | train_rmse: 0.0441 | val_rmse: 2.7692 | val_ll: -116.8757
[2:31:50.948204] epoch: 5050 | elbo: 1443095.44625 | train_rmse: 0.0415 | val_rmse: 2.7653 | val_ll: -115.4775
[2:33:22.134774] epoch: 5100 | elbo: 1438477.48875 | train_rmse: 0.0391 | val_rmse: 2.7613 | val_ll: -116.504
[2:34:52.981023] epoch: 5150 | elbo: 1435868.3125 | train_rmse: 0.0398 | val_rmse: 2.7569 | val_ll: -114.0846
[2:36:23.636387] epoch: 5200 | elbo: 1435008.30125 | train_rmse: 0.0401 | val_rmse: 2.7524 | val_ll: -116.001
[2:37:54.981113] epoch: 5250 | elbo: 1431959.3649999998 | train_rmse: 0.0381 | val_rmse: 2.7495 | val_ll: -117.3567
[2:39:26.037981] epoch: 5300 | elbo: 1432457.53375 | train_rmse: 0.0442 | val_rmse: 2.7446 | val_ll: -114.4952
[2:40:56.515241] epoch: 5350 | elbo: 1426403.45875 | train_rmse: 0.0379 | val_rmse: 2.741 | val_ll: -114.4678
[2:42:27.899265] epoch: 5400 | elbo: 1423528.6575 | train_rmse: 0.0393 | val_rmse: 2.7381 | val_ll: -113.3303
[2:43:59.954931] epoch: 5450 | elbo: 1421059.5162499999 | train_rmse: 0.039 | val_rmse: 2.7338 | val_ll: -115.6484
[2:45:30.365936] epoch: 5500 | elbo: 1419781.01625 | train_rmse: 0.0451 | val_rmse: 2.7297 | val_ll: -112.2877
[2:47:02.055961] epoch: 5550 | elbo: 1416941.3725 | train_rmse: 0.0394 | val_rmse: 2.7275 | val_ll: -112.2836
[2:48:34.346946] epoch: 5600 | elbo: 1413804.4175 | train_rmse: 0.0384 | val_rmse: 2.7222 | val_ll: -114.3593
[2:50:05.708050] epoch: 5650 | elbo: 1414376.1625 | train_rmse: 0.0412 | val_rmse: 2.7184 | val_ll: -115.3503
[2:51:36.654621] epoch: 5700 | elbo: 1410930.86875 | train_rmse: 0.0397 | val_rmse: 2.7144 | val_ll: -113.1084
[2:53:08.142142] epoch: 5750 | elbo: 1408167.26625 | train_rmse: 0.042 | val_rmse: 2.7117 | val_ll: -114.2931
[2:54:38.805498] epoch: 5800 | elbo: 1406882.8900000001 | train_rmse: 0.0444 | val_rmse: 2.7074 | val_ll: -114.7568
[2:56:09.451252] epoch: 5850 | elbo: 1403830.4825 | train_rmse: 0.0451 | val_rmse: 2.7032 | val_ll: -114.2736
[2:57:41.234625] epoch: 5900 | elbo: 1401621.6075000002 | train_rmse: 0.0376 | val_rmse: 2.6996 | val_ll: -112.7224
[2:59:11.485157] epoch: 5950 | elbo: 1398825.845 | train_rmse: 0.0413 | val_rmse: 2.6955 | val_ll: -113.6539
[3:00:41.360492] epoch: 6000 | elbo: 1397500.6125000003 | train_rmse: 0.0414 | val_rmse: 2.6919 | val_ll: -113.0645
[3:02:11.178504] epoch: 6050 | elbo: 1392701.27 | train_rmse: 0.0369 | val_rmse: 2.6887 | val_ll: -112.146
[3:03:40.220666] epoch: 6100 | elbo: 1390869.555 | train_rmse: 0.0382 | val_rmse: 2.6858 | val_ll: -112.9267
[3:05:09.480334] epoch: 6150 | elbo: 1391485.5662500001 | train_rmse: 0.0393 | val_rmse: 2.6795 | val_ll: -112.0861
[3:06:38.686148] epoch: 6200 | elbo: 1388805.10375 | train_rmse: 0.0384 | val_rmse: 2.6766 | val_ll: -112.6913
[3:08:07.503669] epoch: 6250 | elbo: 1386665.0887499999 | train_rmse: 0.0395 | val_rmse: 2.6714 | val_ll: -110.7456
[3:09:40.254848] epoch: 6300 | elbo: 1382039.6012499998 | train_rmse: 0.0381 | val_rmse: 2.6693 | val_ll: -113.6455
[3:11:11.845280] epoch: 6350 | elbo: 1381461.48 | train_rmse: 0.0393 | val_rmse: 2.6637 | val_ll: -109.8677
[3:12:42.614613] epoch: 6400 | elbo: 1380007.4487499997 | train_rmse: 0.0363 | val_rmse: 2.6589 | val_ll: -110.6852
[3:14:14.180012] epoch: 6450 | elbo: 1377302.7825 | train_rmse: 0.0424 | val_rmse: 2.6571 | val_ll: -109.2034
[3:15:44.915025] epoch: 6500 | elbo: 1375137.0150000001 | train_rmse: 0.0431 | val_rmse: 2.6529 | val_ll: -109.6742
[3:17:15.533323] epoch: 6550 | elbo: 1372219.225 | train_rmse: 0.0402 | val_rmse: 2.6489 | val_ll: -110.1803
[3:18:46.342234] epoch: 6600 | elbo: 1370302.6225 | train_rmse: 0.0351 | val_rmse: 2.6463 | val_ll: -110.6825
[3:20:17.060866] epoch: 6650 | elbo: 1370300.38625 | train_rmse: 0.0478 | val_rmse: 2.6415 | val_ll: -109.9484
[3:21:48.086888] epoch: 6700 | elbo: 1366335.5025 | train_rmse: 0.0472 | val_rmse: 2.6371 | val_ll: -109.2077
[3:23:18.428049] epoch: 6750 | elbo: 1360355.34125 | train_rmse: 0.0365 | val_rmse: 2.6328 | val_ll: -108.7694
[3:24:49.111333] epoch: 6800 | elbo: 1360972.40375 | train_rmse: 0.0408 | val_rmse: 2.6301 | val_ll: -110.1479
[3:26:19.747444] epoch: 6850 | elbo: 1358671.215 | train_rmse: 0.041 | val_rmse: 2.6264 | val_ll: -109.1139
[3:27:52.035541] epoch: 6900 | elbo: 1356366.7262499998 | train_rmse: 0.0391 | val_rmse: 2.6205 | val_ll: -108.9836
[3:29:22.714374] epoch: 6950 | elbo: 1352199.0550000002 | train_rmse: 0.0414 | val_rmse: 2.6183 | val_ll: -108.506
[3:30:53.062710] epoch: 7000 | elbo: 1351712.0662500001 | train_rmse: 0.0475 | val_rmse: 2.6138 | val_ll: -108.9866
[3:32:25.014118] epoch: 7050 | elbo: 1347818.8725 | train_rmse: 0.0364 | val_rmse: 2.6091 | val_ll: -106.9671
[3:33:58.029387] epoch: 7100 | elbo: 1346990.11375 | train_rmse: 0.0377 | val_rmse: 2.607 | val_ll: -107.8367
[3:35:29.183235] epoch: 7150 | elbo: 1342483.4137499998 | train_rmse: 0.0353 | val_rmse: 2.6026 | val_ll: -107.3576
[3:37:01.630447] epoch: 7200 | elbo: 1344454.45875 | train_rmse: 0.045 | val_rmse: 2.5993 | val_ll: -106.8392
[3:38:35.099021] epoch: 7250 | elbo: 1342773.8249999997 | train_rmse: 0.0398 | val_rmse: 2.5954 | val_ll: -106.0083
[3:40:06.381628] epoch: 7300 | elbo: 1335961.87125 | train_rmse: 0.0372 | val_rmse: 2.5912 | val_ll: -107.0958
[3:41:36.868140] epoch: 7350 | elbo: 1334824.8299999998 | train_rmse: 0.0376 | val_rmse: 2.5876 | val_ll: -106.4296
[3:43:07.052952] epoch: 7400 | elbo: 1332565.8649999998 | train_rmse: 0.0377 | val_rmse: 2.5833 | val_ll: -106.5217
[3:44:38.050370] epoch: 7450 | elbo: 1329829.69125 | train_rmse: 0.0348 | val_rmse: 2.5798 | val_ll: -105.7059
[3:46:08.630209] epoch: 7500 | elbo: 1328206.97 | train_rmse: 0.0373 | val_rmse: 2.5757 | val_ll: -105.3966
[3:47:37.783077] epoch: 7550 | elbo: 1324957.875 | train_rmse: 0.041 | val_rmse: 2.5714 | val_ll: -106.9142
[3:49:08.027945] epoch: 7600 | elbo: 1323867.9275 | train_rmse: 0.0389 | val_rmse: 2.5679 | val_ll: -106.8023
[3:50:39.376455] epoch: 7650 | elbo: 1322483.56125 | train_rmse: 0.0422 | val_rmse: 2.5641 | val_ll: -105.1757
[3:52:10.575236] epoch: 7700 | elbo: 1317300.37125 | train_rmse: 0.0433 | val_rmse: 2.5611 | val_ll: -106.0948
[3:53:42.118199] epoch: 7750 | elbo: 1315213.0237500002 | train_rmse: 0.0385 | val_rmse: 2.5574 | val_ll: -106.4954
[3:55:11.902333] epoch: 7800 | elbo: 1312554.0699999998 | train_rmse: 0.0358 | val_rmse: 2.5538 | val_ll: -105.4568
[3:56:42.227349] epoch: 7850 | elbo: 1311433.1350000002 | train_rmse: 0.0362 | val_rmse: 2.5501 | val_ll: -104.4888
[3:58:12.703510] epoch: 7900 | elbo: 1311921.7125 | train_rmse: 0.0436 | val_rmse: 2.5467 | val_ll: -104.4036
[3:59:42.953074] epoch: 7950 | elbo: 1306539.9250000003 | train_rmse: 0.0384 | val_rmse: 2.5406 | val_ll: -103.8675
[4:01:13.419991] epoch: 8000 | elbo: 1304132.2224999997 | train_rmse: 0.0365 | val_rmse: 2.5374 | val_ll: -103.9785
[4:02:43.907954] epoch: 8050 | elbo: 1302664.6562499998 | train_rmse: 0.0367 | val_rmse: 2.5346 | val_ll: -103.7543
[4:04:14.200368] epoch: 8100 | elbo: 1298724.23375 | train_rmse: 0.0349 | val_rmse: 2.5293 | val_ll: -101.5762
[4:05:44.973499] epoch: 8150 | elbo: 1295461.9487499997 | train_rmse: 0.0338 | val_rmse: 2.5257 | val_ll: -103.3644
[4:07:15.265687] epoch: 8200 | elbo: 1293891.42875 | train_rmse: 0.0378 | val_rmse: 2.5234 | val_ll: -102.8809
[4:08:45.972891] epoch: 8250 | elbo: 1291343.0149999997 | train_rmse: 0.0357 | val_rmse: 2.5181 | val_ll: -105.2115
[4:10:15.875779] epoch: 8300 | elbo: 1291415.9625 | train_rmse: 0.0369 | val_rmse: 2.5142 | val_ll: -103.2139
[4:11:45.653929] epoch: 8350 | elbo: 1288320.10625 | train_rmse: 0.0354 | val_rmse: 2.511 | val_ll: -100.3932
[4:13:16.073255] epoch: 8400 | elbo: 1286058.76125 | train_rmse: 0.0356 | val_rmse: 2.5065 | val_ll: -102.9208
[4:14:46.298382] epoch: 8450 | elbo: 1283523.23375 | train_rmse: 0.0343 | val_rmse: 2.5033 | val_ll: -103.1227
[4:16:18.874830] epoch: 8500 | elbo: 1282116.2174999998 | train_rmse: 0.0359 | val_rmse: 2.4984 | val_ll: -101.5393
[4:17:49.781469] epoch: 8550 | elbo: 1279873.2449999999 | train_rmse: 0.0503 | val_rmse: 2.4947 | val_ll: -102.4385
[4:19:20.931750] epoch: 8600 | elbo: 1276395.7650000001 | train_rmse: 0.0411 | val_rmse: 2.4907 | val_ll: -100.7566
[4:20:51.985653] epoch: 8650 | elbo: 1272984.37375 | train_rmse: 0.0349 | val_rmse: 2.4865 | val_ll: -100.8602
[4:22:21.472397] epoch: 8700 | elbo: 1271796.5025000002 | train_rmse: 0.0392 | val_rmse: 2.4826 | val_ll: -100.1416
[4:23:51.083463] epoch: 8750 | elbo: 1270256.48 | train_rmse: 0.0347 | val_rmse: 2.4792 | val_ll: -100.0175
[4:25:21.100086] epoch: 8800 | elbo: 1266176.9525 | train_rmse: 0.0317 | val_rmse: 2.4763 | val_ll: -101.5334
[4:26:51.452064] epoch: 8850 | elbo: 1265448.9737500001 | train_rmse: 0.0339 | val_rmse: 2.4715 | val_ll: -99.2268
[4:28:22.123104] epoch: 8900 | elbo: 1261734.7149999999 | train_rmse: 0.0352 | val_rmse: 2.4685 | val_ll: -98.7475
[4:29:51.658139] epoch: 8950 | elbo: 1263234.65375 | train_rmse: 0.035 | val_rmse: 2.4653 | val_ll: -101.0234
[4:31:23.617582] epoch: 9000 | elbo: 1257158.2275 | train_rmse: 0.0331 | val_rmse: 2.4603 | val_ll: -99.2114
