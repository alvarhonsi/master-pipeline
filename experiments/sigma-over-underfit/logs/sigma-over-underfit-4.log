Start time: 2023-07-11 13:11:19.785676
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:3 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0447,  0.0756, -0.2127,  ...,  0.2609,  0.4910,  0.2665],
        [-0.2647, -0.0771,  0.2573,  ..., -0.1631,  0.2988,  0.2101],
        [ 0.0411, -0.3286, -0.2541,  ..., -0.6626, -0.0275,  0.2559],
        ...,
        [-0.4244,  0.2789, -0.2359,  ...,  0.0649, -0.5205, -0.0421],
        [-0.4508, -0.1568,  0.2182,  ..., -0.4388, -0.3859, -0.0195],
        [-0.3507,  0.0489,  0.4109,  ..., -0.0855, -0.2056, -0.2932]],
       device='cuda:3', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6166e-02, -1.3915e-01,  2.9564e-01,  2.9611e-01,  2.2214e-01,
        -9.7888e-02,  5.8897e-01, -1.8524e-01,  2.0443e-01,  5.8446e-02,
        -1.7745e-01, -1.9895e-01, -1.1682e-01, -5.3616e-01,  2.0789e-01,
        -6.4598e-02,  3.3362e-01,  1.3427e-01, -3.1360e-02, -3.8676e-01,
         5.7533e-01,  9.6737e-02, -3.1188e-01, -1.9318e-01, -3.3081e-01,
        -1.8537e-01,  3.6419e-01, -1.3505e-01,  3.0185e-01,  5.7582e-01,
        -3.7531e-01, -4.6969e-01, -6.4841e-02,  4.0309e-01, -4.3244e-01,
        -1.9433e-01,  1.6605e-01, -2.1830e-01,  2.8050e-01, -9.4567e-02,
         1.5417e-01,  1.7902e-01, -2.3644e-01, -2.4925e-01, -2.9956e-01,
        -3.7044e-02,  1.6814e-01, -1.9854e-01, -2.5638e-01,  2.6855e-01,
        -9.5205e-02,  3.6366e-01, -8.0318e-01, -1.6955e-01, -2.2713e-01,
        -2.0035e-02,  1.3097e-01,  2.3065e-01,  3.7070e-01, -1.3243e-01,
        -3.6160e-02, -4.8206e-03, -1.1376e-03,  2.0086e-01, -2.2492e-01,
         3.9010e-01, -2.8781e-01,  4.2783e-02, -3.1186e-01,  2.7993e-01,
         3.9311e-01,  7.0192e-02, -9.7189e-02,  8.0272e-01, -3.1546e-01,
        -5.5958e-01,  2.1423e-02, -1.2455e-01, -3.3646e-01,  2.7934e-01,
        -1.5209e-01,  2.7482e-01, -1.1987e-01,  4.2689e-01,  1.3793e-02,
         2.3633e-01, -3.1819e-01,  5.3045e-02,  1.5081e-01,  3.4590e-01,
        -3.7396e-01, -7.3865e-02, -9.2203e-02,  6.5114e-02,  3.8257e-01,
        -1.5689e-01,  1.7788e-01,  5.1722e-01, -4.2938e-01,  8.0669e-02,
        -6.2920e-03, -1.3110e-01, -2.6427e-02,  4.4446e-01, -5.4677e-01,
        -1.6423e-01,  3.0083e-02,  9.8959e-02, -1.1722e-01,  2.3326e-01,
         1.7362e-01, -4.6287e-01, -5.5670e-01, -9.9926e-02, -3.0622e-01,
         1.4751e-01, -6.1184e-01, -5.0763e-01, -1.6882e-02, -5.2880e-01,
        -7.5939e-01, -1.9816e-01, -1.5566e-01,  3.9489e-01,  3.2330e-01,
        -1.3048e-01,  2.4041e-01, -2.6492e-01, -1.5796e-01,  7.9179e-01,
         1.4048e-01,  5.2655e-01,  2.2442e-01, -9.1619e-02, -1.0366e+00,
         2.6553e-01,  2.0384e-01, -1.7155e-01, -1.6481e-03,  3.2141e-01,
        -5.8581e-01,  6.4256e-01, -3.9943e-01, -1.9391e-01, -2.6091e-01,
         1.1797e-01,  4.4223e-02,  1.6194e-01, -3.5289e-01, -3.2432e-01,
        -2.6665e-01,  3.0188e-01, -8.1698e-02, -1.5214e-01, -6.1498e-01,
        -3.0815e-02, -6.0532e-01,  4.0075e-01, -7.2847e-02,  2.7709e-01,
        -2.6974e-01,  2.5113e-01,  4.1658e-01,  3.3207e-01,  6.8874e-02,
        -5.5198e-02, -4.3679e-01,  5.4634e-01, -1.6460e-01, -1.7971e-01,
         3.5597e-01,  5.3952e-01, -2.0925e-01, -4.3910e-01, -3.2569e-01,
         7.6319e-01, -4.8673e-01,  2.1589e-02,  3.6273e-01, -3.1636e-01,
         9.0259e-02, -1.8414e-02, -2.3747e-01, -5.9817e-02, -1.9344e-01,
        -1.7293e-01, -9.0646e-02, -2.1341e-01, -4.3948e-01, -7.6056e-01,
        -2.0861e-01,  6.6792e-01, -1.3956e-01, -3.2485e-01,  1.1441e-01,
         1.7686e-01, -2.2298e-01,  4.3501e-01, -2.1019e-01, -1.8008e-02,
         3.9982e-01, -1.0706e-01,  3.2638e-01, -5.9527e-01,  1.1342e-01,
        -5.9372e-01, -9.7981e-02, -4.6723e-02,  1.7939e-01, -1.9921e-01,
         6.0286e-01, -5.7475e-01, -2.8755e-01, -2.1078e-02,  3.3662e-01,
        -5.4672e-01, -8.4574e-02,  7.0585e-01, -3.7891e-02, -2.0209e-01,
        -4.6297e-02, -1.0724e-01, -3.7633e-02,  3.6016e-01,  6.9463e-01,
         8.4260e-02,  4.2845e-01, -2.1915e-01,  2.0365e-01,  3.4857e-01,
         2.7607e-01, -4.2154e-01, -7.8276e-01,  3.9964e-02, -1.5784e-02,
        -6.7150e-02,  7.5284e-02,  2.0118e-01,  4.6005e-01,  1.2877e-01,
         1.7722e-01,  2.7403e-01, -4.4400e-01, -3.5898e-01,  3.2615e-01,
        -2.8858e-01,  1.9766e-01,  3.4877e-01,  3.3363e-01, -3.0321e-02,
        -1.4267e-01,  7.5654e-03, -4.5793e-01,  9.4683e-02, -7.8115e-01,
         3.5276e-01, -2.6607e-02, -1.3633e-01,  1.3946e-01, -1.6989e-01,
         9.7561e-02,  1.9092e-01,  1.1244e-01, -1.5142e-02, -2.4859e-01,
         2.6107e-01,  1.3460e-01, -1.9601e-01,  2.2171e-01, -1.1813e-01,
        -2.7271e-02,  2.2744e-01, -6.0848e-02,  1.0853e-01,  2.9351e-01,
         2.4204e-02, -1.1631e-01,  2.2765e-01,  3.5871e-01,  2.9450e-01,
        -5.8593e-01,  3.7996e-02, -4.5191e-01,  2.0098e-01,  1.1031e-01,
         2.5319e-01,  3.8350e-01,  4.1628e-01,  2.2083e-01, -5.2145e-01,
        -1.2084e-04, -2.9774e-01,  5.9605e-01, -1.4718e-01, -2.0328e-01,
        -2.7510e-01,  6.2981e-02,  2.8263e-01,  1.0762e-01, -3.1115e-02,
        -8.6717e-02,  1.2602e-01,  3.4481e-01, -3.1291e-01,  4.4573e-02,
        -5.4093e-02,  1.2785e-02,  5.9368e-01, -9.3458e-02, -3.0025e-02,
        -8.2114e-01,  3.5871e-02, -1.2330e-01, -2.9151e-02, -1.7028e-02,
        -6.4747e-01,  6.7500e-01, -9.3829e-02, -3.5783e-01, -1.1899e-01,
        -1.5495e-01,  1.4848e-01, -1.3111e-02, -4.6007e-01,  7.6984e-02,
        -3.0186e-02,  5.6222e-01, -2.4689e-01, -9.7050e-02, -1.6777e-01,
        -3.7750e-01, -2.4305e-01, -4.1383e-01,  3.1174e-01, -3.7243e-01,
        -7.9324e-02,  1.1726e-01,  6.9679e-01,  1.9514e-01, -5.7582e-02,
        -2.1583e-01,  1.9784e-02,  1.2355e-01,  2.7329e-01,  9.2189e-02,
         1.3324e-01,  1.3457e-01,  1.5220e-01,  5.6737e-01, -4.6108e-01,
        -1.4723e-01,  3.6228e-01,  3.5389e-01, -1.3471e-01, -4.3502e-01,
         5.7319e-02,  6.4020e-01, -3.4931e-01,  5.9252e-01, -3.7241e-01,
         4.8289e-01, -7.1217e-01, -1.7626e-01,  5.8545e-02, -4.4086e-01,
         4.1514e-01,  3.5856e-02,  2.0005e-01, -7.7625e-02,  4.2968e-01,
         4.4622e-01, -2.8619e-01, -5.0256e-01, -6.0599e-01, -1.0697e-01,
        -6.6589e-03, -1.3865e-01, -1.9513e-01, -1.1505e-01, -1.1677e-01,
         1.4042e-01,  1.0355e-01, -1.9033e-01,  2.3525e-02, -3.3168e-01,
         3.7288e-01,  3.2279e-01,  3.5255e-01, -1.1326e-01,  3.3135e-01,
        -1.7131e-01,  3.8987e-01, -1.6718e-01,  2.1797e-02, -6.6744e-01,
        -3.9690e-01, -6.4341e-01,  1.5612e-01,  1.4624e-01, -1.5568e-01,
         1.6977e-02, -1.1612e-01, -2.9578e-01,  3.9115e-01, -1.8719e-01,
         2.2748e-01,  1.3767e-01,  3.3728e-01,  1.0920e-01, -3.8483e-01,
         2.2107e-01,  2.1028e-01,  1.3182e-01,  1.1805e-01,  1.4939e-02,
         4.6891e-01, -3.7607e-01, -1.6891e-01, -1.7116e-01, -6.4544e-02,
        -3.4030e-01,  4.5838e-01,  3.0842e-01,  2.1312e-01, -5.4339e-02,
         2.4640e-01,  9.0786e-02, -6.6366e-01,  3.6862e-02,  3.8134e-01,
        -1.4490e-01, -5.9302e-02, -1.5054e-01,  3.4661e-02, -2.3997e-01,
        -2.0295e-01, -5.0164e-01, -2.1711e-02, -5.0930e-01, -2.2989e-01,
        -1.3395e-01, -7.2049e-02,  5.1076e-01, -1.9724e-01,  1.3791e-01,
         2.4521e-01,  4.4357e-02, -7.0624e-01, -1.9231e-01, -4.9890e-01,
        -3.3431e-01,  9.3956e-02,  3.9203e-02, -3.3491e-01, -7.7078e-01,
        -3.9517e-01, -7.0076e-03, -4.3714e-01, -5.3338e-01, -1.1568e-01,
        -7.7663e-01, -1.9353e-01,  1.6875e-01,  1.5197e-01,  3.1701e-01,
        -2.0656e-01,  2.4972e-01,  6.6190e-02, -2.9960e-02, -8.0230e-01,
        -4.1852e-02,  2.3524e-02,  3.8774e-01, -3.4516e-01,  2.3149e-01,
        -8.1876e-02, -7.5342e-04,  2.5952e-01,  1.2734e-01, -1.9654e-01,
        -2.9427e-02, -4.5514e-02,  5.0196e-01,  4.5032e-01,  4.2538e-01,
         2.0322e-01,  7.1951e-01, -5.9948e-01,  3.0338e-01, -2.7482e-02,
        -9.1157e-02, -2.4218e-01, -2.1302e-02,  2.9174e-01,  3.2023e-02,
         4.5233e-02, -1.0554e-01,  4.9416e-01,  1.0603e-01, -2.3242e-01,
        -1.0399e-01, -2.7018e-01,  2.0762e-01,  3.4917e-01, -7.6422e-02,
         1.4725e-01, -5.7476e-02, -7.6483e-02,  3.3816e-01, -5.8852e-01,
         1.6630e-02,  8.2448e-03], device='cuda:3', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1347,  0.2989, -0.5339,  ..., -0.5265, -0.2761, -0.0241],
        [-0.4257,  0.3315,  0.2079,  ..., -0.1017,  0.1317,  0.5264],
        [-0.6097, -0.1783,  0.2613,  ...,  0.3783, -0.4814, -0.0344],
        ...,
        [ 0.0203, -0.2584, -0.2590,  ..., -0.4645, -0.5785, -0.1913],
        [-0.1921,  0.9825,  0.0440,  ...,  0.2357, -0.6674,  0.3438],
        [ 0.4051, -0.2642,  0.7370,  ...,  0.3095,  0.2294,  0.2429]],
       device='cuda:3', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 5.1383e-01,  2.2676e-01, -1.8156e-01, -2.9724e-01, -1.5685e-01,
         6.0414e-01,  6.1232e-02,  2.7250e-02, -2.3418e-01, -4.5196e-02,
        -2.3660e-01, -6.8167e-01,  5.7750e-02,  3.6466e-01,  3.1874e-01,
         5.3432e-02,  1.8438e-01,  8.8736e-02,  3.0512e-01,  5.2947e-02,
         8.0293e-03,  6.4511e-02,  1.7163e-01,  7.7425e-02,  2.7578e-01,
        -2.7774e-01,  6.8803e-01,  4.6933e-01,  2.0090e-01, -4.3950e-01,
        -2.8161e-01,  3.8266e-01, -3.2890e-02, -6.4451e-01,  6.4740e-01,
        -2.4258e-01, -5.4273e-01, -2.0864e-01, -1.6346e-01,  4.2535e-01,
         6.1291e-02, -1.3541e-01,  3.5634e-01,  2.0267e-01, -3.8041e-01,
         4.7205e-01, -1.3605e-02,  3.2818e-01, -5.6446e-01, -2.6980e-01,
        -3.6690e-01,  1.6149e-01, -2.4639e-01, -4.7106e-02,  4.9669e-02,
        -1.6383e-01, -6.6454e-01, -2.0559e-01,  3.5866e-01, -6.5202e-01,
        -2.5192e-01, -1.2759e-01, -1.0010e-01,  5.5464e-01,  2.6002e-01,
         6.1209e-01,  4.5297e-02, -2.3984e-01,  2.4185e-01, -5.3261e-01,
        -1.5795e-03,  3.7480e-01,  2.2998e-01,  2.5373e-01,  6.2845e-01,
        -4.8846e-01, -2.6556e-01,  1.0543e-01, -1.7961e-01, -4.7442e-02,
         2.0987e-01,  1.4915e-01, -5.4043e-01,  6.3419e-01, -1.4536e-02,
        -1.3952e-01,  1.4541e-03, -1.5868e-01, -4.4381e-02, -7.2536e-03,
         3.0550e-01,  1.7987e-01,  2.4523e-01, -2.6376e-01, -5.4468e-01,
         8.4813e-02, -1.6345e-01, -9.2069e-02,  3.3448e-01,  8.8672e-02,
        -4.2712e-01, -7.8307e-02,  1.5671e-01, -1.4268e-01, -2.6539e-01,
         1.5692e-02, -1.4193e-01,  6.3396e-01, -1.6775e-01, -4.3899e-01,
        -2.9766e-01,  2.0886e-01,  6.2453e-02,  1.4515e-01, -1.7576e-01,
        -5.2782e-01,  2.8647e-01, -3.6118e-01, -1.1295e-01, -1.5333e-01,
         3.8649e-02, -2.3035e-01,  4.8449e-01, -6.5390e-01,  1.7446e-01,
        -2.0998e-01, -2.4014e-01,  3.8094e-01,  5.4638e-01, -9.2301e-02,
         7.2799e-01, -5.4894e-02, -2.6638e-01,  1.8212e-01,  6.3314e-02,
         4.4116e-02, -1.3119e-01,  7.7694e-01,  1.6987e-02, -1.0065e-01,
        -4.2424e-01,  5.3672e-01, -1.0983e-01,  1.8426e-01, -1.5716e-01,
         1.5363e-01,  9.4920e-02, -3.5707e-01, -3.0785e-02, -4.7559e-02,
        -6.1601e-01,  1.6270e-01,  1.1011e-01, -1.8307e-01,  2.0297e-01,
        -3.7307e-01,  1.1780e-01, -8.7353e-01, -6.3581e-01, -9.1177e-02,
         4.5015e-01,  5.8748e-01, -2.0362e-02,  5.2032e-01,  1.2865e-01,
        -2.7387e-01,  3.3855e-01, -8.1991e-02, -3.3930e-02, -2.2129e-01,
        -1.1106e-01,  2.9059e-01,  3.2387e-01, -3.3815e-02, -3.7728e-01,
         5.8272e-01,  6.2464e-01, -2.5074e-01, -4.3780e-01,  5.6216e-01,
         1.2451e-01, -1.2899e-01,  1.2758e-01, -9.5528e-02,  2.0078e-01,
        -2.1147e-01, -1.9996e-01,  2.1658e-01, -4.6028e-01,  1.7152e-01,
        -1.7944e-01,  2.5667e-02,  2.6994e-01, -3.3626e-01, -3.3286e-01,
         2.1351e-01,  3.3059e-01,  8.8407e-02,  1.1184e-01, -9.3410e-01,
        -3.5365e-01, -4.0851e-01,  1.9238e-01,  1.9179e-01,  6.0050e-01,
        -6.1300e-01, -7.7526e-02,  4.3369e-01,  1.9656e-01,  7.5422e-02,
         1.2600e-01,  1.8432e-01,  1.6680e-01,  9.4564e-02,  2.1989e-01,
         6.5338e-02,  3.0670e-01,  2.1004e-01, -3.4678e-02,  3.7545e-01,
        -7.9084e-02, -6.3368e-02,  5.0477e-01,  2.8279e-01, -6.6957e-02,
        -8.2414e-02, -4.5131e-01,  8.7372e-01, -1.2054e-01,  2.9910e-01,
        -7.9745e-04,  1.4130e-01,  2.4211e-01,  1.4722e-01,  2.2276e-02,
         6.4288e-01,  1.9376e-01,  2.3107e-01,  4.2620e-02, -1.0629e-01,
         4.9965e-01,  1.2873e-01, -5.4799e-02,  4.2276e-01, -2.7031e-01,
        -1.9287e-01, -2.8321e-01, -4.5494e-01, -1.1602e-01, -1.0290e-01,
        -1.4740e-01,  4.4534e-01,  5.1917e-01, -1.4431e-01, -7.7550e-01,
         1.6413e-01, -2.1547e-01,  3.4814e-01,  2.9550e-02,  1.2023e-01,
         2.7038e-01, -2.4514e-01, -2.5994e-01, -4.0943e-01,  6.1809e-01,
        -6.4172e-02,  1.4789e-01,  4.9981e-01,  5.8535e-01,  2.1067e-01,
        -7.5197e-02, -2.0989e-01, -6.3907e-02, -4.6315e-01,  2.4584e-01,
        -6.1935e-01,  1.9323e-02,  5.7194e-01, -2.9602e-01, -5.6871e-02,
        -4.8013e-01, -2.9334e-02,  4.9256e-02, -4.2844e-01,  1.4215e-01,
        -1.9628e-01, -5.6353e-01,  2.3770e-01, -1.9504e-01, -6.0946e-02,
         8.6788e-01,  1.5729e-01, -3.3453e-01,  4.2605e-02,  3.4771e-01,
         2.2894e-01,  8.9872e-02,  2.3474e-01,  3.7149e-01, -2.5347e-01,
         1.2892e-01, -3.7038e-01,  6.8718e-02, -2.8476e-01,  3.6732e-01,
        -1.4778e-01,  3.5754e-01,  5.1770e-02, -1.2465e-01,  1.4894e-02,
        -5.7676e-03, -4.7557e-01, -5.1856e-01,  1.0399e-01,  8.1136e-02,
         5.3078e-01, -4.8298e-01,  2.6217e-01,  1.1708e-01, -2.2655e-02,
         3.2414e-01,  3.2335e-01, -3.2487e-01, -3.6452e-01,  4.9279e-01,
         2.7096e-01, -2.5982e-01, -2.0117e-01, -2.4121e-01, -1.1426e-01,
        -7.0331e-02, -3.8127e-01, -3.8204e-01, -1.4516e-01, -3.1634e-01,
         2.4916e-01,  2.1327e-01, -2.6620e-01, -1.7288e-01,  2.1561e-02,
         2.7197e-01, -1.3148e-01,  8.5358e-02, -2.5469e-01,  1.9918e-01,
        -8.2631e-02, -3.2942e-01,  4.1063e-01, -2.2063e-01,  1.4496e-01,
         1.9767e-01, -1.2407e-01, -1.5407e-01, -5.4703e-01,  7.7805e-02,
        -4.6431e-01, -1.4431e-01,  9.7252e-02,  6.0515e-01, -5.5042e-01,
        -2.1915e-01, -2.3620e-01,  6.5295e-02,  8.8855e-01,  1.2050e-01,
        -2.8610e-01,  1.6722e-01,  4.6341e-01,  9.4008e-03, -9.6328e-02,
        -3.3636e-01,  4.1791e-01,  1.4727e-01,  4.5041e-01, -4.3296e-01,
        -3.3456e-02,  6.8088e-02, -1.9341e-01, -2.2595e-01,  1.5026e-01,
        -1.9233e-01, -2.3071e-02, -1.5759e-01,  2.4411e-01, -1.4933e-01,
        -7.8673e-02, -5.1893e-01, -1.8274e-01, -3.6652e-01,  3.9821e-01,
        -1.1308e-03,  1.1077e-01,  1.1272e-01,  1.4247e-01, -2.5590e-01,
        -2.6423e-01, -3.1982e-01,  5.4112e-01,  4.2725e-01,  4.2929e-02,
        -1.4493e-01,  4.0068e-01, -1.5884e-01, -4.5532e-01, -3.6434e-01,
        -2.6256e-01, -1.9314e-01, -3.0976e-01,  2.1947e-01,  1.2527e+00,
         4.6440e-02,  5.4746e-02, -4.3317e-02,  3.7618e-01, -3.7077e-01,
        -2.0447e-01, -1.2383e-01,  2.0692e-01,  1.3606e-01,  1.9516e-01,
         2.2275e-01,  1.7480e-01,  3.2938e-01, -5.1290e-02, -7.7177e-01,
        -2.7293e-01,  3.2536e-01, -1.2914e-01,  8.3979e-02,  3.6308e-02,
        -3.2365e-02, -2.9454e-01,  3.3357e-01, -1.0323e-01,  2.0189e-01,
         2.5102e-02,  4.2577e-01, -3.1884e-01,  4.0641e-01,  8.1616e-03,
         3.8972e-01, -1.5253e-01,  6.5812e-01, -3.0501e-01,  1.1704e-01,
        -3.4881e-01,  1.1053e-02, -3.0909e-01, -7.7253e-02,  3.6673e-01,
        -7.6565e-02,  9.1087e-03, -1.7329e-01,  2.6309e-01,  8.9734e-02,
        -4.5330e-01,  1.5570e-02, -4.8282e-01,  4.0811e-02, -3.2425e-02,
        -3.0552e-01,  9.6641e-02, -1.8799e-02,  1.7777e-02,  7.3005e-01,
         1.4098e-01,  1.0050e-01, -1.0706e-01, -1.7136e-01,  2.0919e-01,
         2.9493e-01, -8.6930e-01, -8.4521e-02, -4.6610e-01,  4.6881e-01,
         2.8357e-01,  2.9123e-01,  5.6292e-01, -7.6437e-01,  9.9124e-02,
        -1.5738e-01,  4.0906e-01,  1.5995e-01, -3.8363e-01,  1.4078e-02,
         2.1916e-01,  2.0725e-01, -1.0849e-01, -4.3832e-01, -4.3885e-01,
        -2.4597e-02,  3.0685e-01, -4.8883e-01, -4.4606e-02, -8.3244e-01,
        -3.0004e-01,  3.6187e-01, -1.8884e-01,  1.1498e-01, -2.2350e-01,
         1.8464e-01, -5.0595e-02,  3.7592e-01, -1.6456e-01,  9.8069e-02,
         2.6785e-01,  2.1655e-01,  2.4135e-01, -4.7972e-01,  1.9272e-01,
         1.7350e-01, -4.1612e-01], device='cuda:3', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1730, -0.2277, -0.1308,  ..., -0.0064,  0.0714,  0.0089],
        [-0.3097, -0.3952,  0.1457,  ...,  0.1606, -0.3123,  0.2392],
        [-0.3884,  0.1280, -0.2651,  ..., -0.3274,  0.0380,  0.5866],
        ...,
        [-0.1138,  0.1471, -0.2607,  ..., -0.1059,  0.1240, -0.2748],
        [ 0.1022, -0.0441,  0.4888,  ...,  0.2420, -0.3309, -0.0214],
        [ 0.3723, -0.2546, -0.5360,  ...,  0.0103,  0.2929, -0.4245]],
       device='cuda:3', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 1.4179e-01,  3.2210e-01, -5.6643e-01,  2.7856e-01,  4.0780e-01,
        -5.2465e-01, -4.4860e-01,  6.3692e-01,  1.2261e-01,  3.3203e-01,
         1.1851e-01,  3.5166e-01,  4.1000e-01, -6.7416e-02,  5.7536e-01,
         9.8060e-02,  4.7483e-02, -9.5358e-02,  5.6268e-02,  4.9899e-02,
         2.9323e-01,  5.6371e-01,  2.5355e-01, -7.4552e-02, -3.9356e-01,
         1.9312e-01, -4.2989e-03, -7.0626e-02, -1.7859e-01,  4.3795e-01,
        -3.8399e-01,  5.2805e-02, -1.2964e-01,  5.6573e-02, -1.4092e-01,
         1.6990e-01,  6.7457e-02, -3.0747e-01,  4.2022e-01,  3.0776e-01,
        -3.1603e-02,  1.7747e-01,  1.3542e-01, -2.8546e-01,  6.4476e-01,
         2.1349e-02, -7.8576e-01,  4.7399e-02, -1.9146e-02,  8.6159e-04,
         4.6737e-01,  3.0309e-01, -8.5541e-02, -2.5931e-02, -1.7506e-01,
        -3.3630e-01,  1.6496e-01, -2.1160e-01,  8.8874e-02,  9.7483e-02,
         1.7767e-01,  8.5455e-01,  3.7553e-01,  1.7289e-01,  3.1729e-01,
        -1.1959e-01,  6.3505e-01, -3.6701e-01, -3.2456e-01,  4.0400e-01,
         3.2255e-01, -6.0352e-01,  2.1656e-02, -3.1416e-01, -3.4468e-01,
        -3.2838e-01,  1.2244e-01,  2.3571e-01,  5.9361e-01,  2.3576e-01,
         2.9457e-01,  2.5872e-01,  5.8149e-01, -4.4340e-01, -5.3872e-01,
        -2.0802e-01,  4.6460e-01, -2.5942e-01, -1.6369e-01, -3.9992e-01,
        -4.5108e-01,  1.5733e-01,  1.2554e-03,  1.2605e-01,  1.7727e-01,
        -1.0899e-01,  4.6134e-01, -1.1725e-01, -1.8230e-01, -3.0823e-01,
         1.0486e-02,  6.9209e-04, -1.7649e-01,  2.0212e-01, -1.1128e-01,
        -2.9200e-01, -4.3355e-01, -2.6220e-02, -3.9349e-01,  7.9136e-02,
         1.3552e-01, -5.1725e-01, -1.9342e-01, -9.7791e-02,  7.8979e-03,
        -3.1055e-01, -3.4341e-01, -1.3593e-01,  1.9896e-01, -1.7927e-01,
        -5.4761e-01, -2.2198e-01,  4.2610e-01,  8.2436e-02, -2.0173e-01,
         2.4040e-02, -1.8343e-01, -2.1978e-01,  3.5884e-01, -7.7458e-01,
         2.3324e-01,  1.3607e-01, -3.9449e-01,  4.1063e-01, -5.3288e-01,
         1.1797e-01, -3.6661e-01,  5.9781e-01, -2.7411e-01,  8.7778e-02,
         1.4884e-01, -3.0281e-01, -6.3868e-01, -1.1967e-01,  2.4771e-01,
         5.4744e-01, -1.4941e-01,  2.3260e-02,  3.9616e-04, -4.2770e-01,
        -4.0471e-01, -2.0044e-01,  3.2872e-01,  4.6651e-02,  2.4371e-01,
         1.9984e-01,  1.5637e-01, -8.4627e-02,  8.0435e-02,  1.6755e-01,
         2.4172e-01, -5.1212e-01,  1.8761e-01, -2.4661e-01,  1.5017e-01,
        -4.1426e-01,  3.1818e-01, -2.4743e-02, -6.2680e-02,  1.5470e-01,
         1.4934e-01,  2.2034e-01,  1.2142e-01,  2.1281e-01,  5.2308e-01,
         1.7412e-01,  2.7777e-01,  1.9550e-01,  5.3673e-01,  1.1650e-01,
        -1.3916e-01, -6.9720e-01,  6.8880e-01,  2.5903e-01, -1.0851e-01,
         1.5277e-01,  3.2483e-01, -2.9587e-02, -2.5160e-01, -8.1416e-02,
         7.9530e-02, -2.6971e-01,  7.0683e-01, -9.2652e-02, -2.7599e-01,
        -2.8310e-01, -3.0244e-01,  2.5042e-01, -2.0662e-01, -1.3467e-01,
        -3.8732e-01, -3.3861e-01, -2.8108e-01, -3.0866e-01, -6.7359e-02,
         3.5845e-01, -1.1492e-01, -2.6174e-01,  7.9780e-01,  6.6983e-02,
        -3.5975e-01,  4.4830e-01,  2.3262e-01, -3.8860e-01,  3.2931e-01,
        -5.0148e-01,  3.9907e-01,  6.3266e-01,  1.0864e-01,  3.5626e-02,
         4.6151e-01, -4.1601e-01,  2.0126e-01,  2.8901e-01, -2.9050e-01,
         1.5636e-01,  1.5314e-01, -2.6690e-01,  2.0631e-01, -1.2344e-01,
         5.0111e-02, -2.8826e-01, -1.3203e-01, -1.0933e-01, -1.2740e-01,
        -1.6363e-01, -3.2539e-02, -4.1435e-02,  2.0555e-01,  2.7631e-01,
        -1.3128e-01,  1.9418e-01,  2.1531e-01,  3.0464e-01,  1.2553e-01,
        -4.3558e-03,  3.5148e-03,  4.5660e-02, -2.6346e-01, -1.2613e-01,
        -1.6006e-01,  2.5396e-01,  2.3544e-03,  2.7709e-01,  2.7365e-01,
         1.5975e-01, -2.0429e-01,  2.6308e-01,  1.1232e-01, -1.7520e-01,
        -2.9343e-02,  1.9986e-01,  3.7351e-01,  1.8321e-03,  2.8144e-01,
         1.6961e-01,  1.1166e-01, -4.7342e-01,  2.5215e-01,  1.2465e-01,
        -1.7574e-01, -2.3487e-01, -3.2847e-02, -3.3511e-01, -1.6397e-01,
         9.2866e-02,  6.8842e-01, -1.2720e-01,  3.8358e-03, -1.8727e-01,
         1.1597e-01,  1.0454e-01, -2.6154e-01,  2.1201e-01, -4.5223e-01,
         2.0397e-01, -6.3676e-01, -1.0293e-01, -2.7385e-01, -3.9719e-01,
         1.6923e-01,  8.8046e-02,  1.1130e-01, -3.8459e-01, -3.7879e-01,
         5.1172e-01,  4.1529e-01, -3.8174e-01, -1.2938e-01,  3.2678e-01,
         4.4141e-01,  5.7247e-01,  5.7335e-01,  2.2937e-01,  1.7478e-01,
         3.0146e-01,  2.0555e-01, -8.7688e-01,  2.6807e-01,  3.9275e-01,
         3.8590e-02, -3.1944e-01, -5.3004e-02,  1.4634e-02,  1.4402e-01,
        -2.0849e-01,  1.3237e-01,  1.5445e-01, -2.2459e-02,  2.4120e-01,
         1.2028e-01, -1.7279e-01,  4.2801e-01, -7.6152e-01, -2.1103e-01,
         6.2899e-01, -1.6014e-01,  5.1742e-02,  4.7674e-01,  5.3797e-02,
        -1.6155e-01,  1.0281e-01, -5.7464e-01,  2.1592e-01, -1.2658e-01,
         4.8350e-02,  3.5443e-02,  1.1428e-01, -4.5882e-01, -9.1370e-02,
        -8.0386e-02, -5.6888e-02,  7.8916e-02,  1.6889e-02,  9.4458e-02,
        -1.0846e-02,  1.4602e-01,  2.9974e-01, -1.2817e-01,  1.4236e-01,
        -5.1285e-01, -4.5173e-01,  3.7114e-01, -6.4035e-02,  3.9342e-01,
         3.0148e-01, -8.0483e-02, -7.5345e-02, -3.7438e-01, -1.1560e-01,
        -7.6414e-01,  1.1941e-01,  3.2946e-01,  1.6418e-01,  2.5149e-01,
         2.8158e-01, -3.1064e-01, -2.1117e-01, -3.6134e-01, -1.7946e-01,
        -3.8067e-01, -1.8442e-02,  5.4178e-01,  1.0826e-01, -4.2489e-01,
         9.8786e-02, -1.9886e-01, -1.9944e-01, -2.6189e-01,  3.2915e-01,
         2.7132e-01, -2.7107e-01,  1.9721e-01, -3.4363e-02, -3.4736e-01,
         7.6883e-01,  7.6609e-01,  1.3300e-01,  1.0317e-01, -9.9651e-02,
        -1.5886e-01,  3.3379e-01,  1.1817e-01,  6.1088e-01, -9.8981e-02,
         3.2033e-02, -3.2494e-01,  6.3448e-01, -1.8965e-02, -1.0558e-01,
         1.8887e-01,  2.4920e-02, -4.7867e-01,  1.6645e-03,  1.6404e-01,
        -2.1174e-01,  5.4358e-01, -1.9561e-01, -6.2061e-01, -5.3127e-01,
         1.8306e-01,  5.1041e-01,  2.6210e-01,  4.7968e-01,  4.0350e-01,
         1.7007e-01, -6.5120e-02,  3.2445e-02, -2.4497e-01,  3.8961e-02,
         3.3875e-01, -2.3760e-01,  4.1712e-01,  1.6046e-01, -2.4134e-01,
         1.4197e-01, -1.9834e-01,  4.0619e-01,  2.6777e-01,  2.4601e-01,
         9.6592e-02,  2.2297e-02,  4.6209e-01,  3.8926e-01,  1.6503e-01,
         1.6308e-01, -2.7902e-01, -1.7507e-01, -1.0870e-01,  4.1737e-02,
         1.2515e-01, -1.6289e-01,  2.8835e-01, -5.0282e-01,  2.9335e-01,
         5.7595e-01, -2.0366e-01,  4.6764e-02, -2.6312e-01,  4.7355e-01,
        -3.3565e-01,  2.4745e-01, -8.0281e-02,  3.2135e-02, -6.0093e-01,
         4.9367e-01, -3.1144e-01,  2.6010e-01,  6.3402e-02, -1.1066e-02,
        -2.8310e-01, -5.9138e-01,  1.9894e-02, -5.6878e-01,  4.3710e-01,
        -1.8511e-01,  1.4252e-01, -4.7774e-01, -1.0077e-01, -2.5265e-01,
         1.0201e-01, -2.4135e-01, -3.3036e-01,  4.7532e-02,  1.9244e-01,
        -1.9187e-01,  3.5796e-02,  2.8442e-01, -2.9295e-01,  2.3329e-02,
         2.8451e-01, -1.1799e-01,  2.6453e-01,  1.4019e-01, -7.1528e-02,
         1.0160e+00,  2.1072e-01,  1.9396e-01, -1.2166e-01, -6.4238e-02,
        -1.6593e-01,  2.4005e-01, -2.9599e-01, -1.6909e-01,  9.1479e-03,
        -5.0724e-02, -3.1212e-01,  4.8560e-01,  3.9701e-01,  1.2635e-01,
        -5.3069e-01, -8.0266e-02,  3.7003e-01, -3.5217e-01,  8.7458e-02,
         5.4936e-01, -4.9864e-01,  1.0496e-01,  1.4389e-01, -7.2095e-01,
         1.7354e-01, -1.0492e-01], device='cuda:3', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 0.7570,  0.0071,  0.1389,  0.6667,  0.1712,  0.3561,  0.6226, -0.3891,
          0.5041, -0.1839, -0.3202, -0.1969,  0.1615,  0.0672,  0.6710, -0.3528,
         -0.3141, -0.2882, -0.8871,  0.1790, -0.3063,  0.4413, -1.0605, -0.2951,
         -0.3255, -0.0646,  0.2149, -0.5881, -0.2235, -0.0753,  0.5737, -0.5544,
         -0.0257, -0.1990, -0.4254, -0.1341,  0.0489, -0.3904, -0.2764, -0.3566,
          0.1979,  0.6079, -0.1551, -0.4622,  0.5872, -0.5354, -0.3222, -0.0805,
         -0.3098, -0.0448,  0.7059,  0.6188,  0.2833,  0.2381, -0.2853, -0.1509,
         -0.4050, -0.2043, -0.2229, -0.4423,  0.1936, -0.2756, -0.6272, -0.0563,
          0.0129,  0.0949,  0.0776,  0.0665,  0.0221, -0.1633, -0.2864,  0.3297,
          0.3653,  0.2385, -0.1601,  0.0213,  0.0414,  0.4344, -0.1426,  0.3209,
         -0.1330, -0.0715,  0.1667, -0.1738, -0.5722, -0.5953, -0.0185, -0.6191,
          0.4194,  0.3419,  0.0859,  0.2053,  0.2619, -0.2133, -0.4736,  0.0040,
         -0.3127, -0.3731, -0.2702,  0.1135,  0.2918,  0.2723, -0.2393, -0.0356,
          0.3593, -0.0179, -0.0283,  0.2146,  0.2183,  0.4363, -0.0966,  0.5011,
          0.4738, -0.2301,  0.3377,  0.5182,  0.2462, -0.2835, -0.2046, -0.0658,
          0.4032,  0.2224, -0.2755,  0.8335,  0.2519, -0.2333,  0.0282,  0.0173,
         -0.2267,  0.3252, -0.0499,  0.0456, -0.2739,  0.3119, -0.1075, -0.3342,
          0.2032, -0.3366, -0.2031,  0.1704, -0.1464, -0.3186,  0.1348, -0.2662,
         -0.4975,  0.1851,  0.2307,  0.0125, -0.1342,  0.5029, -0.3441, -0.0833,
         -0.5997,  0.1109,  0.1963,  0.0987, -0.6184,  0.0578,  0.1665,  0.3195,
          0.2226,  0.3615, -0.7867, -0.3353, -0.0744,  0.2562, -0.1247,  0.2649,
         -0.3539, -0.0216,  0.0795,  0.0297, -0.0140,  0.0654,  0.0853, -0.0177,
          0.1761, -0.1112,  0.1291, -0.4507, -0.3451, -0.2394,  0.4435,  0.4105,
         -0.4136, -0.0931, -0.5896,  0.0572, -0.3367, -0.3010,  0.0908, -0.1063,
         -0.5687,  0.1835, -0.1890, -0.0747,  0.2271,  0.1515, -0.0937,  0.0105,
         -0.5979,  0.5121,  0.2226,  0.2394, -0.1796, -0.5596, -0.1476, -0.3604,
          0.0613, -0.6463, -0.2884,  0.0498,  0.0476, -0.1238,  0.0033, -0.1810,
          0.2991, -0.7201, -0.0066,  0.6760,  0.0838, -0.2200,  0.5599, -0.4012,
          0.0079,  0.4337,  0.0768,  0.5233,  0.0051, -0.1023, -0.2747,  0.1599,
         -0.0071,  0.3136,  0.1999, -0.0581,  0.1474, -0.4202,  0.3128, -0.0721,
          0.1800, -0.6434,  0.2795,  0.0963,  0.1675, -0.2505,  0.5946,  0.0881,
         -0.3420, -0.4583,  0.2318, -0.1727,  0.3149, -0.2287, -0.0798, -0.1796,
          0.2559,  0.4442, -0.1899, -0.2371, -0.5489, -0.4815,  0.0435, -0.3169,
          0.4664,  0.0213,  0.0719, -0.0919,  0.0049,  0.0717, -0.2498, -0.1512,
          0.0769, -0.3439,  0.0776, -0.3771,  0.3978, -0.3865,  0.0013,  0.3145,
         -0.2653,  0.3279, -0.5856, -0.6437, -0.3371,  0.2609, -0.0104, -0.0156,
          0.1098,  0.2341,  0.1565,  0.2076,  0.0886, -0.3759, -0.1986,  0.2679,
          0.2184,  0.0187, -0.2609, -0.0485,  0.2321,  0.2120, -0.3796, -0.1598,
         -0.2731, -0.2402, -0.7589, -0.0833,  0.2278,  0.1961,  0.3413, -0.0987,
          0.4173, -0.0623, -0.5167, -0.5240, -0.3195, -0.2283,  0.4674, -0.0255,
         -0.5547,  0.4798, -0.4382, -0.0420, -0.1176,  0.0931, -0.0501,  0.4722,
          0.6665, -0.0861,  0.6480, -0.0445, -0.3785, -0.0294,  0.3172, -0.1142,
          0.0714, -0.1573,  0.0901, -0.3999,  0.2734,  0.3352,  0.1221,  0.1154,
         -0.2944,  0.4117, -0.2558, -0.0271, -0.0529, -0.1143, -0.3698,  0.3019,
         -0.3937, -0.0406,  0.4142, -0.0227,  0.1544,  0.1405,  0.6259, -0.3281,
         -0.1419,  0.6239,  0.0778, -0.2243, -0.2499,  0.0725, -0.4267, -0.4464,
          0.5441, -0.0022, -0.2501,  0.1621, -0.2522,  0.2555, -0.1617, -0.1284,
         -0.0315,  0.1421, -0.1258, -0.0346,  0.0374,  0.2211, -0.6120,  0.1248,
         -0.3892, -0.1977,  0.4000,  0.2652,  0.1389, -0.1081,  0.6099,  0.2467,
         -0.0990,  0.2355,  0.4636, -0.2268,  0.4451, -0.2121, -0.5002, -0.0408,
          0.2536,  0.3768, -0.6709,  0.2756, -0.2226, -0.4895, -0.5475,  0.0174,
          0.4232,  0.3590,  0.6767,  0.3510, -0.0845, -0.0845, -0.4329, -0.2155,
         -0.0831,  0.1408,  0.3465,  0.3161,  0.3256,  0.1817, -0.4837,  0.2695,
         -0.1407, -0.2695,  0.2053,  0.8556, -0.1175, -0.1025,  0.1077,  0.5011,
         -0.3269,  0.2269, -0.1758, -0.2570,  0.0990, -0.4238, -0.2143, -0.4339,
          0.5493,  0.0367, -0.0246,  0.2124,  0.3997, -0.2968, -0.6228,  0.4566,
          0.2689, -0.6811,  0.2121,  0.0886,  0.2624,  0.4336,  0.1599,  0.4324,
          0.5957, -0.5103,  0.8678, -0.3761, -0.3902, -0.2045, -0.1148,  0.1110,
         -0.3288, -0.1500,  0.7500, -0.5143,  0.4097, -0.2637,  0.2661, -0.3321,
          0.1945,  0.0906, -0.0694,  0.2230, -0.0518,  0.0263,  0.7312,  0.0204,
         -0.3837, -0.0236, -0.0973,  0.4279, -0.1067, -0.4604,  0.3151,  0.2388,
          0.0919,  0.3406, -0.2363, -0.3606,  0.5200,  0.1322, -0.2743,  0.1592,
         -0.4984,  0.0436, -0.2330, -0.0258,  0.0537, -0.4252,  0.3755, -0.1834,
         -0.2058, -0.5455,  0.1601, -0.1842, -0.2745, -0.3696, -0.3067,  0.3493]],
       device='cuda:3', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.1671], device='cuda:3', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:3', grad_fn=<AddBackward0>)
likelihood_guide.likelihood._scale.loc torch.Size([]) Parameter containing:
tensor(0.4195, device='cuda:3', requires_grad=True)
likelihood_guide.likelihood._scale.scale torch.Size([]) tensor(0.0100, device='cuda:3', grad_fn=<AddBackward0>)
Using device: cuda:3
===== Training profile tensin-3x512-sl - 1 =====
[0:00:02.227810] epoch: 0 | elbo: 17845690.02 | train_rmse: 66.8933 | val_rmse: 69.1641 | val_ll: -29.8382
[0:01:45.312813] epoch: 50 | elbo: 2895823.0774999997 | train_rmse: 16.4099 | val_rmse: 19.6974 | val_ll: -5.8718
[0:03:27.649364] epoch: 100 | elbo: 2562294.025 | train_rmse: 11.3803 | val_rmse: 16.5247 | val_ll: -5.2803
[0:05:09.498135] epoch: 150 | elbo: 2421307.3 | train_rmse: 8.6121 | val_rmse: 15.0265 | val_ll: -5.0136
[0:06:51.811200] epoch: 200 | elbo: 2332245.2375 | train_rmse: 6.8096 | val_rmse: 14.1118 | val_ll: -4.8389
[0:08:34.690575] epoch: 250 | elbo: 2273134.32 | train_rmse: 5.479 | val_rmse: 13.4599 | val_ll: -4.755
[0:10:18.072006] epoch: 300 | elbo: 2223463.9525000006 | train_rmse: 4.4366 | val_rmse: 12.9331 | val_ll: -4.6675
[0:12:01.021332] epoch: 350 | elbo: 2182409.965 | train_rmse: 3.6511 | val_rmse: 12.4715 | val_ll: -4.6054
[0:13:44.113424] epoch: 400 | elbo: 2145802.1550000003 | train_rmse: 3.0224 | val_rmse: 12.0141 | val_ll: -4.5211
[0:15:26.900285] epoch: 450 | elbo: 2111701.875 | train_rmse: 2.5681 | val_rmse: 11.6 | val_ll: -4.4604
[0:17:10.416980] epoch: 500 | elbo: 2079581.1512499996 | train_rmse: 2.2597 | val_rmse: 11.1743 | val_ll: -4.3497
[0:18:54.631610] epoch: 550 | elbo: 2048555.36875 | train_rmse: 1.9374 | val_rmse: 10.6917 | val_ll: -4.2374
[0:20:38.527062] epoch: 600 | elbo: 2017058.725 | train_rmse: 1.839 | val_rmse: 10.2247 | val_ll: -4.1525
[0:22:22.550948] epoch: 650 | elbo: 1986456.7312500004 | train_rmse: 1.6436 | val_rmse: 9.6748 | val_ll: -4.0198
[0:24:07.227226] epoch: 700 | elbo: 1954796.4300000002 | train_rmse: 1.5788 | val_rmse: 9.165 | val_ll: -3.9016
[0:25:48.967544] epoch: 750 | elbo: 1924161.99875 | train_rmse: 1.4831 | val_rmse: 8.6358 | val_ll: -3.8016
