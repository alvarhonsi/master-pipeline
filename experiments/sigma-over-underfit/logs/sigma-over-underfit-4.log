Start time: 2023-07-09 21:06:08.333066
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:3 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0447,  0.0756, -0.2127,  ...,  0.2609,  0.4910,  0.2665],
        [-0.2647, -0.0771,  0.2573,  ..., -0.1631,  0.2988,  0.2101],
        [ 0.0411, -0.3286, -0.2541,  ..., -0.6626, -0.0275,  0.2559],
        ...,
        [-0.4244,  0.2789, -0.2359,  ...,  0.0649, -0.5205, -0.0421],
        [-0.4508, -0.1568,  0.2182,  ..., -0.4388, -0.3859, -0.0195],
        [-0.3507,  0.0489,  0.4109,  ..., -0.0855, -0.2056, -0.2932]],
       device='cuda:3', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6166e-02, -1.3915e-01,  2.9564e-01,  2.9611e-01,  2.2214e-01,
        -9.7888e-02,  5.8897e-01, -1.8524e-01,  2.0443e-01,  5.8446e-02,
        -1.7745e-01, -1.9895e-01, -1.1682e-01, -5.3616e-01,  2.0789e-01,
        -6.4598e-02,  3.3362e-01,  1.3427e-01, -3.1360e-02, -3.8676e-01,
         5.7533e-01,  9.6737e-02, -3.1188e-01, -1.9318e-01, -3.3081e-01,
        -1.8537e-01,  3.6419e-01, -1.3505e-01,  3.0185e-01,  5.7582e-01,
        -3.7531e-01, -4.6969e-01, -6.4841e-02,  4.0309e-01, -4.3244e-01,
        -1.9433e-01,  1.6605e-01, -2.1830e-01,  2.8050e-01, -9.4567e-02,
         1.5417e-01,  1.7902e-01, -2.3644e-01, -2.4925e-01, -2.9956e-01,
        -3.7044e-02,  1.6814e-01, -1.9854e-01, -2.5638e-01,  2.6855e-01,
        -9.5205e-02,  3.6366e-01, -8.0318e-01, -1.6955e-01, -2.2713e-01,
        -2.0035e-02,  1.3097e-01,  2.3065e-01,  3.7070e-01, -1.3243e-01,
        -3.6160e-02, -4.8206e-03, -1.1376e-03,  2.0086e-01, -2.2492e-01,
         3.9010e-01, -2.8781e-01,  4.2783e-02, -3.1186e-01,  2.7993e-01,
         3.9311e-01,  7.0192e-02, -9.7189e-02,  8.0272e-01, -3.1546e-01,
        -5.5958e-01,  2.1423e-02, -1.2455e-01, -3.3646e-01,  2.7934e-01,
        -1.5209e-01,  2.7482e-01, -1.1987e-01,  4.2689e-01,  1.3793e-02,
         2.3633e-01, -3.1819e-01,  5.3045e-02,  1.5081e-01,  3.4590e-01,
        -3.7396e-01, -7.3865e-02, -9.2203e-02,  6.5114e-02,  3.8257e-01,
        -1.5689e-01,  1.7788e-01,  5.1722e-01, -4.2938e-01,  8.0669e-02,
        -6.2920e-03, -1.3110e-01, -2.6427e-02,  4.4446e-01, -5.4677e-01,
        -1.6423e-01,  3.0083e-02,  9.8959e-02, -1.1722e-01,  2.3326e-01,
         1.7362e-01, -4.6287e-01, -5.5670e-01, -9.9926e-02, -3.0622e-01,
         1.4751e-01, -6.1184e-01, -5.0763e-01, -1.6882e-02, -5.2880e-01,
        -7.5939e-01, -1.9816e-01, -1.5566e-01,  3.9489e-01,  3.2330e-01,
        -1.3048e-01,  2.4041e-01, -2.6492e-01, -1.5796e-01,  7.9179e-01,
         1.4048e-01,  5.2655e-01,  2.2442e-01, -9.1619e-02, -1.0366e+00,
         2.6553e-01,  2.0384e-01, -1.7155e-01, -1.6481e-03,  3.2141e-01,
        -5.8581e-01,  6.4256e-01, -3.9943e-01, -1.9391e-01, -2.6091e-01,
         1.1797e-01,  4.4223e-02,  1.6194e-01, -3.5289e-01, -3.2432e-01,
        -2.6665e-01,  3.0188e-01, -8.1698e-02, -1.5214e-01, -6.1498e-01,
        -3.0815e-02, -6.0532e-01,  4.0075e-01, -7.2847e-02,  2.7709e-01,
        -2.6974e-01,  2.5113e-01,  4.1658e-01,  3.3207e-01,  6.8874e-02,
        -5.5198e-02, -4.3679e-01,  5.4634e-01, -1.6460e-01, -1.7971e-01,
         3.5597e-01,  5.3952e-01, -2.0925e-01, -4.3910e-01, -3.2569e-01,
         7.6319e-01, -4.8673e-01,  2.1589e-02,  3.6273e-01, -3.1636e-01,
         9.0259e-02, -1.8414e-02, -2.3747e-01, -5.9817e-02, -1.9344e-01,
        -1.7293e-01, -9.0646e-02, -2.1341e-01, -4.3948e-01, -7.6056e-01,
        -2.0861e-01,  6.6792e-01, -1.3956e-01, -3.2485e-01,  1.1441e-01,
         1.7686e-01, -2.2298e-01,  4.3501e-01, -2.1019e-01, -1.8008e-02,
         3.9982e-01, -1.0706e-01,  3.2638e-01, -5.9527e-01,  1.1342e-01,
        -5.9372e-01, -9.7981e-02, -4.6723e-02,  1.7939e-01, -1.9921e-01,
         6.0286e-01, -5.7475e-01, -2.8755e-01, -2.1078e-02,  3.3662e-01,
        -5.4672e-01, -8.4574e-02,  7.0585e-01, -3.7891e-02, -2.0209e-01,
        -4.6297e-02, -1.0724e-01, -3.7633e-02,  3.6016e-01,  6.9463e-01,
         8.4260e-02,  4.2845e-01, -2.1915e-01,  2.0365e-01,  3.4857e-01,
         2.7607e-01, -4.2154e-01, -7.8276e-01,  3.9964e-02, -1.5784e-02,
        -6.7150e-02,  7.5284e-02,  2.0118e-01,  4.6005e-01,  1.2877e-01,
         1.7722e-01,  2.7403e-01, -4.4400e-01, -3.5898e-01,  3.2615e-01,
        -2.8858e-01,  1.9766e-01,  3.4877e-01,  3.3363e-01, -3.0321e-02,
        -1.4267e-01,  7.5654e-03, -4.5793e-01,  9.4683e-02, -7.8115e-01,
         3.5276e-01, -2.6607e-02, -1.3633e-01,  1.3946e-01, -1.6989e-01,
         9.7561e-02,  1.9092e-01,  1.1244e-01, -1.5142e-02, -2.4859e-01,
         2.6107e-01,  1.3460e-01, -1.9601e-01,  2.2171e-01, -1.1813e-01,
        -2.7271e-02,  2.2744e-01, -6.0848e-02,  1.0853e-01,  2.9351e-01,
         2.4204e-02, -1.1631e-01,  2.2765e-01,  3.5871e-01,  2.9450e-01,
        -5.8593e-01,  3.7996e-02, -4.5191e-01,  2.0098e-01,  1.1031e-01,
         2.5319e-01,  3.8350e-01,  4.1628e-01,  2.2083e-01, -5.2145e-01,
        -1.2084e-04, -2.9774e-01,  5.9605e-01, -1.4718e-01, -2.0328e-01,
        -2.7510e-01,  6.2981e-02,  2.8263e-01,  1.0762e-01, -3.1115e-02,
        -8.6717e-02,  1.2602e-01,  3.4481e-01, -3.1291e-01,  4.4573e-02,
        -5.4093e-02,  1.2785e-02,  5.9368e-01, -9.3458e-02, -3.0025e-02,
        -8.2114e-01,  3.5871e-02, -1.2330e-01, -2.9151e-02, -1.7028e-02,
        -6.4747e-01,  6.7500e-01, -9.3829e-02, -3.5783e-01, -1.1899e-01,
        -1.5495e-01,  1.4848e-01, -1.3111e-02, -4.6007e-01,  7.6984e-02,
        -3.0186e-02,  5.6222e-01, -2.4689e-01, -9.7050e-02, -1.6777e-01,
        -3.7750e-01, -2.4305e-01, -4.1383e-01,  3.1174e-01, -3.7243e-01,
        -7.9324e-02,  1.1726e-01,  6.9679e-01,  1.9514e-01, -5.7582e-02,
        -2.1583e-01,  1.9784e-02,  1.2355e-01,  2.7329e-01,  9.2189e-02,
         1.3324e-01,  1.3457e-01,  1.5220e-01,  5.6737e-01, -4.6108e-01,
        -1.4723e-01,  3.6228e-01,  3.5389e-01, -1.3471e-01, -4.3502e-01,
         5.7319e-02,  6.4020e-01, -3.4931e-01,  5.9252e-01, -3.7241e-01,
         4.8289e-01, -7.1217e-01, -1.7626e-01,  5.8545e-02, -4.4086e-01,
         4.1514e-01,  3.5856e-02,  2.0005e-01, -7.7625e-02,  4.2968e-01,
         4.4622e-01, -2.8619e-01, -5.0256e-01, -6.0599e-01, -1.0697e-01,
        -6.6589e-03, -1.3865e-01, -1.9513e-01, -1.1505e-01, -1.1677e-01,
         1.4042e-01,  1.0355e-01, -1.9033e-01,  2.3525e-02, -3.3168e-01,
         3.7288e-01,  3.2279e-01,  3.5255e-01, -1.1326e-01,  3.3135e-01,
        -1.7131e-01,  3.8987e-01, -1.6718e-01,  2.1797e-02, -6.6744e-01,
        -3.9690e-01, -6.4341e-01,  1.5612e-01,  1.4624e-01, -1.5568e-01,
         1.6977e-02, -1.1612e-01, -2.9578e-01,  3.9115e-01, -1.8719e-01,
         2.2748e-01,  1.3767e-01,  3.3728e-01,  1.0920e-01, -3.8483e-01,
         2.2107e-01,  2.1028e-01,  1.3182e-01,  1.1805e-01,  1.4939e-02,
         4.6891e-01, -3.7607e-01, -1.6891e-01, -1.7116e-01, -6.4544e-02,
        -3.4030e-01,  4.5838e-01,  3.0842e-01,  2.1312e-01, -5.4339e-02,
         2.4640e-01,  9.0786e-02, -6.6366e-01,  3.6862e-02,  3.8134e-01,
        -1.4490e-01, -5.9302e-02, -1.5054e-01,  3.4661e-02, -2.3997e-01,
        -2.0295e-01, -5.0164e-01, -2.1711e-02, -5.0930e-01, -2.2989e-01,
        -1.3395e-01, -7.2049e-02,  5.1076e-01, -1.9724e-01,  1.3791e-01,
         2.4521e-01,  4.4357e-02, -7.0624e-01, -1.9231e-01, -4.9890e-01,
        -3.3431e-01,  9.3956e-02,  3.9203e-02, -3.3491e-01, -7.7078e-01,
        -3.9517e-01, -7.0076e-03, -4.3714e-01, -5.3338e-01, -1.1568e-01,
        -7.7663e-01, -1.9353e-01,  1.6875e-01,  1.5197e-01,  3.1701e-01,
        -2.0656e-01,  2.4972e-01,  6.6190e-02, -2.9960e-02, -8.0230e-01,
        -4.1852e-02,  2.3524e-02,  3.8774e-01, -3.4516e-01,  2.3149e-01,
        -8.1876e-02, -7.5342e-04,  2.5952e-01,  1.2734e-01, -1.9654e-01,
        -2.9427e-02, -4.5514e-02,  5.0196e-01,  4.5032e-01,  4.2538e-01,
         2.0322e-01,  7.1951e-01, -5.9948e-01,  3.0338e-01, -2.7482e-02,
        -9.1157e-02, -2.4218e-01, -2.1302e-02,  2.9174e-01,  3.2023e-02,
         4.5233e-02, -1.0554e-01,  4.9416e-01,  1.0603e-01, -2.3242e-01,
        -1.0399e-01, -2.7018e-01,  2.0762e-01,  3.4917e-01, -7.6422e-02,
         1.4725e-01, -5.7476e-02, -7.6483e-02,  3.3816e-01, -5.8852e-01,
         1.6630e-02,  8.2448e-03], device='cuda:3', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1347,  0.2989, -0.5339,  ..., -0.5265, -0.2761, -0.0241],
        [-0.4257,  0.3315,  0.2079,  ..., -0.1017,  0.1317,  0.5264],
        [-0.6097, -0.1783,  0.2613,  ...,  0.3783, -0.4814, -0.0344],
        ...,
        [ 0.0203, -0.2584, -0.2590,  ..., -0.4645, -0.5785, -0.1913],
        [-0.1921,  0.9825,  0.0440,  ...,  0.2357, -0.6674,  0.3438],
        [ 0.4051, -0.2642,  0.7370,  ...,  0.3095,  0.2294,  0.2429]],
       device='cuda:3', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 5.1383e-01,  2.2676e-01, -1.8156e-01, -2.9724e-01, -1.5685e-01,
         6.0414e-01,  6.1232e-02,  2.7250e-02, -2.3418e-01, -4.5196e-02,
        -2.3660e-01, -6.8167e-01,  5.7750e-02,  3.6466e-01,  3.1874e-01,
         5.3432e-02,  1.8438e-01,  8.8736e-02,  3.0512e-01,  5.2947e-02,
         8.0293e-03,  6.4511e-02,  1.7163e-01,  7.7425e-02,  2.7578e-01,
        -2.7774e-01,  6.8803e-01,  4.6933e-01,  2.0090e-01, -4.3950e-01,
        -2.8161e-01,  3.8266e-01, -3.2890e-02, -6.4451e-01,  6.4740e-01,
        -2.4258e-01, -5.4273e-01, -2.0864e-01, -1.6346e-01,  4.2535e-01,
         6.1291e-02, -1.3541e-01,  3.5634e-01,  2.0267e-01, -3.8041e-01,
         4.7205e-01, -1.3605e-02,  3.2818e-01, -5.6446e-01, -2.6980e-01,
        -3.6690e-01,  1.6149e-01, -2.4639e-01, -4.7106e-02,  4.9669e-02,
        -1.6383e-01, -6.6454e-01, -2.0559e-01,  3.5866e-01, -6.5202e-01,
        -2.5192e-01, -1.2759e-01, -1.0010e-01,  5.5464e-01,  2.6002e-01,
         6.1209e-01,  4.5297e-02, -2.3984e-01,  2.4185e-01, -5.3261e-01,
        -1.5795e-03,  3.7480e-01,  2.2998e-01,  2.5373e-01,  6.2845e-01,
        -4.8846e-01, -2.6556e-01,  1.0543e-01, -1.7961e-01, -4.7442e-02,
         2.0987e-01,  1.4915e-01, -5.4043e-01,  6.3419e-01, -1.4536e-02,
        -1.3952e-01,  1.4541e-03, -1.5868e-01, -4.4381e-02, -7.2536e-03,
         3.0550e-01,  1.7987e-01,  2.4523e-01, -2.6376e-01, -5.4468e-01,
         8.4813e-02, -1.6345e-01, -9.2069e-02,  3.3448e-01,  8.8672e-02,
        -4.2712e-01, -7.8307e-02,  1.5671e-01, -1.4268e-01, -2.6539e-01,
         1.5692e-02, -1.4193e-01,  6.3396e-01, -1.6775e-01, -4.3899e-01,
        -2.9766e-01,  2.0886e-01,  6.2453e-02,  1.4515e-01, -1.7576e-01,
        -5.2782e-01,  2.8647e-01, -3.6118e-01, -1.1295e-01, -1.5333e-01,
         3.8649e-02, -2.3035e-01,  4.8449e-01, -6.5390e-01,  1.7446e-01,
        -2.0998e-01, -2.4014e-01,  3.8094e-01,  5.4638e-01, -9.2301e-02,
         7.2799e-01, -5.4894e-02, -2.6638e-01,  1.8212e-01,  6.3314e-02,
         4.4116e-02, -1.3119e-01,  7.7694e-01,  1.6987e-02, -1.0065e-01,
        -4.2424e-01,  5.3672e-01, -1.0983e-01,  1.8426e-01, -1.5716e-01,
         1.5363e-01,  9.4920e-02, -3.5707e-01, -3.0785e-02, -4.7559e-02,
        -6.1601e-01,  1.6270e-01,  1.1011e-01, -1.8307e-01,  2.0297e-01,
        -3.7307e-01,  1.1780e-01, -8.7353e-01, -6.3581e-01, -9.1177e-02,
         4.5015e-01,  5.8748e-01, -2.0362e-02,  5.2032e-01,  1.2865e-01,
        -2.7387e-01,  3.3855e-01, -8.1991e-02, -3.3930e-02, -2.2129e-01,
        -1.1106e-01,  2.9059e-01,  3.2387e-01, -3.3815e-02, -3.7728e-01,
         5.8272e-01,  6.2464e-01, -2.5074e-01, -4.3780e-01,  5.6216e-01,
         1.2451e-01, -1.2899e-01,  1.2758e-01, -9.5528e-02,  2.0078e-01,
        -2.1147e-01, -1.9996e-01,  2.1658e-01, -4.6028e-01,  1.7152e-01,
        -1.7944e-01,  2.5667e-02,  2.6994e-01, -3.3626e-01, -3.3286e-01,
         2.1351e-01,  3.3059e-01,  8.8407e-02,  1.1184e-01, -9.3410e-01,
        -3.5365e-01, -4.0851e-01,  1.9238e-01,  1.9179e-01,  6.0050e-01,
        -6.1300e-01, -7.7526e-02,  4.3369e-01,  1.9656e-01,  7.5422e-02,
         1.2600e-01,  1.8432e-01,  1.6680e-01,  9.4564e-02,  2.1989e-01,
         6.5338e-02,  3.0670e-01,  2.1004e-01, -3.4678e-02,  3.7545e-01,
        -7.9084e-02, -6.3368e-02,  5.0477e-01,  2.8279e-01, -6.6957e-02,
        -8.2414e-02, -4.5131e-01,  8.7372e-01, -1.2054e-01,  2.9910e-01,
        -7.9745e-04,  1.4130e-01,  2.4211e-01,  1.4722e-01,  2.2276e-02,
         6.4288e-01,  1.9376e-01,  2.3107e-01,  4.2620e-02, -1.0629e-01,
         4.9965e-01,  1.2873e-01, -5.4799e-02,  4.2276e-01, -2.7031e-01,
        -1.9287e-01, -2.8321e-01, -4.5494e-01, -1.1602e-01, -1.0290e-01,
        -1.4740e-01,  4.4534e-01,  5.1917e-01, -1.4431e-01, -7.7550e-01,
         1.6413e-01, -2.1547e-01,  3.4814e-01,  2.9550e-02,  1.2023e-01,
         2.7038e-01, -2.4514e-01, -2.5994e-01, -4.0943e-01,  6.1809e-01,
        -6.4172e-02,  1.4789e-01,  4.9981e-01,  5.8535e-01,  2.1067e-01,
        -7.5197e-02, -2.0989e-01, -6.3907e-02, -4.6315e-01,  2.4584e-01,
        -6.1935e-01,  1.9323e-02,  5.7194e-01, -2.9602e-01, -5.6871e-02,
        -4.8013e-01, -2.9334e-02,  4.9256e-02, -4.2844e-01,  1.4215e-01,
        -1.9628e-01, -5.6353e-01,  2.3770e-01, -1.9504e-01, -6.0946e-02,
         8.6788e-01,  1.5729e-01, -3.3453e-01,  4.2605e-02,  3.4771e-01,
         2.2894e-01,  8.9872e-02,  2.3474e-01,  3.7149e-01, -2.5347e-01,
         1.2892e-01, -3.7038e-01,  6.8718e-02, -2.8476e-01,  3.6732e-01,
        -1.4778e-01,  3.5754e-01,  5.1770e-02, -1.2465e-01,  1.4894e-02,
        -5.7676e-03, -4.7557e-01, -5.1856e-01,  1.0399e-01,  8.1136e-02,
         5.3078e-01, -4.8298e-01,  2.6217e-01,  1.1708e-01, -2.2655e-02,
         3.2414e-01,  3.2335e-01, -3.2487e-01, -3.6452e-01,  4.9279e-01,
         2.7096e-01, -2.5982e-01, -2.0117e-01, -2.4121e-01, -1.1426e-01,
        -7.0331e-02, -3.8127e-01, -3.8204e-01, -1.4516e-01, -3.1634e-01,
         2.4916e-01,  2.1327e-01, -2.6620e-01, -1.7288e-01,  2.1561e-02,
         2.7197e-01, -1.3148e-01,  8.5358e-02, -2.5469e-01,  1.9918e-01,
        -8.2631e-02, -3.2942e-01,  4.1063e-01, -2.2063e-01,  1.4496e-01,
         1.9767e-01, -1.2407e-01, -1.5407e-01, -5.4703e-01,  7.7805e-02,
        -4.6431e-01, -1.4431e-01,  9.7252e-02,  6.0515e-01, -5.5042e-01,
        -2.1915e-01, -2.3620e-01,  6.5295e-02,  8.8855e-01,  1.2050e-01,
        -2.8610e-01,  1.6722e-01,  4.6341e-01,  9.4008e-03, -9.6328e-02,
        -3.3636e-01,  4.1791e-01,  1.4727e-01,  4.5041e-01, -4.3296e-01,
        -3.3456e-02,  6.8088e-02, -1.9341e-01, -2.2595e-01,  1.5026e-01,
        -1.9233e-01, -2.3071e-02, -1.5759e-01,  2.4411e-01, -1.4933e-01,
        -7.8673e-02, -5.1893e-01, -1.8274e-01, -3.6652e-01,  3.9821e-01,
        -1.1308e-03,  1.1077e-01,  1.1272e-01,  1.4247e-01, -2.5590e-01,
        -2.6423e-01, -3.1982e-01,  5.4112e-01,  4.2725e-01,  4.2929e-02,
        -1.4493e-01,  4.0068e-01, -1.5884e-01, -4.5532e-01, -3.6434e-01,
        -2.6256e-01, -1.9314e-01, -3.0976e-01,  2.1947e-01,  1.2527e+00,
         4.6440e-02,  5.4746e-02, -4.3317e-02,  3.7618e-01, -3.7077e-01,
        -2.0447e-01, -1.2383e-01,  2.0692e-01,  1.3606e-01,  1.9516e-01,
         2.2275e-01,  1.7480e-01,  3.2938e-01, -5.1290e-02, -7.7177e-01,
        -2.7293e-01,  3.2536e-01, -1.2914e-01,  8.3979e-02,  3.6308e-02,
        -3.2365e-02, -2.9454e-01,  3.3357e-01, -1.0323e-01,  2.0189e-01,
         2.5102e-02,  4.2577e-01, -3.1884e-01,  4.0641e-01,  8.1616e-03,
         3.8972e-01, -1.5253e-01,  6.5812e-01, -3.0501e-01,  1.1704e-01,
        -3.4881e-01,  1.1053e-02, -3.0909e-01, -7.7253e-02,  3.6673e-01,
        -7.6565e-02,  9.1087e-03, -1.7329e-01,  2.6309e-01,  8.9734e-02,
        -4.5330e-01,  1.5570e-02, -4.8282e-01,  4.0811e-02, -3.2425e-02,
        -3.0552e-01,  9.6641e-02, -1.8799e-02,  1.7777e-02,  7.3005e-01,
         1.4098e-01,  1.0050e-01, -1.0706e-01, -1.7136e-01,  2.0919e-01,
         2.9493e-01, -8.6930e-01, -8.4521e-02, -4.6610e-01,  4.6881e-01,
         2.8357e-01,  2.9123e-01,  5.6292e-01, -7.6437e-01,  9.9124e-02,
        -1.5738e-01,  4.0906e-01,  1.5995e-01, -3.8363e-01,  1.4078e-02,
         2.1916e-01,  2.0725e-01, -1.0849e-01, -4.3832e-01, -4.3885e-01,
        -2.4597e-02,  3.0685e-01, -4.8883e-01, -4.4606e-02, -8.3244e-01,
        -3.0004e-01,  3.6187e-01, -1.8884e-01,  1.1498e-01, -2.2350e-01,
         1.8464e-01, -5.0595e-02,  3.7592e-01, -1.6456e-01,  9.8069e-02,
         2.6785e-01,  2.1655e-01,  2.4135e-01, -4.7972e-01,  1.9272e-01,
         1.7350e-01, -4.1612e-01], device='cuda:3', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1730, -0.2277, -0.1308,  ..., -0.0064,  0.0714,  0.0089],
        [-0.3097, -0.3952,  0.1457,  ...,  0.1606, -0.3123,  0.2392],
        [-0.3884,  0.1280, -0.2651,  ..., -0.3274,  0.0380,  0.5866],
        ...,
        [-0.1138,  0.1471, -0.2607,  ..., -0.1059,  0.1240, -0.2748],
        [ 0.1022, -0.0441,  0.4888,  ...,  0.2420, -0.3309, -0.0214],
        [ 0.3723, -0.2546, -0.5360,  ...,  0.0103,  0.2929, -0.4245]],
       device='cuda:3', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 1.4179e-01,  3.2210e-01, -5.6643e-01,  2.7856e-01,  4.0780e-01,
        -5.2465e-01, -4.4860e-01,  6.3692e-01,  1.2261e-01,  3.3203e-01,
         1.1851e-01,  3.5166e-01,  4.1000e-01, -6.7416e-02,  5.7536e-01,
         9.8060e-02,  4.7483e-02, -9.5358e-02,  5.6268e-02,  4.9899e-02,
         2.9323e-01,  5.6371e-01,  2.5355e-01, -7.4552e-02, -3.9356e-01,
         1.9312e-01, -4.2989e-03, -7.0626e-02, -1.7859e-01,  4.3795e-01,
        -3.8399e-01,  5.2805e-02, -1.2964e-01,  5.6573e-02, -1.4092e-01,
         1.6990e-01,  6.7457e-02, -3.0747e-01,  4.2022e-01,  3.0776e-01,
        -3.1603e-02,  1.7747e-01,  1.3542e-01, -2.8546e-01,  6.4476e-01,
         2.1349e-02, -7.8576e-01,  4.7399e-02, -1.9146e-02,  8.6159e-04,
         4.6737e-01,  3.0309e-01, -8.5541e-02, -2.5931e-02, -1.7506e-01,
        -3.3630e-01,  1.6496e-01, -2.1160e-01,  8.8874e-02,  9.7483e-02,
         1.7767e-01,  8.5455e-01,  3.7553e-01,  1.7289e-01,  3.1729e-01,
        -1.1959e-01,  6.3505e-01, -3.6701e-01, -3.2456e-01,  4.0400e-01,
         3.2255e-01, -6.0352e-01,  2.1656e-02, -3.1416e-01, -3.4468e-01,
        -3.2838e-01,  1.2244e-01,  2.3571e-01,  5.9361e-01,  2.3576e-01,
         2.9457e-01,  2.5872e-01,  5.8149e-01, -4.4340e-01, -5.3872e-01,
        -2.0802e-01,  4.6460e-01, -2.5942e-01, -1.6369e-01, -3.9992e-01,
        -4.5108e-01,  1.5733e-01,  1.2554e-03,  1.2605e-01,  1.7727e-01,
        -1.0899e-01,  4.6134e-01, -1.1725e-01, -1.8230e-01, -3.0823e-01,
         1.0486e-02,  6.9209e-04, -1.7649e-01,  2.0212e-01, -1.1128e-01,
        -2.9200e-01, -4.3355e-01, -2.6220e-02, -3.9349e-01,  7.9136e-02,
         1.3552e-01, -5.1725e-01, -1.9342e-01, -9.7791e-02,  7.8979e-03,
        -3.1055e-01, -3.4341e-01, -1.3593e-01,  1.9896e-01, -1.7927e-01,
        -5.4761e-01, -2.2198e-01,  4.2610e-01,  8.2436e-02, -2.0173e-01,
         2.4040e-02, -1.8343e-01, -2.1978e-01,  3.5884e-01, -7.7458e-01,
         2.3324e-01,  1.3607e-01, -3.9449e-01,  4.1063e-01, -5.3288e-01,
         1.1797e-01, -3.6661e-01,  5.9781e-01, -2.7411e-01,  8.7778e-02,
         1.4884e-01, -3.0281e-01, -6.3868e-01, -1.1967e-01,  2.4771e-01,
         5.4744e-01, -1.4941e-01,  2.3260e-02,  3.9616e-04, -4.2770e-01,
        -4.0471e-01, -2.0044e-01,  3.2872e-01,  4.6651e-02,  2.4371e-01,
         1.9984e-01,  1.5637e-01, -8.4627e-02,  8.0435e-02,  1.6755e-01,
         2.4172e-01, -5.1212e-01,  1.8761e-01, -2.4661e-01,  1.5017e-01,
        -4.1426e-01,  3.1818e-01, -2.4743e-02, -6.2680e-02,  1.5470e-01,
         1.4934e-01,  2.2034e-01,  1.2142e-01,  2.1281e-01,  5.2308e-01,
         1.7412e-01,  2.7777e-01,  1.9550e-01,  5.3673e-01,  1.1650e-01,
        -1.3916e-01, -6.9720e-01,  6.8880e-01,  2.5903e-01, -1.0851e-01,
         1.5277e-01,  3.2483e-01, -2.9587e-02, -2.5160e-01, -8.1416e-02,
         7.9530e-02, -2.6971e-01,  7.0683e-01, -9.2652e-02, -2.7599e-01,
        -2.8310e-01, -3.0244e-01,  2.5042e-01, -2.0662e-01, -1.3467e-01,
        -3.8732e-01, -3.3861e-01, -2.8108e-01, -3.0866e-01, -6.7359e-02,
         3.5845e-01, -1.1492e-01, -2.6174e-01,  7.9780e-01,  6.6983e-02,
        -3.5975e-01,  4.4830e-01,  2.3262e-01, -3.8860e-01,  3.2931e-01,
        -5.0148e-01,  3.9907e-01,  6.3266e-01,  1.0864e-01,  3.5626e-02,
         4.6151e-01, -4.1601e-01,  2.0126e-01,  2.8901e-01, -2.9050e-01,
         1.5636e-01,  1.5314e-01, -2.6690e-01,  2.0631e-01, -1.2344e-01,
         5.0111e-02, -2.8826e-01, -1.3203e-01, -1.0933e-01, -1.2740e-01,
        -1.6363e-01, -3.2539e-02, -4.1435e-02,  2.0555e-01,  2.7631e-01,
        -1.3128e-01,  1.9418e-01,  2.1531e-01,  3.0464e-01,  1.2553e-01,
        -4.3558e-03,  3.5148e-03,  4.5660e-02, -2.6346e-01, -1.2613e-01,
        -1.6006e-01,  2.5396e-01,  2.3544e-03,  2.7709e-01,  2.7365e-01,
         1.5975e-01, -2.0429e-01,  2.6308e-01,  1.1232e-01, -1.7520e-01,
        -2.9343e-02,  1.9986e-01,  3.7351e-01,  1.8321e-03,  2.8144e-01,
         1.6961e-01,  1.1166e-01, -4.7342e-01,  2.5215e-01,  1.2465e-01,
        -1.7574e-01, -2.3487e-01, -3.2847e-02, -3.3511e-01, -1.6397e-01,
         9.2866e-02,  6.8842e-01, -1.2720e-01,  3.8358e-03, -1.8727e-01,
         1.1597e-01,  1.0454e-01, -2.6154e-01,  2.1201e-01, -4.5223e-01,
         2.0397e-01, -6.3676e-01, -1.0293e-01, -2.7385e-01, -3.9719e-01,
         1.6923e-01,  8.8046e-02,  1.1130e-01, -3.8459e-01, -3.7879e-01,
         5.1172e-01,  4.1529e-01, -3.8174e-01, -1.2938e-01,  3.2678e-01,
         4.4141e-01,  5.7247e-01,  5.7335e-01,  2.2937e-01,  1.7478e-01,
         3.0146e-01,  2.0555e-01, -8.7688e-01,  2.6807e-01,  3.9275e-01,
         3.8590e-02, -3.1944e-01, -5.3004e-02,  1.4634e-02,  1.4402e-01,
        -2.0849e-01,  1.3237e-01,  1.5445e-01, -2.2459e-02,  2.4120e-01,
         1.2028e-01, -1.7279e-01,  4.2801e-01, -7.6152e-01, -2.1103e-01,
         6.2899e-01, -1.6014e-01,  5.1742e-02,  4.7674e-01,  5.3797e-02,
        -1.6155e-01,  1.0281e-01, -5.7464e-01,  2.1592e-01, -1.2658e-01,
         4.8350e-02,  3.5443e-02,  1.1428e-01, -4.5882e-01, -9.1370e-02,
        -8.0386e-02, -5.6888e-02,  7.8916e-02,  1.6889e-02,  9.4458e-02,
        -1.0846e-02,  1.4602e-01,  2.9974e-01, -1.2817e-01,  1.4236e-01,
        -5.1285e-01, -4.5173e-01,  3.7114e-01, -6.4035e-02,  3.9342e-01,
         3.0148e-01, -8.0483e-02, -7.5345e-02, -3.7438e-01, -1.1560e-01,
        -7.6414e-01,  1.1941e-01,  3.2946e-01,  1.6418e-01,  2.5149e-01,
         2.8158e-01, -3.1064e-01, -2.1117e-01, -3.6134e-01, -1.7946e-01,
        -3.8067e-01, -1.8442e-02,  5.4178e-01,  1.0826e-01, -4.2489e-01,
         9.8786e-02, -1.9886e-01, -1.9944e-01, -2.6189e-01,  3.2915e-01,
         2.7132e-01, -2.7107e-01,  1.9721e-01, -3.4363e-02, -3.4736e-01,
         7.6883e-01,  7.6609e-01,  1.3300e-01,  1.0317e-01, -9.9651e-02,
        -1.5886e-01,  3.3379e-01,  1.1817e-01,  6.1088e-01, -9.8981e-02,
         3.2033e-02, -3.2494e-01,  6.3448e-01, -1.8965e-02, -1.0558e-01,
         1.8887e-01,  2.4920e-02, -4.7867e-01,  1.6645e-03,  1.6404e-01,
        -2.1174e-01,  5.4358e-01, -1.9561e-01, -6.2061e-01, -5.3127e-01,
         1.8306e-01,  5.1041e-01,  2.6210e-01,  4.7968e-01,  4.0350e-01,
         1.7007e-01, -6.5120e-02,  3.2445e-02, -2.4497e-01,  3.8961e-02,
         3.3875e-01, -2.3760e-01,  4.1712e-01,  1.6046e-01, -2.4134e-01,
         1.4197e-01, -1.9834e-01,  4.0619e-01,  2.6777e-01,  2.4601e-01,
         9.6592e-02,  2.2297e-02,  4.6209e-01,  3.8926e-01,  1.6503e-01,
         1.6308e-01, -2.7902e-01, -1.7507e-01, -1.0870e-01,  4.1737e-02,
         1.2515e-01, -1.6289e-01,  2.8835e-01, -5.0282e-01,  2.9335e-01,
         5.7595e-01, -2.0366e-01,  4.6764e-02, -2.6312e-01,  4.7355e-01,
        -3.3565e-01,  2.4745e-01, -8.0281e-02,  3.2135e-02, -6.0093e-01,
         4.9367e-01, -3.1144e-01,  2.6010e-01,  6.3402e-02, -1.1066e-02,
        -2.8310e-01, -5.9138e-01,  1.9894e-02, -5.6878e-01,  4.3710e-01,
        -1.8511e-01,  1.4252e-01, -4.7774e-01, -1.0077e-01, -2.5265e-01,
         1.0201e-01, -2.4135e-01, -3.3036e-01,  4.7532e-02,  1.9244e-01,
        -1.9187e-01,  3.5796e-02,  2.8442e-01, -2.9295e-01,  2.3329e-02,
         2.8451e-01, -1.1799e-01,  2.6453e-01,  1.4019e-01, -7.1528e-02,
         1.0160e+00,  2.1072e-01,  1.9396e-01, -1.2166e-01, -6.4238e-02,
        -1.6593e-01,  2.4005e-01, -2.9599e-01, -1.6909e-01,  9.1479e-03,
        -5.0724e-02, -3.1212e-01,  4.8560e-01,  3.9701e-01,  1.2635e-01,
        -5.3069e-01, -8.0266e-02,  3.7003e-01, -3.5217e-01,  8.7458e-02,
         5.4936e-01, -4.9864e-01,  1.0496e-01,  1.4389e-01, -7.2095e-01,
         1.7354e-01, -1.0492e-01], device='cuda:3', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 0.7570,  0.0071,  0.1389,  0.6667,  0.1712,  0.3561,  0.6226, -0.3891,
          0.5041, -0.1839, -0.3202, -0.1969,  0.1615,  0.0672,  0.6710, -0.3528,
         -0.3141, -0.2882, -0.8871,  0.1790, -0.3063,  0.4413, -1.0605, -0.2951,
         -0.3255, -0.0646,  0.2149, -0.5881, -0.2235, -0.0753,  0.5737, -0.5544,
         -0.0257, -0.1990, -0.4254, -0.1341,  0.0489, -0.3904, -0.2764, -0.3566,
          0.1979,  0.6079, -0.1551, -0.4622,  0.5872, -0.5354, -0.3222, -0.0805,
         -0.3098, -0.0448,  0.7059,  0.6188,  0.2833,  0.2381, -0.2853, -0.1509,
         -0.4050, -0.2043, -0.2229, -0.4423,  0.1936, -0.2756, -0.6272, -0.0563,
          0.0129,  0.0949,  0.0776,  0.0665,  0.0221, -0.1633, -0.2864,  0.3297,
          0.3653,  0.2385, -0.1601,  0.0213,  0.0414,  0.4344, -0.1426,  0.3209,
         -0.1330, -0.0715,  0.1667, -0.1738, -0.5722, -0.5953, -0.0185, -0.6191,
          0.4194,  0.3419,  0.0859,  0.2053,  0.2619, -0.2133, -0.4736,  0.0040,
         -0.3127, -0.3731, -0.2702,  0.1135,  0.2918,  0.2723, -0.2393, -0.0356,
          0.3593, -0.0179, -0.0283,  0.2146,  0.2183,  0.4363, -0.0966,  0.5011,
          0.4738, -0.2301,  0.3377,  0.5182,  0.2462, -0.2835, -0.2046, -0.0658,
          0.4032,  0.2224, -0.2755,  0.8335,  0.2519, -0.2333,  0.0282,  0.0173,
         -0.2267,  0.3252, -0.0499,  0.0456, -0.2739,  0.3119, -0.1075, -0.3342,
          0.2032, -0.3366, -0.2031,  0.1704, -0.1464, -0.3186,  0.1348, -0.2662,
         -0.4975,  0.1851,  0.2307,  0.0125, -0.1342,  0.5029, -0.3441, -0.0833,
         -0.5997,  0.1109,  0.1963,  0.0987, -0.6184,  0.0578,  0.1665,  0.3195,
          0.2226,  0.3615, -0.7867, -0.3353, -0.0744,  0.2562, -0.1247,  0.2649,
         -0.3539, -0.0216,  0.0795,  0.0297, -0.0140,  0.0654,  0.0853, -0.0177,
          0.1761, -0.1112,  0.1291, -0.4507, -0.3451, -0.2394,  0.4435,  0.4105,
         -0.4136, -0.0931, -0.5896,  0.0572, -0.3367, -0.3010,  0.0908, -0.1063,
         -0.5687,  0.1835, -0.1890, -0.0747,  0.2271,  0.1515, -0.0937,  0.0105,
         -0.5979,  0.5121,  0.2226,  0.2394, -0.1796, -0.5596, -0.1476, -0.3604,
          0.0613, -0.6463, -0.2884,  0.0498,  0.0476, -0.1238,  0.0033, -0.1810,
          0.2991, -0.7201, -0.0066,  0.6760,  0.0838, -0.2200,  0.5599, -0.4012,
          0.0079,  0.4337,  0.0768,  0.5233,  0.0051, -0.1023, -0.2747,  0.1599,
         -0.0071,  0.3136,  0.1999, -0.0581,  0.1474, -0.4202,  0.3128, -0.0721,
          0.1800, -0.6434,  0.2795,  0.0963,  0.1675, -0.2505,  0.5946,  0.0881,
         -0.3420, -0.4583,  0.2318, -0.1727,  0.3149, -0.2287, -0.0798, -0.1796,
          0.2559,  0.4442, -0.1899, -0.2371, -0.5489, -0.4815,  0.0435, -0.3169,
          0.4664,  0.0213,  0.0719, -0.0919,  0.0049,  0.0717, -0.2498, -0.1512,
          0.0769, -0.3439,  0.0776, -0.3771,  0.3978, -0.3865,  0.0013,  0.3145,
         -0.2653,  0.3279, -0.5856, -0.6437, -0.3371,  0.2609, -0.0104, -0.0156,
          0.1098,  0.2341,  0.1565,  0.2076,  0.0886, -0.3759, -0.1986,  0.2679,
          0.2184,  0.0187, -0.2609, -0.0485,  0.2321,  0.2120, -0.3796, -0.1598,
         -0.2731, -0.2402, -0.7589, -0.0833,  0.2278,  0.1961,  0.3413, -0.0987,
          0.4173, -0.0623, -0.5167, -0.5240, -0.3195, -0.2283,  0.4674, -0.0255,
         -0.5547,  0.4798, -0.4382, -0.0420, -0.1176,  0.0931, -0.0501,  0.4722,
          0.6665, -0.0861,  0.6480, -0.0445, -0.3785, -0.0294,  0.3172, -0.1142,
          0.0714, -0.1573,  0.0901, -0.3999,  0.2734,  0.3352,  0.1221,  0.1154,
         -0.2944,  0.4117, -0.2558, -0.0271, -0.0529, -0.1143, -0.3698,  0.3019,
         -0.3937, -0.0406,  0.4142, -0.0227,  0.1544,  0.1405,  0.6259, -0.3281,
         -0.1419,  0.6239,  0.0778, -0.2243, -0.2499,  0.0725, -0.4267, -0.4464,
          0.5441, -0.0022, -0.2501,  0.1621, -0.2522,  0.2555, -0.1617, -0.1284,
         -0.0315,  0.1421, -0.1258, -0.0346,  0.0374,  0.2211, -0.6120,  0.1248,
         -0.3892, -0.1977,  0.4000,  0.2652,  0.1389, -0.1081,  0.6099,  0.2467,
         -0.0990,  0.2355,  0.4636, -0.2268,  0.4451, -0.2121, -0.5002, -0.0408,
          0.2536,  0.3768, -0.6709,  0.2756, -0.2226, -0.4895, -0.5475,  0.0174,
          0.4232,  0.3590,  0.6767,  0.3510, -0.0845, -0.0845, -0.4329, -0.2155,
         -0.0831,  0.1408,  0.3465,  0.3161,  0.3256,  0.1817, -0.4837,  0.2695,
         -0.1407, -0.2695,  0.2053,  0.8556, -0.1175, -0.1025,  0.1077,  0.5011,
         -0.3269,  0.2269, -0.1758, -0.2570,  0.0990, -0.4238, -0.2143, -0.4339,
          0.5493,  0.0367, -0.0246,  0.2124,  0.3997, -0.2968, -0.6228,  0.4566,
          0.2689, -0.6811,  0.2121,  0.0886,  0.2624,  0.4336,  0.1599,  0.4324,
          0.5957, -0.5103,  0.8678, -0.3761, -0.3902, -0.2045, -0.1148,  0.1110,
         -0.3288, -0.1500,  0.7500, -0.5143,  0.4097, -0.2637,  0.2661, -0.3321,
          0.1945,  0.0906, -0.0694,  0.2230, -0.0518,  0.0263,  0.7312,  0.0204,
         -0.3837, -0.0236, -0.0973,  0.4279, -0.1067, -0.4604,  0.3151,  0.2388,
          0.0919,  0.3406, -0.2363, -0.3606,  0.5200,  0.1322, -0.2743,  0.1592,
         -0.4984,  0.0436, -0.2330, -0.0258,  0.0537, -0.4252,  0.3755, -0.1834,
         -0.2058, -0.5455,  0.1601, -0.1842, -0.2745, -0.3696, -0.3067,  0.3493]],
       device='cuda:3', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.1671], device='cuda:3', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:3', grad_fn=<AddBackward0>)
likelihood_guide.likelihood._scale.loc torch.Size([]) Parameter containing:
tensor(0.4195, device='cuda:3', requires_grad=True)
likelihood_guide.likelihood._scale.scale torch.Size([]) tensor(0.0100, device='cuda:3', grad_fn=<AddBackward0>)
Using device: cuda:3
===== Training profile tensin-3x512-sl - 1 =====
[0:00:01.819008] epoch: 0 | elbo: 17845690.02 | train_rmse: 66.8933 | val_rmse: 69.1641 | val_ll: -29.8382
[0:01:38.629994] epoch: 50 | elbo: 2895823.0774999997 | train_rmse: 16.4099 | val_rmse: 19.6974 | val_ll: -5.8718
[0:03:14.868856] epoch: 100 | elbo: 2562294.025 | train_rmse: 11.3803 | val_rmse: 16.5247 | val_ll: -5.2803
[0:04:55.096973] epoch: 150 | elbo: 2421307.3 | train_rmse: 8.6121 | val_rmse: 15.0265 | val_ll: -5.0136
[0:06:36.231837] epoch: 200 | elbo: 2332245.2375 | train_rmse: 6.8096 | val_rmse: 14.1118 | val_ll: -4.8389
[0:08:18.135991] epoch: 250 | elbo: 2273134.32 | train_rmse: 5.479 | val_rmse: 13.4599 | val_ll: -4.755
[0:09:59.912263] epoch: 300 | elbo: 2223463.9525000006 | train_rmse: 4.4366 | val_rmse: 12.9331 | val_ll: -4.6675
[0:11:41.772163] epoch: 350 | elbo: 2182409.965 | train_rmse: 3.6511 | val_rmse: 12.4715 | val_ll: -4.6054
[0:13:22.215510] epoch: 400 | elbo: 2145802.1550000003 | train_rmse: 3.0224 | val_rmse: 12.0141 | val_ll: -4.5211
[0:15:02.909700] epoch: 450 | elbo: 2111701.875 | train_rmse: 2.5681 | val_rmse: 11.6 | val_ll: -4.4604
[0:16:43.527382] epoch: 500 | elbo: 2079581.1512499996 | train_rmse: 2.2597 | val_rmse: 11.1743 | val_ll: -4.3497
[0:18:25.464922] epoch: 550 | elbo: 2048555.36875 | train_rmse: 1.9374 | val_rmse: 10.6917 | val_ll: -4.2374
[0:20:05.632967] epoch: 600 | elbo: 2017058.725 | train_rmse: 1.839 | val_rmse: 10.2247 | val_ll: -4.1525
[0:21:45.187709] epoch: 650 | elbo: 1986456.7312500004 | train_rmse: 1.6436 | val_rmse: 9.6748 | val_ll: -4.0198
[0:23:25.427227] epoch: 700 | elbo: 1954796.4300000002 | train_rmse: 1.5788 | val_rmse: 9.165 | val_ll: -3.9016
[0:25:05.906642] epoch: 750 | elbo: 1924161.99875 | train_rmse: 1.4831 | val_rmse: 8.6358 | val_ll: -3.8016
[0:26:48.767205] epoch: 800 | elbo: 1893408.58125 | train_rmse: 1.441 | val_rmse: 8.0886 | val_ll: -3.7004
[0:28:31.347174] epoch: 850 | elbo: 1862660.51625 | train_rmse: 1.4305 | val_rmse: 7.5671 | val_ll: -3.5798
[0:30:14.348024] epoch: 900 | elbo: 1832004.7775000003 | train_rmse: 1.415 | val_rmse: 7.062 | val_ll: -3.4645
[0:31:51.888188] epoch: 950 | elbo: 1801509.20625 | train_rmse: 1.3579 | val_rmse: 6.5469 | val_ll: -3.3725
[0:33:28.957670] epoch: 1000 | elbo: 1771450.1124999996 | train_rmse: 1.3725 | val_rmse: 6.0897 | val_ll: -3.25
[0:35:06.603558] epoch: 1050 | elbo: 1741814.7600000002 | train_rmse: 1.3298 | val_rmse: 5.6551 | val_ll: -3.158
[0:36:49.256852] epoch: 1100 | elbo: 1712321.31875 | train_rmse: 1.3266 | val_rmse: 5.2672 | val_ll: -3.0586
[0:38:33.159450] epoch: 1150 | elbo: 1683325.4137499998 | train_rmse: 1.3061 | val_rmse: 4.9325 | val_ll: -2.9695
[0:40:11.740792] epoch: 1200 | elbo: 1654478.2550000001 | train_rmse: 1.3221 | val_rmse: 4.6422 | val_ll: -2.893
[0:41:49.695430] epoch: 1250 | elbo: 1626068.9337499999 | train_rmse: 1.3035 | val_rmse: 4.3623 | val_ll: -2.8108
[0:43:25.640272] epoch: 1300 | elbo: 1597910.225 | train_rmse: 1.2838 | val_rmse: 4.1427 | val_ll: -2.7488
[0:45:00.938553] epoch: 1350 | elbo: 1569979.00125 | train_rmse: 1.2689 | val_rmse: 3.9457 | val_ll: -2.7007
[0:46:36.909900] epoch: 1400 | elbo: 1542254.88125 | train_rmse: 1.2421 | val_rmse: 3.7644 | val_ll: -2.6564
[0:48:13.558714] epoch: 1450 | elbo: 1514598.81375 | train_rmse: 1.224 | val_rmse: 3.628 | val_ll: -2.6214
[0:49:54.046511] epoch: 1500 | elbo: 1487051.9562500003 | train_rmse: 1.2171 | val_rmse: 3.4826 | val_ll: -2.5874
[0:51:37.660644] epoch: 1550 | elbo: 1459578.2824999997 | train_rmse: 1.1983 | val_rmse: 3.3561 | val_ll: -2.5603
[0:53:21.121731] epoch: 1600 | elbo: 1432118.3887500004 | train_rmse: 1.1781 | val_rmse: 3.2404 | val_ll: -2.5321
[0:55:03.346119] epoch: 1650 | elbo: 1404728.3174999997 | train_rmse: 1.1633 | val_rmse: 3.1347 | val_ll: -2.5086
[0:56:43.773674] epoch: 1700 | elbo: 1377256.74375 | train_rmse: 1.1458 | val_rmse: 3.0369 | val_ll: -2.4838
[0:58:24.690789] epoch: 1750 | elbo: 1349848.3862500002 | train_rmse: 1.1428 | val_rmse: 2.931 | val_ll: -2.4528
[1:00:04.147127] epoch: 1800 | elbo: 1322418.73875 | train_rmse: 1.1062 | val_rmse: 2.8289 | val_ll: -2.4218
[1:01:41.310859] epoch: 1850 | elbo: 1295022.205 | train_rmse: 1.1046 | val_rmse: 2.7247 | val_ll: -2.386
[1:03:17.953028] epoch: 1900 | elbo: 1267560.6987500002 | train_rmse: 1.0853 | val_rmse: 2.6341 | val_ll: -2.3559
[1:04:53.618982] epoch: 1950 | elbo: 1240243.55125 | train_rmse: 1.0782 | val_rmse: 2.5452 | val_ll: -2.3299
[1:06:30.149233] epoch: 2000 | elbo: 1212923.0037500001 | train_rmse: 1.0644 | val_rmse: 2.45 | val_ll: -2.2945
[1:08:05.900600] epoch: 2050 | elbo: 1185598.71875 | train_rmse: 1.0561 | val_rmse: 2.3681 | val_ll: -2.269
[1:09:42.310905] epoch: 2100 | elbo: 1158297.7437500001 | train_rmse: 1.0412 | val_rmse: 2.2791 | val_ll: -2.2446
[1:11:18.930976] epoch: 2150 | elbo: 1131202.3625 | train_rmse: 1.036 | val_rmse: 2.2046 | val_ll: -2.2197
[1:12:56.531357] epoch: 2200 | elbo: 1104089.87375 | train_rmse: 1.0449 | val_rmse: 2.127 | val_ll: -2.1994
[1:14:37.784807] epoch: 2250 | elbo: 1076984.3212500003 | train_rmse: 1.0409 | val_rmse: 2.0526 | val_ll: -2.1798
[1:16:18.328496] epoch: 2300 | elbo: 1049998.57 | train_rmse: 1.0472 | val_rmse: 1.9871 | val_ll: -2.1619
[1:17:55.902914] epoch: 2350 | elbo: 1023088.8225 | train_rmse: 1.0376 | val_rmse: 1.9146 | val_ll: -2.1447
[1:19:36.329866] epoch: 2400 | elbo: 996212.711875 | train_rmse: 1.0368 | val_rmse: 1.8572 | val_ll: -2.1297
[1:21:21.572607] epoch: 2450 | elbo: 969277.3006249999 | train_rmse: 1.0368 | val_rmse: 1.7964 | val_ll: -2.1175
[1:23:04.733514] epoch: 2500 | elbo: 942607.125 | train_rmse: 1.055 | val_rmse: 1.7402 | val_ll: -2.1056
[1:24:45.526641] epoch: 2550 | elbo: 915891.85625 | train_rmse: 1.0453 | val_rmse: 1.6864 | val_ll: -2.0923
[1:26:26.505375] epoch: 2600 | elbo: 889296.553125 | train_rmse: 1.0523 | val_rmse: 1.6371 | val_ll: -2.0817
[1:28:07.676943] epoch: 2650 | elbo: 862781.6050000001 | train_rmse: 1.0508 | val_rmse: 1.5768 | val_ll: -2.0659
[1:29:48.078151] epoch: 2700 | elbo: 836365.670625 | train_rmse: 1.0485 | val_rmse: 1.5347 | val_ll: -2.0577
[1:31:29.523302] epoch: 2750 | elbo: 809916.805625 | train_rmse: 1.0593 | val_rmse: 1.4969 | val_ll: -2.0495
[1:33:10.823006] epoch: 2800 | elbo: 783723.4025 | train_rmse: 1.0526 | val_rmse: 1.4611 | val_ll: -2.0405
[1:34:50.801820] epoch: 2850 | elbo: 757523.1387500002 | train_rmse: 1.0575 | val_rmse: 1.429 | val_ll: -2.0336
[1:36:32.631189] epoch: 2900 | elbo: 731504.6168750001 | train_rmse: 1.0603 | val_rmse: 1.4011 | val_ll: -2.0263
[1:38:14.630915] epoch: 2950 | elbo: 705587.2293750001 | train_rmse: 1.0669 | val_rmse: 1.3664 | val_ll: -2.0204
[1:39:56.112083] epoch: 3000 | elbo: 679792.5093749999 | train_rmse: 1.052 | val_rmse: 1.3413 | val_ll: -2.0169
[1:41:37.515528] epoch: 3050 | elbo: 654179.6018749999 | train_rmse: 1.0692 | val_rmse: 1.3341 | val_ll: -2.015
[1:43:19.414867] epoch: 3100 | elbo: 628713.17 | train_rmse: 1.0737 | val_rmse: 1.3108 | val_ll: -2.012
[1:45:00.476691] epoch: 3150 | elbo: 603415.88625 | train_rmse: 1.0791 | val_rmse: 1.2998 | val_ll: -2.0113
[1:46:41.770043] epoch: 3200 | elbo: 578273.4706250001 | train_rmse: 1.0892 | val_rmse: 1.2918 | val_ll: -2.013
[1:48:21.863639] epoch: 3250 | elbo: 553395.0943750001 | train_rmse: 1.0918 | val_rmse: 1.2829 | val_ll: -2.0133
[1:50:03.324033] epoch: 3300 | elbo: 528624.621875 | train_rmse: 1.1123 | val_rmse: 1.286 | val_ll: -2.0209
[1:51:43.514618] epoch: 3350 | elbo: 504199.140625 | train_rmse: 1.1185 | val_rmse: 1.279 | val_ll: -2.0244
[1:53:25.531856] epoch: 3400 | elbo: 479933.109375 | train_rmse: 1.1362 | val_rmse: 1.291 | val_ll: -2.0282
[1:55:08.613738] epoch: 3450 | elbo: 455948.3078125 | train_rmse: 1.1496 | val_rmse: 1.2936 | val_ll: -2.035
[1:56:49.772411] epoch: 3500 | elbo: 432289.42124999996 | train_rmse: 1.1623 | val_rmse: 1.2872 | val_ll: -2.0423
[1:58:30.748049] epoch: 3550 | elbo: 408878.6575 | train_rmse: 1.193 | val_rmse: 1.3162 | val_ll: -2.0536
[2:00:11.694188] epoch: 3600 | elbo: 385856.86156249995 | train_rmse: 1.2147 | val_rmse: 1.3154 | val_ll: -2.0617
[2:01:53.094077] epoch: 3650 | elbo: 363237.4290625 | train_rmse: 1.2344 | val_rmse: 1.3402 | val_ll: -2.0743
[2:03:34.064181] epoch: 3700 | elbo: 340970.9965625 | train_rmse: 1.2574 | val_rmse: 1.3529 | val_ll: -2.087
[2:05:14.605534] epoch: 3750 | elbo: 319130.20531249995 | train_rmse: 1.2928 | val_rmse: 1.3836 | val_ll: -2.1027
[2:06:54.459687] epoch: 3800 | elbo: 297758.20718749997 | train_rmse: 1.3104 | val_rmse: 1.406 | val_ll: -2.1171
[2:08:34.818812] epoch: 3850 | elbo: 276879.04250000004 | train_rmse: 1.3428 | val_rmse: 1.4283 | val_ll: -2.1335
[2:10:14.954895] epoch: 3900 | elbo: 256684.77921875002 | train_rmse: 1.3732 | val_rmse: 1.4513 | val_ll: -2.1465
[2:11:56.598377] epoch: 3950 | elbo: 237022.54296874994 | train_rmse: 1.4231 | val_rmse: 1.4987 | val_ll: -2.1658
[2:13:38.303163] epoch: 4000 | elbo: 218012.51968750003 | train_rmse: 1.4492 | val_rmse: 1.5219 | val_ll: -2.185
[2:15:19.086487] epoch: 4050 | elbo: 199781.46390625002 | train_rmse: 1.4869 | val_rmse: 1.5601 | val_ll: -2.2006
[2:16:58.822306] epoch: 4100 | elbo: 182249.31046875002 | train_rmse: 1.5408 | val_rmse: 1.5897 | val_ll: -2.2171
[2:18:36.245940] epoch: 4150 | elbo: 165618.92703125 | train_rmse: 1.5762 | val_rmse: 1.6217 | val_ll: -2.237
[2:20:12.518537] epoch: 4200 | elbo: 149861.42140624998 | train_rmse: 1.6224 | val_rmse: 1.6734 | val_ll: -2.2566
[2:21:48.669229] epoch: 4250 | elbo: 135127.231875 | train_rmse: 1.6729 | val_rmse: 1.7189 | val_ll: -2.2737
[2:23:23.918833] epoch: 4300 | elbo: 121401.43421875002 | train_rmse: 1.7123 | val_rmse: 1.7632 | val_ll: -2.2934
[2:25:03.536301] epoch: 4350 | elbo: 108828.97015624998 | train_rmse: 1.7698 | val_rmse: 1.8116 | val_ll: -2.3155
[2:26:43.840277] epoch: 4400 | elbo: 97339.23828124999 | train_rmse: 1.842 | val_rmse: 1.8703 | val_ll: -2.3319
[2:28:22.086583] epoch: 4450 | elbo: 87128.731171875 | train_rmse: 1.8854 | val_rmse: 1.9136 | val_ll: -2.3519
[2:30:03.839156] epoch: 4500 | elbo: 78161.65648437501 | train_rmse: 1.9337 | val_rmse: 1.9631 | val_ll: -2.3709
[2:31:46.698168] epoch: 4550 | elbo: 70464.14 | train_rmse: 1.9958 | val_rmse: 2.0187 | val_ll: -2.3877
[2:33:28.176861] epoch: 4600 | elbo: 64043.726328125005 | train_rmse: 2.0473 | val_rmse: 2.084 | val_ll: -2.406
[2:35:09.961698] epoch: 4650 | elbo: 58873.50074218749 | train_rmse: 2.0933 | val_rmse: 2.117 | val_ll: -2.4215
[2:36:51.428524] epoch: 4700 | elbo: 54853.502070312505 | train_rmse: 2.1715 | val_rmse: 2.1832 | val_ll: -2.4406
[2:38:32.595213] epoch: 4750 | elbo: 51831.842734375 | train_rmse: 2.2115 | val_rmse: 2.2362 | val_ll: -2.4508
[2:40:13.772025] epoch: 4800 | elbo: 49634.579765625 | train_rmse: 2.2653 | val_rmse: 2.2795 | val_ll: -2.4594
[2:41:55.900743] epoch: 4850 | elbo: 48042.842578125004 | train_rmse: 2.2923 | val_rmse: 2.3157 | val_ll: -2.4686
[2:43:38.344388] epoch: 4900 | elbo: 46835.4912890625 | train_rmse: 2.3413 | val_rmse: 2.3527 | val_ll: -2.479
[2:45:20.060832] epoch: 4950 | elbo: 45877.808984375 | train_rmse: 2.3469 | val_rmse: 2.3586 | val_ll: -2.4774
[2:47:01.687649] epoch: 5000 | elbo: 45153.4716796875 | train_rmse: 2.3656 | val_rmse: 2.3711 | val_ll: -2.482
[2:48:43.695740] epoch: 5050 | elbo: 44530.2147265625 | train_rmse: 2.3746 | val_rmse: 2.3804 | val_ll: -2.4839
[2:50:25.955858] epoch: 5100 | elbo: 43936.83261718751 | train_rmse: 2.3657 | val_rmse: 2.3834 | val_ll: -2.4832
[2:52:08.622796] epoch: 5150 | elbo: 43549.45671875 | train_rmse: 2.3639 | val_rmse: 2.3708 | val_ll: -2.4803
[2:53:49.967274] epoch: 5200 | elbo: 43125.4085546875 | train_rmse: 2.3589 | val_rmse: 2.3704 | val_ll: -2.4781
[2:55:30.115540] epoch: 5250 | elbo: 42787.09453125 | train_rmse: 2.354 | val_rmse: 2.3664 | val_ll: -2.4752
[2:57:10.597864] epoch: 5300 | elbo: 42514.585390625005 | train_rmse: 2.3401 | val_rmse: 2.3497 | val_ll: -2.4685
[2:58:51.790187] epoch: 5350 | elbo: 42169.00949218749 | train_rmse: 2.3207 | val_rmse: 2.3283 | val_ll: -2.4616
[3:00:34.117414] epoch: 5400 | elbo: 41920.7223828125 | train_rmse: 2.2884 | val_rmse: 2.3005 | val_ll: -2.4536
[3:02:15.874372] epoch: 5450 | elbo: 41706.565703125 | train_rmse: 2.2745 | val_rmse: 2.2999 | val_ll: -2.4467
[3:03:55.532517] epoch: 5500 | elbo: 41514.888984375 | train_rmse: 2.2481 | val_rmse: 2.2626 | val_ll: -2.4384
[3:05:32.960848] epoch: 5550 | elbo: 41331.63710937499 | train_rmse: 2.204 | val_rmse: 2.2248 | val_ll: -2.4299
[3:07:08.471367] epoch: 5600 | elbo: 41139.862734375 | train_rmse: 2.1784 | val_rmse: 2.2037 | val_ll: -2.4184
[3:08:46.535079] epoch: 5650 | elbo: 41012.612031250006 | train_rmse: 2.142 | val_rmse: 2.1569 | val_ll: -2.408
[3:10:22.654028] epoch: 5700 | elbo: 40861.197031250005 | train_rmse: 2.0923 | val_rmse: 2.1288 | val_ll: -2.3978
[3:11:59.919932] epoch: 5750 | elbo: 40671.20410156251 | train_rmse: 2.07 | val_rmse: 2.098 | val_ll: -2.3858
[3:13:36.443181] epoch: 5800 | elbo: 40531.5458984375 | train_rmse: 2.028 | val_rmse: 2.0461 | val_ll: -2.3677
[3:15:13.002553] epoch: 5850 | elbo: 40412.910585937505 | train_rmse: 1.9844 | val_rmse: 2.023 | val_ll: -2.3568
[3:16:52.028339] epoch: 5900 | elbo: 40312.9376953125 | train_rmse: 1.9508 | val_rmse: 1.9809 | val_ll: -2.3447
[3:18:33.522801] epoch: 5950 | elbo: 40151.9133203125 | train_rmse: 1.8995 | val_rmse: 1.9398 | val_ll: -2.3285
[3:20:15.436935] epoch: 6000 | elbo: 40072.156171875 | train_rmse: 1.8763 | val_rmse: 1.9077 | val_ll: -2.3117
[3:21:57.564782] epoch: 6050 | elbo: 39955.2265234375 | train_rmse: 1.8211 | val_rmse: 1.8589 | val_ll: -2.2988
[3:23:38.176276] epoch: 6100 | elbo: 39790.394062499996 | train_rmse: 1.7703 | val_rmse: 1.7992 | val_ll: -2.2802
[3:25:19.210881] epoch: 6150 | elbo: 39657.7753125 | train_rmse: 1.7421 | val_rmse: 1.7707 | val_ll: -2.2649
[3:27:00.991473] epoch: 6200 | elbo: 39649.636484375005 | train_rmse: 1.7044 | val_rmse: 1.743 | val_ll: -2.2523
[3:28:42.848425] epoch: 6250 | elbo: 39470.932382812505 | train_rmse: 1.6541 | val_rmse: 1.6896 | val_ll: -2.2303
[3:30:26.431624] epoch: 6300 | elbo: 39312.781992187505 | train_rmse: 1.6193 | val_rmse: 1.6691 | val_ll: -2.2171
[3:32:07.808235] epoch: 6350 | elbo: 39238.1698046875 | train_rmse: 1.5681 | val_rmse: 1.6154 | val_ll: -2.1994
[3:33:48.208681] epoch: 6400 | elbo: 39159.8113671875 | train_rmse: 1.5413 | val_rmse: 1.5865 | val_ll: -2.1815
[3:35:28.818001] epoch: 6450 | elbo: 39027.698710937504 | train_rmse: 1.4983 | val_rmse: 1.5572 | val_ll: -2.1618
[3:37:10.275123] epoch: 6500 | elbo: 38887.893789062495 | train_rmse: 1.4601 | val_rmse: 1.5117 | val_ll: -2.1428
[3:38:49.296567] epoch: 6550 | elbo: 38839.7335546875 | train_rmse: 1.4374 | val_rmse: 1.4952 | val_ll: -2.1255
[3:40:26.140440] epoch: 6600 | elbo: 38678.4211328125 | train_rmse: 1.3903 | val_rmse: 1.4495 | val_ll: -2.1053
[3:42:04.523003] epoch: 6650 | elbo: 38555.679531249996 | train_rmse: 1.3541 | val_rmse: 1.4177 | val_ll: -2.0862
[3:43:41.873693] epoch: 6700 | elbo: 38482.787890625004 | train_rmse: 1.3195 | val_rmse: 1.3866 | val_ll: -2.069
[3:45:17.999756] epoch: 6750 | elbo: 38360.4930078125 | train_rmse: 1.287 | val_rmse: 1.3467 | val_ll: -2.0459
[3:46:53.286125] epoch: 6800 | elbo: 38269.12136718751 | train_rmse: 1.2492 | val_rmse: 1.3232 | val_ll: -2.0257
[3:48:31.972415] epoch: 6850 | elbo: 38114.939453125 | train_rmse: 1.2194 | val_rmse: 1.2897 | val_ll: -2.0063
[3:50:07.628012] epoch: 6900 | elbo: 38016.00140625 | train_rmse: 1.1773 | val_rmse: 1.2467 | val_ll: -1.9829
[3:51:43.506831] epoch: 6950 | elbo: 37825.561171875 | train_rmse: 1.1597 | val_rmse: 1.2258 | val_ll: -1.9619
[3:53:19.895675] epoch: 7000 | elbo: 37785.23832031251 | train_rmse: 1.1162 | val_rmse: 1.1857 | val_ll: -1.9409
[3:54:57.025522] epoch: 7050 | elbo: 37690.8941796875 | train_rmse: 1.0993 | val_rmse: 1.1731 | val_ll: -1.9181
[3:56:33.671956] epoch: 7100 | elbo: 37528.1252734375 | train_rmse: 1.0532 | val_rmse: 1.1301 | val_ll: -1.895
[3:58:10.012829] epoch: 7150 | elbo: 37410.616601562506 | train_rmse: 1.0235 | val_rmse: 1.1062 | val_ll: -1.8743
[3:59:49.172263] epoch: 7200 | elbo: 37166.27421875 | train_rmse: 0.9989 | val_rmse: 1.0819 | val_ll: -1.8506
[4:01:30.428165] epoch: 7250 | elbo: 37098.2876953125 | train_rmse: 0.966 | val_rmse: 1.0469 | val_ll: -1.8277
[4:03:11.726161] epoch: 7300 | elbo: 36960.82359374999 | train_rmse: 0.9446 | val_rmse: 1.0307 | val_ll: -1.8032
[4:04:53.361153] epoch: 7350 | elbo: 36872.645742187495 | train_rmse: 0.9133 | val_rmse: 0.9973 | val_ll: -1.7781
[4:06:35.157588] epoch: 7400 | elbo: 36765.6840234375 | train_rmse: 0.9044 | val_rmse: 0.987 | val_ll: -1.7573
[4:08:16.123966] epoch: 7450 | elbo: 36594.55671875 | train_rmse: 0.8795 | val_rmse: 0.9602 | val_ll: -1.7342
[4:09:56.215853] epoch: 7500 | elbo: 36511.9915234375 | train_rmse: 0.85 | val_rmse: 0.9385 | val_ll: -1.7085
[4:11:36.739373] epoch: 7550 | elbo: 36414.545468749995 | train_rmse: 0.8317 | val_rmse: 0.9137 | val_ll: -1.6839
[4:13:18.947083] epoch: 7600 | elbo: 36250.1934375 | train_rmse: 0.8115 | val_rmse: 0.9024 | val_ll: -1.6646
[4:15:00.232812] epoch: 7650 | elbo: 36058.141054687505 | train_rmse: 0.7895 | val_rmse: 0.8802 | val_ll: -1.638
[4:16:40.894765] epoch: 7700 | elbo: 35931.589374999996 | train_rmse: 0.7659 | val_rmse: 0.8592 | val_ll: -1.6145
[4:18:22.863012] epoch: 7750 | elbo: 35882.3539453125 | train_rmse: 0.7526 | val_rmse: 0.8446 | val_ll: -1.5933
[4:20:03.978539] epoch: 7800 | elbo: 35675.724296875 | train_rmse: 0.743 | val_rmse: 0.8297 | val_ll: -1.5696
[4:21:44.175478] epoch: 7850 | elbo: 35606.920468749995 | train_rmse: 0.7286 | val_rmse: 0.8161 | val_ll: -1.547
[4:23:22.167351] epoch: 7900 | elbo: 35466.850312500006 | train_rmse: 0.7128 | val_rmse: 0.8051 | val_ll: -1.5234
[4:24:59.970738] epoch: 7950 | elbo: 35357.91015624999 | train_rmse: 0.6984 | val_rmse: 0.7895 | val_ll: -1.5026
[4:26:38.637597] epoch: 8000 | elbo: 35244.219765625 | train_rmse: 0.6802 | val_rmse: 0.7753 | val_ll: -1.4786
[4:28:16.746516] epoch: 8050 | elbo: 35085.86289062501 | train_rmse: 0.6668 | val_rmse: 0.7596 | val_ll: -1.4562
[4:29:54.485472] epoch: 8100 | elbo: 35032.50812500001 | train_rmse: 0.6536 | val_rmse: 0.7439 | val_ll: -1.4372
[4:31:35.954189] epoch: 8150 | elbo: 34881.901640625 | train_rmse: 0.6446 | val_rmse: 0.7348 | val_ll: -1.4138
[4:33:18.403230] epoch: 8200 | elbo: 34800.672343750004 | train_rmse: 0.6313 | val_rmse: 0.7196 | val_ll: -1.3912
[4:34:59.300232] epoch: 8250 | elbo: 34646.1761328125 | train_rmse: 0.6211 | val_rmse: 0.7093 | val_ll: -1.3716
[4:36:39.157619] epoch: 8300 | elbo: 34509.54171875 | train_rmse: 0.6109 | val_rmse: 0.6997 | val_ll: -1.3513
[4:38:19.809403] epoch: 8350 | elbo: 34370.7318359375 | train_rmse: 0.5971 | val_rmse: 0.6862 | val_ll: -1.3264
[4:40:01.305912] epoch: 8400 | elbo: 34317.17804687501 | train_rmse: 0.5907 | val_rmse: 0.6769 | val_ll: -1.307
[4:41:43.615361] epoch: 8450 | elbo: 34201.56921875001 | train_rmse: 0.5861 | val_rmse: 0.6686 | val_ll: -1.2874
[4:43:25.760681] epoch: 8500 | elbo: 34124.031015625005 | train_rmse: 0.5726 | val_rmse: 0.6574 | val_ll: -1.2669
[4:45:06.180953] epoch: 8550 | elbo: 33949.0391015625 | train_rmse: 0.5602 | val_rmse: 0.6453 | val_ll: -1.2469
[4:46:48.758057] epoch: 8600 | elbo: 33930.4939453125 | train_rmse: 0.5562 | val_rmse: 0.6364 | val_ll: -1.2256
[4:48:30.075125] epoch: 8650 | elbo: 33702.876132812504 | train_rmse: 0.5488 | val_rmse: 0.6294 | val_ll: -1.208
[4:50:12.779547] epoch: 8700 | elbo: 33690.554140625 | train_rmse: 0.5411 | val_rmse: 0.6205 | val_ll: -1.1883
[4:51:53.149235] epoch: 8750 | elbo: 33678.70468750001 | train_rmse: 0.5336 | val_rmse: 0.6101 | val_ll: -1.1681
[4:53:33.935644] epoch: 8800 | elbo: 33444.827656249996 | train_rmse: 0.5257 | val_rmse: 0.6041 | val_ll: -1.1549
[4:55:13.164067] epoch: 8850 | elbo: 33373.596640625 | train_rmse: 0.5207 | val_rmse: 0.5939 | val_ll: -1.133
[4:56:50.495428] epoch: 8900 | elbo: 33254.08251953125 | train_rmse: 0.514 | val_rmse: 0.5867 | val_ll: -1.1185
[4:58:27.754199] epoch: 8950 | elbo: 33206.38384765625 | train_rmse: 0.5103 | val_rmse: 0.5817 | val_ll: -1.1035
[5:00:04.061385] epoch: 9000 | elbo: 33069.83234375 | train_rmse: 0.5047 | val_rmse: 0.5769 | val_ll: -1.0871
[5:01:40.649310] epoch: 9050 | elbo: 32995.5338671875 | train_rmse: 0.496 | val_rmse: 0.5674 | val_ll: -1.0708
[5:03:17.478154] epoch: 9100 | elbo: 32869.86669921875 | train_rmse: 0.491 | val_rmse: 0.5637 | val_ll: -1.0562
[5:04:53.330818] epoch: 9150 | elbo: 32846.424335937496 | train_rmse: 0.4897 | val_rmse: 0.5583 | val_ll: -1.0416
[5:06:29.350397] epoch: 9200 | elbo: 32730.54865234375 | train_rmse: 0.4818 | val_rmse: 0.5533 | val_ll: -1.0282
[5:08:06.400539] epoch: 9250 | elbo: 32583.468417968754 | train_rmse: 0.4778 | val_rmse: 0.5472 | val_ll: -1.0135
[5:09:43.222395] epoch: 9300 | elbo: 32571.835781249996 | train_rmse: 0.4759 | val_rmse: 0.5408 | val_ll: -1.0053
[5:11:19.270228] epoch: 9350 | elbo: 32484.3825 | train_rmse: 0.4699 | val_rmse: 0.5362 | val_ll: -0.9888
[5:12:57.634706] epoch: 9400 | elbo: 32403.225039062498 | train_rmse: 0.4664 | val_rmse: 0.5322 | val_ll: -0.9761
[5:14:38.140446] epoch: 9450 | elbo: 32324.673535156257 | train_rmse: 0.4652 | val_rmse: 0.5314 | val_ll: -0.9679
[5:16:18.260322] epoch: 9500 | elbo: 32257.3047265625 | train_rmse: 0.4579 | val_rmse: 0.5241 | val_ll: -0.9558
[5:17:58.918761] epoch: 9550 | elbo: 32194.621445312503 | train_rmse: 0.4535 | val_rmse: 0.5202 | val_ll: -0.9429
[5:19:39.077442] epoch: 9600 | elbo: 32102.26976562499 | train_rmse: 0.4538 | val_rmse: 0.5194 | val_ll: -0.9319
[5:21:19.337699] epoch: 9650 | elbo: 32044.25802734375 | train_rmse: 0.4489 | val_rmse: 0.5126 | val_ll: -0.9214
[5:22:58.884207] epoch: 9700 | elbo: 31957.24193359375 | train_rmse: 0.4447 | val_rmse: 0.5083 | val_ll: -0.9123
[5:24:38.089376] epoch: 9750 | elbo: 31877.741953125 | train_rmse: 0.4427 | val_rmse: 0.507 | val_ll: -0.9002
[5:26:17.915888] epoch: 9800 | elbo: 31765.947421874997 | train_rmse: 0.4398 | val_rmse: 0.5038 | val_ll: -0.8954
[5:27:57.808558] epoch: 9850 | elbo: 31768.251582031247 | train_rmse: 0.4375 | val_rmse: 0.5002 | val_ll: -0.8846
[5:29:37.625279] epoch: 9900 | elbo: 31673.004921875003 | train_rmse: 0.4339 | val_rmse: 0.4975 | val_ll: -0.8768
[5:31:17.711889] epoch: 9950 | elbo: 31618.26736328125 | train_rmse: 0.4321 | val_rmse: 0.4957 | val_ll: -0.868
Training finished in 5:32:56.928103 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-sl/checkpoint_1.pt
File Size is 4.060103416442871 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:3 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-1.0306e-02,  5.3160e-03,  1.5471e-02,  ...,  1.8613e-03,
         -3.8211e-03, -1.1142e-03],
        [-5.3630e-03,  7.8780e-03,  1.0074e-02,  ..., -6.7320e-03,
         -9.2079e-05, -4.8960e-03],
        [-8.4647e-03,  6.8163e-03,  1.0564e-02,  ..., -3.1092e-03,
         -4.2245e-03, -2.2756e-03],
        ...,
        [-5.4530e-03,  6.2594e-03,  6.9437e-03,  ..., -3.3436e-03,
         -1.8323e-03,  2.0363e-03],
        [-1.1538e-02,  3.9923e-03,  7.8472e-03,  ..., -6.2507e-03,
         -4.5579e-03, -5.0813e-03],
        [-1.0096e-02,  1.5274e-04,  1.2775e-02,  ..., -6.1735e-03,
         -5.6897e-03, -6.5760e-03]], device='cuda:3', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.2583, 0.2487, 0.2492,  ..., 0.2498, 0.2482, 0.2571],
        [0.2585, 0.2498, 0.2499,  ..., 0.2501, 0.2476, 0.2570],
        [0.2582, 0.2500, 0.2495,  ..., 0.2505, 0.2476, 0.2568],
        ...,
        [0.2577, 0.2490, 0.2489,  ..., 0.2479, 0.2476, 0.2565],
        [0.2557, 0.2492, 0.2487,  ..., 0.2484, 0.2482, 0.2565],
        [0.2566, 0.2497, 0.2487,  ..., 0.2480, 0.2474, 0.2556]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-2.9349, -2.9302, -2.9319, -2.9323, -2.9323, -2.8963, -2.8856, -1.7175,
        -2.9314, -2.9360, -2.9349, -2.9305, -2.9343, -2.9344, -2.9355, -2.9346,
        -0.3418, -2.9323, -2.9363, -1.6655, -2.9167, -1.4895, -2.9313, -2.9379,
        -2.9354, -2.9359, -2.9313, -2.9374, -2.9104, -2.9208, -2.9257, -2.9349,
        -2.9343, -2.5061, -1.9640, -2.9332, -2.9325, -2.9326, -1.5843, -2.9338,
        -2.9338, -2.9292, -2.9308, -2.9385, -2.9362, -2.9358, -2.9330, -2.9220,
        -2.9366, -2.7950, -2.9286,  1.9953, -2.9301, -2.4173, -2.9329, -0.6680,
        -2.9347, -2.9321, -2.1303, -2.0573, -2.7314, -2.9336, -2.5458, -2.9345,
        -2.9364, -2.1628, -2.9377, -2.9196, -0.7791, -2.9267, -2.9298, -2.9351,
        -2.9361,  4.9323, -2.9315, -2.9353, -2.9285, -2.9356, -2.9257, -2.9308,
        -2.9355, -2.9333, -1.4107, -1.2809, -2.9348, -2.9284, -2.9355, -2.9328,
        -2.9347, -2.9379, -2.9382, -2.9342, -1.5361, -2.9334, -2.9350, -2.9351,
        -2.9338, -2.9273, -2.9368, -2.9344, -1.9171, -2.9355, -2.3535, -2.9286,
        -1.9578, -2.7287, -2.9347, -2.9302, -2.9352, -2.9353, -2.9342, -2.9357,
        -2.9381, -2.9374, -2.8533, -0.6660, -2.6762, -0.7406, -1.5367, -1.0673,
        -2.9325, -2.9300, -2.5741, -2.9340, -2.9356, -2.9177, -2.9347, -2.9346,
        -1.6468,  2.4631, -2.9355, -2.9085, -2.9312, -2.9363, -2.8828, -2.9372,
        -2.9289, -2.9317, -2.2431, -2.9168, -2.9353,  4.0711, -2.9329, -2.9346,
        -2.9329, -2.9346, -2.9323, -2.4625, -2.9315, -2.9323, -2.9362, -2.9354,
        -2.9341, -2.9357, -1.9521, -2.9344, -2.9342, -2.9328, -2.9372, -1.5822,
        -2.9347, -2.9327, -2.9377, -2.9352, -2.9334, -2.9390, -2.9337, -2.9324,
        -2.5489, -2.9373, -2.9327, -2.7689, -2.9340, -2.9346, -2.9329,  3.6027,
        -2.9353, -2.9372, -2.9328, -1.7435, -2.9329, -2.9360, -2.9360, -2.9373,
        -2.9380, -2.9357, -2.9305, -2.9350, -2.9336, -2.9328, -2.4645,  1.1641,
        -2.9342, -2.7108, -2.9383, -2.9329, -2.9364, -2.7288, -2.9297, -2.9369,
        -2.9329, -2.9312, -2.9235, -2.5611, -2.9343, -2.3822, -2.9353, -2.8997,
        -1.8965, -2.9361,  3.0410, -2.8344, -2.9366, -2.9370, -2.9372, -2.9322,
        -2.9352, -2.9108, -2.9346, -2.5220, -2.9342, -2.9339, -2.9330, -2.9329,
         3.2924, -2.9332,  1.3139, -2.9332, -2.9356, -2.9379, -2.9341, -2.0066,
        -2.9358, -2.9351, -2.9370, -0.6745, -2.8820, -0.7668,  0.6201, -2.9366,
        -2.0359, -1.7655, -2.9319, -1.5938, -1.8014, -2.9324, -2.9327,  2.5252,
        -2.8309, -0.7701, -2.5083, -2.7265, -2.9327, -2.3801, -3.1438, -2.9008,
        -2.9318, -2.9038, -2.9329, -2.9276, -2.9170, -2.6540, -2.9319, -1.7235,
        -2.9338, -2.9311, -1.9790, -2.9340, -2.9363, -2.9372, -2.9405, -2.9366,
        -2.9378, -0.3453, -2.9340, -2.9329, -2.9364, -2.9373, -1.7432, -2.9328,
        -2.7019, -1.9220, -1.5878, -2.9355, -2.9354, -2.9335, -1.7001, -2.3746,
        -2.9357, -2.7879, -2.9324, -2.3262, -2.9204, -2.9338, -2.9383, -2.9377,
        -2.5097, -2.9369, -2.9350, -2.3101, -2.9353, -2.8757, -2.9357, -2.3611,
        -2.9359, -2.9352, -0.4166, -2.8093, -2.9349, -2.9360, -2.9888, -2.7246,
        -2.9364, -2.9348, -2.9382, -2.8478, -0.4961, -2.2369, -2.9331, -2.9356,
        -1.5799, -2.9354, -1.5404, -2.9359, -2.9342, -2.9329, -2.8886, -2.9362,
        -2.9327, -2.9362, -2.9316, -2.9295, -2.3015, -2.8520, -2.9321, -1.7998,
        -2.0240,  3.2463, -2.9354, -2.9373, -2.9359, -2.9334, -2.9404, -2.9330,
        -2.9377, -1.6278, -2.9360, -2.9152, -2.9237, -2.9312, -2.9325, -2.9362,
        -2.5547, -2.9377, -2.8498, -2.9343,  3.3925, -2.5819,  2.1784, -2.6353,
        -2.9296, -2.7803, -2.9374, -1.7575, -2.8429,  1.4754, -2.9382, -2.9299,
        -2.9353, -2.9364, -2.9283, -2.9352, -2.9299, -2.9358, -2.9368, -2.9332,
        -2.7434, -2.9358, -2.9317, -2.9371, -2.9324, -2.9336, -2.9361, -2.9370,
        -2.9327, -2.9339, -2.9297, -2.9364, -2.5652, -2.9327, -2.9378, -2.9334,
        -2.9349,  0.3991, -2.9308, -1.3265, -2.9351, -0.6466, -2.9357, -2.9333,
        -2.9343, -2.6573, -2.9301,  0.3811, -2.9359, -2.9160, -2.9364, -2.9374,
        -2.9320, -2.9326, -2.9362, -2.9320, -2.9348, -2.9320, -2.9312,  0.1938,
        -2.9346, -1.4830, -0.7177, -0.7137, -2.8946, -2.9376, -2.9356, -2.9370,
        -2.9352, -2.9346, -2.9350, -3.0035, -2.9356, -2.9269, -2.9326, -2.9356,
        -2.5089, -2.6986, -2.9360, -2.9315, -2.5728, -2.9360, -2.4413, -2.9359,
        -2.8067, -2.9362, -2.9325, -2.9126, -2.9373, -2.9310, -2.9360, -2.7744,
        -2.9319, -2.9337, -2.9365, -2.9297, -2.9360, -2.9336, -2.6339, -2.7727,
        -0.6639, -2.9331, -2.9346, -2.9355, -2.9354, -1.6512, -2.9305, -2.9300,
        -2.9314, -2.9366, -2.9349, -2.9337, -2.9341, -2.9340, -2.9349, -2.9339,
        -2.2001, -2.5052, -2.9363, -1.3434, -1.4841, -2.9366, -2.9331, -2.5227,
        -2.9397, -2.9327, -2.9351, -2.1303,  1.1770, -1.8274,  4.4372, -2.5205,
        -2.9356, -2.9341, -2.9336, -2.9324, -2.9339, -2.9325, -2.9333, -2.9357,
        -2.9318, -1.8525, -2.9324, -2.9315, -2.9338, -2.9333, -1.6452, -2.9331,
        -2.9378, -2.9310, -2.4502, -2.9340, -0.8913, -2.9359, -2.9246, -2.9386],
       device='cuda:3', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.3215, 0.3222, 0.3221, 0.3220, 0.3218, 0.3181, 0.3185, 0.0182, 0.3222,
        0.3216, 0.3213, 0.3209, 0.3218, 0.3220, 0.3213, 0.3215, 0.0540, 0.3228,
        0.3212, 0.0078, 0.3202, 0.0242, 0.3225, 0.3207, 0.3221, 0.3217, 0.3224,
        0.3213, 0.3193, 0.3205, 0.3211, 0.3216, 0.3216, 0.2849, 0.0230, 0.3220,
        0.3216, 0.3222, 0.0307, 0.3215, 0.3221, 0.3203, 0.3218, 0.3210, 0.3212,
        0.3214, 0.3223, 0.3207, 0.3214, 0.3097, 0.3217, 0.0406, 0.3223, 0.0085,
        0.3220, 0.0109, 0.3214, 0.3221, 0.2509, 0.0164, 0.0196, 0.3214, 0.2899,
        0.3215, 0.3214, 0.2530, 0.3207, 0.3199, 0.0110, 0.3220, 0.3219, 0.3210,
        0.3217, 0.0230, 0.3215, 0.3211, 0.3216, 0.3216, 0.3214, 0.3223, 0.3210,
        0.3219, 0.0151, 0.1688, 0.3216, 0.3233, 0.3212, 0.3224, 0.3216, 0.3213,
        0.3207, 0.3218, 0.0222, 0.3216, 0.3215, 0.3216, 0.3222, 0.3222, 0.3212,
        0.3213, 0.0208, 0.3216, 0.0322, 0.3223, 0.0253, 0.0182, 0.3218, 0.3226,
        0.3224, 0.3213, 0.3214, 0.3215, 0.3201, 0.3208, 0.3159, 0.0135, 0.0172,
        0.0107, 0.0160, 0.0098, 0.3224, 0.3222, 0.0255, 0.3215, 0.3212, 0.3197,
        0.3218, 0.3222, 0.0215, 0.0128, 0.3214, 0.3205, 0.3223, 0.3214, 0.0176,
        0.3210, 0.3200, 0.3228, 0.0532, 0.3193, 0.3214, 0.0212, 0.3220, 0.3213,
        0.3219, 0.3217, 0.3220, 0.2816, 0.3226, 0.3220, 0.3211, 0.3217, 0.3220,
        0.3213, 0.0172, 0.3216, 0.3216, 0.3220, 0.3212, 0.0270, 0.3216, 0.3207,
        0.3202, 0.3216, 0.3218, 0.3203, 0.3215, 0.3217, 0.0285, 0.3212, 0.3216,
        0.3078, 0.3218, 0.3218, 0.3214, 0.0247, 0.3217, 0.3211, 0.3229, 0.0295,
        0.3219, 0.3219, 0.3214, 0.3210, 0.3211, 0.3216, 0.3223, 0.3221, 0.3219,
        0.3221, 0.0369, 0.1024, 0.3222, 0.0201, 0.3208, 0.3220, 0.3210, 0.3040,
        0.3229, 0.3211, 0.3222, 0.3204, 0.3208, 0.2889, 0.3221, 0.2330, 0.3215,
        0.3183, 0.0505, 0.3217, 0.0376, 0.0167, 0.3209, 0.3210, 0.3199, 0.3229,
        0.3215, 0.3185, 0.3219, 0.2848, 0.3219, 0.3221, 0.3219, 0.3220, 0.0282,
        0.3224, 0.0078, 0.3220, 0.3224, 0.3202, 0.3218, 0.0174, 0.3222, 0.3212,
        0.3211, 0.0102, 0.3172, 0.0159, 0.0097, 0.3209, 0.0481, 0.0334, 0.3229,
        0.0135, 0.0294, 0.3225, 0.3215, 0.0050, 0.3129, 0.0111, 0.1198, 0.3054,
        0.3215, 0.0459, 0.0153, 0.3187, 0.3224, 0.3199, 0.3220, 0.3213, 0.3206,
        0.2976, 0.3223, 0.0265, 0.3222, 0.3225, 0.0603, 0.3222, 0.3214, 0.3212,
        0.3202, 0.3211, 0.3215, 0.0121, 0.3221, 0.3227, 0.3213, 0.3208, 0.0562,
        0.3216, 0.0182, 0.1068, 0.0179, 0.3217, 0.3212, 0.3216, 0.0320, 0.2739,
        0.3213, 0.0241, 0.3226, 0.0503, 0.3214, 0.3212, 0.3207, 0.3212, 0.0620,
        0.3208, 0.3216, 0.0573, 0.3213, 0.3164, 0.3209, 0.0653, 0.3213, 0.3216,
        0.0105, 0.3108, 0.3216, 0.3215, 0.0155, 0.3055, 0.3212, 0.3211, 0.3211,
        0.0185, 0.1099, 0.0392, 0.3224, 0.3209, 0.0152, 0.3213, 0.0141, 0.3218,
        0.3209, 0.3220, 0.3188, 0.3217, 0.3219, 0.3215, 0.3224, 0.3227, 0.0121,
        0.3151, 0.3226, 0.0175, 0.0379, 0.0345, 0.3215, 0.3214, 0.3217, 0.3208,
        0.3211, 0.3223, 0.3209, 0.0203, 0.3213, 0.3200, 0.3205, 0.3231, 0.3217,
        0.3214, 0.2880, 0.3210, 0.3153, 0.3223, 0.0349, 0.0305, 0.0405, 0.2970,
        0.3220, 0.0209, 0.3212, 0.0279, 0.0193, 0.0062, 0.3211, 0.3231, 0.3210,
        0.3213, 0.3223, 0.3214, 0.3233, 0.3212, 0.3212, 0.3219, 0.0230, 0.3213,
        0.3225, 0.3208, 0.3219, 0.3214, 0.3216, 0.3214, 0.3219, 0.3215, 0.3226,
        0.3215, 0.0325, 0.3220, 0.3209, 0.3213, 0.3211, 0.0101, 0.3216, 0.0122,
        0.3216, 0.0096, 0.3209, 0.3222, 0.3218, 0.0305, 0.3224, 0.0100, 0.3214,
        0.3193, 0.3210, 0.3208, 0.3227, 0.3223, 0.3216, 0.3225, 0.3217, 0.3232,
        0.3230, 0.0143, 0.3217, 0.0119, 0.0100, 0.0106, 0.0164, 0.3199, 0.3213,
        0.3211, 0.3213, 0.3220, 0.3219, 0.0158, 0.3219, 0.3216, 0.3221, 0.3213,
        0.2853, 0.3018, 0.3211, 0.3233, 0.0393, 0.3214, 0.0093, 0.3215, 0.3103,
        0.3209, 0.3218, 0.3214, 0.3211, 0.3227, 0.3213, 0.0203, 0.3223, 0.3221,
        0.3209, 0.3213, 0.3217, 0.3214, 0.0308, 0.0192, 0.0103, 0.3211, 0.3218,
        0.3216, 0.3215, 0.0162, 0.3219, 0.3223, 0.3219, 0.3209, 0.3219, 0.3217,
        0.3214, 0.3220, 0.3212, 0.3213, 0.1935, 0.1085, 0.3208, 0.0153, 0.0317,
        0.3214, 0.3222, 0.0446, 0.3198, 0.3221, 0.3214, 0.2502, 0.0063, 0.0468,
        0.0206, 0.1172, 0.3221, 0.3214, 0.3224, 0.3223, 0.3217, 0.3218, 0.3223,
        0.3217, 0.3222, 0.0792, 0.3223, 0.3225, 0.3217, 0.3224, 0.0369, 0.3200,
        0.3208, 0.3208, 0.0308, 0.3220, 0.1201, 0.3212, 0.3219, 0.3205],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-1.6327e-03, -1.8920e-04, -3.7892e-05,  ..., -7.5783e-04,
         -1.5643e-04,  4.8804e-06],
        [ 2.5190e-03, -1.2445e-03, -1.4813e-03,  ..., -2.6561e-03,
         -4.5149e-03,  1.9796e-03],
        [-8.2150e-04, -4.1565e-06, -7.1006e-04,  ..., -2.2719e-04,
         -4.3392e-04,  9.1250e-06],
        ...,
        [ 6.4150e-05, -8.8380e-04, -4.3836e-05,  ...,  3.3507e-05,
          6.9358e-04,  2.5336e-05],
        [ 1.5339e-06, -8.6151e-05, -7.9165e-05,  ..., -1.9123e-03,
         -1.5966e-04, -8.4035e-04],
        [ 1.4516e-02,  1.5155e-02,  5.5687e-03,  ...,  9.6242e-03,
          5.3746e-03,  1.3980e-02]], device='cuda:3', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.9998, 1.0000, 1.0000,  ..., 0.9999, 1.0001, 1.0000],
        [0.9994, 0.9994, 0.9995,  ..., 0.9988, 0.9984, 0.9970],
        [1.0000, 1.0000, 0.9999,  ..., 1.0000, 0.9999, 1.0000],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0002,  ..., 0.9998, 0.9998, 1.0001],
        [0.9765, 0.9778, 0.9769,  ..., 0.9784, 0.9752, 0.9772]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.2347, -0.3954, -0.2363, -0.2383, -0.2360, -0.2378, -0.2386, -0.2373,
        -0.2355, -0.2367, -0.2403, -0.3377, -0.2368, -0.2367, -0.2355, -0.2366,
        -0.4213, -0.2378, -0.2362, -0.2367, -0.2354, -0.2353, -0.2353, -0.2358,
        -0.2358, -0.2366, -0.2374, -0.2380, -0.2342, -0.2383, -0.2367, -0.2370,
        -0.2387, -0.2584, -0.2352, -0.2351, -0.2373, -0.2379, -0.5335, -0.2356,
        -0.2385, -0.2336, -0.2352, -0.2359, -0.2347, -0.2363, -0.2381, -0.2360,
        -0.2361, -0.2341, -0.2346, -0.2393, -0.2361, -0.2348, -0.2366, -0.3402,
        -0.2362, -0.2373, -0.2363, -0.2359, -0.2362, -0.2389, -0.2388, -0.2348,
        -0.2356, -0.2395, -0.2376, -0.2376, -0.2392, -0.2385, -0.2364, -0.2323,
        -0.2380, -0.2338, -0.2372, -0.2346, -0.2326, -0.2371, -0.2367, -0.2388,
        -0.2345, -0.2355, -0.2355, -0.2373, -0.2362, -0.2363, -0.2362, -0.2344,
        -0.2360, -0.2378, -0.2446, -0.2369, -0.2347, -0.2345, -0.2358, -0.2358,
        -0.2380, -0.2371, -0.2364, -0.2367, -0.2386, -0.2337, -0.2393, -0.2360,
        -0.2370, -0.2353, -0.2370, -0.2343, -0.2353, -0.2365, -0.2358, -0.2344,
        -0.2351, -0.2379, -0.2373, -0.2354, -0.2375, -0.2370, -0.2376, -0.2400,
        -0.2369, -0.2372, -0.2349, -0.2362, -0.2344, -0.2359, -0.2365, -0.2376,
        -0.2368, -0.2335, -0.2377, -0.2381, -0.2360, -0.2377, -0.2375, -0.2345,
        -0.2361, -0.2344, -0.2364, -0.2351, -0.2395, -0.2351, -0.2375, -0.2353,
        -0.2373, -0.2386,  0.0218, -0.2363,  0.0829, -0.2365, -0.2380, -0.2351,
        -0.2376, -0.2374, -0.2400, -0.2362, -0.2347, -0.2384, -0.4978, -0.2378,
        -0.2344, -0.2402, -0.2354, -0.2361, -0.2361, -0.2344, -0.2363, -0.2372,
         0.8867, -0.2354, -0.2376, -0.2365, -0.2364, -0.2357, -0.2355, -0.2388,
        -0.2343, -0.2392, -0.2350, -0.2355, -0.2358, -0.2364, -0.2359, -0.2352,
        -0.2361, -0.2375, -0.2386, -0.2360, -0.2410, -0.2362, -0.2377, -0.2387,
        -0.2346, -0.2350, -0.2385, -0.2362, -0.2380, -0.2378, -0.2366, -0.2354,
        -0.2384, -0.2353, -0.2370, -0.3854, -0.2390, -0.2387, -0.2361, -0.2365,
        -0.2369, -0.2427, -0.2369, -0.2382, -0.2375, -0.2354,  0.3001, -0.2365,
        -0.2406, -0.2366, -0.2374, -0.2375, -0.2367, -0.2380, -0.2352, -0.3425,
        -0.2385, -0.2405, -0.2356, -0.2348, -0.2352, -0.2375, -0.2353, -0.2467,
        -0.2353, -0.2366, -0.2380, -0.2347, -0.2353, -0.2352, -0.2375, -0.2355,
        -0.2352, -0.2374, -0.2370, -0.2361, -0.2366, -0.2342, -0.3153, -0.3952,
        -0.2356, -0.2385, -0.2357, -0.2375, -0.2691, -0.2344, -0.2374, -0.2354,
        -0.2378, -0.2355, -0.2389, -0.2374, -0.2356, -0.2359, -0.2374, -0.2377,
         0.1627, -0.2367, -0.2348,  0.8353, -0.1416, -0.2365, -0.2359,  2.8611,
        -0.2364, -0.2376, -0.2347, -0.2376, -0.2363, -0.2359, -0.2372, -0.2358,
        -0.2364, -0.2377, -0.2358, -0.2372, -0.2378, -0.2366, -0.2365, -0.2377,
        -0.2352, -0.2381, -0.2369, -0.2340, -0.2362, -0.2369, -0.2348, -0.2374,
        -0.2366, -0.2386, -0.2373, -0.2375, -0.2361, -0.2362, -0.2365, -0.2390,
        -0.2398, -0.2362, -0.2368, -0.2368, -0.2369, -0.2372, -0.2353, -0.2343,
        -0.2350, -0.2375, -0.2354, -0.2333, -0.2363, -0.2349, -0.2364, -0.2346,
        -0.2338, -0.2375, -0.2346, -0.2379, -0.2358, -0.2366, -0.2361, -0.2356,
        -0.2332, -0.2372, -0.2382, -0.2372, -0.2361, -0.2354, -0.2378, -0.2353,
        -0.2362, -0.2344, -0.2354, -0.2370, -0.2356, -0.2364, -0.2348, -0.2366,
        -0.2339, -0.2383, -0.2380, -0.2358, -0.2377, -0.2345, -0.2348, -0.2369,
        -0.2350, -0.2357, -0.2355, -0.2694, -0.2339, -0.2365, -0.2383, -0.2393,
        -0.2361, -0.2382, -0.2362, -0.2383, -0.2396, -0.2356, -0.2380, -0.2361,
        -0.2351, -0.2348, -0.2362, -0.2384, -0.2366, -0.2368, -0.2382, -0.2386,
        -0.2369, -0.4558,  0.9643, -0.2358, -0.2358, -0.2358, -0.2370, -0.2357,
        -0.2379, -0.2361, -0.2389, -0.2365, -0.2401, -0.2330, -0.2352, -0.2361,
        -0.2371, -0.2363, -0.2387, -0.2375, -0.2377, -0.2357, -0.2372, -0.2344,
        -0.2382, -0.2394, -0.2373, -0.4074, -0.5168, -0.2375, -0.2352, -0.2350,
        -0.2360, -0.2365, -0.2384, -0.2402, -0.2354, -0.2390, -0.2343, -0.2325,
        -0.2361, -0.2363, -0.2372, -0.2345, -0.2342, -0.2368, -0.2369, -0.2362,
        -0.2368,  0.5283, -0.2398, -0.2360, -0.2374, -0.2474, -0.2377, -0.2348,
        -0.2351, -0.2368, -0.2364, -0.2382, -0.2362, -0.2357, -0.2358, -0.2371,
        -0.2356, -0.2362, -0.2366, -0.2360, -0.2371, -0.2392, -0.2371,  0.8178,
        -0.2374, -0.2368, -0.2362, -0.2373, -0.2341, -0.2346, -0.2373, -0.2383,
        -0.2361, -0.2369, -0.2377, -0.2380, -0.2362, -0.2391, -0.2386, -0.2374,
        -0.2361, -0.2369, -0.4384, -0.2370, -0.2377, -0.2369, -0.2360, -0.2361,
        -0.2347, -0.2379, -0.2357, -0.2366, -0.2350, -0.2391, -0.2342, -0.2365,
        -0.2373, -0.2368, -0.2379, -0.2383, -0.2386, -0.2339, -0.2373, -0.2378,
        -0.2352, -0.2387, -0.2376, -0.2362, -0.2351, -0.2314, -0.2360, -0.2358,
        -0.2369, -0.2349, -0.2368, -0.2382, -0.2373, -0.2371, -0.2354, -0.2348,
        -0.2360, -0.2385, -0.2360, -0.2346, -0.2357, -0.2346, -0.2377, -0.6075],
       device='cuda:3', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.9688, 0.5765, 0.9673, 0.9651, 0.9677, 0.9666, 0.9658, 0.9670, 0.9671,
        0.9675, 0.9653, 0.0592, 0.9668, 0.9662, 0.9668, 0.9667, 0.1196, 0.9666,
        0.9678, 0.9655, 0.9676, 0.9678, 0.9677, 0.9667, 0.9676, 0.9653, 0.9674,
        0.9662, 0.9686, 0.9663, 0.9677, 0.9680, 0.9665, 0.9660, 0.9682, 0.9673,
        0.9660, 0.9659, 0.1307, 0.9668, 0.9666, 0.9691, 0.9677, 0.9670, 0.9674,
        0.9664, 0.9653, 0.9681, 0.9668, 0.9689, 0.9677, 0.9652, 0.9667, 0.9678,
        0.9676, 0.0586, 0.9664, 0.9660, 0.9673, 0.9676, 0.9659, 0.9642, 0.9658,
        0.9668, 0.9667, 0.9668, 0.9655, 0.9666, 0.9650, 0.9670, 0.9671, 0.9687,
        0.9671, 0.9688, 0.9664, 0.9682, 0.9693, 0.9661, 0.9668, 0.9663, 0.9673,
        0.9674, 0.9659, 0.9672, 0.9662, 0.9668, 0.9671, 0.9695, 0.9669, 0.9650,
        0.9648, 0.9664, 0.9674, 0.9677, 0.9668, 0.9677, 0.9659, 0.9674, 0.9677,
        0.9659, 0.9647, 0.9685, 0.9660, 0.9667, 0.9658, 0.9673, 0.9662, 0.9687,
        0.9676, 0.9664, 0.9674, 0.9689, 0.9671, 0.9658, 0.9665, 0.9680, 0.9665,
        0.9664, 0.9664, 0.9657, 0.9661, 0.9669, 0.9694, 0.9663, 0.9678, 0.9657,
        0.9676, 0.9647, 0.9662, 0.9687, 0.9663, 0.9656, 0.9667, 0.9669, 0.9663,
        0.9674, 0.9675, 0.9682, 0.9657, 0.9662, 0.9648, 0.9673, 0.9669, 0.9680,
        0.9668, 0.9653, 0.3821, 0.9671, 0.0796, 0.9670, 0.9658, 0.9683, 0.9653,
        0.9661, 0.9628, 0.9666, 0.9678, 0.9664, 0.7775, 0.9665, 0.9682, 0.9649,
        0.9668, 0.9666, 0.9670, 0.9677, 0.9673, 0.9669, 0.0394, 0.9678, 0.9655,
        0.9672, 0.9662, 0.9677, 0.9677, 0.9656, 0.9681, 0.9645, 0.9672, 0.9672,
        0.9671, 0.9663, 0.9671, 0.9668, 0.9675, 0.9661, 0.9658, 0.9687, 0.9636,
        0.9661, 0.9665, 0.9663, 0.9678, 0.9681, 0.9664, 0.9673, 0.9661, 0.9665,
        0.9677, 0.9672, 0.9657, 0.9664, 0.9663, 0.1003, 0.9657, 0.9661, 0.9677,
        0.9672, 0.9666, 0.9657, 0.9673, 0.9658, 0.9661, 0.9676, 0.3205, 0.9678,
        0.9653, 0.9668, 0.9668, 0.9659, 0.9673, 0.9654, 0.9664, 0.0538, 0.9656,
        0.9650, 0.9675, 0.9677, 0.9668, 0.9664, 0.9685, 0.9668, 0.9683, 0.9666,
        0.9655, 0.9690, 0.9673, 0.9673, 0.9668, 0.9673, 0.9672, 0.9667, 0.9661,
        0.9665, 0.9673, 0.9669, 0.0715, 0.9395, 0.9667, 0.9667, 0.9675, 0.9664,
        0.0523, 0.9692, 0.9668, 0.9679, 0.9659, 0.9669, 0.9666, 0.9668, 0.9665,
        0.9674, 0.9657, 0.9655, 0.1484, 0.9663, 0.9669, 0.5374, 0.0865, 0.9667,
        0.9679, 0.3942, 0.9673, 0.9658, 0.9684, 0.9665, 0.9676, 0.9676, 0.9670,
        0.9684, 0.9666, 0.9663, 0.9682, 0.9662, 0.9671, 0.9663, 0.9656, 0.9668,
        0.9674, 0.9643, 0.9666, 0.9687, 0.9678, 0.9664, 0.9686, 0.9672, 0.9677,
        0.9656, 0.9669, 0.9656, 0.9670, 0.9680, 0.9663, 0.9665, 0.9653, 0.9661,
        0.9669, 0.9659, 0.9666, 0.9669, 0.9664, 0.9683, 0.9675, 0.9656, 0.9675,
        0.9683, 0.9675, 0.9668, 0.9677, 0.9689, 0.9680, 0.9667, 0.9671, 0.9660,
        0.9683, 0.9676, 0.9667, 0.9675, 0.9689, 0.9667, 0.9661, 0.9671, 0.9668,
        0.9677, 0.9662, 0.9677, 0.9671, 0.9681, 0.9671, 0.9661, 0.9680, 0.9661,
        0.9672, 0.9667, 0.9678, 0.9660, 0.9655, 0.9675, 0.9671, 0.9685, 0.9676,
        0.9668, 0.9675, 0.9668, 0.9678, 0.9671, 0.9676, 0.9657, 0.9686, 0.9645,
        0.9670, 0.9659, 0.9683, 0.9656, 0.9649, 0.9669, 0.9666, 0.9679, 0.9677,
        0.9678, 0.9667, 0.9674, 0.9666, 0.9669, 0.9657, 0.9654, 0.9662, 0.1215,
        0.0292, 0.9670, 0.9665, 0.9668, 0.9671, 0.9670, 0.9648, 0.9682, 0.9653,
        0.9677, 0.9647, 0.9684, 0.9686, 0.9669, 0.9672, 0.9668, 0.9660, 0.9669,
        0.9656, 0.9667, 0.9651, 0.9699, 0.9654, 0.9655, 0.9667, 0.1184, 0.1054,
        0.9654, 0.9684, 0.9668, 0.9661, 0.9670, 0.9651, 0.9639, 0.9673, 0.9652,
        0.9678, 0.9693, 0.9669, 0.9663, 0.9670, 0.9667, 0.9692, 0.9675, 0.9672,
        0.9668, 0.9662, 0.0485, 0.9650, 0.9670, 0.9665, 0.9678, 0.9668, 0.9680,
        0.9673, 0.9669, 0.9662, 0.9652, 0.9665, 0.9677, 0.9670, 0.9658, 0.9681,
        0.9677, 0.9675, 0.9670, 0.9664, 0.9647, 0.9673, 0.1061, 0.9663, 0.9676,
        0.9677, 0.9662, 0.9684, 0.9676, 0.9653, 0.9658, 0.9671, 0.9664, 0.9663,
        0.9652, 0.9670, 0.9646, 0.9670, 0.9660, 0.9676, 0.9672, 0.5210, 0.9654,
        0.9661, 0.9666, 0.9677, 0.9671, 0.9689, 0.9665, 0.9675, 0.9673, 0.9676,
        0.9648, 0.9679, 0.9671, 0.9653, 0.9669, 0.9669, 0.9664, 0.9661, 0.9692,
        0.9673, 0.9668, 0.9665, 0.9648, 0.9666, 0.9666, 0.9677, 0.9708, 0.9668,
        0.9675, 0.9673, 0.9677, 0.9662, 0.9668, 0.9655, 0.9670, 0.9691, 0.9672,
        0.9663, 0.9654, 0.9678, 0.9677, 0.9659, 0.9686, 0.9662, 0.1203],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-4.2826e-06, -2.7203e-04,  1.7821e-07,  ..., -6.6689e-07,
         -3.9331e-07, -1.8504e-02],
        [ 8.5479e-44, -4.1487e-04, -2.8026e-45,  ..., -1.6816e-44,
         -6.2218e-43, -3.5105e-02],
        [-2.8026e-44, -2.8606e-03, -2.1019e-44,  ..., -1.4013e-45,
          7.5875e-07, -4.3840e-02],
        ...,
        [-1.5776e-24, -7.8098e-04, -9.8091e-45,  ..., -8.4078e-45,
         -1.4854e-43, -2.6573e-02],
        [ 1.1210e-44, -2.0802e-03, -1.3587e-04,  ..., -2.2281e-35,
          4.2039e-45, -2.2706e-02],
        [-1.7667e-24, -2.3164e-04, -1.0557e-07,  ..., -5.6758e-06,
          8.2007e-11, -1.6540e-02]], device='cuda:3', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9961],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9915],
        [1.0000, 0.9988, 1.0000,  ..., 1.0000, 1.0000, 0.9831],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9897],
        [1.0000, 0.9999, 1.0000,  ..., 1.0000, 1.0000, 0.9927],
        [1.0000, 1.0001, 1.0000,  ..., 1.0000, 1.0000, 0.9963]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.0082, -0.0129, -0.0115, -0.0217, -0.0133, -0.0153, -0.0105, -0.0075,
        -0.0104, -0.0066, -0.0131, -0.0099, -0.0109, -0.0093, -0.0109, -0.0090,
        -0.0046, -0.0085, -0.0689, -0.0123, -0.0068, -0.0120, -0.0089, -0.1612,
        -0.0107, -0.0100, -0.0091, -0.0119, -0.0099, -0.0081, -0.3475, -0.0083,
        -0.0112, -0.0123, -0.0085, -0.0097, -0.0103, -0.0094, -0.0098, -0.0067,
        -0.0121, -0.0153, -0.0105, -0.0092, -0.0126, -0.0071, -0.0068, -0.0100,
        -0.0107, -0.0078, -0.0075, -0.0103, -0.0117, -0.0055, -0.0062, -0.0094,
        -0.0099, -0.0081, -0.0104, -0.0138, -0.0082, -0.0080, -0.0130, -0.0071,
        -0.0104, -0.0082, -0.0086, -0.0079, -0.0120, -0.0089, -0.0071, -0.0099,
        -0.0116, -0.0075, -0.0120, -0.0109, -0.0085, -0.0100, -0.0087, -0.0076,
        -0.0082, -0.0071, -0.0072, -0.0094, -0.0097, -0.0137, -0.0091, -0.0138,
        -0.0164, -0.0082, -0.0091, -0.0109, -0.0055, -0.0095, -0.0047, -0.0083,
        -0.0082, -0.0104, -0.0098, -0.0076, -0.0084, -0.0085, -0.0108, -0.0092,
        -0.0093, -0.0090, -0.0100, -0.0101, -0.0100, -0.0074, -0.0093, -0.0330,
         0.0912, -0.0112, -0.0075, -0.0063, -0.0082, -0.0113, -0.0096, -0.0128,
        -0.0123, -0.0076, -0.0081, -0.0121, -0.0071, -0.0079, -0.0059, -0.0053,
        -0.0098, -0.0086, -0.0107, -0.0143, -0.0096, -0.0055, -0.0086, -0.0081,
        -0.0070, -0.0057, -0.0115, -0.0084, -0.0103, -0.0085, -0.0075, -0.0118,
        -0.0086, -0.0067, -0.0075, -0.0075, -0.0103, -0.0058, -0.0100, -0.0097,
        -0.0066, -0.0083, -0.0071, -0.0091, -0.0091, -0.0102, -0.0069, -0.0110,
        -0.0095, -0.0068,  0.2670, -0.0126, -0.0107, -0.0077, -0.0080, -0.0073,
        -0.0074, -0.0090, -0.0078, -0.0108, -0.0070, -0.0101, -0.0075, -0.0067,
        -0.0088, -0.0077, -0.0128, -0.0086, -0.0080, -0.0109, -0.0104, -0.0067,
        -0.0098, -0.0177, -0.0089, -0.0092, -0.0122, -0.0094, -0.0084, -0.0073,
        -0.0566, -0.0117, -0.0060, -0.0107, -0.0091, -0.0132, -0.0089, -0.0142,
        -0.0112, -0.0068, -0.0061, -0.0114, -0.0079, -0.0108, -0.0071, -0.0058,
        -0.0044, -0.0111, -0.0088, -0.0065, -0.0069, -0.0082, -0.0071, -0.0108,
        -0.0071,  0.5807, -0.0137, -0.0128, -0.0140, -0.0075,  0.0749, -0.0120,
        -0.0073, -0.0073, -0.0075, -0.0082, -0.0078, -0.0092, -0.0092, -0.0068,
        -0.0098, -0.0074, -0.0095, -0.0079, -0.0073, -0.0070, -0.0153, -0.0092,
        -0.0083, -0.0118, -0.0110, -0.0075, -0.0163, -0.0073, -0.0084, -0.0133,
        -0.0112, -0.0109, -0.0068, -0.0094, -0.0097, -0.0088, -0.0089, -0.0100,
        -0.0069, -0.0098, -0.0074, -0.0118, -0.0129, -0.0144, -0.0080, -0.0062,
        -0.0055, -0.0080, -0.0120, -0.0077, -0.0103, -0.0115, -0.0074, -0.0104,
        -0.0096, -0.0067, -0.0126, -0.0075, -0.0085, -0.0073, -0.0085, -0.0084,
        -0.0073, -0.0067, -0.0069, -0.0141, -0.0158, -0.0080, -0.0097, -0.0086,
        -0.0089, -0.0099, -0.0132, -0.0087, -0.0094, -0.0055, -0.0132, -0.0083,
        -0.0087, -0.0110, -0.0110, -0.0106, -0.0078, -0.0074, -0.0111, -0.0094,
        -0.0086, -0.0081, -0.0113, -0.0106, -0.0109, -0.0107, -0.0205, -0.0104,
        -0.0072, -0.0098, -0.0106, -0.0121, -0.0130, -0.0061, -0.0110, -0.0095,
        -0.0084, -0.0098, -0.0114, -0.0103, -0.0140, -0.0094, -0.0112, -0.0139,
        -0.0100, -0.0104, -0.0180, -0.0090, -0.0096, -0.0088, -0.0079, -0.0140,
        -0.0063, -0.0136, -0.0117, -0.0105, -0.0088, -0.0067, -0.0089, -0.0065,
        -0.0089, -0.0137, -0.0088, -0.0145, -0.0092, -0.0107, -0.0112, -0.0116,
        -0.0074, -0.0090, -0.0055, -0.0077, -0.0061, -0.0070, -0.0069, -0.0067,
        -0.0094, -0.0152, -0.0098, -0.0072, -0.0094, -0.0126, -0.0091, -0.0115,
        -0.0068, -0.0102, -0.0103, -0.0058, -0.0130, -0.0095, -0.0063, -0.0092,
        -0.0096, -0.0104, -0.0106, -0.0094, -0.0064, -0.0073, -0.0060, -0.0070,
        -0.0063, -0.0075, -0.0044, -0.0088, -0.0092, -0.0119, -0.0108, -0.0098,
        -0.0082, -0.0071, -0.0142, -0.0054, -0.0080, -0.0052, -0.0072, -0.0118,
        -0.0122, -0.0058,  0.1378, -0.0088, -0.0067, -0.0124,  0.1153, -0.0101,
        -0.0060, -0.0093, -0.0078, -0.0090, -0.0086, -0.0103, -0.0101, -0.0084,
        -0.0073, -0.0099, -0.0085, -0.0093, -0.0055, -0.0067, -0.0130, -0.0109,
        -0.0117, -0.0074, -0.0117, -0.0163, -0.0089, -0.0065, -0.0087, -0.0072,
        -0.0111, -0.0059, -0.0069, -0.0138, -0.0077, -0.0075, -0.0083, -0.0064,
         0.0150, -0.0072, -0.0079, -0.0098, -0.0086, -0.0075, -0.0026, -0.0077,
        -0.0093, -0.0050, -0.0149, -0.0107, -0.0072, -0.1536, -0.0065, -0.0074,
        -0.0072, -0.0062, -0.0854, -0.0067, -0.0078, -0.0071, -0.0104, -0.0061,
        -0.0095, -0.0094, -0.0099, -0.0069, -0.0099, -0.0092, -0.0079, -0.0123,
        -0.0104, -0.0093, -0.0079, -0.0127, -0.0078, -0.0072, -0.0143, -0.0094,
        -0.0082, -0.0084, -0.0094, -0.0111, -0.0118, -0.0148, -0.0077, -0.0112,
        -0.0117, -0.0078, -0.0083, -0.0077, -0.0061, -0.0080, -0.0092, -0.0097,
        -0.0076, -0.0093, -0.0102, -0.0064, -0.0097, -0.0063, -0.0072, -0.0081,
        -0.0075, -0.0089, -0.0066, -0.0100, -0.0108, -0.0102, -0.0081, -0.0085],
       device='cuda:3', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.9999, 0.9997, 1.0000, 0.9992, 1.0000, 0.9996, 0.9998, 0.9997, 0.9999,
        0.9999, 0.9997, 0.9999, 0.9999, 0.9998, 0.9999, 0.9992, 1.0000, 1.0000,
        0.7381, 1.0000, 1.0000, 0.6254, 1.0000, 0.4194, 1.0000, 0.9999, 1.0000,
        1.0000, 1.0000, 0.9999, 0.3000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9999, 0.9999, 0.9997, 0.9998, 0.9996, 0.9994, 0.9997, 0.9998, 0.9998,
        0.9999, 1.0000, 1.0001, 0.9997, 1.0000, 0.9997, 0.9998, 1.0000, 0.9999,
        1.0000, 1.0001, 0.9996, 0.9999, 1.0000, 0.9989, 0.9999, 1.0000, 0.9997,
        0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 0.9985, 0.9986, 1.0000, 0.9995,
        1.0001, 1.0000, 0.9995, 1.0000, 1.0001, 1.0000, 0.9999, 0.9998, 0.9997,
        0.9999, 0.9999, 0.9999, 1.0000, 0.9992, 0.9995, 0.9995, 0.9999, 0.9998,
        0.9997, 0.9995, 0.9999, 1.0000, 1.0003, 0.9999, 0.9997, 1.0000, 1.0000,
        0.9998, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9994, 1.0001, 1.0000,
        1.0000, 1.0000, 0.9999, 0.9947, 0.3505, 0.9998, 0.9996, 0.9999, 1.0000,
        0.9991, 0.9996, 0.9998, 0.9999, 0.9998, 1.0000, 0.9997, 0.9998, 0.9996,
        0.9999, 1.0000, 0.9998, 0.9997, 1.0000, 0.9998, 1.0000, 0.9992, 0.9998,
        0.9998, 1.0000, 0.9998, 0.9999, 0.9997, 0.9996, 1.0000, 1.0001, 0.9988,
        0.9995, 0.9997, 1.0000, 0.9997, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999,
        0.9997, 0.9998, 0.9994, 1.0000, 0.9998, 1.0000, 0.9990, 1.0000, 1.0000,
        0.1946, 1.0000, 0.9999, 1.0000, 0.9999, 0.9996, 0.9999, 1.0000, 0.9995,
        0.9994, 0.9993, 0.9998, 0.9992, 0.9999, 0.9999, 0.9998, 0.9997, 1.0001,
        0.9999, 0.9999, 0.9996, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9991,
        1.0001, 1.0000, 1.0000, 0.3774, 0.9992, 1.0000, 0.9996, 0.9999, 0.9989,
        1.0000, 0.9999, 1.0000, 0.9999, 0.9997, 1.0000, 0.9998, 0.9997, 0.9997,
        0.9999, 0.9999, 1.0000, 0.9999, 0.9998, 0.9999, 0.9999, 0.9998, 0.9989,
        1.0000, 0.2779, 0.9998, 1.0000, 0.9999, 1.0000, 0.3455, 1.0000, 0.9999,
        0.9999, 1.0000, 0.9998, 0.9997, 1.0000, 0.9999, 0.9995, 0.9994, 1.0001,
        1.0000, 0.9998, 0.9997, 0.9999, 0.9999, 0.9998, 0.9999, 1.0000, 0.9996,
        0.9998, 1.0000, 1.0000, 1.0001, 1.0000, 0.9998, 0.9993, 0.9998, 1.0000,
        1.0000, 0.9996, 0.9999, 0.9992, 1.0000, 0.9997, 1.0000, 0.9997, 0.9996,
        0.9996, 0.9998, 0.9999, 0.9998, 0.9996, 0.9999, 1.0000, 1.0000, 1.0001,
        0.9999, 0.9999, 1.0000, 0.9999, 0.9995, 1.0000, 0.9998, 1.0000, 1.0000,
        0.9992, 0.9999, 0.9998, 0.9998, 0.9994, 0.9998, 0.9996, 0.9995, 1.0000,
        1.0000, 0.9994, 1.0001, 0.9996, 1.0000, 1.0000, 0.9999, 0.9999, 1.0000,
        0.9999, 0.9999, 1.0000, 0.9992, 0.9999, 0.9998, 0.9996, 1.0001, 1.0000,
        1.0000, 0.9995, 0.9999, 0.9997, 0.9997, 0.9985, 0.9997, 0.9999, 0.9995,
        0.9994, 0.9994, 1.0000, 0.9999, 1.0001, 1.0000, 0.9999, 1.0000, 0.9993,
        1.0001, 0.9993, 0.9996, 0.9997, 0.9999, 0.9999, 0.9999, 1.0000, 0.9997,
        1.0000, 1.0000, 0.9998, 1.0000, 0.9998, 1.0001, 1.0000, 0.9998, 0.9999,
        0.9999, 1.0002, 0.9994, 1.0000, 0.9999, 0.9999, 0.9999, 0.9998, 1.0000,
        1.0000, 0.9999, 0.9996, 0.9999, 0.9999, 0.9999, 1.0000, 0.9999, 0.9999,
        1.0000, 0.9978, 0.9997, 1.0001, 0.9999, 1.0000, 0.9999, 0.9996, 1.0000,
        0.9998, 0.9992, 1.0000, 0.9997, 0.9999, 1.0001, 0.9996, 0.9998, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9999, 0.9998, 0.9998, 0.9999, 0.9997, 0.9998,
        1.0000, 0.9994, 0.9994, 1.0000, 1.0000, 0.9999, 1.0000, 0.9993, 0.9998,
        0.9998, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000, 0.6026, 0.9995, 0.9999,
        0.9991, 0.8530, 1.0000, 0.9999, 0.9999, 0.9997, 0.9999, 0.9998, 0.9996,
        1.0001, 0.9993, 0.9998, 0.9998, 0.9998, 1.0000, 1.0001, 1.0000, 0.9999,
        1.0001, 0.9995, 0.9999, 1.0000, 0.9992, 0.9998, 0.9997, 0.9995, 0.9999,
        1.0000, 1.0000, 1.0000, 0.9973, 1.0000, 0.9996, 0.9999, 0.9998, 0.8948,
        1.0000, 0.9999, 0.9999, 1.0000, 0.9999, 0.4638, 0.9999, 0.9997, 0.9999,
        1.0000, 0.9996, 0.9994, 0.4243, 1.0001, 0.9998, 0.9999, 1.0000, 0.2158,
        0.9997, 0.9995, 0.9999, 0.9999, 0.9999, 0.9998, 0.9995, 0.9988, 0.9999,
        0.9995, 0.9998, 0.9999, 1.0000, 1.0000, 0.9999, 0.9997, 0.9999, 1.0000,
        1.0000, 0.9998, 0.9998, 1.0000, 0.9996, 0.9992, 1.0000, 0.9998, 1.0000,
        0.9997, 0.9999, 0.9998, 0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 0.9997,
        1.0000, 0.9999, 1.0001, 1.0000, 0.9998, 1.0001, 0.9999, 0.9998, 1.0000,
        0.9998, 1.0000, 1.0000, 0.9996, 1.0000, 0.9999, 0.9999, 0.9998],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 5.4166e-04,  1.7656e-03,  1.7260e-02, -9.5928e-03,  2.3587e-03,
         -6.7211e-03, -2.8873e-03, -1.0741e-03,  7.7514e-03, -7.0041e-04,
         -1.6123e-03, -3.1042e-03,  6.8899e-03,  9.1459e-04, -4.8164e-03,
         -1.2054e-03, -5.3555e-03,  1.2036e-03, -2.6567e-02, -1.4855e-04,
         -1.7230e-03,  9.7861e-03, -1.5007e-03, -4.3426e-02, -3.6876e-03,
          4.0958e-03,  1.3544e-03,  1.0658e-02, -7.3208e-04, -6.6841e-03,
          4.0252e-02,  3.4562e-03,  2.6879e-03, -4.9402e-03, -8.9922e-04,
         -1.2016e-03,  2.8398e-03,  2.5384e-03, -1.8761e-04, -3.0895e-03,
         -3.9974e-03,  1.6917e-03, -7.9011e-03,  1.1710e-04,  1.6918e-02,
         -3.9249e-03, -1.3206e-03,  2.1409e-03,  2.1812e-03, -2.7294e-03,
         -1.7269e-04, -4.1930e-03,  3.7189e-03,  5.6198e-03, -2.1693e-03,
         -2.1784e-03, -4.8630e-03, -4.2011e-04, -1.1992e-03,  7.6349e-03,
         -1.8139e-03, -2.2325e-03, -9.8176e-03, -2.9747e-03,  2.4940e-03,
         -6.9890e-04,  6.5669e-04, -1.2072e-02,  6.6787e-03, -5.4717e-03,
         -1.6579e-03,  9.6926e-04,  8.2838e-04,  3.5811e-05,  9.3809e-03,
         -1.7483e-04, -6.1444e-03, -6.9188e-03, -1.5951e-03, -3.2826e-03,
         -2.0768e-03, -6.6184e-04, -1.2196e-03,  6.1259e-03,  4.1453e-03,
          4.3374e-04, -1.3973e-03,  2.3626e-03,  4.9994e-03,  6.6313e-05,
         -1.4256e-04,  1.8427e-03, -2.1711e-03,  2.9099e-03, -1.1445e-03,
          2.0459e-04,  5.7198e-03,  3.0114e-04, -9.4385e-04, -3.8037e-04,
         -7.7431e-03, -1.9155e-03,  5.5158e-03, -1.2627e-03, -4.2830e-03,
          1.4429e-03, -3.1101e-03,  2.0241e-04,  1.1647e-03,  6.5191e-04,
         -2.5062e-03,  3.1739e-03,  1.8780e-02, -4.7911e-03,  2.4577e-03,
         -2.9115e-04,  3.0619e-03, -7.1321e-05,  3.5436e-03,  2.0801e-02,
          1.4220e-03,  3.9987e-03, -4.9690e-03,  6.3733e-04,  5.7173e-04,
         -6.0108e-04,  3.0742e-03,  2.2812e-03,  1.3879e-03,  6.0013e-05,
         -8.1139e-03, -7.2263e-03, -4.7895e-03, -3.3826e-03,  1.5455e-03,
         -1.5568e-03, -3.7072e-04, -4.0514e-04,  3.8507e-03,  5.4234e-03,
          2.5308e-03, -4.0094e-03, -4.9045e-04, -7.2475e-03,  1.0210e-03,
          4.0707e-03,  3.0752e-04, -7.3144e-04, -2.8906e-03, -6.6956e-04,
         -4.8785e-04, -7.2509e-03,  1.9096e-03,  9.4919e-04,  2.1061e-04,
          3.6141e-03, -5.9703e-04,  6.8196e-04, -8.0284e-03, -2.7578e-03,
          3.5749e-03,  1.4209e-03, -3.3571e-02,  3.9780e-03,  2.5141e-03,
          4.5834e-03,  1.9435e-03,  9.5690e-04, -5.3447e-03,  4.4374e-03,
         -2.4582e-03, -8.2597e-04, -4.0674e-03, -1.4497e-02, -5.3858e-03,
         -1.9867e-03, -7.5982e-03, -7.4112e-04, -6.0165e-03,  2.7584e-03,
         -6.6495e-03, -7.3456e-03,  2.4229e-03, -3.0117e-03, -4.0261e-03,
         -2.6918e-03, -5.7710e-03,  6.1880e-03, -7.0098e-03,  1.3766e-03,
          1.1567e-03, -1.5399e-03, -4.2761e-02, -2.7391e-03, -1.6481e-03,
          5.6341e-04, -1.0999e-03,  4.1878e-03, -3.4805e-03,  3.1290e-03,
         -5.3613e-03,  1.7219e-03, -1.4641e-03, -3.3399e-03,  5.3010e-04,
          9.2076e-03,  4.6622e-03,  5.3332e-03,  5.7710e-03, -8.3085e-03,
          1.5219e-03, -7.2731e-03, -3.2098e-03, -2.5219e-03,  9.4412e-04,
          2.9937e-03,  9.6819e-03, -2.4543e-02, -1.1764e-02,  3.2842e-03,
         -1.1135e-02, -2.6173e-03,  1.8460e-02,  7.5608e-03, -6.1207e-04,
          2.1545e-04,  1.7636e-03,  1.1302e-03,  2.7094e-03,  1.5797e-03,
         -3.2999e-03,  1.0216e-03,  1.2714e-03, -7.4829e-04,  6.2021e-03,
         -3.1252e-03,  6.6625e-05,  2.4077e-05, -1.1255e-02, -2.3326e-04,
         -7.7069e-05,  2.8809e-03, -1.3292e-03, -9.5098e-04, -2.3397e-03,
          3.6462e-03,  3.1458e-03, -1.2338e-02,  5.9918e-03,  1.8398e-03,
          1.3322e-03, -3.0007e-03,  2.7013e-03,  1.1997e-03, -2.9460e-03,
         -2.3459e-03, -6.0259e-04, -9.8616e-03,  1.2478e-03,  2.4482e-03,
         -3.6160e-03,  3.7207e-03,  1.2974e-04, -2.2524e-04,  2.3763e-04,
          7.2090e-03,  9.4513e-03,  7.9917e-03, -8.1872e-03,  3.6959e-03,
         -1.1713e-03,  5.4552e-03,  9.1242e-03, -1.8320e-04,  5.6789e-03,
         -9.0594e-04, -9.1465e-04, -6.4427e-03, -4.2874e-03,  5.0322e-04,
         -4.3327e-03, -4.5157e-04, -2.1503e-04, -1.0706e-02,  1.7651e-02,
          5.9982e-04,  2.9163e-03,  5.9610e-04, -8.6914e-03,  1.9080e-02,
         -8.6005e-03,  1.3043e-02,  3.4780e-03,  6.2154e-04,  8.3757e-03,
         -2.9246e-03, -3.8572e-03,  5.1433e-03, -1.5991e-03,  6.9871e-03,
          1.8688e-03,  9.0499e-04,  3.5418e-03,  1.8820e-04,  2.6518e-03,
          4.3704e-03, -5.8745e-04,  1.0620e-02,  7.2153e-03, -5.4037e-03,
          3.0849e-03,  3.7246e-03,  8.6988e-04, -2.3998e-03,  8.8189e-03,
          1.4175e-02, -2.3604e-03, -6.9495e-03, -6.0418e-03, -4.0874e-03,
          4.6403e-03,  1.1067e-03,  6.1171e-03, -5.9514e-03,  7.2759e-03,
          1.5092e-03, -5.4240e-03, -3.1659e-03, -6.7151e-03, -2.9467e-03,
         -2.5235e-03, -6.1406e-04, -1.5262e-04,  1.2107e-03, -3.1981e-03,
          6.5084e-03, -2.2412e-03, -1.3415e-02,  5.3226e-03,  5.4611e-03,
          2.6008e-03,  5.7673e-03, -1.2163e-03,  3.9678e-03, -3.1896e-04,
         -1.1221e-03,  4.6454e-03, -1.4075e-02, -7.9009e-03,  2.5443e-03,
         -3.7739e-03, -7.2673e-03,  4.3249e-03, -3.1009e-03, -5.7436e-04,
          3.7327e-03, -2.6564e-03, -1.0991e-02, -5.7718e-04,  2.6553e-03,
         -2.8010e-04, -1.0273e-02,  5.6670e-03,  1.6367e-03, -4.5932e-03,
         -1.8386e-03,  2.0183e-02,  1.5098e-02,  4.8491e-04,  4.3186e-03,
         -4.9481e-03, -4.8333e-03, -7.5013e-03,  1.9421e-03, -4.4020e-03,
         -9.1840e-03, -1.2732e-02,  7.3528e-03,  1.1138e-02,  1.8361e-03,
         -1.1169e-02,  8.3534e-03, -6.4727e-05,  3.5116e-04, -4.6176e-04,
          1.0522e-03, -3.0298e-03, -3.6213e-04,  1.1493e-03, -9.1660e-03,
          6.0943e-03, -1.5305e-03,  1.7973e-03, -3.1801e-03,  1.0686e-02,
         -1.1837e-03,  1.7033e-03, -7.3633e-04, -3.0504e-04, -6.8413e-03,
          6.0358e-03, -4.9153e-04, -1.0299e-02, -1.2502e-05, -5.9537e-03,
         -3.5262e-03, -1.1567e-02,  1.4946e-03, -2.1259e-04,  2.4809e-03,
          4.6741e-04,  6.2132e-03,  3.0272e-03,  3.0670e-03, -1.0658e-02,
          4.0177e-03,  2.0679e-03, -9.1658e-04, -9.1695e-06, -1.7030e-03,
         -1.9965e-03,  5.1584e-03, -1.9987e-04,  1.0481e-03, -2.3799e-03,
          3.1614e-03, -4.1374e-03,  2.2069e-03,  2.1398e-03, -6.2137e-05,
          3.4417e-03,  8.6056e-04,  6.1775e-03,  1.0213e-03, -4.7488e-03,
         -2.1299e-02,  2.1350e-03, -1.9469e-03,  4.5412e-04, -6.8093e-04,
          8.1564e-03,  2.8543e-03,  6.2184e-03, -5.4415e-03,  5.0302e-03,
          3.8614e-03, -3.2626e-02,  3.7225e-05, -3.5472e-04,  2.5311e-04,
          3.3767e-03, -4.1692e-03,  3.1052e-03,  3.7820e-02,  2.0242e-03,
          1.1271e-03, -4.5254e-04, -6.2171e-04,  3.0231e-02,  1.4452e-03,
          1.9694e-03,  2.4192e-03,  6.5872e-04, -6.7540e-03,  2.7261e-03,
          1.6547e-03,  7.0710e-03,  1.0657e-04, -1.0714e-03,  2.7465e-03,
         -3.0719e-04,  3.3466e-03, -2.4944e-03,  3.3802e-03, -2.6474e-03,
          1.1954e-02, -3.4975e-04,  3.0665e-04, -5.1337e-04,  3.4786e-03,
         -3.0539e-03, -8.0170e-04,  5.1251e-03,  3.5589e-03, -5.3200e-03,
         -1.3334e-03,  4.0606e-03,  5.8925e-03, -5.6604e-03, -1.0692e-03,
          4.6158e-03, -3.0562e-03, -2.7551e-04,  5.1050e-05,  3.9919e-03,
          3.4014e-03, -6.7532e-04,  7.5120e-03,  9.7217e-03,  3.2572e-03,
          2.1953e-03, -4.6082e-03,  1.0028e-06, -4.7633e-03,  3.6170e-05,
          4.0307e-04,  1.9899e-03, -1.9121e-03, -3.6732e-03,  8.2380e-04,
         -5.6435e-03,  1.6490e-04]], device='cuda:3', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[4.5681e-03, 8.3260e-02, 9.7217e-02, 2.0369e-01, 5.8015e-02, 1.2127e-01,
         1.0223e-01, 6.5726e-03, 9.9344e-02, 5.4084e-02, 1.9711e-01, 7.2669e-02,
         5.9942e-02, 5.6187e-02, 1.0794e-01, 4.0847e-02, 9.2308e-02, 6.0575e-02,
         5.7540e-04, 5.8232e-02, 4.2585e-02, 7.2492e-05, 7.8340e-03, 2.2652e-04,
         2.4818e-02, 6.0123e-02, 9.4614e-02, 2.0672e-01, 1.4667e-02, 3.7553e-02,
         1.5632e-04, 2.0402e-01, 6.4954e-02, 7.1688e-02, 1.8277e-02, 5.7072e-02,
         4.7145e-02, 5.8336e-02, 2.3942e-02, 1.7503e-02, 4.8697e-02, 1.0448e-01,
         5.2728e-02, 3.6601e-03, 1.2835e-01, 5.5896e-02, 3.0366e-02, 5.1315e-02,
         5.9529e-02, 4.7502e-02, 6.0067e-03, 2.3342e-02, 8.5970e-02, 5.8122e-02,
         5.0178e-02, 8.8180e-02, 6.4737e-02, 1.3346e-02, 3.8263e-02, 9.2178e-02,
         5.2783e-02, 6.3609e-02, 1.4783e-01, 6.5697e-02, 2.7910e-02, 4.0827e-02,
         4.8053e-02, 4.5854e-02, 1.6361e-01, 4.6929e-02, 3.9000e-02, 5.9610e-02,
         7.3198e-02, 4.1416e-02, 5.1420e-02, 3.9398e-02, 5.3422e-02, 9.6244e-02,
         2.1414e-02, 2.7464e-02, 4.6837e-02, 2.1921e-02, 1.7556e-02, 5.5457e-02,
         3.3861e-02, 9.9509e-02, 5.5129e-02, 1.1021e-01, 9.1630e-02, 2.3088e-03,
         4.1091e-02, 5.9189e-02, 1.9356e-02, 1.3570e-01, 4.0828e-02, 6.9808e-02,
         5.8668e-02, 4.8525e-02, 4.2179e-02, 1.8440e-02, 1.2121e-01, 6.7203e-02,
         1.2335e-01, 5.6354e-02, 1.0207e-01, 3.0190e-02, 5.2995e-02, 1.2550e-02,
         3.4068e-02, 6.1697e-03, 1.0951e-01, 7.0177e-04, 4.8283e-05, 6.0527e-02,
         1.0990e-02, 3.3063e-03, 4.5975e-02, 8.3623e-02, 3.5106e-02, 1.2934e-01,
         5.1044e-02, 4.4052e-02, 6.6497e-02, 1.1148e-01, 3.7988e-02, 1.7272e-02,
         3.8700e-02, 4.6546e-02, 3.2343e-02, 1.7108e-02, 5.2153e-02, 5.9041e-02,
         5.6073e-02, 9.6918e-02, 4.5481e-02, 4.2073e-02, 2.2440e-02, 3.8707e-03,
         5.4246e-02, 2.4970e-02, 3.2985e-02, 6.4215e-02, 1.9110e-02, 5.2723e-02,
         1.7843e-02, 3.6010e-02, 4.0468e-02, 3.2573e-02, 4.8516e-02, 3.0530e-03,
         7.1352e-02, 7.4637e-02, 7.3764e-02, 3.4668e-02, 1.0223e-02, 7.3184e-02,
         3.9468e-02, 5.0763e-02, 5.2252e-02, 6.0949e-02, 8.1565e-02, 9.7310e-03,
         2.6205e-05, 7.1057e-02, 5.0659e-02, 6.4179e-02, 3.6030e-02, 1.7208e-02,
         3.4729e-02, 5.1304e-02, 4.5365e-02, 3.5490e-02, 6.8332e-02, 4.8441e-02,
         6.0441e-02, 6.4336e-02, 5.9817e-02, 4.2750e-02, 4.7316e-02, 5.3375e-02,
         6.1834e-02, 4.9825e-02, 6.7032e-02, 1.2800e-02, 8.9612e-02, 1.6882e-01,
         3.9023e-02, 4.6977e-02, 1.0481e-01, 2.8501e-02, 2.5315e-02, 4.8008e-02,
         2.0430e-04, 1.7271e-01, 5.8019e-02, 4.5072e-02, 1.8174e-02, 6.0129e-02,
         6.0362e-02, 6.5291e-02, 1.1451e-01, 1.1472e-02, 5.4843e-02, 9.7620e-02,
         2.5554e-02, 8.5196e-02, 5.5796e-02, 8.0533e-02, 4.7818e-02, 1.1128e-01,
         5.5784e-02, 5.4229e-02, 2.5581e-02, 7.8773e-02, 1.0872e-02, 6.4069e-02,
         3.6426e-02, 4.5073e-05, 9.0907e-02, 1.8222e-01, 1.3828e-01, 5.9041e-02,
         4.1564e-05, 8.6887e-02, 7.0858e-02, 8.2148e-03, 4.1356e-02, 1.3119e-02,
         3.3630e-02, 5.2480e-02, 1.6260e-02, 6.4048e-02, 6.4175e-02, 1.0326e-02,
         6.7071e-02, 2.3323e-02, 7.8236e-03, 3.5215e-03, 1.0639e-01, 2.6735e-02,
         1.9783e-02, 1.4944e-01, 3.0693e-02, 6.9599e-02, 1.4697e-01, 1.2974e-01,
         1.0654e-01, 1.3339e-01, 2.0462e-01, 3.0926e-02, 1.2771e-02, 5.7648e-02,
         8.7294e-02, 5.7775e-02, 4.5006e-02, 7.3348e-02, 5.8068e-03, 8.5935e-02,
         4.7748e-02, 1.8415e-01, 1.3335e-01, 1.1948e-01, 1.4160e-02, 2.8168e-03,
         5.4300e-04, 4.5612e-02, 1.2335e-01, 5.9643e-02, 5.5827e-02, 6.2025e-02,
         1.6865e-02, 3.7680e-02, 6.8916e-02, 5.3887e-03, 4.7839e-02, 3.1829e-02,
         3.9701e-02, 3.4841e-02, 2.0867e-01, 7.0522e-02, 4.4478e-02, 6.4841e-03,
         1.6553e-03, 1.4797e-01, 1.3263e-01, 5.3776e-03, 8.6566e-02, 5.9021e-02,
         4.4073e-02, 9.0112e-02, 5.8202e-02, 8.2681e-02, 7.8376e-02, 4.1832e-02,
         7.7751e-02, 1.1649e-02, 1.7911e-02, 5.8127e-02, 7.9298e-02, 5.4864e-02,
         4.2620e-02, 1.5425e-02, 3.5665e-02, 1.7479e-02, 1.9333e-02, 4.0648e-02,
         1.2163e-01, 5.3033e-02, 5.0189e-02, 3.6546e-02, 1.8739e-01, 5.6581e-02,
         4.3155e-03, 3.7046e-02, 9.4158e-02, 1.2477e-01, 9.6962e-02, 4.6820e-02,
         7.1410e-02, 5.7958e-02, 1.4440e-01, 5.9141e-02, 7.3026e-02, 5.6857e-02,
         4.8837e-02, 5.1165e-02, 1.0219e-01, 1.4532e-01, 1.0789e-01, 4.8633e-02,
         1.2053e-01, 5.1352e-02, 1.6169e-02, 9.7055e-02, 6.5922e-02, 6.7129e-02,
         4.2608e-02, 1.1118e-01, 6.3858e-02, 5.5166e-02, 8.1185e-02, 5.5862e-02,
         3.1205e-02, 5.7232e-02, 4.9945e-02, 1.3998e-01, 6.3848e-02, 6.9591e-02,
         5.6835e-02, 7.5157e-02, 1.7045e-01, 6.7692e-02, 5.5354e-02, 5.5492e-02,
         2.8956e-03, 5.2738e-02, 2.4631e-02, 4.2155e-02, 3.4280e-03, 2.4521e-02,
         5.5396e-02, 1.4122e-01, 3.8073e-02, 4.1778e-02, 5.1956e-02, 5.9860e-02,
         1.1502e-01, 1.1190e-01, 1.7359e-03, 7.9175e-02, 7.2827e-02, 7.5077e-02,
         8.3063e-02, 5.7931e-02, 6.5647e-02, 6.8596e-02, 8.1748e-02, 8.0682e-02,
         8.5010e-02, 5.3379e-02, 5.7774e-02, 4.5301e-02, 1.3784e-03, 5.6049e-02,
         1.0256e-02, 9.9113e-03, 2.8095e-02, 7.3674e-03, 5.0671e-02, 6.0056e-02,
         1.6000e-01, 6.1895e-02, 5.1937e-02, 5.3896e-02, 1.4224e-01, 3.4608e-02,
         1.3433e-02, 3.7041e-02, 1.2345e-02, 6.9976e-02, 8.0903e-02, 5.1898e-02,
         7.4681e-05, 3.6229e-02, 5.8856e-02, 3.7385e-02, 4.7326e-04, 4.4125e-02,
         7.6644e-04, 1.2255e-02, 1.1682e-01, 8.2413e-02, 8.0477e-02, 3.0432e-02,
         5.9212e-02, 8.9803e-02, 2.4096e-02, 2.2383e-02, 1.1035e-02, 3.1186e-02,
         3.8197e-03, 6.8287e-02, 1.9977e-01, 3.0714e-02, 6.3054e-02, 4.1493e-02,
         7.5867e-02, 2.0487e-01, 5.2757e-02, 8.1285e-03, 4.4118e-02, 1.7291e-02,
         7.7057e-02, 1.0159e-02, 6.2993e-02, 1.9300e-01, 3.1157e-02, 1.5145e-02,
         1.7682e-02, 5.4494e-03, 3.9852e-04, 5.6245e-02, 6.0361e-02, 5.4068e-02,
         9.5000e-02, 2.0211e-02, 2.0159e-04, 2.4428e-03, 6.7652e-02, 5.3451e-04,
         1.1453e-01, 4.7301e-02, 2.5610e-02, 2.0589e-04, 1.4781e-02, 7.9902e-03,
         7.2309e-03, 4.0721e-03, 2.7729e-05, 9.9471e-03, 2.6840e-02, 1.4786e-02,
         6.6410e-02, 4.5095e-02, 1.9212e-02, 7.7528e-02, 1.1057e-01, 3.4969e-03,
         1.0195e-01, 6.4809e-02, 1.4180e-02, 1.2202e-01, 1.0558e-01, 4.2620e-02,
         4.6487e-02, 1.0867e-01, 4.7345e-02, 4.9109e-02, 1.2684e-01, 5.4048e-02,
         5.9878e-02, 5.3570e-02, 5.3460e-02, 6.8268e-02, 7.3285e-02, 1.8186e-01,
         4.5690e-02, 5.6863e-02, 7.6161e-02, 3.9845e-02, 6.9079e-02, 9.5030e-02,
         2.3585e-03, 5.3681e-02, 2.1301e-02, 5.6740e-02, 5.2602e-03, 6.6921e-02,
         1.0971e-01, 5.2521e-02, 2.7951e-02, 5.1202e-02, 1.5764e-03, 6.4032e-02,
         7.7140e-02, 8.3507e-02, 7.3357e-02, 3.8504e-02, 6.5938e-02, 6.1809e-02,
         4.9945e-02, 1.0213e-02]], device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.5008], device='cuda:3', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0066], device='cuda:3', grad_fn=<AddBackward0>)
likelihood_guide.likelihood._scale.loc torch.Size([]) Parameter containing:
tensor(-0.4466, device='cuda:3', requires_grad=True)
likelihood_guide.likelihood._scale.scale torch.Size([]) tensor(0.0071, device='cuda:3', grad_fn=<AddBackward0>)
Using device: cuda:3
===== Training profile tensin-3x512-sl - 2 =====
[0:00:01.950217] epoch: 0 | elbo: 31586.659785156247 | train_rmse: 0.4324 | val_rmse: 0.496 | val_ll: -0.864
[0:01:44.041893] epoch: 50 | elbo: 31552.053789062502 | train_rmse: 0.4278 | val_rmse: 0.4917 | val_ll: -0.8547
[0:03:25.776567] epoch: 100 | elbo: 31473.592851562502 | train_rmse: 0.4235 | val_rmse: 0.4878 | val_ll: -0.8532
[0:05:05.448625] epoch: 150 | elbo: 31400.460488281242 | train_rmse: 0.4234 | val_rmse: 0.484 | val_ll: -0.8431
[0:06:45.979957] epoch: 200 | elbo: 31280.261328125 | train_rmse: 0.4214 | val_rmse: 0.4821 | val_ll: -0.8361
[0:08:22.672564] epoch: 250 | elbo: 31306.465390624995 | train_rmse: 0.4181 | val_rmse: 0.4804 | val_ll: -0.827
[0:10:03.071239] epoch: 300 | elbo: 31261.389824218746 | train_rmse: 0.4174 | val_rmse: 0.4797 | val_ll: -0.8225
[0:11:41.685903] epoch: 350 | elbo: 31248.527773437498 | train_rmse: 0.4147 | val_rmse: 0.4765 | val_ll: -0.8169
[0:13:18.500406] epoch: 400 | elbo: 31094.6359375 | train_rmse: 0.4103 | val_rmse: 0.4763 | val_ll: -0.8076
[0:14:55.323054] epoch: 450 | elbo: 31122.5187890625 | train_rmse: 0.4129 | val_rmse: 0.4745 | val_ll: -0.8097
[0:16:32.141706] epoch: 500 | elbo: 31034.1704296875 | train_rmse: 0.4096 | val_rmse: 0.4713 | val_ll: -0.7979
[0:18:08.617715] epoch: 550 | elbo: 30960.620488281253 | train_rmse: 0.406 | val_rmse: 0.4677 | val_ll: -0.7919
[0:19:46.128823] epoch: 600 | elbo: 30908.431269531255 | train_rmse: 0.4043 | val_rmse: 0.4672 | val_ll: -0.7859
[0:21:27.577609] epoch: 650 | elbo: 30806.390859375 | train_rmse: 0.4017 | val_rmse: 0.4649 | val_ll: -0.7801
[0:23:09.474822] epoch: 700 | elbo: 30770.745859375 | train_rmse: 0.4015 | val_rmse: 0.4635 | val_ll: -0.7765
[0:24:50.463973] epoch: 750 | elbo: 30816.435097656253 | train_rmse: 0.4009 | val_rmse: 0.4644 | val_ll: -0.7697
[0:26:32.810297] epoch: 800 | elbo: 30710.992207031253 | train_rmse: 0.3985 | val_rmse: 0.4614 | val_ll: -0.7682
[0:28:15.132687] epoch: 850 | elbo: 30700.526152343744 | train_rmse: 0.3982 | val_rmse: 0.4628 | val_ll: -0.7603
[0:29:57.099775] epoch: 900 | elbo: 30618.38916015625 | train_rmse: 0.3967 | val_rmse: 0.4591 | val_ll: -0.758
[0:31:38.915069] epoch: 950 | elbo: 30544.8201171875 | train_rmse: 0.3939 | val_rmse: 0.4563 | val_ll: -0.7523
[0:33:19.495540] epoch: 1000 | elbo: 30485.535624999997 | train_rmse: 0.393 | val_rmse: 0.4556 | val_ll: -0.7471
[0:35:00.940535] epoch: 1050 | elbo: 30508.042578124994 | train_rmse: 0.3921 | val_rmse: 0.4544 | val_ll: -0.7433
[0:36:42.289169] epoch: 1100 | elbo: 30460.121718750004 | train_rmse: 0.3911 | val_rmse: 0.4533 | val_ll: -0.7411
[0:38:23.306995] epoch: 1150 | elbo: 30426.982207031244 | train_rmse: 0.3918 | val_rmse: 0.4528 | val_ll: -0.737
[0:40:04.737776] epoch: 1200 | elbo: 30400.019121093745 | train_rmse: 0.3876 | val_rmse: 0.4505 | val_ll: -0.7291
[0:41:44.789490] epoch: 1250 | elbo: 30467.078281250004 | train_rmse: 0.3874 | val_rmse: 0.4496 | val_ll: -0.7306
[0:43:23.046088] epoch: 1300 | elbo: 30314.896484375 | train_rmse: 0.3845 | val_rmse: 0.4482 | val_ll: -0.724
[0:45:00.116536] epoch: 1350 | elbo: 30173.265234375 | train_rmse: 0.3838 | val_rmse: 0.447 | val_ll: -0.7185
[0:46:37.207322] epoch: 1400 | elbo: 30173.8549609375 | train_rmse: 0.384 | val_rmse: 0.4471 | val_ll: -0.7169
[0:48:14.014128] epoch: 1450 | elbo: 30161.240175781248 | train_rmse: 0.3819 | val_rmse: 0.4448 | val_ll: -0.7107
[0:49:52.775793] epoch: 1500 | elbo: 30052.362441406254 | train_rmse: 0.3807 | val_rmse: 0.4432 | val_ll: -0.7081
[0:51:29.478382] epoch: 1550 | elbo: 30072.379140625002 | train_rmse: 0.3787 | val_rmse: 0.4422 | val_ll: -0.7035
[0:53:04.872345] epoch: 1600 | elbo: 30022.782285156252 | train_rmse: 0.3794 | val_rmse: 0.4406 | val_ll: -0.6999
[0:54:40.926361] epoch: 1650 | elbo: 29966.652695312503 | train_rmse: 0.3776 | val_rmse: 0.4394 | val_ll: -0.6983
[0:56:16.529615] epoch: 1700 | elbo: 29942.726269531257 | train_rmse: 0.376 | val_rmse: 0.4403 | val_ll: -0.6978
[0:57:52.520690] epoch: 1750 | elbo: 29944.8673828125 | train_rmse: 0.3762 | val_rmse: 0.4382 | val_ll: -0.6906
[0:59:29.825043] epoch: 1800 | elbo: 29839.139121093747 | train_rmse: 0.3746 | val_rmse: 0.4363 | val_ll: -0.6895
[1:01:05.981653] epoch: 1850 | elbo: 29871.98654296875 | train_rmse: 0.3729 | val_rmse: 0.4354 | val_ll: -0.6836
[1:02:43.154992] epoch: 1900 | elbo: 29873.086562499997 | train_rmse: 0.3733 | val_rmse: 0.4369 | val_ll: -0.6839
[1:04:21.470071] epoch: 1950 | elbo: 29801.374492187497 | train_rmse: 0.3711 | val_rmse: 0.435 | val_ll: -0.6797
[1:05:57.436491] epoch: 2000 | elbo: 29655.422011718747 | train_rmse: 0.3707 | val_rmse: 0.433 | val_ll: -0.6765
[1:07:33.997561] epoch: 2050 | elbo: 29971.993925781244 | train_rmse: 0.3698 | val_rmse: 0.4323 | val_ll: -0.6737
[1:09:09.917432] epoch: 2100 | elbo: 29702.325039062496 | train_rmse: 0.3691 | val_rmse: 0.4328 | val_ll: -0.671
[1:10:45.711522] epoch: 2150 | elbo: 29683.08818359375 | train_rmse: 0.3692 | val_rmse: 0.4306 | val_ll: -0.6745
[1:12:22.159795] epoch: 2200 | elbo: 29628.435546875 | train_rmse: 0.3708 | val_rmse: 0.432 | val_ll: -0.6696
[1:13:59.268309] epoch: 2250 | elbo: 29600.08273437499 | train_rmse: 0.3678 | val_rmse: 0.4294 | val_ll: -0.6661
[1:15:42.387149] epoch: 2300 | elbo: 29607.632246093748 | train_rmse: 0.3658 | val_rmse: 0.4287 | val_ll: -0.6628
[1:17:25.395335] epoch: 2350 | elbo: 29606.7490625 | train_rmse: 0.3648 | val_rmse: 0.4273 | val_ll: -0.658
[1:19:05.759691] epoch: 2400 | elbo: 29547.05326171875 | train_rmse: 0.3663 | val_rmse: 0.4277 | val_ll: -0.6562
[1:20:43.332624] epoch: 2450 | elbo: 29507.65548828125 | train_rmse: 0.3639 | val_rmse: 0.4279 | val_ll: -0.6572
[1:22:19.978802] epoch: 2500 | elbo: 29515.23271484375 | train_rmse: 0.3639 | val_rmse: 0.4271 | val_ll: -0.6511
[1:23:57.494447] epoch: 2550 | elbo: 29625.384648437503 | train_rmse: 0.3632 | val_rmse: 0.4254 | val_ll: -0.6488
[1:25:35.235328] epoch: 2600 | elbo: 29393.25607421875 | train_rmse: 0.3615 | val_rmse: 0.4246 | val_ll: -0.6461
[1:27:15.239044] epoch: 2650 | elbo: 29320.444140625004 | train_rmse: 0.3624 | val_rmse: 0.4257 | val_ll: -0.648
[1:28:56.984552] epoch: 2700 | elbo: 29357.44923828125 | train_rmse: 0.3608 | val_rmse: 0.4233 | val_ll: -0.6419
[1:30:38.819736] epoch: 2750 | elbo: 29384.281816406252 | train_rmse: 0.3602 | val_rmse: 0.4234 | val_ll: -0.642
[1:32:19.249911] epoch: 2800 | elbo: 29368.734003906247 | train_rmse: 0.3596 | val_rmse: 0.4225 | val_ll: -0.6399
[1:33:58.756535] epoch: 2850 | elbo: 29321.182578125 | train_rmse: 0.3601 | val_rmse: 0.4229 | val_ll: -0.6408
[1:35:39.115838] epoch: 2900 | elbo: 29239.42236328125 | train_rmse: 0.359 | val_rmse: 0.4207 | val_ll: -0.6357
[1:37:20.064863] epoch: 2950 | elbo: 29189.71787109375 | train_rmse: 0.3587 | val_rmse: 0.4214 | val_ll: -0.6377
[1:39:01.341888] epoch: 3000 | elbo: 29265.923886718745 | train_rmse: 0.3592 | val_rmse: 0.423 | val_ll: -0.6339
[1:40:42.061678] epoch: 3050 | elbo: 29143.619746093747 | train_rmse: 0.361 | val_rmse: 0.4219 | val_ll: -0.6355
[1:42:22.729031] epoch: 3100 | elbo: 29203.646835937503 | train_rmse: 0.3574 | val_rmse: 0.4199 | val_ll: -0.6286
[1:44:04.530850] epoch: 3150 | elbo: 29214.767402343754 | train_rmse: 0.356 | val_rmse: 0.4195 | val_ll: -0.6249
[1:45:44.548157] epoch: 3200 | elbo: 29181.334726562498 | train_rmse: 0.3576 | val_rmse: 0.4185 | val_ll: -0.6279
[1:47:25.579286] epoch: 3250 | elbo: 29153.714238281245 | train_rmse: 0.3568 | val_rmse: 0.4195 | val_ll: -0.6269
[1:49:08.111756] epoch: 3300 | elbo: 29059.049062500002 | train_rmse: 0.356 | val_rmse: 0.418 | val_ll: -0.6245
[1:50:51.096473] epoch: 3350 | elbo: 29059.23751953125 | train_rmse: 0.3565 | val_rmse: 0.4199 | val_ll: -0.6255
[1:52:31.737109] epoch: 3400 | elbo: 29032.88056640625 | train_rmse: 0.3563 | val_rmse: 0.4208 | val_ll: -0.6229
[1:54:12.199458] epoch: 3450 | elbo: 29070.914296875 | train_rmse: 0.3543 | val_rmse: 0.4173 | val_ll: -0.6223
[1:55:52.150431] epoch: 3500 | elbo: 28967.896289062504 | train_rmse: 0.3536 | val_rmse: 0.418 | val_ll: -0.6172
[1:57:32.381735] epoch: 3550 | elbo: 28980.638125000005 | train_rmse: 0.3531 | val_rmse: 0.4172 | val_ll: -0.6204
[1:59:14.053631] epoch: 3600 | elbo: 28910.417402343755 | train_rmse: 0.3539 | val_rmse: 0.4159 | val_ll: -0.6159
[2:00:55.597040] epoch: 3650 | elbo: 28920.745429687493 | train_rmse: 0.3535 | val_rmse: 0.4169 | val_ll: -0.6192
[2:02:35.828369] epoch: 3700 | elbo: 28918.0541796875 | train_rmse: 0.3521 | val_rmse: 0.4155 | val_ll: -0.6101
[2:04:15.965969] epoch: 3750 | elbo: 28887.692109375 | train_rmse: 0.3531 | val_rmse: 0.4165 | val_ll: -0.6132
[2:05:56.498350] epoch: 3800 | elbo: 28897.994218750002 | train_rmse: 0.3515 | val_rmse: 0.4148 | val_ll: -0.6105
[2:07:36.862976] epoch: 3850 | elbo: 28883.623164062497 | train_rmse: 0.3515 | val_rmse: 0.4158 | val_ll: -0.6102
[2:09:16.273238] epoch: 3900 | elbo: 28834.6737890625 | train_rmse: 0.351 | val_rmse: 0.4146 | val_ll: -0.6063
[2:10:56.677908] epoch: 3950 | elbo: 28848.01611328125 | train_rmse: 0.3503 | val_rmse: 0.4146 | val_ll: -0.6087
[2:12:36.043477] epoch: 4000 | elbo: 28896.229003906257 | train_rmse: 0.3503 | val_rmse: 0.416 | val_ll: -0.6061
[2:14:16.218542] epoch: 4050 | elbo: 28857.2912109375 | train_rmse: 0.3498 | val_rmse: 0.4139 | val_ll: -0.6059
[2:15:57.418551] epoch: 4100 | elbo: 28904.55029296875 | train_rmse: 0.3496 | val_rmse: 0.4147 | val_ll: -0.6051
[2:17:39.353206] epoch: 4150 | elbo: 28688.916738281252 | train_rmse: 0.3501 | val_rmse: 0.4128 | val_ll: -0.6028
[2:19:19.533543] epoch: 4200 | elbo: 28844.595312500005 | train_rmse: 0.3489 | val_rmse: 0.4127 | val_ll: -0.5998
[2:20:58.817631] epoch: 4250 | elbo: 28734.81267578125 | train_rmse: 0.3498 | val_rmse: 0.4118 | val_ll: -0.6026
[2:22:39.141819] epoch: 4300 | elbo: 28742.935878906253 | train_rmse: 0.3498 | val_rmse: 0.4133 | val_ll: -0.5977
[2:24:19.209643] epoch: 4350 | elbo: 28699.610429687502 | train_rmse: 0.3502 | val_rmse: 0.4132 | val_ll: -0.6006
[2:25:59.537311] epoch: 4400 | elbo: 28597.096347656254 | train_rmse: 0.3494 | val_rmse: 0.4119 | val_ll: -0.6007
[2:27:39.652879] epoch: 4450 | elbo: 28732.891035156248 | train_rmse: 0.349 | val_rmse: 0.4115 | val_ll: -0.5961
[2:29:19.128829] epoch: 4500 | elbo: 28664.3171875 | train_rmse: 0.348 | val_rmse: 0.412 | val_ll: -0.5968
[2:30:58.988042] epoch: 4550 | elbo: 28572.418808593753 | train_rmse: 0.3491 | val_rmse: 0.4134 | val_ll: -0.6009
[2:32:39.196685] epoch: 4600 | elbo: 28608.60431640625 | train_rmse: 0.3483 | val_rmse: 0.4123 | val_ll: -0.6001
[2:34:19.603826] epoch: 4650 | elbo: 28613.59072265625 | train_rmse: 0.3496 | val_rmse: 0.412 | val_ll: -0.5973
[2:36:00.384665] epoch: 4700 | elbo: 28513.22337890625 | train_rmse: 0.3479 | val_rmse: 0.4122 | val_ll: -0.5976
[2:37:41.327180] epoch: 4750 | elbo: 28483.536992187503 | train_rmse: 0.3473 | val_rmse: 0.4115 | val_ll: -0.5968
[2:39:21.167623] epoch: 4800 | elbo: 28589.365253906253 | train_rmse: 0.347 | val_rmse: 0.4116 | val_ll: -0.5956
[2:41:00.917092] epoch: 4850 | elbo: 28504.323828125 | train_rmse: 0.3472 | val_rmse: 0.4113 | val_ll: -0.5966
[2:42:41.805504] epoch: 4900 | elbo: 28524.331328124994 | train_rmse: 0.347 | val_rmse: 0.4107 | val_ll: -0.5929
[2:44:22.064500] epoch: 4950 | elbo: 28491.248906250003 | train_rmse: 0.3487 | val_rmse: 0.4119 | val_ll: -0.5957
[2:46:04.991437] epoch: 5000 | elbo: 28378.18642578125 | train_rmse: 0.3454 | val_rmse: 0.4094 | val_ll: -0.5923
[2:47:47.492026] epoch: 5050 | elbo: 28389.38689453125 | train_rmse: 0.3452 | val_rmse: 0.4103 | val_ll: -0.5936
[2:49:29.653284] epoch: 5100 | elbo: 28284.484902343753 | train_rmse: 0.3455 | val_rmse: 0.4108 | val_ll: -0.5945
[2:51:10.568960] epoch: 5150 | elbo: 28412.677363281247 | train_rmse: 0.346 | val_rmse: 0.4102 | val_ll: -0.5901
[2:52:52.700805] epoch: 5200 | elbo: 28322.84251953125 | train_rmse: 0.3458 | val_rmse: 0.4105 | val_ll: -0.5906
[2:54:34.662066] epoch: 5250 | elbo: 28279.7740234375 | train_rmse: 0.3453 | val_rmse: 0.4119 | val_ll: -0.5958
[2:56:16.403500] epoch: 5300 | elbo: 28315.035312500004 | train_rmse: 0.3449 | val_rmse: 0.4093 | val_ll: -0.5877
[2:57:57.890865] epoch: 5350 | elbo: 28282.189101562497 | train_rmse: 0.345 | val_rmse: 0.4095 | val_ll: -0.5907
[2:59:38.183715] epoch: 5400 | elbo: 28225.576484375004 | train_rmse: 0.3447 | val_rmse: 0.4104 | val_ll: -0.5919
[3:01:16.145892] epoch: 5450 | elbo: 28307.4936328125 | train_rmse: 0.3448 | val_rmse: 0.4098 | val_ll: -0.5904
[3:02:55.296098] epoch: 5500 | elbo: 28305.03203125 | train_rmse: 0.3458 | val_rmse: 0.4114 | val_ll: -0.5903
[3:04:32.902370] epoch: 5550 | elbo: 28211.55779296875 | train_rmse: 0.3433 | val_rmse: 0.4093 | val_ll: -0.5894
[3:06:10.316292] epoch: 5600 | elbo: 28202.37134765625 | train_rmse: 0.3441 | val_rmse: 0.4087 | val_ll: -0.5879
[3:07:48.750126] epoch: 5650 | elbo: 28208.6379296875 | train_rmse: 0.345 | val_rmse: 0.4104 | val_ll: -0.5873
[3:09:26.463530] epoch: 5700 | elbo: 28210.022402343755 | train_rmse: 0.3449 | val_rmse: 0.4101 | val_ll: -0.5916
[3:11:03.468036] epoch: 5750 | elbo: 28131.7433984375 | train_rmse: 0.3442 | val_rmse: 0.4091 | val_ll: -0.5855
[3:12:40.918305] epoch: 5800 | elbo: 28095.94673828125 | train_rmse: 0.345 | val_rmse: 0.4106 | val_ll: -0.5887
[3:14:18.428511] epoch: 5850 | elbo: 28169.135859374997 | train_rmse: 0.3438 | val_rmse: 0.4093 | val_ll: -0.5886
[3:15:55.624875] epoch: 5900 | elbo: 28362.4084375 | train_rmse: 0.3434 | val_rmse: 0.4099 | val_ll: -0.5868
[3:17:32.394487] epoch: 5950 | elbo: 28111.222578124998 | train_rmse: 0.3429 | val_rmse: 0.4088 | val_ll: -0.5844
[3:19:08.516684] epoch: 6000 | elbo: 28010.177089843746 | train_rmse: 0.3436 | val_rmse: 0.4093 | val_ll: -0.5844
[3:20:46.386629] epoch: 6050 | elbo: 28126.65970703125 | train_rmse: 0.3432 | val_rmse: 0.4097 | val_ll: -0.5836
[3:22:23.617791] epoch: 6100 | elbo: 27993.569492187504 | train_rmse: 0.3433 | val_rmse: 0.4096 | val_ll: -0.5897
[3:24:00.792861] epoch: 6150 | elbo: 28019.994472656246 | train_rmse: 0.3434 | val_rmse: 0.4085 | val_ll: -0.5851
[3:25:38.566512] epoch: 6200 | elbo: 28003.823203124997 | train_rmse: 0.3423 | val_rmse: 0.4085 | val_ll: -0.5826
[3:27:17.919752] epoch: 6250 | elbo: 28021.33447265625 | train_rmse: 0.3424 | val_rmse: 0.4091 | val_ll: -0.5822
[3:28:59.934598] epoch: 6300 | elbo: 27950.140625000007 | train_rmse: 0.3421 | val_rmse: 0.4087 | val_ll: -0.5848
[3:30:40.837217] epoch: 6350 | elbo: 27946.2116015625 | train_rmse: 0.3434 | val_rmse: 0.4097 | val_ll: -0.5807
[3:32:22.581828] epoch: 6400 | elbo: 27889.296132812502 | train_rmse: 0.3429 | val_rmse: 0.4079 | val_ll: -0.5845
[3:33:59.955023] epoch: 6450 | elbo: 27919.88765625 | train_rmse: 0.3421 | val_rmse: 0.4073 | val_ll: -0.5815
[3:35:37.777824] epoch: 6500 | elbo: 27823.36357421875 | train_rmse: 0.3435 | val_rmse: 0.4092 | val_ll: -0.5816
[3:37:13.941298] epoch: 6550 | elbo: 27934.269199218747 | train_rmse: 0.3425 | val_rmse: 0.4082 | val_ll: -0.5817
[3:38:50.377588] epoch: 6600 | elbo: 27922.727519531247 | train_rmse: 0.3443 | val_rmse: 0.411 | val_ll: -0.5822
[3:40:27.353955] epoch: 6650 | elbo: 27787.896464843758 | train_rmse: 0.3416 | val_rmse: 0.4076 | val_ll: -0.579
[3:42:05.617036] epoch: 6700 | elbo: 27879.801191406248 | train_rmse: 0.342 | val_rmse: 0.4086 | val_ll: -0.5813
[3:43:43.396821] epoch: 6750 | elbo: 27813.66744140625 | train_rmse: 0.342 | val_rmse: 0.4088 | val_ll: -0.5769
[3:45:21.088661] epoch: 6800 | elbo: 27724.27939453125 | train_rmse: 0.3408 | val_rmse: 0.4083 | val_ll: -0.5781
[3:46:58.955867] epoch: 6850 | elbo: 27733.720078124996 | train_rmse: 0.341 | val_rmse: 0.4073 | val_ll: -0.5771
[3:48:36.830971] epoch: 6900 | elbo: 27767.27 | train_rmse: 0.3419 | val_rmse: 0.4073 | val_ll: -0.5764
[3:50:20.176268] epoch: 6950 | elbo: 27857.567324218748 | train_rmse: 0.341 | val_rmse: 0.408 | val_ll: -0.5711
[3:52:02.150224] epoch: 7000 | elbo: 27801.259824218752 | train_rmse: 0.3416 | val_rmse: 0.4075 | val_ll: -0.5719
[3:53:46.298311] epoch: 7050 | elbo: 27655.851914062503 | train_rmse: 0.3411 | val_rmse: 0.4077 | val_ll: -0.5799
[3:55:29.055668] epoch: 7100 | elbo: 27808.67080078125 | train_rmse: 0.341 | val_rmse: 0.4082 | val_ll: -0.5741
[3:57:12.392529] epoch: 7150 | elbo: 27627.564335937503 | train_rmse: 0.3408 | val_rmse: 0.4068 | val_ll: -0.5729
[3:58:54.979636] epoch: 7200 | elbo: 27860.80185546875 | train_rmse: 0.3413 | val_rmse: 0.4064 | val_ll: -0.5719
[4:00:36.376040] epoch: 7250 | elbo: 27693.553046875 | train_rmse: 0.3404 | val_rmse: 0.4079 | val_ll: -0.5755
[4:02:17.946922] epoch: 7300 | elbo: 27707.09015625 | train_rmse: 0.3403 | val_rmse: 0.4056 | val_ll: -0.5676
[4:04:00.969733] epoch: 7350 | elbo: 27602.261679687497 | train_rmse: 0.3399 | val_rmse: 0.4068 | val_ll: -0.5728
[4:05:39.720392] epoch: 7400 | elbo: 27610.017539062497 | train_rmse: 0.3399 | val_rmse: 0.4071 | val_ll: -0.5724
[4:07:18.640068] epoch: 7450 | elbo: 27647.34599609375 | train_rmse: 0.3421 | val_rmse: 0.4087 | val_ll: -0.571
[4:08:57.930687] epoch: 7500 | elbo: 27487.695546875002 | train_rmse: 0.3401 | val_rmse: 0.4067 | val_ll: -0.5704
[4:10:35.195982] epoch: 7550 | elbo: 27499.181269531247 | train_rmse: 0.341 | val_rmse: 0.4072 | val_ll: -0.5708
[4:12:15.795362] epoch: 7600 | elbo: 27472.398125 | train_rmse: 0.3403 | val_rmse: 0.4052 | val_ll: -0.5692
[4:13:59.111606] epoch: 7650 | elbo: 27491.36865234375 | train_rmse: 0.3398 | val_rmse: 0.4062 | val_ll: -0.5668
[4:15:36.249032] epoch: 7700 | elbo: 27436.600625000003 | train_rmse: 0.3397 | val_rmse: 0.4065 | val_ll: -0.5691
[4:17:12.531174] epoch: 7750 | elbo: 27505.997148437502 | train_rmse: 0.3434 | val_rmse: 0.4088 | val_ll: -0.5733
[4:18:48.029798] epoch: 7800 | elbo: 27447.8176953125 | train_rmse: 0.3389 | val_rmse: 0.4056 | val_ll: -0.5663
[4:20:24.364365] epoch: 7850 | elbo: 27411.30171875 | train_rmse: 0.3399 | val_rmse: 0.4063 | val_ll: -0.5655
[4:22:03.995033] epoch: 7900 | elbo: 27498.32490234375 | train_rmse: 0.3387 | val_rmse: 0.4047 | val_ll: -0.5641
[4:23:45.453876] epoch: 7950 | elbo: 27391.1682421875 | train_rmse: 0.3395 | val_rmse: 0.4063 | val_ll: -0.5681
[4:25:25.645637] epoch: 8000 | elbo: 27415.023593749997 | train_rmse: 0.338 | val_rmse: 0.4047 | val_ll: -0.5673
[4:27:05.454130] epoch: 8050 | elbo: 27441.279765625 | train_rmse: 0.3394 | val_rmse: 0.4052 | val_ll: -0.5659
[4:28:46.183445] epoch: 8100 | elbo: 27312.625820312496 | train_rmse: 0.3383 | val_rmse: 0.4049 | val_ll: -0.567
[4:30:27.210425] epoch: 8150 | elbo: 27411.289804687498 | train_rmse: 0.3392 | val_rmse: 0.4057 | val_ll: -0.5688
[4:32:08.574506] epoch: 8200 | elbo: 27289.34640625 | train_rmse: 0.3385 | val_rmse: 0.4048 | val_ll: -0.5642
[4:33:50.449012] epoch: 8250 | elbo: 27455.1634765625 | train_rmse: 0.338 | val_rmse: 0.4039 | val_ll: -0.5671
[4:35:33.168135] epoch: 8300 | elbo: 27269.17412109375 | train_rmse: 0.3386 | val_rmse: 0.4047 | val_ll: -0.5649
[4:37:15.173598] epoch: 8350 | elbo: 27222.14697265625 | train_rmse: 0.3382 | val_rmse: 0.4052 | val_ll: -0.5636
[4:38:57.832989] epoch: 8400 | elbo: 27227.53779296875 | train_rmse: 0.3395 | val_rmse: 0.4056 | val_ll: -0.565
[4:40:39.803530] epoch: 8450 | elbo: 27272.86806640625 | train_rmse: 0.3382 | val_rmse: 0.4042 | val_ll: -0.5628
[4:42:21.602667] epoch: 8500 | elbo: 27189.4602734375 | train_rmse: 0.339 | val_rmse: 0.4053 | val_ll: -0.5638
[4:44:03.182517] epoch: 8550 | elbo: 27252.981503906252 | train_rmse: 0.338 | val_rmse: 0.4044 | val_ll: -0.5629
[4:45:43.497596] epoch: 8600 | elbo: 27136.09716796875 | train_rmse: 0.3374 | val_rmse: 0.4044 | val_ll: -0.5645
[4:47:25.260706] epoch: 8650 | elbo: 27189.334707031252 | train_rmse: 0.3393 | val_rmse: 0.4062 | val_ll: -0.5704
[4:49:05.384223] epoch: 8700 | elbo: 27150.266210937494 | train_rmse: 0.3373 | val_rmse: 0.4043 | val_ll: -0.5635
[4:50:45.762026] epoch: 8750 | elbo: 27160.046230468746 | train_rmse: 0.3376 | val_rmse: 0.4038 | val_ll: -0.5671
[4:52:26.185216] epoch: 8800 | elbo: 27166.203554687498 | train_rmse: 0.3378 | val_rmse: 0.405 | val_ll: -0.5622
[4:54:06.597324] epoch: 8850 | elbo: 27081.371796875 | train_rmse: 0.3373 | val_rmse: 0.404 | val_ll: -0.5642
[4:55:47.131873] epoch: 8900 | elbo: 27241.285976562503 | train_rmse: 0.3374 | val_rmse: 0.4043 | val_ll: -0.5644
[4:57:29.177682] epoch: 8950 | elbo: 27024.81181640625 | train_rmse: 0.338 | val_rmse: 0.4039 | val_ll: -0.5657
[4:59:11.264174] epoch: 9000 | elbo: 27055.421464843752 | train_rmse: 0.337 | val_rmse: 0.4031 | val_ll: -0.5654
[5:00:52.573209] epoch: 9050 | elbo: 27042.780410156247 | train_rmse: 0.3371 | val_rmse: 0.4038 | val_ll: -0.5594
[5:02:32.858395] epoch: 9100 | elbo: 27098.55095703125 | train_rmse: 0.3384 | val_rmse: 0.4057 | val_ll: -0.5607
[5:04:13.285880] epoch: 9150 | elbo: 27119.655781250003 | train_rmse: 0.337 | val_rmse: 0.4037 | val_ll: -0.5631
[5:05:53.061526] epoch: 9200 | elbo: 26989.943437500006 | train_rmse: 0.3367 | val_rmse: 0.4034 | val_ll: -0.5586
[5:07:33.333944] epoch: 9250 | elbo: 26961.92966796875 | train_rmse: 0.3365 | val_rmse: 0.4042 | val_ll: -0.5616
[5:09:13.190501] epoch: 9300 | elbo: 26948.948710937497 | train_rmse: 0.3372 | val_rmse: 0.4042 | val_ll: -0.5603
[5:10:54.181676] epoch: 9350 | elbo: 26896.135859374997 | train_rmse: 0.3366 | val_rmse: 0.4031 | val_ll: -0.5604
[5:12:36.251297] epoch: 9400 | elbo: 27125.23365234375 | train_rmse: 0.3367 | val_rmse: 0.4036 | val_ll: -0.5605
[5:14:18.056229] epoch: 9450 | elbo: 27109.0193359375 | train_rmse: 0.339 | val_rmse: 0.4064 | val_ll: -0.5611
[5:15:59.756486] epoch: 9500 | elbo: 26849.61015625 | train_rmse: 0.3366 | val_rmse: 0.4034 | val_ll: -0.5571
[5:17:41.428650] epoch: 9550 | elbo: 27130.7807421875 | train_rmse: 0.3369 | val_rmse: 0.4032 | val_ll: -0.5573
[5:19:22.994833] epoch: 9600 | elbo: 26898.582519531243 | train_rmse: 0.3393 | val_rmse: 0.4055 | val_ll: -0.5652
[5:21:04.977159] epoch: 9650 | elbo: 26852.7148046875 | train_rmse: 0.337 | val_rmse: 0.4032 | val_ll: -0.5595
[5:22:46.280457] epoch: 9700 | elbo: 26948.640449218743 | train_rmse: 0.3373 | val_rmse: 0.4027 | val_ll: -0.558
[5:24:27.597037] epoch: 9750 | elbo: 26828.951816406254 | train_rmse: 0.3365 | val_rmse: 0.4028 | val_ll: -0.5599
[5:26:09.059473] epoch: 9800 | elbo: 27078.98197265625 | train_rmse: 0.3374 | val_rmse: 0.4041 | val_ll: -0.5586
[5:27:50.952527] epoch: 9850 | elbo: 26847.195917968747 | train_rmse: 0.3361 | val_rmse: 0.4032 | val_ll: -0.5585
[5:29:32.697218] epoch: 9900 | elbo: 26778.478769531248 | train_rmse: 0.3375 | val_rmse: 0.4048 | val_ll: -0.5592
[5:31:13.992555] epoch: 9950 | elbo: 26807.12208984375 | train_rmse: 0.336 | val_rmse: 0.4022 | val_ll: -0.556
Training finished in 5:32:53.105026 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-sl/checkpoint_2.pt
File Size is 4.060103416442871 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:3 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0070,  0.0132,  0.0057,  ..., -0.0089, -0.0032,  0.0033],
        [-0.0050,  0.0017,  0.0110,  ..., -0.0041,  0.0006, -0.0077],
        [-0.0151,  0.0098,  0.0046,  ..., -0.0087, -0.0027, -0.0012],
        ...,
        [-0.0043,  0.0018,  0.0140,  ..., -0.0025,  0.0013, -0.0084],
        [-0.0043,  0.0037,  0.0052,  ..., -0.0099, -0.0048,  0.0030],
        [-0.0132,  0.0007,  0.0074,  ..., -0.0035, -0.0048, -0.0081]],
       device='cuda:3', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.2285, 0.2214, 0.2219,  ..., 0.2205, 0.2216, 0.2283],
        [0.2281, 0.2213, 0.2217,  ..., 0.2203, 0.2202, 0.2269],
        [0.2272, 0.2216, 0.2213,  ..., 0.2214, 0.2205, 0.2255],
        ...,
        [0.2281, 0.2220, 0.2213,  ..., 0.2209, 0.2199, 0.2296],
        [0.2289, 0.2217, 0.2221,  ..., 0.2206, 0.2207, 0.2281],
        [0.2287, 0.2233, 0.2228,  ..., 0.2218, 0.2212, 0.2273]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-2.9771, -2.9786, -2.9788, -2.9726, -2.9763, -2.9747, -2.9741, -1.9086,
        -2.9764, -2.9779, -2.9778, -2.9780, -2.9797, -2.9762, -2.9751, -2.9796,
        -2.9439, -2.9786, -2.9768, -1.4351, -2.9781, -1.5738, -2.9782, -2.9747,
        -2.9764, -2.9780, -2.9773, -2.9756, -2.9734, -2.9756, -2.9747, -2.9770,
        -2.9771, -2.9757, -2.0522, -2.9773, -2.9772, -2.9798, -2.7123, -2.9761,
        -2.9786, -2.9779, -2.9770, -2.9799, -2.9790, -2.9769, -2.9752, -2.9768,
        -2.9778, -2.9765, -2.9743, -2.3863, -2.9764, -2.4544, -2.9780, -0.5470,
        -2.9741, -2.9776, -2.9702, -2.2205, -3.0072, -2.9766, -2.9748, -2.9764,
        -2.9831, -2.9707, -2.9764, -2.9792, -0.7924, -2.9782, -2.9778, -2.9764,
        -2.9781,  8.5685, -2.9781, -2.9768, -2.9776, -2.9789, -2.9799, -2.9810,
        -2.9793, -2.9776, -1.5016, -2.9655, -2.9825, -2.9757, -2.9751, -2.9790,
        -2.9762, -2.9753, -2.9780, -2.9767, -1.6161, -2.9762, -2.9809, -2.9757,
        -2.9732, -2.9774, -2.9798, -2.9764, -2.1685, -2.9816, -2.4485, -2.9778,
        -1.9175, -2.9722, -2.9808, -2.9779, -2.9788, -2.9796, -2.9795, -2.9794,
        -2.9756, -2.9805, -2.9803, -0.4825, -2.9341, -0.6261, -1.5471, -0.8550,
        -2.9767, -2.9782, -2.8522, -2.9788, -2.9830, -2.9749, -2.9781, -2.9812,
        -1.7809,  2.4616, -2.9780, -2.9813, -2.9755, -2.9755, -3.1043, -2.9774,
        -2.9797, -2.9806, -2.9540, -2.9769, -2.9760,  5.4965, -2.9792, -2.9814,
        -2.9764, -2.9802, -2.9792, -2.9726, -2.9762, -2.9772, -2.9793, -2.9807,
        -2.9741, -2.9760, -2.0517, -2.9764, -2.9760, -2.9799, -2.9750, -1.5207,
        -2.9783, -2.9767, -2.9747, -2.9766, -2.9797, -2.9777, -2.9786, -2.9759,
        -2.7917, -2.9750, -2.9800, -2.9734, -2.9761, -2.9771, -2.9761,  4.4680,
        -2.9740, -2.9727, -2.9762, -1.8122, -2.9771, -2.9775, -2.9790, -2.9804,
        -2.9773, -2.9794, -2.9788, -2.9803, -2.9801, -2.9752, -2.8883, -2.9123,
        -2.9785, -2.9127, -2.9803, -2.9769, -2.9805, -2.9755, -2.9790, -2.9801,
        -2.9791, -2.9782, -2.9768, -2.9728, -2.9776, -2.9727, -2.9770, -2.9753,
        -2.9510, -2.9815,  3.2239, -3.0560, -2.9754, -2.9750, -2.9788, -2.9755,
        -2.9791, -2.9763, -2.9757, -2.9693, -2.9732, -2.9816, -2.9789, -2.9805,
         4.0092, -2.9771,  0.9041, -2.9777, -2.9763, -2.9759, -2.9780, -2.1856,
        -2.9779, -2.9794, -2.9798, -0.6045, -2.9799, -0.4668,  0.7217, -2.9817,
        -2.8779, -2.8590, -2.9829, -1.5564, -2.4470, -2.9785, -2.9737,  2.4358,
        -2.9785, -0.6729, -2.9710, -2.9790, -2.9752, -2.9295, -3.3567, -2.9795,
        -2.9772, -2.9810, -2.9795, -2.9784, -2.9801, -2.9754, -2.9749, -1.6795,
        -2.9784, -2.9778, -2.9502, -2.9775, -2.9776, -2.9728, -2.9775, -2.9795,
        -2.9776, -0.3838, -2.9788, -2.9766, -2.9762, -2.9790, -2.9525, -2.9806,
        -2.8835, -2.9585, -1.5681, -2.9773, -2.9820, -2.9784, -1.9933, -2.9718,
        -2.9782, -3.0198, -2.9754, -2.9397, -2.9792, -2.9777, -2.9755, -2.9780,
        -2.9629, -2.9755, -2.9798, -2.9531, -2.9783, -2.9784, -2.9778, -2.9635,
        -2.9759, -2.9789, -0.4971, -2.9775, -2.9754, -2.9750, -3.2529, -2.9748,
        -2.9777, -2.9762, -2.9763, -3.0886, -2.9545, -2.5142, -2.9776, -2.9779,
        -1.7900, -2.9762, -1.5232, -2.9794, -2.9740, -2.9716, -2.9779, -2.9750,
        -2.9757, -2.9802, -2.9784, -2.9752, -2.2677, -2.9789, -2.9750, -2.0074,
        -2.8774,  3.7892, -2.9797, -2.9756, -2.9791, -2.9761, -2.9775, -2.9751,
        -2.9787, -2.0469, -2.9783, -2.9788, -2.9745, -2.9796, -2.9787, -2.9741,
        -2.9745, -2.9778, -2.9813, -2.9811,  5.0375, -2.8502, -2.1145, -2.9731,
        -2.9770, -3.0186, -2.9812, -1.9688, -3.1789,  1.1238, -2.9742, -2.9784,
        -2.9785, -2.9782, -2.9800, -2.9772, -2.9787, -2.9710, -2.9789, -2.9759,
        -2.9745, -2.9781, -2.9794, -2.9819, -2.9781, -2.9776, -2.9759, -2.9776,
        -2.9816, -2.9794, -2.9736, -2.9789, -2.6869, -2.9799, -2.9771, -2.9775,
        -2.9754,  0.1255, -2.9784, -1.3786, -2.9758, -0.6101, -2.9766, -2.9757,
        -2.9789, -2.7792, -2.9799,  0.3525, -2.9807, -2.9752, -2.9780, -2.9766,
        -2.9787, -2.9768, -2.9787, -2.9739, -2.9743, -2.9758, -2.9756, -2.7202,
        -2.9767, -1.6257, -0.6048, -0.6198, -3.1929, -2.9773, -2.9772, -2.9765,
        -2.9792, -2.9759, -2.9772, -3.1820, -2.9747, -2.9714, -2.9809, -2.9749,
        -2.9729, -2.9724, -2.9775, -2.9746, -2.8344, -2.9828, -2.3953, -2.9791,
        -2.9752, -2.9764, -2.9805, -2.9793, -2.9790, -2.9827, -2.9795, -2.9685,
        -2.9761, -2.9762, -2.9783, -2.9804, -2.9789, -2.9762, -2.8936, -2.9596,
        -0.5930, -2.9800, -2.9802, -2.9786, -2.9746, -1.6220, -2.9790, -2.9792,
        -2.9743, -2.9773, -2.9780, -2.9805, -2.9772, -2.9755, -2.9780, -2.9784,
        -2.9716, -2.9687, -2.9809, -1.4086, -2.8172, -2.9792, -2.9788, -2.3921,
        -2.9797, -2.9771, -2.9830, -2.9655,  1.1565, -2.9440,  6.3333, -2.9687,
        -2.9794, -2.9779, -2.9755, -2.9739, -2.9820, -2.9797, -2.9775, -2.9765,
        -2.9798, -2.9579, -2.9776, -2.9811, -2.9825, -2.9792, -2.8978, -2.9784,
        -2.9786, -2.9796, -2.7731, -2.9790, -2.9568, -2.9784, -2.9780, -2.9780],
       device='cuda:3', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.2937, 0.2932, 0.2930, 0.2949, 0.2941, 0.2945, 0.2946, 0.0122, 0.2938,
        0.2935, 0.2938, 0.2929, 0.2933, 0.2944, 0.2947, 0.2929, 0.2918, 0.2936,
        0.2940, 0.0048, 0.2933, 0.0153, 0.2935, 0.2944, 0.2941, 0.2937, 0.2937,
        0.2938, 0.2956, 0.2936, 0.2939, 0.2939, 0.2935, 0.2938, 0.0179, 0.2936,
        0.2934, 0.2927, 0.2726, 0.2937, 0.2932, 0.2932, 0.2940, 0.2930, 0.2938,
        0.2938, 0.2944, 0.2940, 0.2939, 0.2936, 0.2944, 0.2467, 0.2939, 0.0057,
        0.2940, 0.0066, 0.2941, 0.2934, 0.2935, 0.0128, 0.0082, 0.2941, 0.2936,
        0.2937, 0.2925, 0.2937, 0.2940, 0.2929, 0.0063, 0.2928, 0.2938, 0.2937,
        0.2937, 0.0172, 0.2933, 0.2938, 0.2937, 0.2934, 0.2938, 0.2931, 0.2938,
        0.2939, 0.0082, 0.2922, 0.2917, 0.2943, 0.2939, 0.2936, 0.2942, 0.2936,
        0.2930, 0.2937, 0.0154, 0.2936, 0.2936, 0.2944, 0.2950, 0.2943, 0.2935,
        0.2937, 0.0135, 0.2925, 0.0184, 0.2935, 0.0170, 0.0069, 0.2929, 0.2931,
        0.2934, 0.2934, 0.2935, 0.2934, 0.2945, 0.2934, 0.2926, 0.0073, 0.0056,
        0.0067, 0.0095, 0.0061, 0.2940, 0.2936, 0.0116, 0.2928, 0.2924, 0.2940,
        0.2943, 0.2930, 0.0124, 0.0059, 0.2934, 0.2923, 0.2944, 0.2944, 0.0058,
        0.2938, 0.2931, 0.2930, 0.2906, 0.2944, 0.2939, 0.0164, 0.2930, 0.2929,
        0.2939, 0.2930, 0.2931, 0.2933, 0.2946, 0.2942, 0.2931, 0.2931, 0.2946,
        0.2948, 0.0088, 0.2936, 0.2943, 0.2934, 0.2944, 0.0157, 0.2935, 0.2946,
        0.2939, 0.2936, 0.2933, 0.2935, 0.2942, 0.2946, 0.0132, 0.2947, 0.2938,
        0.2946, 0.2937, 0.2944, 0.2940, 0.0158, 0.2947, 0.2944, 0.2940, 0.0635,
        0.2940, 0.2941, 0.2931, 0.2934, 0.2944, 0.2939, 0.2930, 0.2933, 0.2931,
        0.2943, 0.2865, 0.2891, 0.2937, 0.0098, 0.2929, 0.2941, 0.2931, 0.2933,
        0.2936, 0.2925, 0.2936, 0.2934, 0.2940, 0.2940, 0.2937, 0.2930, 0.2937,
        0.2940, 0.2919, 0.2927, 0.0539, 0.0057, 0.2946, 0.2943, 0.2930, 0.2938,
        0.2936, 0.2945, 0.2947, 0.2944, 0.2941, 0.2927, 0.2930, 0.2930, 0.0265,
        0.2936, 0.0076, 0.2939, 0.2936, 0.2945, 0.2937, 0.0094, 0.2938, 0.2935,
        0.2931, 0.0060, 0.2932, 0.0102, 0.0034, 0.2930, 0.2853, 0.2844, 0.2927,
        0.0082, 0.2513, 0.2930, 0.2944, 0.0024, 0.2929, 0.0070, 0.2921, 0.2928,
        0.2945, 0.2882, 0.0060, 0.2929, 0.2938, 0.2927, 0.2939, 0.2934, 0.2933,
        0.2930, 0.2943, 0.0194, 0.2936, 0.2939, 0.2914, 0.2938, 0.2940, 0.2949,
        0.2939, 0.2934, 0.2937, 0.0067, 0.2934, 0.2932, 0.2938, 0.2934, 0.2913,
        0.2937, 0.0075, 0.2931, 0.0093, 0.2937, 0.2919, 0.2934, 0.0403, 0.2935,
        0.2936, 0.0124, 0.2941, 0.2912, 0.2937, 0.2938, 0.2940, 0.2939, 0.2922,
        0.2936, 0.2931, 0.2932, 0.2931, 0.2938, 0.2938, 0.2917, 0.2943, 0.2935,
        0.0055, 0.2929, 0.2940, 0.2939, 0.0062, 0.2941, 0.2935, 0.2946, 0.2948,
        0.0077, 0.2917, 0.2575, 0.2936, 0.2936, 0.0094, 0.2938, 0.0085, 0.2931,
        0.2945, 0.2952, 0.2936, 0.2944, 0.2943, 0.2935, 0.2933, 0.2943, 0.0057,
        0.2932, 0.2945, 0.0129, 0.2865, 0.0375, 0.2933, 0.2945, 0.2933, 0.2938,
        0.2942, 0.2943, 0.2939, 0.0123, 0.2939, 0.2931, 0.2951, 0.2932, 0.2930,
        0.2945, 0.2937, 0.2935, 0.2928, 0.2930, 0.0396, 0.0155, 0.2265, 0.2941,
        0.2943, 0.0075, 0.2924, 0.0286, 0.0075, 0.0033, 0.2944, 0.2932, 0.2929,
        0.2936, 0.2928, 0.2941, 0.2939, 0.2958, 0.2929, 0.2938, 0.0087, 0.2935,
        0.2937, 0.2922, 0.2933, 0.2936, 0.2932, 0.2937, 0.2933, 0.2935, 0.2941,
        0.2932, 0.2718, 0.2929, 0.2939, 0.2940, 0.2941, 0.0062, 0.2939, 0.0077,
        0.2944, 0.0054, 0.2935, 0.2939, 0.2928, 0.0129, 0.2931, 0.0051, 0.2936,
        0.2946, 0.2934, 0.2932, 0.2936, 0.2943, 0.2940, 0.2947, 0.2942, 0.2939,
        0.2946, 0.2725, 0.2935, 0.0076, 0.0062, 0.0061, 0.0079, 0.2934, 0.2937,
        0.2944, 0.2934, 0.2931, 0.2934, 0.0077, 0.2943, 0.2947, 0.2931, 0.2940,
        0.2941, 0.2946, 0.2936, 0.2950, 0.2829, 0.2919, 0.0053, 0.2929, 0.2941,
        0.2942, 0.2932, 0.2938, 0.2936, 0.2926, 0.2932, 0.0061, 0.2940, 0.2938,
        0.2932, 0.2926, 0.2934, 0.2938, 0.0103, 0.0070, 0.0062, 0.2929, 0.2931,
        0.2929, 0.2949, 0.0089, 0.2935, 0.2940, 0.2941, 0.2941, 0.2933, 0.2931,
        0.2936, 0.2943, 0.2936, 0.2947, 0.2924, 0.2925, 0.2934, 0.0089, 0.2808,
        0.2934, 0.2939, 0.2196, 0.2929, 0.2944, 0.2925, 0.2942, 0.0031, 0.2922,
        0.0143, 0.2930, 0.2931, 0.2936, 0.2943, 0.2945, 0.2927, 0.2936, 0.2941,
        0.2938, 0.2928, 0.2922, 0.2939, 0.2925, 0.2925, 0.2932, 0.2878, 0.2934,
        0.2935, 0.2927, 0.0124, 0.2941, 0.2915, 0.2938, 0.2942, 0.2937],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-2.5842e-05,  6.6877e-33, -1.8402e-16,  ..., -3.6859e-05,
         -3.0262e-06,  1.0701e-05],
        [ 8.0765e-07,  9.1146e-17,  1.6493e-09,  ..., -9.0077e-17,
         -6.8118e-04, -1.1141e-11],
        [-1.3332e-08, -1.3125e-03, -1.3519e-14,  ..., -6.0141e-21,
          1.9211e-13, -2.6815e-10],
        ...,
        [-1.0132e-07, -2.5214e-05,  2.6607e-21,  ..., -1.7920e-20,
          4.5876e-10, -1.9033e-09],
        [-2.8771e-11,  9.4728e-10,  1.3672e-08,  ..., -3.6160e-05,
         -1.3223e-05,  1.5939e-11],
        [-4.1511e-03,  2.2775e-03,  3.6511e-03,  ...,  4.9145e-03,
         -3.0393e-03, -6.3860e-04]], device='cuda:3', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 0.9997, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [0.9935, 0.9961, 0.9974,  ..., 0.9947, 0.9930, 0.9961]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.1685, -0.1690, -0.1680, -0.1663, -0.1685, -0.1684, -0.1663, -0.1671,
        -0.1670, -0.1656, -0.1675, -0.3453, -0.1695, -0.1697, -0.1708, -0.1682,
        -0.2769, -0.1680, -0.1689, -0.1696, -0.1669, -0.1724, -0.1711, -0.1673,
        -0.1663, -0.1674, -0.1700, -0.1684, -0.1699, -0.1681, -0.1705, -0.1678,
        -0.1708, -0.1685, -0.1687, -0.1644, -0.1678, -0.1676, -0.3493, -0.1698,
        -0.1678, -0.1700, -0.1659, -0.1674, -0.1681, -0.1687, -0.1661, -0.1697,
        -0.1723, -0.1668, -0.1685, -0.1684, -0.1666, -0.1657, -0.1664, -0.3588,
        -0.1667, -0.1708, -0.1680, -0.1681, -0.1701, -0.1688, -0.1695, -0.1689,
        -0.1697, -0.1651, -0.1672, -0.1698, -0.1667, -0.1712, -0.1698, -0.1686,
        -0.1659, -0.1668, -0.1692, -0.1668, -0.1701, -0.1674, -0.1682, -0.1717,
        -0.1681, -0.1688, -0.1687, -0.1677, -0.1679, -0.1668, -0.1667, -0.1687,
        -0.1661, -0.1679, -0.1682, -0.1730, -0.1712, -0.1677, -0.1681, -0.1693,
        -0.1682, -0.1662, -0.1664, -0.1663, -0.1708, -0.1669, -0.1672, -0.1723,
        -0.1675, -0.1678, -0.1684, -0.1659, -0.1645, -0.1685, -0.1678, -0.1670,
        -0.1689, -0.1726, -0.1688, -0.1696, -0.1706, -0.1681, -0.1697, -0.1703,
        -0.1689, -0.1678, -0.1670, -0.1691, -0.1680, -0.1669, -0.1653, -0.1682,
        -0.1697, -0.1667, -0.1673, -0.1673, -0.1699, -0.1685, -0.1637, -0.1698,
        -0.1658, -0.1694, -0.1676, -0.1677, -0.1666, -0.1709, -0.1649, -0.1680,
        -0.1675, -0.1680, -0.1686, -0.1670,  0.0080, -0.1715, -0.1639, -0.1688,
        -0.1654, -0.1690, -0.1686, -0.1691, -0.1685, -0.1673, -0.1693, -0.1684,
        -0.1691, -0.1674, -0.1682, -0.1661, -0.1652, -0.1701, -0.1686, -0.1657,
         0.8303, -0.1684, -0.1677, -0.1670, -0.1689, -0.1681, -0.1706, -0.1709,
        -0.1648, -0.1674, -0.1692, -0.1667, -0.1673, -0.1674, -0.1645, -0.1685,
        -0.1671, -0.1675, -0.1710, -0.1672, -0.1719, -0.1685, -0.1700, -0.1663,
        -0.1662, -0.1693, -0.1680, -0.1665, -0.1653, -0.1688, -0.1671, -0.1669,
        -0.1674, -0.1691, -0.1689, -0.3522, -0.1682, -0.1674, -0.1667, -0.1682,
        -0.1676, -0.1694, -0.1724, -0.1690, -0.1706, -0.1683, -0.1732, -0.1651,
        -0.1660, -0.1670, -0.1703, -0.1662, -0.1745, -0.1711, -0.1711, -0.3457,
        -0.1665, -0.1689, -0.1689, -0.1671, -0.1687, -0.1685, -0.1657, -0.1684,
        -0.1654, -0.1694, -0.1708, -0.1687, -0.1665, -0.1694, -0.1686, -0.1701,
        -0.1699, -0.1710, -0.1665, -0.1665, -0.1678, -0.1680, -0.3292, -0.1695,
        -0.1676, -0.1691, -0.1675, -0.1690, -0.2397, -0.1693, -0.1667, -0.1684,
        -0.1701, -0.1709, -0.1703, -0.1656, -0.1714, -0.1731, -0.1707, -0.1688,
         0.0485, -0.1672, -0.1688, -0.4833, -0.1297, -0.1703, -0.1701,  2.9463,
        -0.1709, -0.1687, -0.1676, -0.1685, -0.1690, -0.1705, -0.1708, -0.1661,
        -0.1706, -0.1690, -0.1706, -0.1678, -0.1652, -0.1664, -0.1696, -0.1676,
        -0.1677, -0.1685, -0.1685, -0.1660, -0.1687, -0.1684, -0.1694, -0.1682,
        -0.1684, -0.1709, -0.1700, -0.1667, -0.1714, -0.1696, -0.1651, -0.1696,
        -0.1698, -0.1681, -0.1667, -0.1702, -0.1691, -0.1716, -0.1696, -0.1678,
        -0.1696, -0.1715, -0.1715, -0.1655, -0.1680, -0.1668, -0.1673, -0.1692,
        -0.1671, -0.1697, -0.1685, -0.1665, -0.1686, -0.1686, -0.1724, -0.1709,
        -0.1708, -0.1679, -0.1690, -0.1697, -0.1675, -0.1709, -0.1713, -0.1676,
        -0.1694, -0.1715, -0.1705, -0.1682, -0.1687, -0.1649, -0.1705, -0.1698,
        -0.1711, -0.1701, -0.1699, -0.1692, -0.1714, -0.1678, -0.1659, -0.1684,
        -0.1685, -0.1670, -0.1641, -0.1675, -0.1695, -0.1687, -0.1663, -0.1717,
        -0.1689, -0.1685, -0.1676, -0.1688, -0.1718, -0.1711, -0.1713, -0.1675,
        -0.1697, -0.1678, -0.1705, -0.1682, -0.1707, -0.1686, -0.1700, -0.1656,
        -0.1692, -0.4713,  0.7436, -0.1725, -0.1689, -0.1697, -0.1674, -0.1676,
        -0.1683, -0.1703, -0.1685, -0.1668, -0.1682, -0.1668, -0.1685, -0.1677,
        -0.1680, -0.1687, -0.1688, -0.1722, -0.1682, -0.1673, -0.1688, -0.1692,
        -0.1686, -0.1683, -0.1643, -0.3156, -0.4764, -0.1682, -0.1672, -0.1679,
        -0.1690, -0.1717, -0.1685, -0.1690, -0.1695, -0.1690, -0.1707, -0.1696,
        -0.1693, -0.1701, -0.1639, -0.1678, -0.1671, -0.1710, -0.1700, -0.1682,
        -0.1646,  0.4343, -0.1672, -0.1723, -0.1697, -0.1695, -0.1687, -0.1689,
        -0.1680, -0.1703, -0.1717, -0.1715, -0.1684, -0.1666, -0.1667, -0.1689,
        -0.1677, -0.1703, -0.1661, -0.1691, -0.1709, -0.1686, -0.1703,  0.4030,
        -0.1700, -0.1664, -0.1659, -0.1675, -0.1680, -0.1679, -0.1676, -0.1695,
        -0.1657, -0.1673, -0.1661, -0.1673, -0.1660, -0.1675, -0.1669, -0.1687,
        -0.1671, -0.1694, -0.1676, -0.1708, -0.1690, -0.1649, -0.1666, -0.1688,
        -0.1668, -0.1675, -0.1682, -0.1690, -0.1664, -0.1659, -0.1678, -0.1701,
        -0.1640, -0.1667, -0.1692, -0.1670, -0.1704, -0.1682, -0.1643, -0.1665,
        -0.1691, -0.1698, -0.1688, -0.1689, -0.1691, -0.1673, -0.1679, -0.1698,
        -0.1697, -0.1716, -0.1660, -0.1681, -0.1700, -0.1694, -0.1687, -0.1690,
        -0.1694, -0.1646, -0.1676, -0.1709, -0.1698, -0.1675, -0.1670, -0.3533],
       device='cuda:3', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.9727, 0.9743, 0.9742, 0.9766, 0.9737, 0.9748, 0.9764, 0.9760, 0.9753,
        0.9757, 0.9755, 0.0281, 0.9746, 0.9737, 0.9724, 0.9748, 0.1318, 0.9738,
        0.9741, 0.9743, 0.9752, 0.9718, 0.9739, 0.9749, 0.9766, 0.9760, 0.9734,
        0.9749, 0.9729, 0.9750, 0.9737, 0.9725, 0.9733, 0.9745, 0.9733, 0.9767,
        0.9746, 0.9750, 0.1409, 0.9739, 0.9737, 0.9741, 0.9756, 0.9755, 0.9734,
        0.9732, 0.9759, 0.9737, 0.9722, 0.9755, 0.9742, 0.9736, 0.9745, 0.9770,
        0.9756, 0.0269, 0.9752, 0.9718, 0.9753, 0.9744, 0.9723, 0.9738, 0.9742,
        0.9731, 0.9748, 0.9773, 0.9752, 0.9742, 0.9750, 0.9734, 0.9750, 0.9745,
        0.9765, 0.9771, 0.9739, 0.9753, 0.9732, 0.9746, 0.9756, 0.9736, 0.9754,
        0.9736, 0.9752, 0.9745, 0.9761, 0.9756, 0.9758, 0.9747, 0.9756, 0.9745,
        0.9750, 0.9718, 0.9733, 0.9740, 0.9766, 0.9724, 0.9750, 0.9764, 0.9772,
        0.9753, 0.9736, 0.9762, 0.9741, 0.9723, 0.9741, 0.9759, 0.9743, 0.9761,
        0.9787, 0.9735, 0.9750, 0.9753, 0.9745, 0.9716, 0.9724, 0.9730, 0.9741,
        0.9741, 0.9739, 0.9743, 0.9748, 0.9751, 0.9750, 0.9729, 0.9749, 0.9758,
        0.9753, 0.9745, 0.9747, 0.9767, 0.9758, 0.9758, 0.9727, 0.9755, 0.9768,
        0.9725, 0.9768, 0.9744, 0.9741, 0.9748, 0.9751, 0.9752, 0.9771, 0.9750,
        0.9757, 0.9747, 0.9756, 0.9761, 0.0343, 0.9736, 0.9770, 0.9743, 0.9768,
        0.9738, 0.9751, 0.9733, 0.9748, 0.9742, 0.9744, 0.9750, 0.9737, 0.9753,
        0.9733, 0.9765, 0.9769, 0.9743, 0.9753, 0.9765, 0.0171, 0.9732, 0.9757,
        0.9747, 0.9745, 0.9742, 0.9729, 0.9734, 0.9760, 0.9750, 0.9735, 0.9761,
        0.9744, 0.9748, 0.9765, 0.9743, 0.9756, 0.9750, 0.9730, 0.9758, 0.9729,
        0.9760, 0.9735, 0.9763, 0.9761, 0.9743, 0.9761, 0.9753, 0.9761, 0.9732,
        0.9751, 0.9747, 0.9744, 0.9733, 0.9751, 0.0657, 0.9740, 0.9753, 0.9764,
        0.9734, 0.9740, 0.9749, 0.9723, 0.9745, 0.9726, 0.9754, 0.9762, 0.9759,
        0.9759, 0.9750, 0.9731, 0.9768, 0.9716, 0.9728, 0.9731, 0.0263, 0.9758,
        0.9742, 0.9751, 0.9754, 0.9742, 0.9737, 0.9767, 0.9737, 0.9767, 0.9731,
        0.9738, 0.9748, 0.9742, 0.9747, 0.9737, 0.9734, 0.9749, 0.9734, 0.9765,
        0.9750, 0.9738, 0.9751, 0.0293, 0.9734, 0.9743, 0.9733, 0.9757, 0.9746,
        0.0266, 0.9734, 0.9759, 0.9754, 0.9741, 0.9722, 0.9724, 0.9765, 0.9720,
        0.9717, 0.9743, 0.9738, 0.1743, 0.9760, 0.9750, 0.9580, 0.0353, 0.9741,
        0.9736, 0.2428, 0.9712, 0.9731, 0.9757, 0.9737, 0.9747, 0.9722, 0.9733,
        0.9753, 0.9718, 0.9744, 0.9735, 0.9746, 0.9754, 0.9751, 0.9740, 0.9755,
        0.9761, 0.9742, 0.9755, 0.9763, 0.9730, 0.9746, 0.9746, 0.9750, 0.9749,
        0.9728, 0.9733, 0.9758, 0.9719, 0.9742, 0.9765, 0.9732, 0.9740, 0.9748,
        0.9741, 0.9731, 0.9744, 0.9734, 0.9749, 0.9748, 0.9737, 0.9742, 0.9731,
        0.9748, 0.9752, 0.9763, 0.9752, 0.9734, 0.9740, 0.9727, 0.9757, 0.9763,
        0.9747, 0.9731, 0.9726, 0.9726, 0.9735, 0.9744, 0.9749, 0.9746, 0.9751,
        0.9747, 0.9714, 0.9754, 0.9740, 0.9741, 0.9739, 0.9740, 0.9736, 0.9783,
        0.9725, 0.9743, 0.9742, 0.9733, 0.9719, 0.9738, 0.9726, 0.9766, 0.9762,
        0.9741, 0.9749, 0.9755, 0.9779, 0.9750, 0.9743, 0.9744, 0.9741, 0.9721,
        0.9744, 0.9738, 0.9745, 0.9752, 0.9730, 0.9727, 0.9736, 0.9752, 0.9728,
        0.9754, 0.9736, 0.9738, 0.9727, 0.9746, 0.9735, 0.9747, 0.9738, 0.1194,
        0.0133, 0.9727, 0.9749, 0.9747, 0.9740, 0.9743, 0.9754, 0.9744, 0.9747,
        0.9755, 0.9746, 0.9756, 0.9744, 0.9751, 0.9745, 0.9735, 0.9749, 0.9726,
        0.9746, 0.9757, 0.9745, 0.9738, 0.9757, 0.9750, 0.9774, 0.1007, 0.0842,
        0.9744, 0.9752, 0.9748, 0.9738, 0.9726, 0.9753, 0.9748, 0.9742, 0.9735,
        0.9716, 0.9742, 0.9750, 0.9737, 0.9780, 0.9751, 0.9760, 0.9724, 0.9742,
        0.9742, 0.9778, 0.0276, 0.9734, 0.9722, 0.9724, 0.9733, 0.9730, 0.9735,
        0.9760, 0.9731, 0.9720, 0.9729, 0.9738, 0.9742, 0.9747, 0.9718, 0.9753,
        0.9743, 0.9754, 0.9733, 0.9725, 0.9735, 0.9737, 0.0676, 0.9738, 0.9762,
        0.9756, 0.9759, 0.9747, 0.9754, 0.9749, 0.9738, 0.9768, 0.9747, 0.9762,
        0.9758, 0.9744, 0.9745, 0.9766, 0.9741, 0.9747, 0.9751, 0.9755, 0.9738,
        0.9747, 0.9768, 0.9753, 0.9743, 0.9750, 0.9758, 0.9747, 0.9738, 0.9757,
        0.9767, 0.9748, 0.9729, 0.9763, 0.9751, 0.9740, 0.9764, 0.9734, 0.9754,
        0.9769, 0.9747, 0.9753, 0.9724, 0.9739, 0.9731, 0.9742, 0.9752, 0.9735,
        0.9736, 0.9729, 0.9727, 0.9756, 0.9742, 0.9730, 0.9745, 0.9741, 0.9754,
        0.9733, 0.9758, 0.9751, 0.9743, 0.9749, 0.9752, 0.9759, 0.1283],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[ 1.9125e-06, -3.3271e-06,  1.3488e-08,  ...,  4.5384e-07,
          5.2789e-07, -3.9368e-03],
        [ 5.3091e-08, -8.4078e-45, -2.8026e-45,  ...,  3.9350e-07,
          2.6308e-07, -4.4974e-04],
        [-3.4548e-06, -2.5728e-09, -5.6052e-45,  ..., -2.7232e-07,
          1.9522e-06, -9.9054e-03],
        ...,
        [-2.6367e-08, -3.0908e-07,  1.7271e-07,  ...,  3.9350e-07,
          2.7673e-06, -2.4882e-03],
        [-3.9350e-07,  1.0472e-07, -5.7444e-09,  ...,  1.8401e-07,
          1.4504e-07, -9.1725e-04],
        [-1.8529e-06,  2.4610e-08,  7.3329e-08,  ..., -1.2472e-43,
         -3.2411e-08, -4.5014e-03]], device='cuda:3', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9978],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9939],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9993],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9993],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9989]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.0048, -0.0082, -0.0076, -0.0119, -0.0024, -0.0051, -0.0100, -0.0027,
        -0.0086, -0.0111, -0.0075, -0.0048, -0.0072, -0.0061, -0.0090, -0.0040,
        -0.0027, -0.0053, -0.0588, -0.0042, -0.0080, -0.0274, -0.0077, -0.1270,
        -0.0065, -0.0069, -0.0110, -0.0167, -0.0081, -0.0028, -0.3127, -0.0190,
        -0.0064, -0.0078, -0.0038, -0.0074, -0.0074, -0.0108, -0.0034, -0.0104,
        -0.0117, -0.0131, -0.0070, -0.0065, -0.0098, -0.0074, -0.0050, -0.0083,
        -0.0060, -0.0068, -0.0064, -0.0131, -0.0100, -0.0097, -0.0042, -0.0023,
        -0.0069, -0.0062, -0.0042, -0.0073, -0.0045, -0.0023, -0.0099, -0.0055,
        -0.0086, -0.0087, -0.0049, -0.0062, -0.0095, -0.0104, -0.0036, -0.0058,
        -0.0054, -0.0052, -0.0029, -0.0077, -0.0034, -0.0052, -0.0031, -0.0069,
        -0.0048, -0.0061, -0.0055, -0.0119, -0.0043, -0.0053, -0.0059, -0.0037,
        -0.0071, -0.0050, -0.0072, -0.0098, -0.0080, -0.0104, -0.0026, -0.0037,
        -0.0063, -0.0078, -0.0074, -0.0058, -0.0106, -0.0068, -0.0081, -0.0091,
        -0.0031, -0.0057, -0.0080, -0.0079, -0.0066, -0.0076, -0.0053, -0.0067,
        -0.0469, -0.0075, -0.0033, -0.0079, -0.0031, -0.0093, -0.0092, -0.0039,
        -0.0042, -0.0066, -0.0005, -0.0100, -0.0057, -0.0043, -0.0101, -0.0085,
        -0.0055, -0.0032, -0.0069, -0.0036, -0.0090, -0.0073, -0.0075, -0.0102,
        -0.0102, -0.0056, -0.0028, -0.0052, -0.0053, -0.0121, -0.0088, -0.0056,
        -0.0034, -0.0008, -0.0072, -0.0066, -0.0071, -0.0018, -0.0050, -0.0043,
        -0.0076, -0.0085, -0.0040, -0.0046, -0.0045, -0.0092, -0.0071, -0.0039,
        -0.0071, -0.0054,  0.3298, -0.0038, -0.0035, -0.0081, -0.0072, -0.0058,
        -0.0094, -0.0123, -0.0046, -0.0035, -0.0090, -0.0080, -0.0111, -0.0151,
        -0.0109, -0.0085, -0.0060, -0.0075, -0.0046, -0.0052, -0.0068, -0.0050,
        -0.0006, -0.0114, -0.0082, -0.0065, -0.0083, -0.0025, -0.0053, -0.0029,
        -0.0262, -0.0058, -0.0070, -0.0073, -0.0098, -0.0054, -0.0038, -0.0061,
        -0.0075, -0.0041, -0.0027, -0.0071, -0.0048, -0.0074, -0.0055, -0.0140,
        -0.0039, -0.0011, -0.0050, -0.0027, -0.0045, -0.0102, -0.0061, -0.0056,
        -0.0044,  0.4144, -0.0057, -0.0062, -0.0068, -0.0048, -0.0752, -0.0034,
        -0.0119, -0.0012, -0.0061, -0.0128, -0.0064, -0.0036, -0.0064, -0.0024,
        -0.0065, -0.0055, -0.0075, -0.0044, -0.0054, -0.0022, -0.0068, -0.0074,
        -0.0066, -0.0107, -0.0067, -0.0052, -0.0057, -0.0042, -0.0092, -0.0056,
        -0.0091, -0.0082, -0.0045, -0.0078, -0.0065, -0.0067, -0.0092, -0.0065,
        -0.0049, -0.0115, -0.0055, -0.0068, -0.0037, -0.0093, -0.0049, -0.0034,
        -0.0080, -0.0096, -0.0090, -0.0074, -0.0087, -0.0105, -0.0053, -0.0040,
        -0.0085, -0.0055, -0.0081, -0.0035, -0.0052, -0.0032, -0.0110, -0.0049,
        -0.0081, -0.0032, -0.0097, -0.0122, -0.0143, -0.0049, -0.0100, -0.0158,
        -0.0072, -0.0055, -0.0028, -0.0052, -0.0049, -0.0067, -0.0080, -0.0066,
        -0.0075, -0.0060, -0.0037, -0.0063, -0.0048, -0.0045, -0.0052, -0.0079,
        -0.0051, -0.0078, -0.0044, -0.0032, -0.0054, -0.0091, -0.0140, -0.0071,
        -0.0058, -0.0034, -0.0059, -0.0072, -0.0098, -0.0053, -0.0108, -0.0140,
        -0.0049, -0.0070, -0.0058, -0.0141, -0.0128, -0.0025, -0.0059, -0.0043,
        -0.0071, -0.0076, -0.0065, -0.0049, -0.0049, -0.0044, -0.0126, -0.0067,
        -0.0037, -0.0064, -0.0098, -0.0017, -0.0130, -0.0080, -0.0030, -0.0043,
        -0.0061, -0.0030, -0.0042, -0.0069, -0.0077, -0.0076, -0.0063, -0.0101,
        -0.0048, -0.0052, -0.0047, -0.0060, -0.0042, -0.0069, -0.0091, -0.0077,
        -0.0060, -0.0121, -0.0101, -0.0040, -0.0102, -0.0064, -0.0089, -0.0048,
        -0.0048, -0.0084, -0.0066, -0.0090, -0.0066, -0.0053, -0.0031, -0.0005,
        -0.0047, -0.0066, -0.0011, -0.0080, -0.0009, -0.0085, -0.0037, -0.0049,
        -0.0073, -0.0022, -0.0075, -0.0083, -0.0098, -0.0084, -0.0053, -0.0096,
        -0.0009, -0.0072, -0.0153, -0.0078, -0.0085, -0.0060, -0.0060, -0.0065,
        -0.0069, -0.0070,  0.2031, -0.0085, -0.0136, -0.0060,  0.0556, -0.0032,
        -0.0049, -0.0064, -0.0077, -0.0079, -0.0023, -0.0099, -0.0060, -0.0089,
        -0.0066, -0.0020, -0.0009, -0.0031, -0.0045, -0.0051, -0.0085, -0.0091,
        -0.0047, -0.0048, -0.0089, -0.0110, -0.0097, -0.0054, -0.0023, -0.0057,
        -0.0110, -0.0020, -0.0069, -0.0088, -0.0069, -0.0059, -0.0104, -0.0062,
        -0.0240, -0.0083, -0.0042, -0.0075, -0.0094, -0.0057,  0.0165, -0.0054,
        -0.0054, -0.0026, -0.0150, -0.0024, -0.0056, -0.1633, -0.0073, -0.0040,
        -0.0047, -0.0056, -0.1949, -0.0047, -0.0082, -0.0068, -0.0064, -0.0052,
        -0.0041, -0.0073, -0.0047, -0.0042, -0.0073, -0.0027, -0.0064, -0.0056,
        -0.0049, -0.0051, -0.0051, -0.0075, -0.0031, -0.0094, -0.0040, -0.0110,
        -0.0050, -0.0049, -0.0085, -0.0049, -0.0054, -0.0145, -0.0081, -0.0021,
        -0.0087, -0.0025, -0.0052, -0.0049, -0.0020, -0.0073, -0.0033, -0.0077,
        -0.0066, -0.0022, -0.0057, -0.0083, -0.0059, -0.0073, -0.0039, -0.0062,
        -0.0095, -0.0124, -0.0097, -0.0031, -0.0077, -0.0045, -0.0038, -0.0065],
       device='cuda:3', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([1.0000, 0.9999, 1.0000, 0.9977, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9988, 1.0000, 0.9984, 0.9972, 1.0000, 1.0000, 1.0000, 1.0000,
        0.5015, 1.0000, 1.0000, 0.9020, 1.0000, 0.3408, 0.9995, 1.0000, 0.9998,
        0.9993, 1.0000, 1.0000, 0.2538, 0.9972, 0.9991, 1.0000, 1.0000, 1.0000,
        0.9998, 1.0000, 1.0000, 0.9990, 0.9998, 0.9999, 1.0000, 0.9999, 0.9990,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9996, 0.9982, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 0.9996,
        1.0000, 0.9975, 1.0000, 0.9996, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000,
        0.9999, 1.0000, 0.9998, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9999, 0.9988, 1.0000, 1.0000, 1.0000, 0.9992, 1.0000, 1.0000,
        0.9980, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9994, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0001, 0.4390, 1.0000, 1.0000, 0.9983, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.9988, 1.0000, 0.9979, 1.0000, 1.0000,
        0.9997, 0.9993, 1.0000, 1.0000, 0.9996, 1.0000, 0.9997, 0.9996, 1.0000,
        0.9994, 0.9994, 0.9999, 1.0000, 1.0000, 1.0000, 0.9995, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9999, 0.9975, 0.9995, 1.0000,
        0.9993, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9974, 0.9995, 1.0000,
        0.0766, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9999, 1.0000, 0.9995, 0.9995, 0.9998, 0.9976, 1.0000, 0.9996, 0.9984,
        1.0000, 0.9995, 0.9989, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9986,
        1.0000, 0.9999, 1.0000, 0.2830, 1.0000, 1.0000, 1.0000, 0.9998, 1.0000,
        1.0000, 1.0000, 0.9976, 1.0000, 1.0000, 1.0000, 0.9987, 1.0000, 1.0000,
        0.9982, 1.0000, 1.0000, 1.0000, 1.0000, 1.0001, 0.9996, 1.0000, 1.0000,
        0.9999, 0.2569, 1.0000, 0.9996, 0.9995, 1.0000, 0.2974, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9997, 0.9981,
        1.0000, 1.0000, 1.0000, 0.9980, 1.0000, 0.9988, 1.0000, 1.0000, 0.9991,
        0.9997, 1.0000, 0.9997, 0.9995, 1.0000, 1.0000, 1.0000, 1.0000, 0.9989,
        1.0000, 0.9998, 0.9999, 0.9993, 0.9984, 1.0000, 1.0000, 1.0000, 0.9999,
        1.0000, 1.0000, 1.0000, 1.0000, 0.9981, 1.0000, 1.0000, 1.0001, 0.9998,
        1.0000, 0.9971, 1.0000, 1.0001, 0.9980, 1.0000, 1.0000, 0.9983, 0.9970,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.9985, 0.9991, 0.9999, 1.0000, 0.9997, 1.0000, 1.0000,
        1.0000, 0.9998, 1.0001, 1.0000, 1.0000, 0.9999, 1.0000, 0.9999, 1.0000,
        1.0000, 0.9996, 0.9989, 1.0000, 0.9991, 1.0000, 0.9999, 1.0000, 0.9981,
        0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 0.9997, 1.0000, 0.9999, 1.0000,
        1.0000, 0.9996, 1.0000, 1.0000, 0.9973, 0.9975, 1.0000, 1.0000, 0.9998,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9998, 1.0000,
        1.0000, 1.0000, 1.0000, 0.9991, 0.9999, 1.0000, 1.0000, 0.9999, 0.9993,
        0.9999, 0.9999, 0.9996, 1.0000, 1.0000, 1.0001, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.9981, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9998, 1.0000, 1.0000, 0.9995, 1.0000, 0.9999, 1.0000, 1.0000,
        0.9998, 0.9994, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2886, 1.0000, 0.9976,
        0.9999, 0.7617, 1.0000, 1.0000, 0.9996, 0.9995, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9992, 0.9999, 1.0000, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9981, 1.0000, 1.0000, 1.0000, 0.9999, 0.9993, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.9978, 0.9997, 0.9998, 1.0000, 1.0000, 1.0000, 0.9673,
        1.0000, 1.0000, 1.0000, 0.9995, 0.9998, 0.3905, 1.0000, 1.0000, 1.0000,
        0.9999, 1.0000, 1.0001, 0.3289, 0.9995, 1.0000, 1.0000, 1.0000, 0.0685,
        1.0000, 0.9995, 1.0001, 1.0000, 1.0000, 0.9997, 1.0000, 1.0000, 0.9999,
        1.0000, 1.0000, 0.9999, 1.0000, 0.9996, 1.0000, 1.0000, 0.9969, 1.0000,
        0.9997, 1.0000, 0.9999, 1.0000, 1.0000, 0.9996, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.9994, 0.9989, 1.0000, 0.9999, 1.0000,
        1.0000, 1.0000, 1.0001, 1.0000, 0.9973, 1.0000, 1.0000, 1.0000, 0.9997,
        1.0000, 1.0000, 0.9992, 1.0000, 1.0000, 1.0000, 0.9986, 1.0000],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 1.8082e-03,  2.1468e-02,  1.0025e-03,  1.2303e-02,  8.0858e-03,
         -1.4875e-02,  4.0338e-03, -6.8185e-04,  6.7180e-04,  1.4341e-02,
         -9.6311e-03, -4.1341e-03,  5.4739e-04,  2.7545e-02,  4.3288e-03,
          7.5745e-03, -6.8468e-03,  2.4568e-02, -3.4149e-02, -1.2599e-02,
          1.1763e-03,  3.1685e-03, -1.6215e-03, -3.7280e-02, -1.2058e-02,
         -1.5052e-02, -8.3654e-03,  4.2811e-03,  6.6072e-03,  1.6065e-03,
          3.4395e-02,  1.9168e-02, -8.4430e-03, -1.0391e-02,  1.4668e-02,
         -9.8518e-03, -3.4520e-03, -7.4394e-03, -1.1152e-02,  4.1030e-03,
          1.1916e-02, -6.4551e-03, -2.1960e-02,  6.8808e-03,  1.6399e-03,
         -9.6384e-03,  1.3854e-02, -7.4363e-03,  6.3979e-04,  1.1952e-02,
          3.4983e-03,  1.0253e-02,  5.0856e-03, -7.3126e-03, -5.1226e-03,
          6.9693e-03, -8.8693e-03,  2.9017e-03,  7.3594e-03, -5.0590e-04,
          1.1360e-02, -4.3937e-03,  1.1728e-02, -1.5553e-02, -5.1288e-03,
          5.7662e-03, -1.0879e-02, -7.8272e-03,  1.7511e-02, -1.5852e-02,
         -3.2952e-03, -1.8457e-02, -1.3087e-02, -2.1036e-02,  7.6426e-03,
          2.1108e-02,  1.2144e-03,  4.7408e-03,  9.9176e-04,  5.5947e-03,
         -1.1766e-02,  4.7343e-03,  6.6026e-03, -1.3956e-03, -6.2190e-03,
          3.4378e-02,  2.7761e-03,  1.1173e-03, -8.9846e-03, -5.9189e-03,
         -6.7279e-04, -8.4201e-03, -5.6613e-03, -1.8002e-02,  5.2716e-03,
         -1.4351e-02,  3.0955e-03,  1.2460e-02, -1.9752e-03,  1.7713e-03,
         -1.2518e-02, -2.0037e-05, -4.0332e-03, -2.9445e-03,  2.3229e-02,
          5.5779e-03, -1.4738e-03, -2.1534e-04, -1.0014e-02, -3.8802e-03,
          6.7945e-03, -6.5538e-03,  9.4401e-03,  2.5636e-03, -7.4558e-03,
         -3.9924e-03, -1.2989e-02, -3.1052e-03, -7.4098e-03,  2.3440e-03,
          2.2632e-02,  1.3110e-02, -1.6564e-03,  4.1921e-03,  4.0997e-03,
         -9.5850e-03, -5.5776e-03,  2.8417e-03, -6.3649e-03, -8.5787e-03,
          1.2644e-03,  3.5199e-04, -2.9771e-03,  1.2337e-02,  8.9350e-05,
          1.2835e-02, -8.1281e-03, -1.0090e-03, -1.7468e-02, -9.5692e-03,
          1.5330e-02,  7.3749e-03,  1.9029e-03, -2.1145e-02, -6.1701e-03,
          8.3617e-04, -1.6043e-02,  1.1233e-02,  2.6255e-02,  5.4530e-03,
          4.3852e-03, -5.1460e-03, -5.6132e-03,  6.1615e-03, -8.7402e-03,
         -1.2349e-02,  9.8681e-03,  5.3275e-03, -6.5418e-03,  3.3180e-03,
          5.9471e-03, -3.2808e-03, -5.6360e-02, -6.7656e-03, -9.5947e-03,
          4.0732e-03, -1.8753e-02, -1.4603e-02, -4.1882e-03,  8.2434e-03,
          1.0144e-03,  4.1580e-03,  7.9642e-03,  8.0583e-03,  1.3849e-02,
          1.4375e-02,  4.1903e-03, -1.4858e-03, -1.0616e-02,  4.2107e-04,
          8.5682e-03,  1.9439e-02,  1.8905e-03, -1.9188e-03,  4.5604e-04,
          3.4173e-03, -1.1190e-02,  6.1425e-04,  1.4274e-02,  2.9887e-03,
          7.4084e-04, -1.1694e-02, -4.1544e-02, -1.0379e-03,  5.5519e-03,
         -1.7783e-02,  2.0793e-03, -4.7358e-03,  9.7451e-03,  2.3194e-03,
         -1.0079e-02, -7.6466e-03,  5.4802e-03,  1.5699e-02, -2.8171e-03,
          2.7732e-04,  9.2521e-03,  1.0998e-02,  6.5801e-03, -8.4154e-03,
          1.3568e-02,  3.1503e-03,  4.2320e-03, -3.1413e-02,  2.4066e-03,
         -5.4782e-03,  6.2126e-03, -1.7217e-02,  7.5171e-03,  1.9262e-02,
          8.6910e-03, -4.1401e-03,  1.4215e-02, -4.4480e-03,  3.7296e-03,
         -5.5674e-03,  3.1183e-03,  1.8721e-02,  8.9773e-03,  1.3644e-02,
         -5.9747e-03,  4.6770e-03, -4.1419e-03, -2.6384e-04,  4.1778e-03,
          6.7515e-03, -5.8717e-04, -1.7293e-03, -1.6741e-02,  2.9510e-03,
         -5.1124e-03, -1.1322e-02, -7.9712e-03, -1.4001e-02,  5.2866e-03,
          7.6528e-03,  1.4406e-02,  2.2689e-02,  3.8980e-03,  1.6227e-02,
         -1.4477e-02,  6.8782e-03,  6.4781e-03,  2.3219e-03,  2.6916e-04,
          4.2129e-03, -4.4192e-03, -7.1674e-03,  4.2363e-03,  7.6629e-03,
          1.3669e-02, -2.8116e-03, -2.4917e-03,  5.4435e-03, -2.2068e-03,
          1.5427e-03, -2.5838e-02,  7.9927e-03, -4.1273e-03, -1.9687e-02,
         -2.6788e-03, -1.9690e-02,  1.6964e-02, -4.1710e-03, -1.1713e-02,
         -2.4697e-03,  1.4662e-02,  1.8207e-03, -3.2243e-02,  5.5322e-04,
          1.0621e-02, -5.0930e-03, -2.6703e-03,  1.9971e-03,  2.5245e-03,
          5.0461e-03,  5.8220e-03, -1.6531e-02,  3.3506e-03,  3.3430e-03,
         -1.0051e-04,  1.1464e-03,  3.9467e-03, -5.7859e-03,  1.0488e-02,
          9.1001e-03, -1.8332e-03, -6.7380e-03, -1.2086e-02,  1.4569e-02,
         -3.9430e-03,  4.6238e-03,  1.9714e-03, -5.8611e-03,  1.8946e-03,
         -2.0257e-02, -2.0107e-02,  8.0092e-03, -7.6810e-03, -6.6913e-04,
          9.0904e-03,  5.4657e-03, -4.5279e-03,  1.4350e-02,  5.0040e-03,
          7.0092e-03, -4.5103e-03, -2.1909e-03,  5.6090e-03,  6.3287e-03,
         -2.5764e-03,  7.3373e-03,  3.3489e-03, -1.8239e-02, -9.0835e-03,
          8.8736e-03, -9.8782e-03, -2.4441e-03, -2.9112e-02, -5.1722e-03,
          2.5591e-02, -2.0368e-03, -3.3063e-03, -1.0942e-02, -6.0247e-03,
         -1.0545e-02, -1.5277e-02, -1.8100e-03,  4.9012e-03, -2.5488e-02,
         -9.9725e-03, -4.8257e-03,  1.7438e-03,  6.7615e-03, -6.0573e-04,
         -1.7780e-02, -7.9269e-03,  1.0450e-02,  1.0737e-02, -6.6992e-03,
          3.6077e-03, -7.0490e-03, -3.6335e-03, -2.9733e-03,  5.2038e-03,
          7.0415e-03, -5.4297e-03, -1.0276e-03, -7.9725e-03, -2.3057e-03,
         -8.3036e-03, -1.8149e-02, -8.1157e-03, -2.5231e-03, -1.4519e-02,
         -9.0667e-03,  3.5214e-03,  1.0483e-02, -3.3195e-03, -1.5724e-03,
         -4.4044e-03,  1.5716e-02,  3.4777e-03, -3.6862e-03, -7.3056e-04,
          7.4366e-03,  7.2673e-03, -3.0213e-03, -7.2149e-03, -1.2460e-02,
         -2.6432e-04, -6.5792e-03, -2.3955e-03, -5.2389e-03, -6.7151e-03,
          1.7824e-03,  1.7879e-02, -1.7355e-03, -8.9335e-03,  2.4423e-03,
          8.9270e-03, -2.0181e-02, -6.9910e-03, -1.0738e-02, -3.6945e-04,
          5.7628e-03,  2.6639e-03,  1.6148e-02,  9.2301e-03, -5.5309e-05,
         -5.4534e-03,  6.8471e-03, -1.6204e-02,  2.8142e-03,  7.4422e-03,
          5.7419e-03, -6.1944e-03,  1.9580e-03,  5.1205e-03, -2.3250e-03,
         -1.3161e-02,  5.6077e-03,  2.7127e-02,  7.5368e-03, -7.2556e-04,
         -6.3668e-03,  1.0902e-02, -7.3971e-06,  3.8300e-03, -3.4925e-04,
          5.8744e-03,  1.2517e-03, -2.0299e-02,  2.5988e-03, -1.6906e-02,
          9.4960e-03, -5.2047e-03,  6.6884e-03, -8.0984e-03,  6.2463e-03,
          3.2799e-03,  4.8749e-03,  4.3751e-03,  4.1598e-03,  2.6795e-03,
          1.7048e-03, -1.1406e-02, -2.0729e-03,  3.3503e-04,  1.0272e-02,
          3.0692e-03, -3.7637e-03,  5.1218e-03,  3.8803e-03,  1.6127e-02,
         -6.8809e-04, -2.8332e-02,  8.0944e-03, -5.0411e-03, -5.0976e-03,
         -3.6777e-03, -1.1488e-02, -7.6276e-03,  3.5269e-02,  1.2020e-02,
          2.0502e-03, -1.1554e-03, -6.6420e-03,  6.3187e-02, -4.2616e-04,
          3.2763e-03,  7.3466e-03,  1.2155e-02, -1.5931e-02, -2.4276e-03,
          1.6591e-03,  5.7290e-03, -2.5391e-05,  7.8240e-03,  3.2611e-03,
          1.2618e-02, -2.1009e-02,  1.8365e-03,  1.0918e-02,  1.1983e-04,
         -7.0293e-03,  1.3472e-02,  1.3662e-02, -1.0700e-02, -1.3798e-02,
         -1.0028e-02, -1.7867e-02, -2.9564e-03, -1.8700e-02,  1.4043e-02,
          9.9284e-03,  1.3492e-02, -4.0157e-03, -9.2426e-04, -1.3759e-03,
         -7.3671e-03,  1.9026e-03, -1.2435e-03, -2.3961e-02, -1.7454e-03,
         -2.6042e-04,  8.1416e-03,  7.3195e-03,  1.4421e-02, -7.8128e-03,
         -4.3838e-03, -2.0212e-02, -6.9308e-04, -2.7186e-03, -1.5976e-03,
         -5.2082e-03,  8.1889e-03, -7.8345e-04, -6.7626e-03, -2.5470e-03,
         -2.2122e-02, -5.9071e-03]], device='cuda:3', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[5.9790e-02, 1.9540e-01, 2.0003e-01, 3.9117e-01, 1.5947e-01, 2.2651e-01,
         2.0382e-01, 7.6359e-02, 2.0247e-01, 1.8871e-01, 3.4199e-01, 1.9544e-01,
         1.8736e-01, 1.8439e-01, 2.1052e-01, 1.4188e-01, 2.2846e-01, 1.6222e-01,
         4.2547e-04, 1.7404e-01, 1.2529e-01, 1.0827e-04, 7.8163e-02, 1.6861e-04,
         1.2775e-01, 1.4262e-01, 2.2190e-01, 3.4790e-01, 9.9085e-02, 1.3222e-01,
         1.5121e-04, 3.1272e-01, 1.8718e-01, 1.7486e-01, 1.1590e-01, 1.6182e-01,
         1.6402e-01, 1.8742e-01, 1.1628e-01, 1.0141e-01, 1.9387e-01, 2.3689e-01,
         1.5338e-01, 6.1965e-02, 3.0244e-01, 1.8364e-01, 1.5326e-01, 1.6740e-01,
         1.6583e-01, 1.5431e-01, 6.6698e-02, 1.3396e-01, 1.9580e-01, 1.4179e-01,
         1.5079e-01, 1.9499e-01, 1.7737e-01, 8.6960e-02, 1.1018e-01, 2.1802e-01,
         1.6410e-01, 1.7103e-01, 2.4774e-01, 1.7284e-01, 1.3581e-01, 1.3371e-01,
         1.4076e-01, 1.3346e-01, 2.7404e-01, 1.3950e-01, 1.3891e-01, 1.8659e-01,
         2.1190e-01, 1.3712e-01, 1.6227e-01, 1.2540e-01, 1.5843e-01, 2.4570e-01,
         1.0556e-01, 1.3165e-01, 1.4027e-01, 1.1386e-01, 9.7266e-02, 1.7288e-01,
         1.2330e-01, 2.2789e-01, 1.4035e-01, 2.4331e-01, 2.1419e-01, 5.7746e-02,
         1.6433e-01, 1.5646e-01, 1.3020e-01, 2.8207e-01, 1.4112e-01, 1.9682e-01,
         1.7616e-01, 1.4683e-01, 1.2543e-01, 9.6318e-02, 2.6324e-01, 1.3919e-01,
         2.2164e-01, 1.5055e-01, 2.0231e-01, 1.2043e-01, 1.3638e-01, 9.5263e-02,
         1.3306e-01, 7.7499e-02, 2.2462e-01, 3.5877e-02, 3.8589e-05, 1.6713e-01,
         8.5504e-02, 6.1702e-02, 1.3201e-01, 2.3494e-01, 1.4500e-01, 2.7057e-01,
         1.5770e-01, 1.5505e-01, 2.0889e-01, 2.7057e-01, 1.3437e-01, 1.0512e-01,
         1.2355e-01, 1.8441e-01, 1.2771e-01, 1.0745e-01, 1.3891e-01, 2.0492e-01,
         1.5864e-01, 2.4229e-01, 1.4471e-01, 1.5273e-01, 1.1752e-01, 6.2408e-02,
         1.5793e-01, 1.2206e-01, 1.3359e-01, 1.6916e-01, 1.1507e-01, 1.4653e-01,
         1.0800e-01, 1.4167e-01, 1.3680e-01, 1.2080e-01, 1.3835e-01, 5.6803e-02,
         2.0852e-01, 2.1104e-01, 1.6905e-01, 1.2580e-01, 8.9152e-02, 2.1165e-01,
         1.5215e-01, 1.5169e-01, 1.7551e-01, 1.8043e-01, 2.0638e-01, 7.0002e-02,
         1.6995e-05, 1.8469e-01, 1.3847e-01, 1.5913e-01, 1.3648e-01, 1.0537e-01,
         1.4634e-01, 1.4737e-01, 1.3716e-01, 1.2948e-01, 1.4453e-01, 1.9511e-01,
         1.7366e-01, 2.0615e-01, 1.8177e-01, 1.3770e-01, 1.4059e-01, 1.7210e-01,
         1.6083e-01, 1.9539e-01, 2.0343e-01, 9.2055e-02, 2.3993e-01, 2.9586e-01,
         1.4208e-01, 1.7013e-01, 2.2105e-01, 1.1566e-01, 1.2304e-01, 1.4007e-01,
         1.5552e-04, 2.7723e-01, 1.6977e-01, 1.3864e-01, 1.0265e-01, 2.0007e-01,
         1.6338e-01, 1.8385e-01, 2.3615e-01, 7.6595e-02, 1.7488e-01, 2.2203e-01,
         1.1920e-01, 1.9547e-01, 1.6967e-01, 2.1010e-01, 1.7138e-01, 2.3788e-01,
         1.8789e-01, 1.4753e-01, 1.0730e-01, 2.0389e-01, 8.9947e-02, 1.9100e-01,
         1.3861e-01, 2.8083e-05, 2.0164e-01, 3.1132e-01, 2.7403e-01, 1.6896e-01,
         2.5877e-05, 2.0930e-01, 1.7159e-01, 7.6755e-02, 1.6727e-01, 8.8247e-02,
         1.4110e-01, 1.4090e-01, 9.2918e-02, 2.3538e-01, 1.8592e-01, 8.0191e-02,
         1.7127e-01, 1.2827e-01, 7.7520e-02, 5.9683e-02, 2.2575e-01, 1.5233e-01,
         9.7949e-02, 2.8649e-01, 1.0696e-01, 1.7699e-01, 2.5760e-01, 2.7306e-01,
         2.6793e-01, 2.3501e-01, 4.1720e-01, 1.2674e-01, 1.0328e-01, 1.9115e-01,
         2.4192e-01, 1.7734e-01, 1.2965e-01, 1.9177e-01, 7.5011e-02, 1.9871e-01,
         1.3766e-01, 3.3100e-01, 2.5670e-01, 2.2511e-01, 9.3572e-02, 5.5311e-02,
         3.6345e-02, 1.4771e-01, 2.4793e-01, 1.5342e-01, 1.6888e-01, 1.6424e-01,
         9.3032e-02, 1.4656e-01, 2.0535e-01, 7.5352e-02, 1.2539e-01, 1.2312e-01,
         1.2854e-01, 1.5632e-01, 3.3200e-01, 1.5896e-01, 1.7111e-01, 7.9512e-02,
         5.0415e-02, 2.7919e-01, 2.8728e-01, 7.4993e-02, 2.0593e-01, 1.9150e-01,
         1.5561e-01, 2.1248e-01, 1.7308e-01, 2.2685e-01, 1.8794e-01, 1.4875e-01,
         2.1241e-01, 1.0375e-01, 1.0440e-01, 1.7217e-01, 1.9745e-01, 1.5924e-01,
         1.3968e-01, 1.0575e-01, 1.4402e-01, 9.8601e-02, 1.0364e-01, 1.3288e-01,
         2.3873e-01, 1.7934e-01, 1.5683e-01, 1.3158e-01, 3.2168e-01, 1.8236e-01,
         8.1594e-02, 1.4394e-01, 2.1699e-01, 2.4742e-01, 1.9475e-01, 1.5304e-01,
         2.0659e-01, 1.5888e-01, 2.8456e-01, 1.7324e-01, 1.7663e-01, 1.8159e-01,
         1.5838e-01, 1.5179e-01, 2.2721e-01, 2.8458e-01, 2.6283e-01, 1.5448e-01,
         2.2591e-01, 1.7689e-01, 9.9679e-02, 1.9489e-01, 1.7883e-01, 1.9940e-01,
         1.7031e-01, 2.2171e-01, 1.8663e-01, 1.6251e-01, 2.2274e-01, 1.7457e-01,
         1.3482e-01, 1.4774e-01, 1.6853e-01, 2.5098e-01, 1.4024e-01, 2.0249e-01,
         1.7898e-01, 1.7110e-01, 2.9931e-01, 1.8922e-01, 1.5937e-01, 1.5550e-01,
         5.7815e-02, 1.5863e-01, 9.7695e-02, 1.4316e-01, 7.1875e-02, 1.5399e-01,
         1.7370e-01, 2.6982e-01, 1.3749e-01, 1.4361e-01, 1.6150e-01, 1.7369e-01,
         2.4731e-01, 2.3319e-01, 4.8095e-02, 1.7471e-01, 1.7779e-01, 1.8183e-01,
         2.0768e-01, 1.5473e-01, 1.7353e-01, 1.7898e-01, 1.8431e-01, 2.2861e-01,
         2.0791e-01, 1.7565e-01, 1.5591e-01, 1.4489e-01, 5.2088e-02, 1.6451e-01,
         8.2123e-02, 7.8206e-02, 1.2030e-01, 6.7625e-02, 1.7128e-01, 1.4929e-01,
         2.9058e-01, 1.5529e-01, 1.7102e-01, 1.4742e-01, 2.7209e-01, 1.3827e-01,
         9.1252e-02, 1.5341e-01, 8.7992e-02, 1.9572e-01, 1.8516e-01, 1.5571e-01,
         5.2689e-05, 1.5576e-01, 2.0336e-01, 1.2647e-01, 2.9102e-04, 1.5679e-01,
         4.8157e-02, 1.0287e-01, 2.5905e-01, 2.3937e-01, 2.0607e-01, 1.3266e-01,
         1.8416e-01, 2.0973e-01, 1.0078e-01, 9.8417e-02, 9.9973e-02, 1.1684e-01,
         7.1563e-02, 2.1810e-01, 3.2017e-01, 1.2789e-01, 1.7785e-01, 1.2251e-01,
         1.9238e-01, 3.4706e-01, 1.5224e-01, 7.7188e-02, 1.4781e-01, 1.1356e-01,
         1.7380e-01, 8.2632e-02, 1.8271e-01, 3.0664e-01, 1.0944e-01, 8.8804e-02,
         9.8838e-02, 7.9017e-02, 4.4780e-04, 1.7389e-01, 1.7718e-01, 1.5090e-01,
         1.9206e-01, 8.6508e-02, 1.5046e-04, 5.8571e-02, 1.4976e-01, 4.0441e-02,
         2.3121e-01, 1.3222e-01, 1.0367e-01, 1.6119e-04, 9.5308e-02, 7.9532e-02,
         6.8073e-02, 5.7463e-02, 1.7653e-05, 7.4756e-02, 1.0809e-01, 9.3524e-02,
         1.8182e-01, 1.3086e-01, 1.1720e-01, 2.0400e-01, 2.1453e-01, 6.7203e-02,
         1.8088e-01, 1.7466e-01, 1.0957e-01, 2.3123e-01, 2.7274e-01, 1.6739e-01,
         1.4648e-01, 2.5423e-01, 1.3982e-01, 1.3999e-01, 2.7777e-01, 1.9191e-01,
         1.9837e-01, 1.6827e-01, 1.5924e-01, 1.9413e-01, 1.7106e-01, 3.4093e-01,
         1.4186e-01, 1.8766e-01, 1.6965e-01, 1.2414e-01, 1.5640e-01, 2.2571e-01,
         5.7329e-02, 1.5417e-01, 1.1186e-01, 1.7831e-01, 7.5536e-02, 1.9581e-01,
         2.2921e-01, 1.8923e-01, 1.2232e-01, 1.5378e-01, 4.6691e-02, 1.5690e-01,
         1.8840e-01, 2.0146e-01, 1.8728e-01, 1.3538e-01, 1.7386e-01, 1.5493e-01,
         1.5041e-01, 7.4995e-02]], device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.6310], device='cuda:3', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0043], device='cuda:3', grad_fn=<AddBackward0>)
likelihood_guide.likelihood._scale.loc torch.Size([]) Parameter containing:
tensor(-0.8402, device='cuda:3', requires_grad=True)
likelihood_guide.likelihood._scale.scale torch.Size([]) tensor(0.0071, device='cuda:3', grad_fn=<AddBackward0>)
Using device: cuda:3
===== Training profile tensin-3x512-sl - 3 =====
[0:00:01.833046] epoch: 0 | elbo: 27056.776699218746 | train_rmse: 0.3431 | val_rmse: 0.4082 | val_ll: -0.5597
[0:01:42.406954] epoch: 50 | elbo: 26851.04677734375 | train_rmse: 0.3384 | val_rmse: 0.4052 | val_ll: -0.5665
[0:03:21.806058] epoch: 100 | elbo: 26993.88359375 | train_rmse: 0.3364 | val_rmse: 0.4032 | val_ll: -0.5641
[0:05:01.502840] epoch: 150 | elbo: 26799.990605468753 | train_rmse: 0.3364 | val_rmse: 0.4029 | val_ll: -0.5678
[0:06:41.879301] epoch: 200 | elbo: 26910.6516796875 | train_rmse: 0.3374 | val_rmse: 0.403 | val_ll: -0.5669
[0:08:19.195940] epoch: 250 | elbo: 27327.1994921875 | train_rmse: 0.3398 | val_rmse: 0.4052 | val_ll: -0.5692
[0:09:55.452599] epoch: 300 | elbo: 26715.544804687503 | train_rmse: 0.3362 | val_rmse: 0.4038 | val_ll: -0.5628
[0:11:32.957167] epoch: 350 | elbo: 26785.077089843748 | train_rmse: 0.3372 | val_rmse: 0.4035 | val_ll: -0.5643
[0:13:10.249547] epoch: 400 | elbo: 26739.493203125 | train_rmse: 0.3386 | val_rmse: 0.404 | val_ll: -0.5696
[0:14:48.592251] epoch: 450 | elbo: 26930.582656249997 | train_rmse: 0.3364 | val_rmse: 0.4032 | val_ll: -0.5614
[0:16:26.218833] epoch: 500 | elbo: 26899.87587890625 | train_rmse: 0.3369 | val_rmse: 0.4024 | val_ll: -0.5586
[0:18:09.390007] epoch: 550 | elbo: 26673.0481640625 | train_rmse: 0.3378 | val_rmse: 0.4039 | val_ll: -0.565
[0:19:50.020834] epoch: 600 | elbo: 26610.615839843747 | train_rmse: 0.3362 | val_rmse: 0.4021 | val_ll: -0.558
[0:21:30.816584] epoch: 650 | elbo: 26638.1437890625 | train_rmse: 0.3358 | val_rmse: 0.4026 | val_ll: -0.5584
[0:23:10.788294] epoch: 700 | elbo: 26673.96482421875 | train_rmse: 0.3358 | val_rmse: 0.402 | val_ll: -0.5579
[0:24:50.968465] epoch: 750 | elbo: 26615.122480468755 | train_rmse: 0.3361 | val_rmse: 0.4011 | val_ll: -0.558
[0:26:31.657251] epoch: 800 | elbo: 26608.20314453125 | train_rmse: 0.3383 | val_rmse: 0.4038 | val_ll: -0.5636
[0:28:13.136162] epoch: 850 | elbo: 26641.825058593746 | train_rmse: 0.3366 | val_rmse: 0.4018 | val_ll: -0.5591
[0:29:54.133131] epoch: 900 | elbo: 26741.114921875007 | train_rmse: 0.3353 | val_rmse: 0.4021 | val_ll: -0.5564
[0:31:34.016579] epoch: 950 | elbo: 26681.113398437497 | train_rmse: 0.3357 | val_rmse: 0.4023 | val_ll: -0.5569
[0:33:15.574952] epoch: 1000 | elbo: 26631.621035156255 | train_rmse: 0.3358 | val_rmse: 0.4021 | val_ll: -0.557
[0:34:57.136624] epoch: 1050 | elbo: 26675.715937499997 | train_rmse: 0.3353 | val_rmse: 0.4019 | val_ll: -0.5562
[0:36:37.885156] epoch: 1100 | elbo: 26564.20986328125 | train_rmse: 0.3356 | val_rmse: 0.4017 | val_ll: -0.5577
[0:38:18.237483] epoch: 1150 | elbo: 26595.463457031252 | train_rmse: 0.3395 | val_rmse: 0.4051 | val_ll: -0.5629
[0:39:58.616098] epoch: 1200 | elbo: 26536.844921875 | train_rmse: 0.3357 | val_rmse: 0.401 | val_ll: -0.5533
[0:41:40.004222] epoch: 1250 | elbo: 26560.080449218745 | train_rmse: 0.3353 | val_rmse: 0.4016 | val_ll: -0.5523
[0:43:21.527633] epoch: 1300 | elbo: 26504.4525390625 | train_rmse: 0.335 | val_rmse: 0.4017 | val_ll: -0.5536
[0:45:02.823650] epoch: 1350 | elbo: 26557.813046875002 | train_rmse: 0.3357 | val_rmse: 0.4023 | val_ll: -0.5557
[0:46:42.638773] epoch: 1400 | elbo: 27228.45189453125 | train_rmse: 0.3367 | val_rmse: 0.4033 | val_ll: -0.5552
[0:48:19.734574] epoch: 1450 | elbo: 26438.637910156256 | train_rmse: 0.3349 | val_rmse: 0.4029 | val_ll: -0.5569
[0:49:57.095379] epoch: 1500 | elbo: 26498.931953125 | train_rmse: 0.3354 | val_rmse: 0.4025 | val_ll: -0.5563
[0:51:35.363672] epoch: 1550 | elbo: 26452.25880859375 | train_rmse: 0.3351 | val_rmse: 0.4027 | val_ll: -0.5541
[0:53:13.373300] epoch: 1600 | elbo: 26458.235625 | train_rmse: 0.3348 | val_rmse: 0.4015 | val_ll: -0.5548
[0:54:49.802986] epoch: 1650 | elbo: 26491.634648437503 | train_rmse: 0.3344 | val_rmse: 0.4016 | val_ll: -0.5521
[0:56:26.106637] epoch: 1700 | elbo: 26624.14296875 | train_rmse: 0.3352 | val_rmse: 0.4016 | val_ll: -0.5541
[0:58:01.707813] epoch: 1750 | elbo: 26947.867441406248 | train_rmse: 0.335 | val_rmse: 0.4009 | val_ll: -0.5595
[0:59:43.032187] epoch: 1800 | elbo: 26551.85830078125 | train_rmse: 0.3357 | val_rmse: 0.4013 | val_ll: -0.5504
[1:01:23.858073] epoch: 1850 | elbo: 26434.67115234375 | train_rmse: 0.3348 | val_rmse: 0.4013 | val_ll: -0.5554
[1:03:04.530495] epoch: 1900 | elbo: 26470.131289062498 | train_rmse: 0.3356 | val_rmse: 0.4012 | val_ll: -0.5513
[1:04:45.618398] epoch: 1950 | elbo: 26442.638027343746 | train_rmse: 0.3361 | val_rmse: 0.4034 | val_ll: -0.5585
[1:06:26.994153] epoch: 2000 | elbo: 26357.428652343748 | train_rmse: 0.3354 | val_rmse: 0.4015 | val_ll: -0.5543
[1:08:08.416473] epoch: 2050 | elbo: 26684.032890624996 | train_rmse: 0.3346 | val_rmse: 0.4007 | val_ll: -0.5528
[1:09:50.042446] epoch: 2100 | elbo: 26394.291640624997 | train_rmse: 0.3351 | val_rmse: 0.4018 | val_ll: -0.5563
[1:11:30.807831] epoch: 2150 | elbo: 26418.047519531247 | train_rmse: 0.335 | val_rmse: 0.4019 | val_ll: -0.5532
[1:13:12.359173] epoch: 2200 | elbo: 26383.9677734375 | train_rmse: 0.3348 | val_rmse: 0.4012 | val_ll: -0.5536
[1:14:54.122254] epoch: 2250 | elbo: 26438.485000000004 | train_rmse: 0.336 | val_rmse: 0.4008 | val_ll: -0.5518
[1:16:35.294899] epoch: 2300 | elbo: 26288.41779296875 | train_rmse: 0.3358 | val_rmse: 0.4028 | val_ll: -0.5539
[1:18:15.271519] epoch: 2350 | elbo: 26308.123554687503 | train_rmse: 0.335 | val_rmse: 0.401 | val_ll: -0.552
[1:19:55.834895] epoch: 2400 | elbo: 26310.9696875 | train_rmse: 0.3338 | val_rmse: 0.4013 | val_ll: -0.5565
[1:21:36.646376] epoch: 2450 | elbo: 26357.03250000001 | train_rmse: 0.3351 | val_rmse: 0.4022 | val_ll: -0.5514
[1:23:18.774314] epoch: 2500 | elbo: 26207.478105468752 | train_rmse: 0.3347 | val_rmse: 0.402 | val_ll: -0.5505
[1:24:58.175590] epoch: 2550 | elbo: 26286.19404296875 | train_rmse: 0.3344 | val_rmse: 0.4008 | val_ll: -0.5517
[1:26:38.508373] epoch: 2600 | elbo: 26178.367597656248 | train_rmse: 0.335 | val_rmse: 0.4019 | val_ll: -0.5572
[1:28:18.638103] epoch: 2650 | elbo: 26932.5453125 | train_rmse: 0.3355 | val_rmse: 0.4022 | val_ll: -0.5513
[1:29:58.880273] epoch: 2700 | elbo: 26264.918417968747 | train_rmse: 0.3348 | val_rmse: 0.4021 | val_ll: -0.5547
[1:31:39.604646] epoch: 2750 | elbo: 26170.81705078125 | train_rmse: 0.3355 | val_rmse: 0.4013 | val_ll: -0.5515
[1:33:20.882928] epoch: 2800 | elbo: 26179.711191406248 | train_rmse: 0.3345 | val_rmse: 0.4014 | val_ll: -0.5521
[1:35:02.916880] epoch: 2850 | elbo: 26384.29248046875 | train_rmse: 0.3351 | val_rmse: 0.4018 | val_ll: -0.5536
[1:36:45.525690] epoch: 2900 | elbo: 26194.154453125 | train_rmse: 0.3339 | val_rmse: 0.4012 | val_ll: -0.5539
[1:38:28.548268] epoch: 2950 | elbo: 26163.42369140625 | train_rmse: 0.3348 | val_rmse: 0.4015 | val_ll: -0.5559
[1:40:10.944395] epoch: 3000 | elbo: 26147.5043359375 | train_rmse: 0.3362 | val_rmse: 0.4025 | val_ll: -0.5538
[1:41:51.051694] epoch: 3050 | elbo: 26179.060312499998 | train_rmse: 0.3375 | val_rmse: 0.4039 | val_ll: -0.5566
[1:43:31.308211] epoch: 3100 | elbo: 26116.2891796875 | train_rmse: 0.3342 | val_rmse: 0.4015 | val_ll: -0.5511
[1:45:12.396729] epoch: 3150 | elbo: 26176.2907421875 | train_rmse: 0.3339 | val_rmse: 0.4011 | val_ll: -0.5528
[1:46:52.271741] epoch: 3200 | elbo: 26166.104589843748 | train_rmse: 0.3351 | val_rmse: 0.4017 | val_ll: -0.5512
[1:48:33.661286] epoch: 3250 | elbo: 26176.04765625 | train_rmse: 0.3342 | val_rmse: 0.4012 | val_ll: -0.5502
[1:50:14.127066] epoch: 3300 | elbo: 26192.750078125006 | train_rmse: 0.3348 | val_rmse: 0.4015 | val_ll: -0.5505
[1:51:53.836386] epoch: 3350 | elbo: 26228.244941406254 | train_rmse: 0.3351 | val_rmse: 0.4017 | val_ll: -0.5497
[1:53:35.062188] epoch: 3400 | elbo: 26089.440175781252 | train_rmse: 0.3345 | val_rmse: 0.4015 | val_ll: -0.5515
[1:55:16.531276] epoch: 3450 | elbo: 26014.08923828125 | train_rmse: 0.3339 | val_rmse: 0.4006 | val_ll: -0.5466
[1:56:57.619044] epoch: 3500 | elbo: 26287.37189453125 | train_rmse: 0.3341 | val_rmse: 0.4017 | val_ll: -0.5537
[1:58:39.632781] epoch: 3550 | elbo: 26051.416640625 | train_rmse: 0.3339 | val_rmse: 0.4016 | val_ll: -0.5521
[2:00:19.583704] epoch: 3600 | elbo: 25989.915468749998 | train_rmse: 0.3345 | val_rmse: 0.4008 | val_ll: -0.5498
[2:02:00.100405] epoch: 3650 | elbo: 26481.908828124997 | train_rmse: 0.3349 | val_rmse: 0.4024 | val_ll: -0.5539
[2:03:41.127777] epoch: 3700 | elbo: 26061.0046875 | train_rmse: 0.3346 | val_rmse: 0.4016 | val_ll: -0.5528
[2:05:24.295839] epoch: 3750 | elbo: 26074.7830859375 | train_rmse: 0.3355 | val_rmse: 0.4016 | val_ll: -0.5528
[2:07:07.058437] epoch: 3800 | elbo: 26040.24720703125 | train_rmse: 0.3358 | val_rmse: 0.4027 | val_ll: -0.5511
[2:08:48.022472] epoch: 3850 | elbo: 26084.308281250003 | train_rmse: 0.3335 | val_rmse: 0.4009 | val_ll: -0.5488
[2:10:29.638186] epoch: 3900 | elbo: 26078.65208984375 | train_rmse: 0.3357 | val_rmse: 0.4023 | val_ll: -0.5522
[2:12:09.193924] epoch: 3950 | elbo: 26028.17560546875 | train_rmse: 0.3335 | val_rmse: 0.4009 | val_ll: -0.5503
[2:13:48.575421] epoch: 4000 | elbo: 26046.069570312502 | train_rmse: 0.3353 | val_rmse: 0.4013 | val_ll: -0.5472
[2:15:27.934068] epoch: 4050 | elbo: 25916.73630859375 | train_rmse: 0.3352 | val_rmse: 0.4019 | val_ll: -0.5556
[2:17:07.140779] epoch: 4100 | elbo: 26084.094218749997 | train_rmse: 0.3347 | val_rmse: 0.4012 | val_ll: -0.5502
[2:18:46.592982] epoch: 4150 | elbo: 25918.2438671875 | train_rmse: 0.3356 | val_rmse: 0.4019 | val_ll: -0.5567
[2:20:27.535117] epoch: 4200 | elbo: 25926.120136718746 | train_rmse: 0.3337 | val_rmse: 0.4017 | val_ll: -0.5516
[2:22:09.889538] epoch: 4250 | elbo: 25888.3937109375 | train_rmse: 0.3338 | val_rmse: 0.4012 | val_ll: -0.5468
[2:23:52.025255] epoch: 4300 | elbo: 25889.82013671875 | train_rmse: 0.3346 | val_rmse: 0.4006 | val_ll: -0.5484
[2:25:33.940360] epoch: 4350 | elbo: 26248.69375 | train_rmse: 0.3387 | val_rmse: 0.4045 | val_ll: -0.555
[2:27:14.894037] epoch: 4400 | elbo: 25853.11125 | train_rmse: 0.3345 | val_rmse: 0.4018 | val_ll: -0.5473
[2:28:55.959611] epoch: 4450 | elbo: 25960.6037109375 | train_rmse: 0.3337 | val_rmse: 0.4015 | val_ll: -0.5517
[2:30:38.358976] epoch: 4500 | elbo: 26083.403925781247 | train_rmse: 0.3341 | val_rmse: 0.4006 | val_ll: -0.5488
[2:32:20.362768] epoch: 4550 | elbo: 25964.4501171875 | train_rmse: 0.3339 | val_rmse: 0.4011 | val_ll: -0.5519
[2:34:00.952130] epoch: 4600 | elbo: 25880.35966796875 | train_rmse: 0.3346 | val_rmse: 0.4025 | val_ll: -0.5527
[2:35:42.523587] epoch: 4650 | elbo: 25835.289921875003 | train_rmse: 0.3339 | val_rmse: 0.4007 | val_ll: -0.5478
[2:37:22.625073] epoch: 4700 | elbo: 25973.6784375 | train_rmse: 0.334 | val_rmse: 0.401 | val_ll: -0.5472
[2:39:04.419809] epoch: 4750 | elbo: 25823.972558593752 | train_rmse: 0.3336 | val_rmse: 0.4005 | val_ll: -0.5461
[2:40:45.437640] epoch: 4800 | elbo: 25812.97326171875 | train_rmse: 0.3354 | val_rmse: 0.403 | val_ll: -0.5468
[2:42:26.125501] epoch: 4850 | elbo: 25833.557441406247 | train_rmse: 0.3336 | val_rmse: 0.4009 | val_ll: -0.5489
[2:44:07.208492] epoch: 4900 | elbo: 25741.67048828125 | train_rmse: 0.3333 | val_rmse: 0.4007 | val_ll: -0.5487
[2:45:47.064668] epoch: 4950 | elbo: 25752.40705078125 | train_rmse: 0.3346 | val_rmse: 0.4009 | val_ll: -0.5506
[2:47:26.978106] epoch: 5000 | elbo: 25885.582460937498 | train_rmse: 0.3333 | val_rmse: 0.4 | val_ll: -0.5461
[2:49:07.832777] epoch: 5050 | elbo: 25791.169062499997 | train_rmse: 0.3336 | val_rmse: 0.4015 | val_ll: -0.5494
[2:50:47.290804] epoch: 5100 | elbo: 25734.44806640625 | train_rmse: 0.3352 | val_rmse: 0.4026 | val_ll: -0.5519
[2:52:28.288818] epoch: 5150 | elbo: 25791.32759765625 | train_rmse: 0.3353 | val_rmse: 0.4016 | val_ll: -0.5476
[2:54:10.795060] epoch: 5200 | elbo: 25795.87322265625 | train_rmse: 0.3344 | val_rmse: 0.4013 | val_ll: -0.5476
[2:55:52.628150] epoch: 5250 | elbo: 25709.156445312503 | train_rmse: 0.3335 | val_rmse: 0.4013 | val_ll: -0.5484
[2:57:33.330177] epoch: 5300 | elbo: 25733.98029296875 | train_rmse: 0.3337 | val_rmse: 0.4005 | val_ll: -0.5458
[2:59:13.504655] epoch: 5350 | elbo: 25795.459375 | train_rmse: 0.3337 | val_rmse: 0.4003 | val_ll: -0.5467
[3:00:50.612970] epoch: 5400 | elbo: 25712.68375 | train_rmse: 0.3349 | val_rmse: 0.4016 | val_ll: -0.5491
[3:02:26.568650] epoch: 5450 | elbo: 25733.45072265625 | train_rmse: 0.3351 | val_rmse: 0.403 | val_ll: -0.5484
[3:04:02.065138] epoch: 5500 | elbo: 25669.112851562502 | train_rmse: 0.3341 | val_rmse: 0.4011 | val_ll: -0.5472
[3:05:37.625695] epoch: 5550 | elbo: 25644.936757812502 | train_rmse: 0.3348 | val_rmse: 0.403 | val_ll: -0.5486
[3:07:13.844812] epoch: 5600 | elbo: 25782.758398437498 | train_rmse: 0.3338 | val_rmse: 0.4018 | val_ll: -0.5499
[3:08:51.078115] epoch: 5650 | elbo: 25662.159648437497 | train_rmse: 0.3337 | val_rmse: 0.4015 | val_ll: -0.5484
[3:10:28.573164] epoch: 5700 | elbo: 25700.589003906247 | train_rmse: 0.3339 | val_rmse: 0.4019 | val_ll: -0.5469
[3:12:11.404152] epoch: 5750 | elbo: 25605.26080078125 | train_rmse: 0.3345 | val_rmse: 0.4016 | val_ll: -0.5478
[3:13:52.402558] epoch: 5800 | elbo: 25610.78435546875 | train_rmse: 0.3344 | val_rmse: 0.4012 | val_ll: -0.5455
[3:15:33.744671] epoch: 5850 | elbo: 29031.859238281253 | train_rmse: 0.3367 | val_rmse: 0.4027 | val_ll: -0.5508
[3:17:15.318755] epoch: 5900 | elbo: 25641.07326171875 | train_rmse: 0.3342 | val_rmse: 0.401 | val_ll: -0.5442
[3:18:56.710914] epoch: 5950 | elbo: 25611.573222656247 | train_rmse: 0.3353 | val_rmse: 0.4022 | val_ll: -0.5473
[3:20:38.030891] epoch: 6000 | elbo: 26410.536191406252 | train_rmse: 0.3343 | val_rmse: 0.4008 | val_ll: -0.5469
[3:22:20.553939] epoch: 6050 | elbo: 25753.264238281252 | train_rmse: 0.334 | val_rmse: 0.4022 | val_ll: -0.5475
[3:24:02.857974] epoch: 6100 | elbo: 25727.13234375 | train_rmse: 0.3361 | val_rmse: 0.4037 | val_ll: -0.5541
[3:25:44.961510] epoch: 6150 | elbo: 25526.196132812496 | train_rmse: 0.3342 | val_rmse: 0.4024 | val_ll: -0.545
[3:27:27.621902] epoch: 6200 | elbo: 25528.280195312505 | train_rmse: 0.3362 | val_rmse: 0.4035 | val_ll: -0.5496
[3:29:09.852103] epoch: 6250 | elbo: 25772.28494140625 | train_rmse: 0.3354 | val_rmse: 0.4033 | val_ll: -0.5473
[3:30:50.798577] epoch: 6300 | elbo: 25495.8258203125 | train_rmse: 0.3339 | val_rmse: 0.4021 | val_ll: -0.5491
[3:32:31.240668] epoch: 6350 | elbo: 25793.30341796875 | train_rmse: 0.3363 | val_rmse: 0.4033 | val_ll: -0.5477
[3:34:12.252653] epoch: 6400 | elbo: 25473.48431640625 | train_rmse: 0.3349 | val_rmse: 0.4034 | val_ll: -0.5493
[3:35:53.299907] epoch: 6450 | elbo: 25483.1313671875 | train_rmse: 0.3342 | val_rmse: 0.4018 | val_ll: -0.5456
[3:37:35.522069] epoch: 6500 | elbo: 25551.86978515625 | train_rmse: 0.3344 | val_rmse: 0.4022 | val_ll: -0.5433
[3:39:16.815986] epoch: 6550 | elbo: 25441.497050781247 | train_rmse: 0.3344 | val_rmse: 0.4023 | val_ll: -0.5453
[3:40:57.727790] epoch: 6600 | elbo: 25423.51455078125 | train_rmse: 0.3346 | val_rmse: 0.4032 | val_ll: -0.5476
[3:42:40.075072] epoch: 6650 | elbo: 25552.22345703125 | train_rmse: 0.3358 | val_rmse: 0.4038 | val_ll: -0.5478
[3:44:22.385084] epoch: 6700 | elbo: 25721.393632812495 | train_rmse: 0.3369 | val_rmse: 0.4062 | val_ll: -0.5497
[3:46:04.353090] epoch: 6750 | elbo: 25369.16185546875 | train_rmse: 0.3343 | val_rmse: 0.4033 | val_ll: -0.5441
[3:47:45.454323] epoch: 6800 | elbo: 26219.953847656252 | train_rmse: 0.335 | val_rmse: 0.403 | val_ll: -0.5463
[3:49:25.968635] epoch: 6850 | elbo: 25323.806386718752 | train_rmse: 0.3355 | val_rmse: 0.4038 | val_ll: -0.5427
[3:51:06.526190] epoch: 6900 | elbo: 25455.112929687497 | train_rmse: 0.3343 | val_rmse: 0.4026 | val_ll: -0.5426
[3:52:46.226115] epoch: 6950 | elbo: 25360.778691406253 | train_rmse: 0.3381 | val_rmse: 0.4064 | val_ll: -0.5507
[3:54:26.442732] epoch: 7000 | elbo: 25521.9250390625 | train_rmse: 0.3349 | val_rmse: 0.4028 | val_ll: -0.5441
[3:56:08.296649] epoch: 7050 | elbo: 25292.967714843755 | train_rmse: 0.3346 | val_rmse: 0.4031 | val_ll: -0.5488
[3:57:49.899605] epoch: 7100 | elbo: 25322.383085937505 | train_rmse: 0.3358 | val_rmse: 0.4044 | val_ll: -0.5428
[3:59:31.428468] epoch: 7150 | elbo: 25281.4851953125 | train_rmse: 0.3349 | val_rmse: 0.4029 | val_ll: -0.544
[4:01:13.991869] epoch: 7200 | elbo: 25448.492304687497 | train_rmse: 0.3357 | val_rmse: 0.4032 | val_ll: -0.5411
[4:02:56.525987] epoch: 7250 | elbo: 25739.266093749997 | train_rmse: 0.3355 | val_rmse: 0.4035 | val_ll: -0.5412
[4:04:37.321959] epoch: 7300 | elbo: 25305.643046875 | train_rmse: 0.3366 | val_rmse: 0.4035 | val_ll: -0.5458
[4:06:14.943481] epoch: 7350 | elbo: 25252.59091796875 | train_rmse: 0.3356 | val_rmse: 0.4031 | val_ll: -0.5463
[4:07:51.412168] epoch: 7400 | elbo: 25266.617519531246 | train_rmse: 0.3349 | val_rmse: 0.4022 | val_ll: -0.5414
[4:09:28.178146] epoch: 7450 | elbo: 25303.08763671875 | train_rmse: 0.3346 | val_rmse: 0.4028 | val_ll: -0.5396
[4:11:11.221671] epoch: 7500 | elbo: 25333.633535156252 | train_rmse: 0.3353 | val_rmse: 0.4025 | val_ll: -0.5424
[4:12:55.084709] epoch: 7550 | elbo: 25174.459921875 | train_rmse: 0.3367 | val_rmse: 0.404 | val_ll: -0.5461
[4:14:38.936775] epoch: 7600 | elbo: 25202.6770703125 | train_rmse: 0.3343 | val_rmse: 0.4034 | val_ll: -0.5475
[4:16:20.150536] epoch: 7650 | elbo: 25220.33375 | train_rmse: 0.3358 | val_rmse: 0.4032 | val_ll: -0.5411
[4:18:01.498475] epoch: 7700 | elbo: 25208.8555078125 | train_rmse: 0.3345 | val_rmse: 0.4022 | val_ll: -0.5407
[4:19:42.519439] epoch: 7750 | elbo: 25109.1955078125 | train_rmse: 0.3351 | val_rmse: 0.4029 | val_ll: -0.5407
[4:21:23.069949] epoch: 7800 | elbo: 25242.607031249998 | train_rmse: 0.3346 | val_rmse: 0.4026 | val_ll: -0.5424
[4:23:03.788378] epoch: 7850 | elbo: 25184.57267578125 | train_rmse: 0.3368 | val_rmse: 0.4051 | val_ll: -0.544
[4:24:44.610737] epoch: 7900 | elbo: 25147.304335937508 | train_rmse: 0.3351 | val_rmse: 0.4025 | val_ll: -0.5412
[4:26:25.274718] epoch: 7950 | elbo: 25532.23353515625 | train_rmse: 0.3341 | val_rmse: 0.4024 | val_ll: -0.5423
