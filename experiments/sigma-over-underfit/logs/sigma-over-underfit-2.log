Start time: 2023-07-09 21:06:04.137510
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:1 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.3 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0447,  0.0756, -0.2127,  ...,  0.2609,  0.4910,  0.2665],
        [-0.2647, -0.0771,  0.2573,  ..., -0.1631,  0.2988,  0.2101],
        [ 0.0411, -0.3286, -0.2541,  ..., -0.6626, -0.0275,  0.2559],
        ...,
        [-0.4244,  0.2789, -0.2359,  ...,  0.0649, -0.5205, -0.0421],
        [-0.4508, -0.1568,  0.2182,  ..., -0.4388, -0.3859, -0.0195],
        [-0.3507,  0.0489,  0.4109,  ..., -0.0855, -0.2056, -0.2932]],
       device='cuda:1', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6166e-02, -1.3915e-01,  2.9564e-01,  2.9611e-01,  2.2214e-01,
        -9.7888e-02,  5.8897e-01, -1.8524e-01,  2.0443e-01,  5.8446e-02,
        -1.7745e-01, -1.9895e-01, -1.1682e-01, -5.3616e-01,  2.0789e-01,
        -6.4598e-02,  3.3362e-01,  1.3427e-01, -3.1360e-02, -3.8676e-01,
         5.7533e-01,  9.6737e-02, -3.1188e-01, -1.9318e-01, -3.3081e-01,
        -1.8537e-01,  3.6419e-01, -1.3505e-01,  3.0185e-01,  5.7582e-01,
        -3.7531e-01, -4.6969e-01, -6.4841e-02,  4.0309e-01, -4.3244e-01,
        -1.9433e-01,  1.6605e-01, -2.1830e-01,  2.8050e-01, -9.4567e-02,
         1.5417e-01,  1.7902e-01, -2.3644e-01, -2.4925e-01, -2.9956e-01,
        -3.7044e-02,  1.6814e-01, -1.9854e-01, -2.5638e-01,  2.6855e-01,
        -9.5205e-02,  3.6366e-01, -8.0318e-01, -1.6955e-01, -2.2713e-01,
        -2.0035e-02,  1.3097e-01,  2.3065e-01,  3.7070e-01, -1.3243e-01,
        -3.6160e-02, -4.8206e-03, -1.1376e-03,  2.0086e-01, -2.2492e-01,
         3.9010e-01, -2.8781e-01,  4.2783e-02, -3.1186e-01,  2.7993e-01,
         3.9311e-01,  7.0192e-02, -9.7189e-02,  8.0272e-01, -3.1546e-01,
        -5.5958e-01,  2.1423e-02, -1.2455e-01, -3.3646e-01,  2.7934e-01,
        -1.5209e-01,  2.7482e-01, -1.1987e-01,  4.2689e-01,  1.3793e-02,
         2.3633e-01, -3.1819e-01,  5.3045e-02,  1.5081e-01,  3.4590e-01,
        -3.7396e-01, -7.3865e-02, -9.2203e-02,  6.5114e-02,  3.8257e-01,
        -1.5689e-01,  1.7788e-01,  5.1722e-01, -4.2938e-01,  8.0669e-02,
        -6.2920e-03, -1.3110e-01, -2.6427e-02,  4.4446e-01, -5.4677e-01,
        -1.6423e-01,  3.0083e-02,  9.8959e-02, -1.1722e-01,  2.3326e-01,
         1.7362e-01, -4.6287e-01, -5.5670e-01, -9.9926e-02, -3.0622e-01,
         1.4751e-01, -6.1184e-01, -5.0763e-01, -1.6882e-02, -5.2880e-01,
        -7.5939e-01, -1.9816e-01, -1.5566e-01,  3.9489e-01,  3.2330e-01,
        -1.3048e-01,  2.4041e-01, -2.6492e-01, -1.5796e-01,  7.9179e-01,
         1.4048e-01,  5.2655e-01,  2.2442e-01, -9.1619e-02, -1.0366e+00,
         2.6553e-01,  2.0384e-01, -1.7155e-01, -1.6481e-03,  3.2141e-01,
        -5.8581e-01,  6.4256e-01, -3.9943e-01, -1.9391e-01, -2.6091e-01,
         1.1797e-01,  4.4223e-02,  1.6194e-01, -3.5289e-01, -3.2432e-01,
        -2.6665e-01,  3.0188e-01, -8.1698e-02, -1.5214e-01, -6.1498e-01,
        -3.0815e-02, -6.0532e-01,  4.0075e-01, -7.2847e-02,  2.7709e-01,
        -2.6974e-01,  2.5113e-01,  4.1658e-01,  3.3207e-01,  6.8874e-02,
        -5.5198e-02, -4.3679e-01,  5.4634e-01, -1.6460e-01, -1.7971e-01,
         3.5597e-01,  5.3952e-01, -2.0925e-01, -4.3910e-01, -3.2569e-01,
         7.6319e-01, -4.8673e-01,  2.1589e-02,  3.6273e-01, -3.1636e-01,
         9.0259e-02, -1.8414e-02, -2.3747e-01, -5.9817e-02, -1.9344e-01,
        -1.7293e-01, -9.0646e-02, -2.1341e-01, -4.3948e-01, -7.6056e-01,
        -2.0861e-01,  6.6792e-01, -1.3956e-01, -3.2485e-01,  1.1441e-01,
         1.7686e-01, -2.2298e-01,  4.3501e-01, -2.1019e-01, -1.8008e-02,
         3.9982e-01, -1.0706e-01,  3.2638e-01, -5.9527e-01,  1.1342e-01,
        -5.9372e-01, -9.7981e-02, -4.6723e-02,  1.7939e-01, -1.9921e-01,
         6.0286e-01, -5.7475e-01, -2.8755e-01, -2.1078e-02,  3.3662e-01,
        -5.4672e-01, -8.4574e-02,  7.0585e-01, -3.7891e-02, -2.0209e-01,
        -4.6297e-02, -1.0724e-01, -3.7633e-02,  3.6016e-01,  6.9463e-01,
         8.4260e-02,  4.2845e-01, -2.1915e-01,  2.0365e-01,  3.4857e-01,
         2.7607e-01, -4.2154e-01, -7.8276e-01,  3.9964e-02, -1.5784e-02,
        -6.7150e-02,  7.5284e-02,  2.0118e-01,  4.6005e-01,  1.2877e-01,
         1.7722e-01,  2.7403e-01, -4.4400e-01, -3.5898e-01,  3.2615e-01,
        -2.8858e-01,  1.9766e-01,  3.4877e-01,  3.3363e-01, -3.0321e-02,
        -1.4267e-01,  7.5654e-03, -4.5793e-01,  9.4683e-02, -7.8115e-01,
         3.5276e-01, -2.6607e-02, -1.3633e-01,  1.3946e-01, -1.6989e-01,
         9.7561e-02,  1.9092e-01,  1.1244e-01, -1.5142e-02, -2.4859e-01,
         2.6107e-01,  1.3460e-01, -1.9601e-01,  2.2171e-01, -1.1813e-01,
        -2.7271e-02,  2.2744e-01, -6.0848e-02,  1.0853e-01,  2.9351e-01,
         2.4204e-02, -1.1631e-01,  2.2765e-01,  3.5871e-01,  2.9450e-01,
        -5.8593e-01,  3.7996e-02, -4.5191e-01,  2.0098e-01,  1.1031e-01,
         2.5319e-01,  3.8350e-01,  4.1628e-01,  2.2083e-01, -5.2145e-01,
        -1.2084e-04, -2.9774e-01,  5.9605e-01, -1.4718e-01, -2.0328e-01,
        -2.7510e-01,  6.2981e-02,  2.8263e-01,  1.0762e-01, -3.1115e-02,
        -8.6717e-02,  1.2602e-01,  3.4481e-01, -3.1291e-01,  4.4573e-02,
        -5.4093e-02,  1.2785e-02,  5.9368e-01, -9.3458e-02, -3.0025e-02,
        -8.2114e-01,  3.5871e-02, -1.2330e-01, -2.9151e-02, -1.7028e-02,
        -6.4747e-01,  6.7500e-01, -9.3829e-02, -3.5783e-01, -1.1899e-01,
        -1.5495e-01,  1.4848e-01, -1.3111e-02, -4.6007e-01,  7.6984e-02,
        -3.0186e-02,  5.6222e-01, -2.4689e-01, -9.7050e-02, -1.6777e-01,
        -3.7750e-01, -2.4305e-01, -4.1383e-01,  3.1174e-01, -3.7243e-01,
        -7.9324e-02,  1.1726e-01,  6.9679e-01,  1.9514e-01, -5.7582e-02,
        -2.1583e-01,  1.9784e-02,  1.2355e-01,  2.7329e-01,  9.2189e-02,
         1.3324e-01,  1.3457e-01,  1.5220e-01,  5.6737e-01, -4.6108e-01,
        -1.4723e-01,  3.6228e-01,  3.5389e-01, -1.3471e-01, -4.3502e-01,
         5.7319e-02,  6.4020e-01, -3.4931e-01,  5.9252e-01, -3.7241e-01,
         4.8289e-01, -7.1217e-01, -1.7626e-01,  5.8545e-02, -4.4086e-01,
         4.1514e-01,  3.5856e-02,  2.0005e-01, -7.7625e-02,  4.2968e-01,
         4.4622e-01, -2.8619e-01, -5.0256e-01, -6.0599e-01, -1.0697e-01,
        -6.6589e-03, -1.3865e-01, -1.9513e-01, -1.1505e-01, -1.1677e-01,
         1.4042e-01,  1.0355e-01, -1.9033e-01,  2.3525e-02, -3.3168e-01,
         3.7288e-01,  3.2279e-01,  3.5255e-01, -1.1326e-01,  3.3135e-01,
        -1.7131e-01,  3.8987e-01, -1.6718e-01,  2.1797e-02, -6.6744e-01,
        -3.9690e-01, -6.4341e-01,  1.5612e-01,  1.4624e-01, -1.5568e-01,
         1.6977e-02, -1.1612e-01, -2.9578e-01,  3.9115e-01, -1.8719e-01,
         2.2748e-01,  1.3767e-01,  3.3728e-01,  1.0920e-01, -3.8483e-01,
         2.2107e-01,  2.1028e-01,  1.3182e-01,  1.1805e-01,  1.4939e-02,
         4.6891e-01, -3.7607e-01, -1.6891e-01, -1.7116e-01, -6.4544e-02,
        -3.4030e-01,  4.5838e-01,  3.0842e-01,  2.1312e-01, -5.4339e-02,
         2.4640e-01,  9.0786e-02, -6.6366e-01,  3.6862e-02,  3.8134e-01,
        -1.4490e-01, -5.9302e-02, -1.5054e-01,  3.4661e-02, -2.3997e-01,
        -2.0295e-01, -5.0164e-01, -2.1711e-02, -5.0930e-01, -2.2989e-01,
        -1.3395e-01, -7.2049e-02,  5.1076e-01, -1.9724e-01,  1.3791e-01,
         2.4521e-01,  4.4357e-02, -7.0624e-01, -1.9231e-01, -4.9890e-01,
        -3.3431e-01,  9.3956e-02,  3.9203e-02, -3.3491e-01, -7.7078e-01,
        -3.9517e-01, -7.0076e-03, -4.3714e-01, -5.3338e-01, -1.1568e-01,
        -7.7663e-01, -1.9353e-01,  1.6875e-01,  1.5197e-01,  3.1701e-01,
        -2.0656e-01,  2.4972e-01,  6.6190e-02, -2.9960e-02, -8.0230e-01,
        -4.1852e-02,  2.3524e-02,  3.8774e-01, -3.4516e-01,  2.3149e-01,
        -8.1876e-02, -7.5342e-04,  2.5952e-01,  1.2734e-01, -1.9654e-01,
        -2.9427e-02, -4.5514e-02,  5.0196e-01,  4.5032e-01,  4.2538e-01,
         2.0322e-01,  7.1951e-01, -5.9948e-01,  3.0338e-01, -2.7482e-02,
        -9.1157e-02, -2.4218e-01, -2.1302e-02,  2.9174e-01,  3.2023e-02,
         4.5233e-02, -1.0554e-01,  4.9416e-01,  1.0603e-01, -2.3242e-01,
        -1.0399e-01, -2.7018e-01,  2.0762e-01,  3.4917e-01, -7.6422e-02,
         1.4725e-01, -5.7476e-02, -7.6483e-02,  3.3816e-01, -5.8852e-01,
         1.6630e-02,  8.2448e-03], device='cuda:1', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1347,  0.2989, -0.5339,  ..., -0.5265, -0.2761, -0.0241],
        [-0.4257,  0.3315,  0.2079,  ..., -0.1017,  0.1317,  0.5264],
        [-0.6097, -0.1783,  0.2613,  ...,  0.3783, -0.4814, -0.0344],
        ...,
        [ 0.0203, -0.2584, -0.2590,  ..., -0.4645, -0.5785, -0.1913],
        [-0.1921,  0.9825,  0.0440,  ...,  0.2357, -0.6674,  0.3438],
        [ 0.4051, -0.2642,  0.7370,  ...,  0.3095,  0.2294,  0.2429]],
       device='cuda:1', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 5.1383e-01,  2.2676e-01, -1.8156e-01, -2.9724e-01, -1.5685e-01,
         6.0414e-01,  6.1232e-02,  2.7250e-02, -2.3418e-01, -4.5196e-02,
        -2.3660e-01, -6.8167e-01,  5.7750e-02,  3.6466e-01,  3.1874e-01,
         5.3432e-02,  1.8438e-01,  8.8736e-02,  3.0512e-01,  5.2947e-02,
         8.0293e-03,  6.4511e-02,  1.7163e-01,  7.7425e-02,  2.7578e-01,
        -2.7774e-01,  6.8803e-01,  4.6933e-01,  2.0090e-01, -4.3950e-01,
        -2.8161e-01,  3.8266e-01, -3.2890e-02, -6.4451e-01,  6.4740e-01,
        -2.4258e-01, -5.4273e-01, -2.0864e-01, -1.6346e-01,  4.2535e-01,
         6.1291e-02, -1.3541e-01,  3.5634e-01,  2.0267e-01, -3.8041e-01,
         4.7205e-01, -1.3605e-02,  3.2818e-01, -5.6446e-01, -2.6980e-01,
        -3.6690e-01,  1.6149e-01, -2.4639e-01, -4.7106e-02,  4.9669e-02,
        -1.6383e-01, -6.6454e-01, -2.0559e-01,  3.5866e-01, -6.5202e-01,
        -2.5192e-01, -1.2759e-01, -1.0010e-01,  5.5464e-01,  2.6002e-01,
         6.1209e-01,  4.5297e-02, -2.3984e-01,  2.4185e-01, -5.3261e-01,
        -1.5795e-03,  3.7480e-01,  2.2998e-01,  2.5373e-01,  6.2845e-01,
        -4.8846e-01, -2.6556e-01,  1.0543e-01, -1.7961e-01, -4.7442e-02,
         2.0987e-01,  1.4915e-01, -5.4043e-01,  6.3419e-01, -1.4536e-02,
        -1.3952e-01,  1.4541e-03, -1.5868e-01, -4.4381e-02, -7.2536e-03,
         3.0550e-01,  1.7987e-01,  2.4523e-01, -2.6376e-01, -5.4468e-01,
         8.4813e-02, -1.6345e-01, -9.2069e-02,  3.3448e-01,  8.8672e-02,
        -4.2712e-01, -7.8307e-02,  1.5671e-01, -1.4268e-01, -2.6539e-01,
         1.5692e-02, -1.4193e-01,  6.3396e-01, -1.6775e-01, -4.3899e-01,
        -2.9766e-01,  2.0886e-01,  6.2453e-02,  1.4515e-01, -1.7576e-01,
        -5.2782e-01,  2.8647e-01, -3.6118e-01, -1.1295e-01, -1.5333e-01,
         3.8649e-02, -2.3035e-01,  4.8449e-01, -6.5390e-01,  1.7446e-01,
        -2.0998e-01, -2.4014e-01,  3.8094e-01,  5.4638e-01, -9.2301e-02,
         7.2799e-01, -5.4894e-02, -2.6638e-01,  1.8212e-01,  6.3314e-02,
         4.4116e-02, -1.3119e-01,  7.7694e-01,  1.6987e-02, -1.0065e-01,
        -4.2424e-01,  5.3672e-01, -1.0983e-01,  1.8426e-01, -1.5716e-01,
         1.5363e-01,  9.4920e-02, -3.5707e-01, -3.0785e-02, -4.7559e-02,
        -6.1601e-01,  1.6270e-01,  1.1011e-01, -1.8307e-01,  2.0297e-01,
        -3.7307e-01,  1.1780e-01, -8.7353e-01, -6.3581e-01, -9.1177e-02,
         4.5015e-01,  5.8748e-01, -2.0362e-02,  5.2032e-01,  1.2865e-01,
        -2.7387e-01,  3.3855e-01, -8.1991e-02, -3.3930e-02, -2.2129e-01,
        -1.1106e-01,  2.9059e-01,  3.2387e-01, -3.3815e-02, -3.7728e-01,
         5.8272e-01,  6.2464e-01, -2.5074e-01, -4.3780e-01,  5.6216e-01,
         1.2451e-01, -1.2899e-01,  1.2758e-01, -9.5528e-02,  2.0078e-01,
        -2.1147e-01, -1.9996e-01,  2.1658e-01, -4.6028e-01,  1.7152e-01,
        -1.7944e-01,  2.5667e-02,  2.6994e-01, -3.3626e-01, -3.3286e-01,
         2.1351e-01,  3.3059e-01,  8.8407e-02,  1.1184e-01, -9.3410e-01,
        -3.5365e-01, -4.0851e-01,  1.9238e-01,  1.9179e-01,  6.0050e-01,
        -6.1300e-01, -7.7526e-02,  4.3369e-01,  1.9656e-01,  7.5422e-02,
         1.2600e-01,  1.8432e-01,  1.6680e-01,  9.4564e-02,  2.1989e-01,
         6.5338e-02,  3.0670e-01,  2.1004e-01, -3.4678e-02,  3.7545e-01,
        -7.9084e-02, -6.3368e-02,  5.0477e-01,  2.8279e-01, -6.6957e-02,
        -8.2414e-02, -4.5131e-01,  8.7372e-01, -1.2054e-01,  2.9910e-01,
        -7.9745e-04,  1.4130e-01,  2.4211e-01,  1.4722e-01,  2.2276e-02,
         6.4288e-01,  1.9376e-01,  2.3107e-01,  4.2620e-02, -1.0629e-01,
         4.9965e-01,  1.2873e-01, -5.4799e-02,  4.2276e-01, -2.7031e-01,
        -1.9287e-01, -2.8321e-01, -4.5494e-01, -1.1602e-01, -1.0290e-01,
        -1.4740e-01,  4.4534e-01,  5.1917e-01, -1.4431e-01, -7.7550e-01,
         1.6413e-01, -2.1547e-01,  3.4814e-01,  2.9550e-02,  1.2023e-01,
         2.7038e-01, -2.4514e-01, -2.5994e-01, -4.0943e-01,  6.1809e-01,
        -6.4172e-02,  1.4789e-01,  4.9981e-01,  5.8535e-01,  2.1067e-01,
        -7.5197e-02, -2.0989e-01, -6.3907e-02, -4.6315e-01,  2.4584e-01,
        -6.1935e-01,  1.9323e-02,  5.7194e-01, -2.9602e-01, -5.6871e-02,
        -4.8013e-01, -2.9334e-02,  4.9256e-02, -4.2844e-01,  1.4215e-01,
        -1.9628e-01, -5.6353e-01,  2.3770e-01, -1.9504e-01, -6.0946e-02,
         8.6788e-01,  1.5729e-01, -3.3453e-01,  4.2605e-02,  3.4771e-01,
         2.2894e-01,  8.9872e-02,  2.3474e-01,  3.7149e-01, -2.5347e-01,
         1.2892e-01, -3.7038e-01,  6.8718e-02, -2.8476e-01,  3.6732e-01,
        -1.4778e-01,  3.5754e-01,  5.1770e-02, -1.2465e-01,  1.4894e-02,
        -5.7676e-03, -4.7557e-01, -5.1856e-01,  1.0399e-01,  8.1136e-02,
         5.3078e-01, -4.8298e-01,  2.6217e-01,  1.1708e-01, -2.2655e-02,
         3.2414e-01,  3.2335e-01, -3.2487e-01, -3.6452e-01,  4.9279e-01,
         2.7096e-01, -2.5982e-01, -2.0117e-01, -2.4121e-01, -1.1426e-01,
        -7.0331e-02, -3.8127e-01, -3.8204e-01, -1.4516e-01, -3.1634e-01,
         2.4916e-01,  2.1327e-01, -2.6620e-01, -1.7288e-01,  2.1561e-02,
         2.7197e-01, -1.3148e-01,  8.5358e-02, -2.5469e-01,  1.9918e-01,
        -8.2631e-02, -3.2942e-01,  4.1063e-01, -2.2063e-01,  1.4496e-01,
         1.9767e-01, -1.2407e-01, -1.5407e-01, -5.4703e-01,  7.7805e-02,
        -4.6431e-01, -1.4431e-01,  9.7252e-02,  6.0515e-01, -5.5042e-01,
        -2.1915e-01, -2.3620e-01,  6.5295e-02,  8.8855e-01,  1.2050e-01,
        -2.8610e-01,  1.6722e-01,  4.6341e-01,  9.4008e-03, -9.6328e-02,
        -3.3636e-01,  4.1791e-01,  1.4727e-01,  4.5041e-01, -4.3296e-01,
        -3.3456e-02,  6.8088e-02, -1.9341e-01, -2.2595e-01,  1.5026e-01,
        -1.9233e-01, -2.3071e-02, -1.5759e-01,  2.4411e-01, -1.4933e-01,
        -7.8673e-02, -5.1893e-01, -1.8274e-01, -3.6652e-01,  3.9821e-01,
        -1.1308e-03,  1.1077e-01,  1.1272e-01,  1.4247e-01, -2.5590e-01,
        -2.6423e-01, -3.1982e-01,  5.4112e-01,  4.2725e-01,  4.2929e-02,
        -1.4493e-01,  4.0068e-01, -1.5884e-01, -4.5532e-01, -3.6434e-01,
        -2.6256e-01, -1.9314e-01, -3.0976e-01,  2.1947e-01,  1.2527e+00,
         4.6440e-02,  5.4746e-02, -4.3317e-02,  3.7618e-01, -3.7077e-01,
        -2.0447e-01, -1.2383e-01,  2.0692e-01,  1.3606e-01,  1.9516e-01,
         2.2275e-01,  1.7480e-01,  3.2938e-01, -5.1290e-02, -7.7177e-01,
        -2.7293e-01,  3.2536e-01, -1.2914e-01,  8.3979e-02,  3.6308e-02,
        -3.2365e-02, -2.9454e-01,  3.3357e-01, -1.0323e-01,  2.0189e-01,
         2.5102e-02,  4.2577e-01, -3.1884e-01,  4.0641e-01,  8.1616e-03,
         3.8972e-01, -1.5253e-01,  6.5812e-01, -3.0501e-01,  1.1704e-01,
        -3.4881e-01,  1.1053e-02, -3.0909e-01, -7.7253e-02,  3.6673e-01,
        -7.6565e-02,  9.1087e-03, -1.7329e-01,  2.6309e-01,  8.9734e-02,
        -4.5330e-01,  1.5570e-02, -4.8282e-01,  4.0811e-02, -3.2425e-02,
        -3.0552e-01,  9.6641e-02, -1.8799e-02,  1.7777e-02,  7.3005e-01,
         1.4098e-01,  1.0050e-01, -1.0706e-01, -1.7136e-01,  2.0919e-01,
         2.9493e-01, -8.6930e-01, -8.4521e-02, -4.6610e-01,  4.6881e-01,
         2.8357e-01,  2.9123e-01,  5.6292e-01, -7.6437e-01,  9.9124e-02,
        -1.5738e-01,  4.0906e-01,  1.5995e-01, -3.8363e-01,  1.4078e-02,
         2.1916e-01,  2.0725e-01, -1.0849e-01, -4.3832e-01, -4.3885e-01,
        -2.4597e-02,  3.0685e-01, -4.8883e-01, -4.4606e-02, -8.3244e-01,
        -3.0004e-01,  3.6187e-01, -1.8884e-01,  1.1498e-01, -2.2350e-01,
         1.8464e-01, -5.0595e-02,  3.7592e-01, -1.6456e-01,  9.8069e-02,
         2.6785e-01,  2.1655e-01,  2.4135e-01, -4.7972e-01,  1.9272e-01,
         1.7350e-01, -4.1612e-01], device='cuda:1', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1730, -0.2277, -0.1308,  ..., -0.0064,  0.0714,  0.0089],
        [-0.3097, -0.3952,  0.1457,  ...,  0.1606, -0.3123,  0.2392],
        [-0.3884,  0.1280, -0.2651,  ..., -0.3274,  0.0380,  0.5866],
        ...,
        [-0.1138,  0.1471, -0.2607,  ..., -0.1059,  0.1240, -0.2748],
        [ 0.1022, -0.0441,  0.4888,  ...,  0.2420, -0.3309, -0.0214],
        [ 0.3723, -0.2546, -0.5360,  ...,  0.0103,  0.2929, -0.4245]],
       device='cuda:1', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 1.4179e-01,  3.2210e-01, -5.6643e-01,  2.7856e-01,  4.0780e-01,
        -5.2465e-01, -4.4860e-01,  6.3692e-01,  1.2261e-01,  3.3203e-01,
         1.1851e-01,  3.5166e-01,  4.1000e-01, -6.7416e-02,  5.7536e-01,
         9.8060e-02,  4.7483e-02, -9.5358e-02,  5.6268e-02,  4.9899e-02,
         2.9323e-01,  5.6371e-01,  2.5355e-01, -7.4552e-02, -3.9356e-01,
         1.9312e-01, -4.2989e-03, -7.0626e-02, -1.7859e-01,  4.3795e-01,
        -3.8399e-01,  5.2805e-02, -1.2964e-01,  5.6573e-02, -1.4092e-01,
         1.6990e-01,  6.7457e-02, -3.0747e-01,  4.2022e-01,  3.0776e-01,
        -3.1603e-02,  1.7747e-01,  1.3542e-01, -2.8546e-01,  6.4476e-01,
         2.1349e-02, -7.8576e-01,  4.7399e-02, -1.9146e-02,  8.6159e-04,
         4.6737e-01,  3.0309e-01, -8.5541e-02, -2.5931e-02, -1.7506e-01,
        -3.3630e-01,  1.6496e-01, -2.1160e-01,  8.8874e-02,  9.7483e-02,
         1.7767e-01,  8.5455e-01,  3.7553e-01,  1.7289e-01,  3.1729e-01,
        -1.1959e-01,  6.3505e-01, -3.6701e-01, -3.2456e-01,  4.0400e-01,
         3.2255e-01, -6.0352e-01,  2.1656e-02, -3.1416e-01, -3.4468e-01,
        -3.2838e-01,  1.2244e-01,  2.3571e-01,  5.9361e-01,  2.3576e-01,
         2.9457e-01,  2.5872e-01,  5.8149e-01, -4.4340e-01, -5.3872e-01,
        -2.0802e-01,  4.6460e-01, -2.5942e-01, -1.6369e-01, -3.9992e-01,
        -4.5108e-01,  1.5733e-01,  1.2554e-03,  1.2605e-01,  1.7727e-01,
        -1.0899e-01,  4.6134e-01, -1.1725e-01, -1.8230e-01, -3.0823e-01,
         1.0486e-02,  6.9209e-04, -1.7649e-01,  2.0212e-01, -1.1128e-01,
        -2.9200e-01, -4.3355e-01, -2.6220e-02, -3.9349e-01,  7.9136e-02,
         1.3552e-01, -5.1725e-01, -1.9342e-01, -9.7791e-02,  7.8979e-03,
        -3.1055e-01, -3.4341e-01, -1.3593e-01,  1.9896e-01, -1.7927e-01,
        -5.4761e-01, -2.2198e-01,  4.2610e-01,  8.2436e-02, -2.0173e-01,
         2.4040e-02, -1.8343e-01, -2.1978e-01,  3.5884e-01, -7.7458e-01,
         2.3324e-01,  1.3607e-01, -3.9449e-01,  4.1063e-01, -5.3288e-01,
         1.1797e-01, -3.6661e-01,  5.9781e-01, -2.7411e-01,  8.7778e-02,
         1.4884e-01, -3.0281e-01, -6.3868e-01, -1.1967e-01,  2.4771e-01,
         5.4744e-01, -1.4941e-01,  2.3260e-02,  3.9616e-04, -4.2770e-01,
        -4.0471e-01, -2.0044e-01,  3.2872e-01,  4.6651e-02,  2.4371e-01,
         1.9984e-01,  1.5637e-01, -8.4627e-02,  8.0435e-02,  1.6755e-01,
         2.4172e-01, -5.1212e-01,  1.8761e-01, -2.4661e-01,  1.5017e-01,
        -4.1426e-01,  3.1818e-01, -2.4743e-02, -6.2680e-02,  1.5470e-01,
         1.4934e-01,  2.2034e-01,  1.2142e-01,  2.1281e-01,  5.2308e-01,
         1.7412e-01,  2.7777e-01,  1.9550e-01,  5.3673e-01,  1.1650e-01,
        -1.3916e-01, -6.9720e-01,  6.8880e-01,  2.5903e-01, -1.0851e-01,
         1.5277e-01,  3.2483e-01, -2.9587e-02, -2.5160e-01, -8.1416e-02,
         7.9530e-02, -2.6971e-01,  7.0683e-01, -9.2652e-02, -2.7599e-01,
        -2.8310e-01, -3.0244e-01,  2.5042e-01, -2.0662e-01, -1.3467e-01,
        -3.8732e-01, -3.3861e-01, -2.8108e-01, -3.0866e-01, -6.7359e-02,
         3.5845e-01, -1.1492e-01, -2.6174e-01,  7.9780e-01,  6.6983e-02,
        -3.5975e-01,  4.4830e-01,  2.3262e-01, -3.8860e-01,  3.2931e-01,
        -5.0148e-01,  3.9907e-01,  6.3266e-01,  1.0864e-01,  3.5626e-02,
         4.6151e-01, -4.1601e-01,  2.0126e-01,  2.8901e-01, -2.9050e-01,
         1.5636e-01,  1.5314e-01, -2.6690e-01,  2.0631e-01, -1.2344e-01,
         5.0111e-02, -2.8826e-01, -1.3203e-01, -1.0933e-01, -1.2740e-01,
        -1.6363e-01, -3.2539e-02, -4.1435e-02,  2.0555e-01,  2.7631e-01,
        -1.3128e-01,  1.9418e-01,  2.1531e-01,  3.0464e-01,  1.2553e-01,
        -4.3558e-03,  3.5148e-03,  4.5660e-02, -2.6346e-01, -1.2613e-01,
        -1.6006e-01,  2.5396e-01,  2.3544e-03,  2.7709e-01,  2.7365e-01,
         1.5975e-01, -2.0429e-01,  2.6308e-01,  1.1232e-01, -1.7520e-01,
        -2.9343e-02,  1.9986e-01,  3.7351e-01,  1.8321e-03,  2.8144e-01,
         1.6961e-01,  1.1166e-01, -4.7342e-01,  2.5215e-01,  1.2465e-01,
        -1.7574e-01, -2.3487e-01, -3.2847e-02, -3.3511e-01, -1.6397e-01,
         9.2866e-02,  6.8842e-01, -1.2720e-01,  3.8358e-03, -1.8727e-01,
         1.1597e-01,  1.0454e-01, -2.6154e-01,  2.1201e-01, -4.5223e-01,
         2.0397e-01, -6.3676e-01, -1.0293e-01, -2.7385e-01, -3.9719e-01,
         1.6923e-01,  8.8046e-02,  1.1130e-01, -3.8459e-01, -3.7879e-01,
         5.1172e-01,  4.1529e-01, -3.8174e-01, -1.2938e-01,  3.2678e-01,
         4.4141e-01,  5.7247e-01,  5.7335e-01,  2.2937e-01,  1.7478e-01,
         3.0146e-01,  2.0555e-01, -8.7688e-01,  2.6807e-01,  3.9275e-01,
         3.8590e-02, -3.1944e-01, -5.3004e-02,  1.4634e-02,  1.4402e-01,
        -2.0849e-01,  1.3237e-01,  1.5445e-01, -2.2459e-02,  2.4120e-01,
         1.2028e-01, -1.7279e-01,  4.2801e-01, -7.6152e-01, -2.1103e-01,
         6.2899e-01, -1.6014e-01,  5.1742e-02,  4.7674e-01,  5.3797e-02,
        -1.6155e-01,  1.0281e-01, -5.7464e-01,  2.1592e-01, -1.2658e-01,
         4.8350e-02,  3.5443e-02,  1.1428e-01, -4.5882e-01, -9.1370e-02,
        -8.0386e-02, -5.6888e-02,  7.8916e-02,  1.6889e-02,  9.4458e-02,
        -1.0846e-02,  1.4602e-01,  2.9974e-01, -1.2817e-01,  1.4236e-01,
        -5.1285e-01, -4.5173e-01,  3.7114e-01, -6.4035e-02,  3.9342e-01,
         3.0148e-01, -8.0483e-02, -7.5345e-02, -3.7438e-01, -1.1560e-01,
        -7.6414e-01,  1.1941e-01,  3.2946e-01,  1.6418e-01,  2.5149e-01,
         2.8158e-01, -3.1064e-01, -2.1117e-01, -3.6134e-01, -1.7946e-01,
        -3.8067e-01, -1.8442e-02,  5.4178e-01,  1.0826e-01, -4.2489e-01,
         9.8786e-02, -1.9886e-01, -1.9944e-01, -2.6189e-01,  3.2915e-01,
         2.7132e-01, -2.7107e-01,  1.9721e-01, -3.4363e-02, -3.4736e-01,
         7.6883e-01,  7.6609e-01,  1.3300e-01,  1.0317e-01, -9.9651e-02,
        -1.5886e-01,  3.3379e-01,  1.1817e-01,  6.1088e-01, -9.8981e-02,
         3.2033e-02, -3.2494e-01,  6.3448e-01, -1.8965e-02, -1.0558e-01,
         1.8887e-01,  2.4920e-02, -4.7867e-01,  1.6645e-03,  1.6404e-01,
        -2.1174e-01,  5.4358e-01, -1.9561e-01, -6.2061e-01, -5.3127e-01,
         1.8306e-01,  5.1041e-01,  2.6210e-01,  4.7968e-01,  4.0350e-01,
         1.7007e-01, -6.5120e-02,  3.2445e-02, -2.4497e-01,  3.8961e-02,
         3.3875e-01, -2.3760e-01,  4.1712e-01,  1.6046e-01, -2.4134e-01,
         1.4197e-01, -1.9834e-01,  4.0619e-01,  2.6777e-01,  2.4601e-01,
         9.6592e-02,  2.2297e-02,  4.6209e-01,  3.8926e-01,  1.6503e-01,
         1.6308e-01, -2.7902e-01, -1.7507e-01, -1.0870e-01,  4.1737e-02,
         1.2515e-01, -1.6289e-01,  2.8835e-01, -5.0282e-01,  2.9335e-01,
         5.7595e-01, -2.0366e-01,  4.6764e-02, -2.6312e-01,  4.7355e-01,
        -3.3565e-01,  2.4745e-01, -8.0281e-02,  3.2135e-02, -6.0093e-01,
         4.9367e-01, -3.1144e-01,  2.6010e-01,  6.3402e-02, -1.1066e-02,
        -2.8310e-01, -5.9138e-01,  1.9894e-02, -5.6878e-01,  4.3710e-01,
        -1.8511e-01,  1.4252e-01, -4.7774e-01, -1.0077e-01, -2.5265e-01,
         1.0201e-01, -2.4135e-01, -3.3036e-01,  4.7532e-02,  1.9244e-01,
        -1.9187e-01,  3.5796e-02,  2.8442e-01, -2.9295e-01,  2.3329e-02,
         2.8451e-01, -1.1799e-01,  2.6453e-01,  1.4019e-01, -7.1528e-02,
         1.0160e+00,  2.1072e-01,  1.9396e-01, -1.2166e-01, -6.4238e-02,
        -1.6593e-01,  2.4005e-01, -2.9599e-01, -1.6909e-01,  9.1479e-03,
        -5.0724e-02, -3.1212e-01,  4.8560e-01,  3.9701e-01,  1.2635e-01,
        -5.3069e-01, -8.0266e-02,  3.7003e-01, -3.5217e-01,  8.7458e-02,
         5.4936e-01, -4.9864e-01,  1.0496e-01,  1.4389e-01, -7.2095e-01,
         1.7354e-01, -1.0492e-01], device='cuda:1', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 0.7570,  0.0071,  0.1389,  0.6667,  0.1712,  0.3561,  0.6226, -0.3891,
          0.5041, -0.1839, -0.3202, -0.1969,  0.1615,  0.0672,  0.6710, -0.3528,
         -0.3141, -0.2882, -0.8871,  0.1790, -0.3063,  0.4413, -1.0605, -0.2951,
         -0.3255, -0.0646,  0.2149, -0.5881, -0.2235, -0.0753,  0.5737, -0.5544,
         -0.0257, -0.1990, -0.4254, -0.1341,  0.0489, -0.3904, -0.2764, -0.3566,
          0.1979,  0.6079, -0.1551, -0.4622,  0.5872, -0.5354, -0.3222, -0.0805,
         -0.3098, -0.0448,  0.7059,  0.6188,  0.2833,  0.2381, -0.2853, -0.1509,
         -0.4050, -0.2043, -0.2229, -0.4423,  0.1936, -0.2756, -0.6272, -0.0563,
          0.0129,  0.0949,  0.0776,  0.0665,  0.0221, -0.1633, -0.2864,  0.3297,
          0.3653,  0.2385, -0.1601,  0.0213,  0.0414,  0.4344, -0.1426,  0.3209,
         -0.1330, -0.0715,  0.1667, -0.1738, -0.5722, -0.5953, -0.0185, -0.6191,
          0.4194,  0.3419,  0.0859,  0.2053,  0.2619, -0.2133, -0.4736,  0.0040,
         -0.3127, -0.3731, -0.2702,  0.1135,  0.2918,  0.2723, -0.2393, -0.0356,
          0.3593, -0.0179, -0.0283,  0.2146,  0.2183,  0.4363, -0.0966,  0.5011,
          0.4738, -0.2301,  0.3377,  0.5182,  0.2462, -0.2835, -0.2046, -0.0658,
          0.4032,  0.2224, -0.2755,  0.8335,  0.2519, -0.2333,  0.0282,  0.0173,
         -0.2267,  0.3252, -0.0499,  0.0456, -0.2739,  0.3119, -0.1075, -0.3342,
          0.2032, -0.3366, -0.2031,  0.1704, -0.1464, -0.3186,  0.1348, -0.2662,
         -0.4975,  0.1851,  0.2307,  0.0125, -0.1342,  0.5029, -0.3441, -0.0833,
         -0.5997,  0.1109,  0.1963,  0.0987, -0.6184,  0.0578,  0.1665,  0.3195,
          0.2226,  0.3615, -0.7867, -0.3353, -0.0744,  0.2562, -0.1247,  0.2649,
         -0.3539, -0.0216,  0.0795,  0.0297, -0.0140,  0.0654,  0.0853, -0.0177,
          0.1761, -0.1112,  0.1291, -0.4507, -0.3451, -0.2394,  0.4435,  0.4105,
         -0.4136, -0.0931, -0.5896,  0.0572, -0.3367, -0.3010,  0.0908, -0.1063,
         -0.5687,  0.1835, -0.1890, -0.0747,  0.2271,  0.1515, -0.0937,  0.0105,
         -0.5979,  0.5121,  0.2226,  0.2394, -0.1796, -0.5596, -0.1476, -0.3604,
          0.0613, -0.6463, -0.2884,  0.0498,  0.0476, -0.1238,  0.0033, -0.1810,
          0.2991, -0.7201, -0.0066,  0.6760,  0.0838, -0.2200,  0.5599, -0.4012,
          0.0079,  0.4337,  0.0768,  0.5233,  0.0051, -0.1023, -0.2747,  0.1599,
         -0.0071,  0.3136,  0.1999, -0.0581,  0.1474, -0.4202,  0.3128, -0.0721,
          0.1800, -0.6434,  0.2795,  0.0963,  0.1675, -0.2505,  0.5946,  0.0881,
         -0.3420, -0.4583,  0.2318, -0.1727,  0.3149, -0.2287, -0.0798, -0.1796,
          0.2559,  0.4442, -0.1899, -0.2371, -0.5489, -0.4815,  0.0435, -0.3169,
          0.4664,  0.0213,  0.0719, -0.0919,  0.0049,  0.0717, -0.2498, -0.1512,
          0.0769, -0.3439,  0.0776, -0.3771,  0.3978, -0.3865,  0.0013,  0.3145,
         -0.2653,  0.3279, -0.5856, -0.6437, -0.3371,  0.2609, -0.0104, -0.0156,
          0.1098,  0.2341,  0.1565,  0.2076,  0.0886, -0.3759, -0.1986,  0.2679,
          0.2184,  0.0187, -0.2609, -0.0485,  0.2321,  0.2120, -0.3796, -0.1598,
         -0.2731, -0.2402, -0.7589, -0.0833,  0.2278,  0.1961,  0.3413, -0.0987,
          0.4173, -0.0623, -0.5167, -0.5240, -0.3195, -0.2283,  0.4674, -0.0255,
         -0.5547,  0.4798, -0.4382, -0.0420, -0.1176,  0.0931, -0.0501,  0.4722,
          0.6665, -0.0861,  0.6480, -0.0445, -0.3785, -0.0294,  0.3172, -0.1142,
          0.0714, -0.1573,  0.0901, -0.3999,  0.2734,  0.3352,  0.1221,  0.1154,
         -0.2944,  0.4117, -0.2558, -0.0271, -0.0529, -0.1143, -0.3698,  0.3019,
         -0.3937, -0.0406,  0.4142, -0.0227,  0.1544,  0.1405,  0.6259, -0.3281,
         -0.1419,  0.6239,  0.0778, -0.2243, -0.2499,  0.0725, -0.4267, -0.4464,
          0.5441, -0.0022, -0.2501,  0.1621, -0.2522,  0.2555, -0.1617, -0.1284,
         -0.0315,  0.1421, -0.1258, -0.0346,  0.0374,  0.2211, -0.6120,  0.1248,
         -0.3892, -0.1977,  0.4000,  0.2652,  0.1389, -0.1081,  0.6099,  0.2467,
         -0.0990,  0.2355,  0.4636, -0.2268,  0.4451, -0.2121, -0.5002, -0.0408,
          0.2536,  0.3768, -0.6709,  0.2756, -0.2226, -0.4895, -0.5475,  0.0174,
          0.4232,  0.3590,  0.6767,  0.3510, -0.0845, -0.0845, -0.4329, -0.2155,
         -0.0831,  0.1408,  0.3465,  0.3161,  0.3256,  0.1817, -0.4837,  0.2695,
         -0.1407, -0.2695,  0.2053,  0.8556, -0.1175, -0.1025,  0.1077,  0.5011,
         -0.3269,  0.2269, -0.1758, -0.2570,  0.0990, -0.4238, -0.2143, -0.4339,
          0.5493,  0.0367, -0.0246,  0.2124,  0.3997, -0.2968, -0.6228,  0.4566,
          0.2689, -0.6811,  0.2121,  0.0886,  0.2624,  0.4336,  0.1599,  0.4324,
          0.5957, -0.5103,  0.8678, -0.3761, -0.3902, -0.2045, -0.1148,  0.1110,
         -0.3288, -0.1500,  0.7500, -0.5143,  0.4097, -0.2637,  0.2661, -0.3321,
          0.1945,  0.0906, -0.0694,  0.2230, -0.0518,  0.0263,  0.7312,  0.0204,
         -0.3837, -0.0236, -0.0973,  0.4279, -0.1067, -0.4604,  0.3151,  0.2388,
          0.0919,  0.3406, -0.2363, -0.3606,  0.5200,  0.1322, -0.2743,  0.1592,
         -0.4984,  0.0436, -0.2330, -0.0258,  0.0537, -0.4252,  0.3755, -0.1834,
         -0.2058, -0.5455,  0.1601, -0.1842, -0.2745, -0.3696, -0.3067,  0.3493]],
       device='cuda:1', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.1671], device='cuda:1', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:1', grad_fn=<AddBackward0>)
Using device: cuda:1
===== Training profile tensin-3x512-s03 - 1 =====
[0:00:01.752161] epoch: 0 | elbo: 404235831.0400001 | train_rmse: 66.8983 | val_rmse: 69.1621 | val_ll: -30.9205
[0:01:33.842587] epoch: 50 | elbo: 20280836.62 | train_rmse: 16.3065 | val_rmse: 19.6247 | val_ll: -5.9572
[0:03:06.241011] epoch: 100 | elbo: 12139745.540000003 | train_rmse: 11.2061 | val_rmse: 16.481 | val_ll: -5.4108
[0:04:38.168166] epoch: 150 | elbo: 8865461.179999998 | train_rmse: 8.4355 | val_rmse: 14.9907 | val_ll: -5.2054
[0:06:10.599855] epoch: 200 | elbo: 7044814.004999998 | train_rmse: 6.5802 | val_rmse: 14.1418 | val_ll: -5.169
[0:07:42.666430] epoch: 250 | elbo: 5922570.375 | train_rmse: 5.1914 | val_rmse: 13.4985 | val_ll: -5.1271
[0:09:14.729394] epoch: 300 | elbo: 5145707.154999999 | train_rmse: 4.1448 | val_rmse: 13.035 | val_ll: -5.1999
[0:10:46.294373] epoch: 350 | elbo: 4631466.175 | train_rmse: 3.3332 | val_rmse: 12.6674 | val_ll: -5.2674
[0:12:19.680764] epoch: 400 | elbo: 4205053.047499999 | train_rmse: 2.6349 | val_rmse: 12.3011 | val_ll: -5.3704
[0:13:52.790531] epoch: 450 | elbo: 3903310.7300000004 | train_rmse: 2.1245 | val_rmse: 12.0067 | val_ll: -5.4739
[0:15:28.210349] epoch: 500 | elbo: 3661392.3674999997 | train_rmse: 1.7073 | val_rmse: 11.7023 | val_ll: -5.5728
[0:17:04.084835] epoch: 550 | elbo: 3473510.3525000005 | train_rmse: 1.4216 | val_rmse: 11.4241 | val_ll: -5.6727
[0:18:39.774113] epoch: 600 | elbo: 3317441.705 | train_rmse: 1.1667 | val_rmse: 11.1132 | val_ll: -5.8235
[0:20:16.170179] epoch: 650 | elbo: 3185576.37 | train_rmse: 0.9903 | val_rmse: 10.8108 | val_ll: -5.9473
[0:21:52.317141] epoch: 700 | elbo: 3075016.6325 | train_rmse: 0.8838 | val_rmse: 10.5008 | val_ll: -6.0736
[0:23:27.728037] epoch: 750 | elbo: 2976431.9324999996 | train_rmse: 0.7779 | val_rmse: 10.2019 | val_ll: -6.2628
[0:25:02.818722] epoch: 800 | elbo: 2883314.5625000005 | train_rmse: 0.7613 | val_rmse: 9.9246 | val_ll: -6.3558
[0:26:36.712956] epoch: 850 | elbo: 2804986.5050000004 | train_rmse: 0.6719 | val_rmse: 9.6165 | val_ll: -6.5042
[0:28:11.543736] epoch: 900 | elbo: 2744414.06 | train_rmse: 0.6506 | val_rmse: 9.3205 | val_ll: -6.6762
[0:29:45.418952] epoch: 950 | elbo: 2684393.9175 | train_rmse: 0.5789 | val_rmse: 9.054 | val_ll: -6.8862
[0:31:19.542433] epoch: 1000 | elbo: 2623750.9999999995 | train_rmse: 0.5539 | val_rmse: 8.7827 | val_ll: -6.9988
[0:32:54.003857] epoch: 1050 | elbo: 2578370.2849999997 | train_rmse: 0.5421 | val_rmse: 8.5375 | val_ll: -7.1334
[0:34:26.856807] epoch: 1100 | elbo: 2533255.4725 | train_rmse: 0.5068 | val_rmse: 8.2668 | val_ll: -7.3345
[0:35:59.771283] epoch: 1150 | elbo: 2485688.7375 | train_rmse: 0.4795 | val_rmse: 8.0223 | val_ll: -7.4365
[0:37:34.310597] epoch: 1200 | elbo: 2452550.6275 | train_rmse: 0.4663 | val_rmse: 7.7798 | val_ll: -7.5655
[0:39:08.094145] epoch: 1250 | elbo: 2413161.5025 | train_rmse: 0.4537 | val_rmse: 7.5439 | val_ll: -7.6498
[0:40:40.963598] epoch: 1300 | elbo: 2378132.0625 | train_rmse: 0.4469 | val_rmse: 7.331 | val_ll: -7.6868
[0:42:13.434646] epoch: 1350 | elbo: 2346438.7424999997 | train_rmse: 0.4144 | val_rmse: 7.1224 | val_ll: -7.7772
[0:43:46.652996] epoch: 1400 | elbo: 2317657.9299999997 | train_rmse: 0.3981 | val_rmse: 6.9056 | val_ll: -7.7807
[0:45:19.349232] epoch: 1450 | elbo: 2287508.375 | train_rmse: 0.3826 | val_rmse: 6.7115 | val_ll: -7.8818
[0:46:52.989690] epoch: 1500 | elbo: 2260870.585 | train_rmse: 0.3888 | val_rmse: 6.5045 | val_ll: -7.8245
[0:48:25.725542] epoch: 1550 | elbo: 2232099.7525000004 | train_rmse: 0.3648 | val_rmse: 6.3085 | val_ll: -7.8378
[0:49:59.391177] epoch: 1600 | elbo: 2204153.9725 | train_rmse: 0.3576 | val_rmse: 6.1237 | val_ll: -7.6904
[0:51:33.751399] epoch: 1650 | elbo: 2176070.5599999996 | train_rmse: 0.3666 | val_rmse: 5.9487 | val_ll: -7.7233
[0:53:07.561878] epoch: 1700 | elbo: 2150259.5925 | train_rmse: 0.3719 | val_rmse: 5.7724 | val_ll: -7.6887
[0:54:40.823476] epoch: 1750 | elbo: 2122776.2725 | train_rmse: 0.3319 | val_rmse: 5.6056 | val_ll: -7.5882
[0:56:14.005252] epoch: 1800 | elbo: 2098435.5687500006 | train_rmse: 0.3284 | val_rmse: 5.4331 | val_ll: -7.3013
[0:57:46.992932] epoch: 1850 | elbo: 2073492.95625 | train_rmse: 0.3408 | val_rmse: 5.2745 | val_ll: -7.3082
[0:59:19.857998] epoch: 1900 | elbo: 2047295.12375 | train_rmse: 0.3483 | val_rmse: 5.1214 | val_ll: -7.0172
[1:00:51.497521] epoch: 1950 | elbo: 2019165.4449999998 | train_rmse: 0.303 | val_rmse: 4.988 | val_ll: -6.84
[1:02:23.978272] epoch: 2000 | elbo: 1994957.875 | train_rmse: 0.3113 | val_rmse: 4.8416 | val_ll: -6.722
[1:03:56.254609] epoch: 2050 | elbo: 1969434.7075000003 | train_rmse: 0.2943 | val_rmse: 4.7141 | val_ll: -6.5762
[1:05:28.371573] epoch: 2100 | elbo: 1944344.4287500004 | train_rmse: 0.3183 | val_rmse: 4.5993 | val_ll: -6.4598
[1:07:00.779921] epoch: 2150 | elbo: 1917891.35 | train_rmse: 0.2929 | val_rmse: 4.4848 | val_ll: -6.2358
[1:08:33.231449] epoch: 2200 | elbo: 1892308.5837499998 | train_rmse: 0.2859 | val_rmse: 4.3685 | val_ll: -6.0017
[1:10:05.000621] epoch: 2250 | elbo: 1867313.9324999999 | train_rmse: 0.2834 | val_rmse: 4.258 | val_ll: -5.8915
[1:11:37.557005] epoch: 2300 | elbo: 1842076.0875 | train_rmse: 0.2817 | val_rmse: 4.1581 | val_ll: -5.8053
[1:13:10.199043] epoch: 2350 | elbo: 1816682.9575 | train_rmse: 0.2815 | val_rmse: 4.0645 | val_ll: -5.5849
[1:14:43.177446] epoch: 2400 | elbo: 1790975.92 | train_rmse: 0.2836 | val_rmse: 3.9704 | val_ll: -5.4376
[1:16:15.294313] epoch: 2450 | elbo: 1766087.1612500001 | train_rmse: 0.2786 | val_rmse: 3.8787 | val_ll: -5.3244
[1:17:49.986397] epoch: 2500 | elbo: 1740443.8800000001 | train_rmse: 0.2949 | val_rmse: 3.7875 | val_ll: -5.1259
[1:19:24.105301] epoch: 2550 | elbo: 1715243.355 | train_rmse: 0.2679 | val_rmse: 3.7068 | val_ll: -4.9865
[1:20:58.077344] epoch: 2600 | elbo: 1690149.2774999999 | train_rmse: 0.2836 | val_rmse: 3.6347 | val_ll: -4.8654
[1:22:33.963823] epoch: 2650 | elbo: 1664916.3187500003 | train_rmse: 0.2752 | val_rmse: 3.5687 | val_ll: -4.7364
[1:24:06.586014] epoch: 2700 | elbo: 1640399.455 | train_rmse: 0.2712 | val_rmse: 3.4911 | val_ll: -4.6556
[1:25:41.027727] epoch: 2750 | elbo: 1615835.7550000001 | train_rmse: 0.2781 | val_rmse: 3.4296 | val_ll: -4.5856
[1:27:13.250545] epoch: 2800 | elbo: 1590791.6075 | train_rmse: 0.276 | val_rmse: 3.3718 | val_ll: -4.4581
[1:28:47.040652] epoch: 2850 | elbo: 1566402.5137500002 | train_rmse: 0.2773 | val_rmse: 3.3079 | val_ll: -4.324
[1:30:19.325674] epoch: 2900 | elbo: 1542317.3275000001 | train_rmse: 0.2815 | val_rmse: 3.2555 | val_ll: -4.2492
[1:31:52.933134] epoch: 2950 | elbo: 1517716.1925000001 | train_rmse: 0.2791 | val_rmse: 3.1957 | val_ll: -4.1987
[1:33:26.227405] epoch: 3000 | elbo: 1493663.8150000002 | train_rmse: 0.28 | val_rmse: 3.139 | val_ll: -4.0643
[1:34:59.492855] epoch: 3050 | elbo: 1470083.81375 | train_rmse: 0.2853 | val_rmse: 3.0887 | val_ll: -3.9786
[1:36:31.999326] epoch: 3100 | elbo: 1445495.09 | train_rmse: 0.2927 | val_rmse: 3.0427 | val_ll: -3.9252
[1:38:05.937542] epoch: 3150 | elbo: 1421764.7312499997 | train_rmse: 0.3032 | val_rmse: 2.9964 | val_ll: -3.8448
[1:39:41.178569] epoch: 3200 | elbo: 1398290.8275000001 | train_rmse: 0.3009 | val_rmse: 2.9418 | val_ll: -3.7893
[1:41:13.838590] epoch: 3250 | elbo: 1374215.61 | train_rmse: 0.2955 | val_rmse: 2.8928 | val_ll: -3.699
[1:42:46.902846] epoch: 3300 | elbo: 1351477.045 | train_rmse: 0.3053 | val_rmse: 2.8436 | val_ll: -3.5617
[1:44:19.653074] epoch: 3350 | elbo: 1327738.6999999997 | train_rmse: 0.3081 | val_rmse: 2.7964 | val_ll: -3.4838
[1:45:51.405217] epoch: 3400 | elbo: 1304988.83375 | train_rmse: 0.3141 | val_rmse: 2.7511 | val_ll: -3.3976
[1:47:23.520704] epoch: 3450 | elbo: 1281396.36375 | train_rmse: 0.3176 | val_rmse: 2.7067 | val_ll: -3.3642
[1:48:55.967210] epoch: 3500 | elbo: 1258014.135 | train_rmse: 0.3185 | val_rmse: 2.66 | val_ll: -3.2463
[1:50:28.202448] epoch: 3550 | elbo: 1234922.42375 | train_rmse: 0.3203 | val_rmse: 2.6101 | val_ll: -3.1693
[1:52:00.693497] epoch: 3600 | elbo: 1211884.44 | train_rmse: 0.3295 | val_rmse: 2.5598 | val_ll: -3.1066
[1:53:33.389840] epoch: 3650 | elbo: 1189453.1525 | train_rmse: 0.3301 | val_rmse: 2.5141 | val_ll: -3.0281
[1:55:06.764160] epoch: 3700 | elbo: 1166311.23875 | train_rmse: 0.3337 | val_rmse: 2.4611 | val_ll: -2.9599
[1:56:38.280885] epoch: 3750 | elbo: 1143781.46875 | train_rmse: 0.3442 | val_rmse: 2.4219 | val_ll: -2.8708
[1:58:09.538409] epoch: 3800 | elbo: 1120668.22 | train_rmse: 0.3447 | val_rmse: 2.3657 | val_ll: -2.8636
[1:59:40.561288] epoch: 3850 | elbo: 1097844.0150000001 | train_rmse: 0.3511 | val_rmse: 2.3193 | val_ll: -2.76
[2:01:10.994357] epoch: 3900 | elbo: 1075264.4674999998 | train_rmse: 0.3612 | val_rmse: 2.2735 | val_ll: -2.6805
[2:02:42.607925] epoch: 3950 | elbo: 1053912.66 | train_rmse: 0.3632 | val_rmse: 2.2225 | val_ll: -2.6448
[2:04:13.867578] epoch: 4000 | elbo: 1031339.9668750002 | train_rmse: 0.3656 | val_rmse: 2.1739 | val_ll: -2.5496
[2:05:44.758209] epoch: 4050 | elbo: 1008474.15625 | train_rmse: 0.3621 | val_rmse: 2.1269 | val_ll: -2.5203
[2:07:16.101799] epoch: 4100 | elbo: 986607.5156249998 | train_rmse: 0.3687 | val_rmse: 2.078 | val_ll: -2.4551
[2:08:47.310332] epoch: 4150 | elbo: 963678.9706250001 | train_rmse: 0.376 | val_rmse: 2.0325 | val_ll: -2.3997
[2:10:18.737288] epoch: 4200 | elbo: 941719.0818750001 | train_rmse: 0.3796 | val_rmse: 1.9862 | val_ll: -2.3421
[2:11:50.433183] epoch: 4250 | elbo: 920362.325 | train_rmse: 0.3809 | val_rmse: 1.94 | val_ll: -2.3172
[2:13:22.204839] epoch: 4300 | elbo: 897620.781875 | train_rmse: 0.3865 | val_rmse: 1.8924 | val_ll: -2.2528
[2:14:55.031868] epoch: 4350 | elbo: 876731.8918750001 | train_rmse: 0.3976 | val_rmse: 1.8528 | val_ll: -2.2149
[2:16:28.321215] epoch: 4400 | elbo: 854681.891875 | train_rmse: 0.3891 | val_rmse: 1.8054 | val_ll: -2.1616
[2:18:03.259414] epoch: 4450 | elbo: 833383.191875 | train_rmse: 0.395 | val_rmse: 1.7594 | val_ll: -2.1357
[2:19:35.750416] epoch: 4500 | elbo: 811737.53375 | train_rmse: 0.4016 | val_rmse: 1.7126 | val_ll: -2.0697
[2:21:08.707826] epoch: 4550 | elbo: 790808.235625 | train_rmse: 0.4013 | val_rmse: 1.668 | val_ll: -2.035
[2:22:41.617615] epoch: 4600 | elbo: 769384.7974999999 | train_rmse: 0.4029 | val_rmse: 1.6228 | val_ll: -1.9809
[2:24:14.577689] epoch: 4650 | elbo: 748336.0725000001 | train_rmse: 0.4047 | val_rmse: 1.5788 | val_ll: -1.9227
[2:25:47.152635] epoch: 4700 | elbo: 728081.0937500001 | train_rmse: 0.408 | val_rmse: 1.5367 | val_ll: -1.8789
[2:27:20.032029] epoch: 4750 | elbo: 707804.166875 | train_rmse: 0.4099 | val_rmse: 1.4965 | val_ll: -1.8419
[2:28:52.591309] epoch: 4800 | elbo: 687547.3275 | train_rmse: 0.4087 | val_rmse: 1.4502 | val_ll: -1.7902
[2:30:27.356742] epoch: 4850 | elbo: 667855.8425 | train_rmse: 0.4157 | val_rmse: 1.4174 | val_ll: -1.7724
[2:32:01.738049] epoch: 4900 | elbo: 648220.8368749999 | train_rmse: 0.4145 | val_rmse: 1.3787 | val_ll: -1.7269
[2:33:34.855794] epoch: 4950 | elbo: 628588.2256250001 | train_rmse: 0.4095 | val_rmse: 1.3424 | val_ll: -1.6884
[2:35:07.731031] epoch: 5000 | elbo: 609881.05625 | train_rmse: 0.421 | val_rmse: 1.3106 | val_ll: -1.6564
[2:36:40.970595] epoch: 5050 | elbo: 590851.4912500001 | train_rmse: 0.4159 | val_rmse: 1.2741 | val_ll: -1.6314
[2:38:13.557380] epoch: 5100 | elbo: 572473.3250000001 | train_rmse: 0.4178 | val_rmse: 1.2456 | val_ll: -1.6059
[2:39:46.241693] epoch: 5150 | elbo: 554157.8825 | train_rmse: 0.4161 | val_rmse: 1.2141 | val_ll: -1.5673
[2:41:19.384709] epoch: 5200 | elbo: 536505.6912499999 | train_rmse: 0.4171 | val_rmse: 1.1901 | val_ll: -1.5523
[2:42:52.981459] epoch: 5250 | elbo: 519250.99437500007 | train_rmse: 0.4163 | val_rmse: 1.1625 | val_ll: -1.552
[2:44:25.933645] epoch: 5300 | elbo: 502205.86062500003 | train_rmse: 0.4182 | val_rmse: 1.1377 | val_ll: -1.523
[2:45:59.431462] epoch: 5350 | elbo: 485188.59187500004 | train_rmse: 0.4181 | val_rmse: 1.1113 | val_ll: -1.505
[2:47:33.241744] epoch: 5400 | elbo: 468672.69343750004 | train_rmse: 0.4175 | val_rmse: 1.0872 | val_ll: -1.4891
[2:49:06.968154] epoch: 5450 | elbo: 452690.67281250004 | train_rmse: 0.4157 | val_rmse: 1.0636 | val_ll: -1.4692
[2:50:41.287135] epoch: 5500 | elbo: 437457.29375 | train_rmse: 0.4123 | val_rmse: 1.039 | val_ll: -1.4563
[2:52:15.366537] epoch: 5550 | elbo: 422036.34187500004 | train_rmse: 0.4129 | val_rmse: 1.0196 | val_ll: -1.4593
[2:53:49.666810] epoch: 5600 | elbo: 407053.5134375 | train_rmse: 0.4163 | val_rmse: 1.0028 | val_ll: -1.4428
[2:55:23.037005] epoch: 5650 | elbo: 392729.12187500007 | train_rmse: 0.4173 | val_rmse: 0.9848 | val_ll: -1.4308
[2:56:57.274892] epoch: 5700 | elbo: 378767.4403125 | train_rmse: 0.4118 | val_rmse: 0.9664 | val_ll: -1.4122
[2:58:31.163882] epoch: 5750 | elbo: 365076.65312500007 | train_rmse: 0.4141 | val_rmse: 0.9541 | val_ll: -1.4246
[3:00:05.475209] epoch: 5800 | elbo: 351470.72875 | train_rmse: 0.4091 | val_rmse: 0.9368 | val_ll: -1.3932
[3:01:40.277226] epoch: 5850 | elbo: 338832.69718749996 | train_rmse: 0.407 | val_rmse: 0.9217 | val_ll: -1.3819
[3:03:14.908618] epoch: 5900 | elbo: 326573.30937499995 | train_rmse: 0.4059 | val_rmse: 0.906 | val_ll: -1.369
[3:04:50.017715] epoch: 5950 | elbo: 315031.1515625 | train_rmse: 0.4089 | val_rmse: 0.8932 | val_ll: -1.3566
[3:06:23.242083] epoch: 6000 | elbo: 303061.3603124999 | train_rmse: 0.4024 | val_rmse: 0.8826 | val_ll: -1.3234
[3:07:54.518746] epoch: 6050 | elbo: 291740.7734375 | train_rmse: 0.3995 | val_rmse: 0.8692 | val_ll: -1.3346
[3:09:27.237394] epoch: 6100 | elbo: 280982.37781250005 | train_rmse: 0.4009 | val_rmse: 0.8573 | val_ll: -1.3174
[3:11:00.815892] epoch: 6150 | elbo: 270690.639375 | train_rmse: 0.3972 | val_rmse: 0.8476 | val_ll: -1.3074
[3:12:34.856156] epoch: 6200 | elbo: 260729.69265625003 | train_rmse: 0.3945 | val_rmse: 0.836 | val_ll: -1.2924
[3:14:06.735147] epoch: 6250 | elbo: 251453.68375 | train_rmse: 0.3902 | val_rmse: 0.828 | val_ll: -1.2871
[3:15:40.758484] epoch: 6300 | elbo: 242159.06359374998 | train_rmse: 0.3893 | val_rmse: 0.815 | val_ll: -1.2695
[3:17:15.204100] epoch: 6350 | elbo: 233316.46484375 | train_rmse: 0.3872 | val_rmse: 0.8059 | val_ll: -1.251
[3:18:49.349282] epoch: 6400 | elbo: 224849.73265625 | train_rmse: 0.3867 | val_rmse: 0.8012 | val_ll: -1.2671
[3:20:23.893146] epoch: 6450 | elbo: 216708.45953125 | train_rmse: 0.3845 | val_rmse: 0.7883 | val_ll: -1.2466
[3:21:58.193144] epoch: 6500 | elbo: 208882.7765625 | train_rmse: 0.383 | val_rmse: 0.7806 | val_ll: -1.2379
[3:23:32.091459] epoch: 6550 | elbo: 201762.94250000003 | train_rmse: 0.3815 | val_rmse: 0.7738 | val_ll: -1.2323
[3:25:07.049840] epoch: 6600 | elbo: 194591.85249999998 | train_rmse: 0.3816 | val_rmse: 0.7688 | val_ll: -1.2167
[3:26:40.117702] epoch: 6650 | elbo: 187815.2934375 | train_rmse: 0.3795 | val_rmse: 0.7596 | val_ll: -1.212
[3:28:12.277053] epoch: 6700 | elbo: 182024.9690625 | train_rmse: 0.3781 | val_rmse: 0.7521 | val_ll: -1.207
[3:29:46.527038] epoch: 6750 | elbo: 175434.43656249996 | train_rmse: 0.3749 | val_rmse: 0.7455 | val_ll: -1.2006
[3:31:20.994504] epoch: 6800 | elbo: 169782.32296874997 | train_rmse: 0.3768 | val_rmse: 0.7388 | val_ll: -1.1769
[3:32:55.995239] epoch: 6850 | elbo: 164174.5909375 | train_rmse: 0.3736 | val_rmse: 0.7306 | val_ll: -1.1814
[3:34:31.058689] epoch: 6900 | elbo: 158883.955 | train_rmse: 0.3707 | val_rmse: 0.7228 | val_ll: -1.17
[3:36:05.448316] epoch: 6950 | elbo: 154159.67875 | train_rmse: 0.3706 | val_rmse: 0.7179 | val_ll: -1.1654
[3:37:40.442148] epoch: 7000 | elbo: 149224.03671875 | train_rmse: 0.3699 | val_rmse: 0.7121 | val_ll: -1.1553
[3:39:13.235042] epoch: 7050 | elbo: 144749.29406249998 | train_rmse: 0.3674 | val_rmse: 0.7064 | val_ll: -1.1476
[3:40:44.870017] epoch: 7100 | elbo: 140587.71390625002 | train_rmse: 0.3655 | val_rmse: 0.7026 | val_ll: -1.1446
[3:42:17.909679] epoch: 7150 | elbo: 136472.93624999997 | train_rmse: 0.3644 | val_rmse: 0.6945 | val_ll: -1.1381
[3:43:49.874500] epoch: 7200 | elbo: 132689.11960937502 | train_rmse: 0.3628 | val_rmse: 0.6913 | val_ll: -1.1247
[3:45:21.426669] epoch: 7250 | elbo: 129223.882109375 | train_rmse: 0.3615 | val_rmse: 0.6873 | val_ll: -1.1151
[3:46:53.227610] epoch: 7300 | elbo: 125758.54734375002 | train_rmse: 0.3587 | val_rmse: 0.6831 | val_ll: -1.1255
[3:48:26.046613] epoch: 7350 | elbo: 122367.28367187499 | train_rmse: 0.3598 | val_rmse: 0.6781 | val_ll: -1.1126
[3:49:57.778288] epoch: 7400 | elbo: 119232.789765625 | train_rmse: 0.3554 | val_rmse: 0.6731 | val_ll: -1.1117
[3:51:29.477932] epoch: 7450 | elbo: 116227.581953125 | train_rmse: 0.3552 | val_rmse: 0.6713 | val_ll: -1.1041
[3:53:01.840918] epoch: 7500 | elbo: 113449.91406250003 | train_rmse: 0.3538 | val_rmse: 0.6645 | val_ll: -1.0975
[3:54:34.743899] epoch: 7550 | elbo: 110901.670703125 | train_rmse: 0.3515 | val_rmse: 0.6619 | val_ll: -1.0939
[3:56:07.046385] epoch: 7600 | elbo: 108373.8375 | train_rmse: 0.3507 | val_rmse: 0.6576 | val_ll: -1.1071
[3:57:39.698389] epoch: 7650 | elbo: 105926.94875000001 | train_rmse: 0.3495 | val_rmse: 0.6534 | val_ll: -1.0934
[3:59:10.685132] epoch: 7700 | elbo: 103722.17531249998 | train_rmse: 0.3476 | val_rmse: 0.6489 | val_ll: -1.0888
[4:00:43.181645] epoch: 7750 | elbo: 101585.804375 | train_rmse: 0.3456 | val_rmse: 0.6464 | val_ll: -1.0867
[4:02:17.817677] epoch: 7800 | elbo: 99695.984765625 | train_rmse: 0.3477 | val_rmse: 0.6422 | val_ll: -1.0844
[4:03:50.348652] epoch: 7850 | elbo: 97608.38273437502 | train_rmse: 0.3427 | val_rmse: 0.6383 | val_ll: -1.0809
[4:05:23.201618] epoch: 7900 | elbo: 95888.21453124999 | train_rmse: 0.3425 | val_rmse: 0.6358 | val_ll: -1.0878
[4:06:55.259604] epoch: 7950 | elbo: 94102.36640625 | train_rmse: 0.3417 | val_rmse: 0.6335 | val_ll: -1.0717
[4:08:28.121735] epoch: 8000 | elbo: 92440.388125 | train_rmse: 0.3419 | val_rmse: 0.6319 | val_ll: -1.0707
[4:10:01.315690] epoch: 8050 | elbo: 90898.72734375001 | train_rmse: 0.3407 | val_rmse: 0.6289 | val_ll: -1.0833
[4:11:33.441380] epoch: 8100 | elbo: 89375.144140625 | train_rmse: 0.3392 | val_rmse: 0.6255 | val_ll: -1.063
[4:13:06.740328] epoch: 8150 | elbo: 88027.861171875 | train_rmse: 0.3372 | val_rmse: 0.623 | val_ll: -1.0654
[4:14:39.411518] epoch: 8200 | elbo: 86646.94757812501 | train_rmse: 0.3366 | val_rmse: 0.6205 | val_ll: -1.061
[4:16:11.652056] epoch: 8250 | elbo: 85339.384765625 | train_rmse: 0.3375 | val_rmse: 0.6168 | val_ll: -1.0511
[4:17:44.023964] epoch: 8300 | elbo: 83977.630703125 | train_rmse: 0.3367 | val_rmse: 0.6162 | val_ll: -1.0596
[4:19:17.555899] epoch: 8350 | elbo: 82625.55320312499 | train_rmse: 0.3354 | val_rmse: 0.6113 | val_ll: -1.053
[4:20:50.052394] epoch: 8400 | elbo: 81431.39609375 | train_rmse: 0.3344 | val_rmse: 0.6107 | val_ll: -1.0439
[4:22:24.206703] epoch: 8450 | elbo: 80560.20656250001 | train_rmse: 0.3359 | val_rmse: 0.6056 | val_ll: -1.0448
[4:23:56.433414] epoch: 8500 | elbo: 79540.75367187502 | train_rmse: 0.3355 | val_rmse: 0.6056 | val_ll: -1.0448
[4:25:30.113739] epoch: 8550 | elbo: 78300.84914062501 | train_rmse: 0.3348 | val_rmse: 0.603 | val_ll: -1.0468
[4:27:03.745232] epoch: 8600 | elbo: 77453.535 | train_rmse: 0.3345 | val_rmse: 0.601 | val_ll: -1.0334
[4:28:38.122848] epoch: 8650 | elbo: 76415.75953125 | train_rmse: 0.3342 | val_rmse: 0.5976 | val_ll: -1.0369
[4:30:11.047935] epoch: 8700 | elbo: 75456.44085937501 | train_rmse: 0.3345 | val_rmse: 0.5947 | val_ll: -1.0283
[4:31:43.721847] epoch: 8750 | elbo: 74617.80796875 | train_rmse: 0.3337 | val_rmse: 0.5927 | val_ll: -1.0059
[4:33:16.778785] epoch: 8800 | elbo: 73684.479921875 | train_rmse: 0.3343 | val_rmse: 0.592 | val_ll: -1.0197
[4:34:50.534377] epoch: 8850 | elbo: 72861.99343750002 | train_rmse: 0.3334 | val_rmse: 0.5886 | val_ll: -1.0246
[4:36:25.134139] epoch: 8900 | elbo: 71903.13312500001 | train_rmse: 0.334 | val_rmse: 0.5878 | val_ll: -1.0036
[4:37:57.558701] epoch: 8950 | elbo: 71201.298515625 | train_rmse: 0.3327 | val_rmse: 0.585 | val_ll: -0.9954
[4:39:29.512860] epoch: 9000 | elbo: 70477.26164062499 | train_rmse: 0.3334 | val_rmse: 0.583 | val_ll: -0.9999
[4:41:02.306439] epoch: 9050 | elbo: 69725.0178125 | train_rmse: 0.3323 | val_rmse: 0.5813 | val_ll: -0.9985
[4:42:38.005540] epoch: 9100 | elbo: 68937.979375 | train_rmse: 0.3314 | val_rmse: 0.5818 | val_ll: -0.9993
[4:44:11.278814] epoch: 9150 | elbo: 68149.51484374999 | train_rmse: 0.3329 | val_rmse: 0.5773 | val_ll: -0.9848
[4:45:45.838228] epoch: 9200 | elbo: 67419.69 | train_rmse: 0.3309 | val_rmse: 0.5756 | val_ll: -0.989
[4:47:20.461444] epoch: 9250 | elbo: 66987.40968750001 | train_rmse: 0.3321 | val_rmse: 0.5744 | val_ll: -0.9703
[4:48:53.930731] epoch: 9300 | elbo: 66368.8669140625 | train_rmse: 0.3312 | val_rmse: 0.5713 | val_ll: -0.9772
[4:50:28.194433] epoch: 9350 | elbo: 65638.40734374998 | train_rmse: 0.3309 | val_rmse: 0.5688 | val_ll: -0.9824
[4:52:01.327124] epoch: 9400 | elbo: 65130.52738281251 | train_rmse: 0.3301 | val_rmse: 0.569 | val_ll: -0.9812
[4:53:34.519946] epoch: 9450 | elbo: 64358.822265625 | train_rmse: 0.3306 | val_rmse: 0.5663 | val_ll: -0.9708
[4:55:08.222169] epoch: 9500 | elbo: 63815.851640625 | train_rmse: 0.3299 | val_rmse: 0.563 | val_ll: -0.9653
[4:56:41.625409] epoch: 9550 | elbo: 63292.87089843749 | train_rmse: 0.329 | val_rmse: 0.5624 | val_ll: -0.9669
[4:58:16.060581] epoch: 9600 | elbo: 62581.671796874994 | train_rmse: 0.3287 | val_rmse: 0.5612 | val_ll: -0.9665
[4:59:49.655486] epoch: 9650 | elbo: 62117.38921875002 | train_rmse: 0.3276 | val_rmse: 0.559 | val_ll: -0.9516
[5:01:24.998943] epoch: 9700 | elbo: 61664.345937499995 | train_rmse: 0.3271 | val_rmse: 0.5565 | val_ll: -0.952
[5:03:00.237992] epoch: 9750 | elbo: 61018.08652343751 | train_rmse: 0.3269 | val_rmse: 0.554 | val_ll: -0.9373
[5:04:35.460096] epoch: 9800 | elbo: 60659.42511718751 | train_rmse: 0.3263 | val_rmse: 0.5527 | val_ll: -0.9451
[5:06:10.093653] epoch: 9850 | elbo: 60195.847499999996 | train_rmse: 0.3254 | val_rmse: 0.5517 | val_ll: -0.9516
[5:07:45.150788] epoch: 9900 | elbo: 59641.61953125 | train_rmse: 0.3257 | val_rmse: 0.5494 | val_ll: -0.941
[5:09:19.126572] epoch: 9950 | elbo: 59193.7246484375 | train_rmse: 0.3248 | val_rmse: 0.5479 | val_ll: -0.9364
Training finished in 5:10:50.972438 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-s03/checkpoint_1.pt
File Size is 4.0595598220825195 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:1 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.3 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.0628, -0.0818, -0.0182,  ...,  0.0275,  0.0213, -0.0486],
        [-0.0303, -0.0131,  0.0504,  ...,  0.0784,  0.1368,  0.0055],
        [-0.0049, -0.0044,  0.0035,  ..., -0.0101, -0.0070,  0.0008],
        ...,
        [-0.0076,  0.0871, -0.0900,  ...,  0.0540, -0.1160,  0.0100],
        [-0.0024,  0.0005,  0.0002,  ...,  0.0009, -0.0014,  0.0017],
        [-0.0233, -0.0393,  0.0646,  ...,  0.0071, -0.0653,  0.0524]],
       device='cuda:1', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0070, 0.0065, 0.0071,  ..., 0.0084, 0.0081, 0.0072],
        [0.0094, 0.0099, 0.0087,  ..., 0.0080, 0.0068, 0.0093],
        [0.0883, 0.0866, 0.0866,  ..., 0.0827, 0.0835, 0.0872],
        ...,
        [0.0078, 0.0078, 0.0078,  ..., 0.0075, 0.0077, 0.0078],
        [0.0545, 0.0532, 0.0535,  ..., 0.0533, 0.0537, 0.0542],
        [0.0177, 0.0172, 0.0150,  ..., 0.0188, 0.0150, 0.0169]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.5790, -0.4421, -1.2009, -1.0001, -0.2766, -1.1343,  0.4396, -0.4978,
        -0.3643, -1.1746, -0.6277, -0.4323, -0.3285, -0.9620, -1.0961, -0.4013,
        -0.4559, -1.2127, -0.4340, -0.5998,  0.0491, -0.0222, -1.2852, -0.5602,
        -0.9865, -0.4110,  0.2473, -0.6288, -0.0905,  0.2245, -0.8089, -0.8676,
        -1.2514,  0.0249, -0.3038, -1.0561, -0.2335, -1.4391, -0.0119, -0.2259,
        -0.7301,  0.0339, -0.8325, -1.3447, -1.2653, -0.3591, -0.2022, -0.8278,
        -0.7530, -0.0495, -0.6963,  0.2975, -1.0946, -0.5868, -0.4671, -0.4321,
        -1.1749, -1.0033,  0.3401, -0.6315, -0.5906, -0.3383, -0.7100, -0.8963,
        -0.5182,  0.0113, -0.5834, -0.2908, -0.7207,  0.1879, -0.6935, -0.1689,
        -0.4171,  0.9773, -0.7974, -0.8556, -0.4619, -0.3059, -0.9575,  0.0789,
        -0.6773, -1.1003, -0.5207, -0.1096, -0.6031, -1.3167, -0.5849, -0.3097,
        -0.0136, -0.0398, -0.8563, -0.7473, -0.1483, -0.0111, -1.1786, -0.3288,
        -0.3061,  0.1390, -0.6890, -1.0296, -0.9142, -0.4106, -0.4808,  0.2732,
        -1.0148, -0.8862, -0.6874, -0.3960, -0.3896, -0.0530, -1.0210, -0.7428,
        -0.8211, -1.1726, -0.6684,  0.0648, -1.0662, -0.6377, -0.4950, -0.1952,
        -1.3895, -0.7114, -1.2558, -0.0686, -1.1501, -0.5221, -1.1329, -0.5063,
        -0.4799,  0.9310, -0.3731, -0.0621,  0.0469, -0.6938, -1.3540, -0.0339,
        -0.5602, -0.7158, -1.1375,  0.1583, -0.7199,  0.6442, -1.0194, -0.3747,
        -1.3245, -0.5885, -0.0279, -0.8759, -0.8423, -1.0196, -0.5360,  0.0041,
        -0.0918, -0.6721, -1.2831, -0.4414, -1.0340,  0.1785, -0.7010, -0.3723,
        -0.7792, -0.1053,  0.1488, -0.4053, -0.3188, -0.3117, -1.0646,  0.2652,
        -0.6581, -0.7231,  0.0383,  0.1403, -0.5377, -1.1346, -0.9650,  1.5952,
        -0.9507, -0.3673, -1.1561, -0.5527, -1.0607, -0.9866, -1.1844, -0.7500,
        -0.7256, -0.5819, -0.5368, -0.4241, -0.9105, -1.0870, -0.7328,  1.1856,
        -1.5167, -0.8647, -0.4317, -0.2039, -0.7466,  0.0737, -0.9171, -0.2791,
        -0.7385, -0.6572, -1.0183, -1.0830, -0.5082, -1.1551, -0.5546, -1.1121,
         0.0527, -0.3396,  0.6763, -0.8385, -0.8010, -0.6683, -0.6674, -1.3311,
        -0.2317,  0.4952, -0.7665, -0.8036, -0.3226, -0.8938, -0.3318,  0.0373,
         1.0692, -0.4795,  0.3003, -0.4837, -1.2410,  0.0156, -0.9698, -1.0988,
        -1.1494, -0.5910, -0.9649, -0.5714, -0.1248, -0.2254,  0.7199, -0.4182,
        -0.0781, -0.0622, -0.6897, -0.5451, -0.3419, -0.7197, -0.9102,  0.5684,
         0.0044,  0.0079, -0.6585, -0.3831, -0.7918, -0.4172, -1.4057, -0.8726,
        -0.3891, -0.8372, -0.6608, -0.5819, -0.7162,  0.2025, -0.3436, -1.2312,
        -0.7959, -1.1568, -0.3678, -0.6919, -0.0033, -0.3022, -0.4116, -1.0512,
        -1.4305,  0.0516, -0.6443, -0.4664, -1.5308, -0.4922, -0.0320, -1.2056,
        -0.9585, -0.0425, -0.8155, -1.1279, -0.6678, -1.2764,  0.0618,  0.2833,
        -1.3961, -1.0763, -0.3261, -0.8519,  0.0301, -0.7620, -0.5373, -0.5091,
        -0.5813, -0.0205, -0.1267, -0.2984, -0.5024, -0.5928, -1.1189, -0.7583,
        -1.3096, -0.6236,  0.0090,  0.2857, -0.2839, -0.5221, -1.0507, -0.3311,
        -0.5650, -0.4089, -0.3158, -1.2367,  0.6318, -0.3619, -0.9118, -0.2026,
        -0.5252, -0.0597, -0.3836, -0.7170, -0.1820, -0.4745,  0.4954, -0.5961,
        -0.4502, -1.1810, -0.6127, -0.4581, -0.6219,  0.0688, -0.6681, -0.3849,
        -0.2412,  1.1509, -0.2877, -0.4401, -0.5852, -0.4128, -0.8548, -0.0282,
        -0.8995, -0.0865, -1.1957,  0.0366,  0.2350, -0.8701, -0.5490, -0.4741,
        -0.2015, -0.1366, -0.9645, -0.8521,  0.9736, -0.8942,  0.3088, -0.8924,
        -1.2091, -1.1497, -0.1732, -0.4168, -0.9723,  0.3892, -0.9244, -0.3007,
        -0.4274, -0.6195,  0.2321, -0.7625, -1.5892, -0.8296, -0.4774, -0.8891,
        -0.9340, -0.5012, -0.8422, -0.3285, -0.1440, -1.0715, -0.8515, -0.4342,
        -0.5156,  0.2403, -0.1535, -0.1085, -0.6336, -1.1476, -1.0097,  0.4504,
        -0.6704, -0.0042, -1.1096, -0.5460, -0.9433,  0.0166, -0.9670, -1.4044,
        -1.0695, -0.5749, -1.1700,  0.1476, -0.5939, -1.1847, -1.3011, -1.1815,
        -0.5452, -0.7182, -0.1777, -0.3656, -0.0638, -0.1705, -0.7245,  0.1844,
        -0.6703, -0.5829, -0.5938, -0.3458, -0.7213, -0.1170,  0.0529, -0.1303,
        -0.6436, -1.3051, -0.6339, -1.2694, -0.7231,  1.3610, -0.5413, -0.9794,
        -0.7858, -0.8439, -1.0502, -0.5489, -0.8366, -0.4342, -0.8553, -0.7833,
        -1.6294, -0.5326,  0.1614, -1.0450, -0.6259, -0.2257, -0.4305, -1.0981,
        -0.6919, -0.6758, -0.9335, -0.2207, -1.2143, -0.7062, -1.2924, -1.0181,
        -0.3164, -0.9882, -1.4971, -0.6108, -1.1684, -0.3416, -0.0914, -0.4373,
        -1.2215, -1.3439, -1.1541, -1.2108, -0.7021, -1.0578, -0.3788, -0.6642,
        -0.1282, -0.8231, -0.0794, -0.5413, -0.0077, -1.2125, -0.6787, -0.7784,
        -0.4327, -1.2618,  0.1941,  0.3335, -1.3534, -0.1211,  1.8061, -0.9382,
        -0.2971, -0.3949, -1.0812, -0.5697, -0.5620,  0.0526, -0.2115, -1.0616,
        -1.2728,  0.1109, -0.1250, -0.5721, -1.0372, -0.6833, -0.2677, -0.1944,
        -0.4383, -0.0850, -0.4057, -0.5491, -1.1851, -0.6358, -0.7943, -0.5733],
       device='cuda:1', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0089, 0.0117, 0.1185, 0.0876, 0.0269, 0.1101, 0.0031, 0.0142, 0.0020,
        0.1037, 0.0232, 0.0334, 0.0215, 0.0012, 0.1007, 0.0085, 0.0488, 0.1052,
        0.0079, 0.0036, 0.0205, 0.0026, 0.1173, 0.0195, 0.0927, 0.0161, 0.0240,
        0.0026, 0.0065, 0.0193, 0.0254, 0.0202, 0.1278, 0.0090, 0.0027, 0.1015,
        0.0037, 0.1262, 0.0011, 0.0031, 0.0658, 0.0133, 0.0140, 0.1207, 0.1255,
        0.0016, 0.0014, 0.0012, 0.0016, 0.0168, 0.0357, 0.0014, 0.0140, 0.0029,
        0.0212, 0.0060, 0.1058, 0.0880, 0.0032, 0.0016, 0.0069, 0.0015, 0.0066,
        0.0922, 0.0052, 0.0013, 0.0115, 0.0171, 0.0185, 0.0037, 0.0777, 0.0130,
        0.0111, 0.0067, 0.0098, 0.0264, 0.0112, 0.0111, 0.0822, 0.0067, 0.0014,
        0.1060, 0.0019, 0.0160, 0.0024, 0.1133, 0.0478, 0.0146, 0.0038, 0.0036,
        0.0016, 0.0780, 0.0042, 0.0049, 0.1023, 0.0109, 0.0310, 0.0018, 0.0211,
        0.1054, 0.0435, 0.0013, 0.0288, 0.0093, 0.0112, 0.0022, 0.0902, 0.0403,
        0.0080, 0.0049, 0.1007, 0.0072, 0.0089, 0.1059, 0.0060, 0.0015, 0.0015,
        0.0115, 0.0060, 0.0016, 0.0489, 0.0088, 0.1184, 0.0074, 0.1036, 0.0104,
        0.1016, 0.0108, 0.0019, 0.0018, 0.0347, 0.0061, 0.0022, 0.0031, 0.0063,
        0.0033, 0.0366, 0.0640, 0.1036, 0.0045, 0.0152, 0.0020, 0.0022, 0.0019,
        0.1272, 0.0080, 0.0041, 0.0841, 0.0032, 0.0602, 0.0056, 0.0012, 0.0074,
        0.0255, 0.0013, 0.0125, 0.0176, 0.0152, 0.0014, 0.0111, 0.0030, 0.0151,
        0.0040, 0.0380, 0.0075, 0.0058, 0.0018, 0.0049, 0.0270, 0.0017, 0.0062,
        0.0178, 0.0383, 0.1209, 0.0881, 0.0097, 0.0040, 0.0035, 0.1024, 0.0014,
        0.0933, 0.0887, 0.1059, 0.0016, 0.0457, 0.0017, 0.0224, 0.0075, 0.0013,
        0.0207, 0.0122, 0.0129, 0.1458, 0.0200, 0.0037, 0.0081, 0.0026, 0.0034,
        0.0015, 0.0100, 0.0970, 0.0215, 0.0938, 0.0019, 0.0107, 0.0049, 0.0203,
        0.1083, 0.0022, 0.0066, 0.0066, 0.0644, 0.0086, 0.0675, 0.0783, 0.1302,
        0.0089, 0.0043, 0.0574, 0.0822, 0.0014, 0.0815, 0.0029, 0.0053, 0.0081,
        0.0077, 0.0027, 0.0061, 0.1063, 0.0042, 0.0919, 0.0114, 0.0274, 0.0046,
        0.0969, 0.0018, 0.0031, 0.0109, 0.0120, 0.0084, 0.0107, 0.0016, 0.0072,
        0.0042, 0.0283, 0.0384, 0.0946, 0.0010, 0.0166, 0.0011, 0.0214, 0.0073,
        0.0048, 0.0086, 0.0046, 0.0792, 0.0103, 0.0011, 0.0674, 0.0225, 0.0546,
        0.0017, 0.0018, 0.1099, 0.0187, 0.1076, 0.0017, 0.0208, 0.0052, 0.0016,
        0.0018, 0.0949, 0.1264, 0.0008, 0.0856, 0.0016, 0.1372, 0.0682, 0.0036,
        0.1047, 0.0042, 0.0020, 0.0152, 0.1009, 0.0700, 0.1121, 0.0053, 0.0050,
        0.1245, 0.0012, 0.0115, 0.0613, 0.0037, 0.0828, 0.0156, 0.0080, 0.0156,
        0.0111, 0.0126, 0.0390, 0.0015, 0.0529, 0.1070, 0.0299, 0.1223, 0.0140,
        0.0011, 0.0047, 0.0183, 0.0460, 0.0092, 0.0069, 0.0018, 0.0154, 0.0353,
        0.0008, 0.0053, 0.0119, 0.0017, 0.0069, 0.0021, 0.0019, 0.0049, 0.0176,
        0.0077, 0.0218, 0.0147, 0.0126, 0.0019, 0.1119, 0.0212, 0.0086, 0.0030,
        0.0015, 0.0439, 0.0015, 0.0144, 0.0106, 0.0094, 0.0023, 0.0206, 0.0037,
        0.0043, 0.0081, 0.0803, 0.0115, 0.1047, 0.0019, 0.0042, 0.0095, 0.0234,
        0.0565, 0.0137, 0.0085, 0.0420, 0.0781, 0.0048, 0.0048, 0.0090, 0.0307,
        0.1175, 0.0020, 0.0068, 0.0128, 0.0008, 0.0009, 0.0916, 0.0312, 0.0169,
        0.0549, 0.0099, 0.0623, 0.1460, 0.0127, 0.0592, 0.0846, 0.0185, 0.0044,
        0.0888, 0.0078, 0.0055, 0.0974, 0.0877, 0.0033, 0.0149, 0.0021, 0.0086,
        0.0034, 0.0048, 0.1034, 0.1072, 0.0076, 0.0016, 0.0040, 0.0013, 0.0036,
        0.0136, 0.0020, 0.0882, 0.1197, 0.1106, 0.0208, 0.1128, 0.0021, 0.0134,
        0.1099, 0.1128, 0.1068, 0.0017, 0.0148, 0.0036, 0.0051, 0.0042, 0.0012,
        0.0654, 0.0056, 0.0090, 0.0041, 0.0019, 0.0134, 0.0322, 0.0052, 0.0152,
        0.0107, 0.0331, 0.1240, 0.0603, 0.0011, 0.0568, 0.0319, 0.0523, 0.0890,
        0.0159, 0.0886, 0.1085, 0.0441, 0.0120, 0.0096, 0.0035, 0.0056, 0.1581,
        0.0101, 0.0057, 0.1157, 0.0588, 0.0285, 0.0180, 0.0230, 0.0019, 0.0345,
        0.0015, 0.0030, 0.1072, 0.0042, 0.0043, 0.0439, 0.0017, 0.0931, 0.1277,
        0.0059, 0.0142, 0.0068, 0.0027, 0.0681, 0.1053, 0.1153, 0.1081, 0.1297,
        0.0018, 0.0011, 0.0115, 0.0349, 0.0244, 0.0845, 0.0015, 0.0382, 0.0011,
        0.1090, 0.0732, 0.0557, 0.0149, 0.1155, 0.0024, 0.0057, 0.1226, 0.0059,
        0.0066, 0.0652, 0.0254, 0.0310, 0.1163, 0.0044, 0.0011, 0.0062, 0.0089,
        0.0991, 0.1173, 0.0019, 0.0056, 0.0038, 0.0977, 0.0014, 0.0293, 0.0179,
        0.0283, 0.0031, 0.0032, 0.0495, 0.1247, 0.0096, 0.0741, 0.0212],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[ 3.3150e-06, -1.1676e-04,  1.1210e-44,  ..., -3.3236e-02,
         -1.7187e-41, -8.5758e-09],
        [-1.5701e-08, -3.7374e-03,  1.0468e-42,  ..., -4.4884e-02,
         -1.4013e-45, -1.8011e-03],
        [-2.1303e-13, -4.0538e-04,  5.6052e-45,  ..., -5.0053e-02,
         -5.6052e-45, -5.4697e-17],
        ...,
        [-1.3839e-04, -2.1207e-03, -9.0964e-34,  ..., -4.1516e-02,
         -3.0077e-18, -2.1807e-03],
        [-5.0881e-04, -1.0767e-04, -7.0065e-45,  ..., -4.9880e-02,
          1.8918e-43, -1.6068e-03],
        [-4.3015e-04, -1.9399e-04, -2.6697e-09,  ..., -3.1641e-02,
         -2.0146e-27,  3.2309e-08]], device='cuda:1', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[1.0000, 0.9999, 1.0000,  ..., 0.9962, 1.0000, 1.0000],
        [1.0000, 1.0003, 1.0000,  ..., 0.9939, 1.0000, 0.9999],
        [1.0000, 1.0000, 1.0000,  ..., 0.9916, 1.0000, 1.0000],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 0.9945, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 0.9925, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 0.9967, 1.0000, 1.0000]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.7756, -0.8478, -1.0018, -1.0109, -0.9751, -0.6722, -0.9218, -0.8922,
        -0.8808, -0.8833, -0.9843, -1.0916, -0.8436, -0.7894, -0.8149, -0.8512,
        -0.7011, -0.8681, -0.7636, -0.9108, -0.8199, -0.8566, -0.8931, -0.8714,
        -0.3449, -0.9379, -0.4146, -0.7990, -0.7822, -1.0185, -0.9750, -0.6881,
        -0.9163, -1.1197, -0.6867, -0.9984, -1.0378, -0.9836, -0.8048, -0.7673,
        -0.8863, -0.8605, -0.8055, -0.8296, -1.0561, -0.7585, -0.8596, -0.7999,
        -1.1198, -1.0081, -1.0731, -0.7353, -1.0419, -0.9228, -0.9380, -0.8744,
        -1.1777, -0.9813, -0.7729, -1.1026, -1.0286, -0.9144, -0.8184, -0.7421,
        -0.8676, -0.7320, -0.8824, -0.9438, -0.7845, -1.0982, -0.9239, -0.8157,
        -0.8333, -0.8317, -0.7047, -1.0980, -0.7323, -0.8790, -0.9707, -0.9361,
        -0.8359, -0.8228, -1.1225, -0.7463, -0.7254, -0.9380, -0.8632, -0.9500,
        -0.9660, -0.9345, -0.6393, -0.8600, -0.7895, -1.0044, -1.1113, -0.9138,
        -0.9572, -0.5101, -0.7191, -0.9101, -1.0336, -0.9392, -0.7219, -0.9107,
        -1.0104, -0.8807, -0.9616, -0.2241, -0.9411, -1.0615, -1.0166, -0.8513,
        -0.8970, -0.6925, -0.9814, -1.0701, -0.7598, -1.0357, -0.9443, -0.9803,
        -0.9079, -0.9535, -0.7706, -0.9483, -0.8656, -0.6321, -0.8086, -0.6976,
        -0.7467, -0.8897, -0.6208, -0.9096, -0.9827, -0.8044, -0.8789, -0.8271,
        -0.9607, -0.6235, -0.9229, -0.8809, -0.9810, -0.7444, -0.8103, -0.8207,
        -0.9704, -0.9089, -0.8115, -0.7548, -0.8539, -0.5422, -1.0893, -0.8651,
        -0.8860, -1.0154, -0.8174, -0.9968, -0.8458, -1.2127, -1.0631, -0.9223,
        -0.7758,  0.2021, -0.9501, -0.7035, -0.9037, -1.0193, -0.7748, -0.9886,
         0.9148, -0.9787, -0.9872, -0.7710, -0.8504, -0.9429, -0.9542, -0.7082,
        -0.6413, -1.0258, -1.0628, -0.4691, -0.2416, -0.9623, -0.8754, -0.9338,
        -0.7866, -0.9511, -0.9531, -0.8068, -1.0715, -0.8741, -0.9758, -0.8432,
        -0.8021, -1.0298, -0.9961, -0.8901, -0.8096, -0.9010, -0.7918, -1.1930,
        -1.0296, -1.0127, -0.7765, -0.8083, -0.6270, -1.0777, -0.9365, -0.7955,
        -0.8591, -0.8055, -0.8882, -0.8490, -0.8351, -0.8636, -0.7902, -0.8941,
        -0.8549, -0.8067, -0.9206, -0.8003, -0.8983, -0.9166, -0.7860, -0.7217,
        -0.9028, -0.9482, -1.0482, -0.5743, -0.8370, -0.7970, -0.9515,  0.2814,
        -0.8382, -0.8894, -0.8763, -0.7243, -0.6499, -0.8391, -0.8623, -0.8845,
        -0.7237, -0.8614, -0.8973, -0.7696, -1.0032, -0.9885, -0.9192, -0.9284,
        -0.9996, -0.9263, -0.9224, -0.7725, -0.6185, -0.9713, -1.1804, -0.7783,
        -0.9682, -0.7392, -0.8801, -0.9062, -0.8135, -0.9919, -1.0340, -1.0728,
        -0.6814, -0.9583, -0.8446,  0.6646, -0.3106, -0.7314, -0.9191, -0.2544,
        -0.9556, -1.0409, -0.0179, -1.0566, -0.9173, -0.7301, -0.9994, -0.7627,
        -1.0506, -0.8633, -0.7485, -1.0430, -0.8701, -1.0085, -1.0121, -0.8343,
        -0.9177, -0.9336, -0.6339, -0.8833, -0.9481, -0.9126, -0.7520, -0.8400,
        -0.8595, -0.6462, -0.7609, -1.0085, -0.8758, -0.6105, -0.8877, -1.0231,
        -0.7704, -0.9414, -0.8079, -0.8961, -0.9917, -0.8739, -0.9023, -1.0280,
        -1.1166, -0.8740, -0.8781, -0.7129, -1.0700, -0.8527, -0.8642, -0.8764,
        -0.7462, -0.7642, -1.0106, -1.0389, -0.7144, -0.8088, -1.0162, -0.9554,
        -1.0377, -0.9526, -0.9485, -0.9917, -1.0782, -0.9167, -1.0531, -0.8524,
        -0.8057, -0.9585, -0.9886, -0.9170, -0.7842, -0.8911, -0.8799, -1.0240,
        -0.8187, -0.9366, -1.0328, -0.5823, -1.0261, -0.7756, -0.8737, -0.9867,
        -1.0117, -1.0886, -0.8659, -1.0369, -0.9446, -0.8431, -0.6973, -1.0697,
        -0.9392, -0.9663, -0.7450, -0.5608, -0.8143, -0.9595, -0.8966, -0.7515,
        -0.8678, -0.9970, -0.9837, -0.3510, -0.8502, -0.6950, -1.0662, -0.9275,
        -0.9018, -0.9531, -0.1205, -0.6849, -0.8901, -0.9572, -0.9672, -0.8275,
        -0.9081, -0.9129, -1.0425, -0.8583, -0.9978, -0.8123, -0.8961, -0.9139,
        -0.8789, -0.8410, -1.0036, -0.9164, -1.0250, -0.6780, -0.7486, -0.7884,
        -0.9127, -0.8048, -0.9697, -0.9652, -0.9325, -0.9944, -0.9510, -0.9567,
        -0.8055, -0.3462, -0.8733, -0.9061, -0.9558, -0.7538, -1.0316, -0.9862,
        -0.8962, -0.7450, -0.8178, -0.8623, -0.7504, -0.8198, -0.7459, -0.9254,
        -1.1778, -0.9053, -0.7749, -0.9604, -0.7732, -0.7346, -0.9685, -1.0237,
        -0.8008, -0.9731, -0.8876, -0.8624, -0.7673, -0.9712, -0.8036, -0.9532,
        -0.8505, -0.9846, -0.7044, -0.9806, -0.8328, -0.9897, -0.9511, -0.1414,
        -0.8598, -0.7946, -0.9632, -0.9172, -0.9777, -0.7716, -0.9154, -0.9761,
        -0.8308, -1.0645, -0.9317, -0.9323, -1.0070, -0.8207, -0.8362, -0.8739,
        -0.6844, -0.8028, -0.8965, -0.9207, -0.9797, -0.8497, -0.8419, -1.1121,
        -0.9564, -1.0755, -0.7399, -0.8638, -0.7957, -0.6655, -1.0359, -0.8870,
        -0.9902, -0.7869, -0.8146, -1.0421, -0.8806, -0.8737, -0.8569, -0.9288,
        -0.4355, -1.0537, -0.8274, -0.7676, -1.0888, -0.8333, -1.0645, -0.9807,
        -0.7009, -0.6321, -0.8670, -0.9673, -0.7877, -0.8507, -0.8407, -0.9779,
        -0.9295, -0.8104, -0.8442, -0.6662, -1.0713, -0.8457, -0.7918, -0.9647],
       device='cuda:1', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.7389, 0.7485, 0.7191, 0.7029, 0.7098, 0.7705, 0.7369, 0.7212, 0.7258,
        0.7467, 0.7160, 0.7285, 0.7394, 0.7138, 0.7159, 0.7206, 0.7509, 0.7480,
        0.7726, 0.7228, 0.7547, 0.7141, 0.7150, 0.7119, 0.3805, 0.7282, 0.7618,
        0.7259, 0.6863, 0.7182, 0.7047, 0.7448, 0.7134, 0.7183, 0.7264, 0.7048,
        0.7267, 0.7171, 0.6491, 0.7115, 0.7243, 0.7121, 0.7426, 0.6896, 0.7329,
        0.7435, 0.7677, 0.7037, 0.7272, 0.7284, 0.6902, 0.7632, 0.7075, 0.7046,
        0.7009, 0.7453, 0.7078, 0.7058, 0.7141, 0.6989, 0.6861, 0.7060, 0.7564,
        0.6777, 0.7309, 0.7223, 0.7248, 0.7501, 0.7619, 0.7195, 0.7174, 0.6979,
        0.7136, 0.7240, 0.7475, 0.6980, 0.8038, 0.7377, 0.7008, 0.7214, 0.7297,
        0.6737, 0.7117, 0.7347, 0.7415, 0.7407, 0.7405, 0.7158, 0.7044, 0.7037,
        0.7898, 0.7412, 0.7580, 0.7020, 0.7330, 0.7045, 0.7142, 0.4866, 0.7172,
        0.7243, 0.7229, 0.7348, 0.7526, 0.7276, 0.7482, 0.7122, 0.7324, 0.4173,
        0.7156, 0.7149, 0.7396, 0.7643, 0.7473, 0.7828, 0.6977, 0.7129, 0.7153,
        0.7085, 0.6954, 0.7379, 0.7158, 0.7503, 0.7223, 0.0169, 0.7087, 0.2286,
        0.7726, 0.7290, 0.7241, 0.7266, 0.7506, 0.6874, 0.6926, 0.7296, 0.6787,
        0.7600, 0.7490, 0.7272, 0.7244, 0.7510, 0.7566, 0.7382, 0.7251, 0.7075,
        0.7146, 0.6979, 0.7665, 0.2231, 0.7713, 0.1859, 0.7090, 0.7360, 0.6762,
        0.7192, 0.7524, 0.7435, 0.6455, 0.6681, 0.7487, 0.7282, 0.7202, 0.0133,
        0.7557, 0.7452, 0.7219, 0.6938, 0.7271, 0.7112, 0.3968, 0.7067, 0.7238,
        0.7236, 0.7351, 0.7491, 0.7398, 0.7206, 0.7835, 0.7227, 0.6950, 0.7461,
        0.0290, 0.6783, 0.7143, 0.6584, 0.7829, 0.7471, 0.7311, 0.7320, 0.7115,
        0.7248, 0.7350, 0.7079, 0.7086, 0.7209, 0.7415, 0.7254, 0.7135, 0.7051,
        0.7094, 0.7296, 0.7174, 0.7199, 0.7510, 0.7421, 0.7153, 0.7327, 0.7083,
        0.7200, 0.7319, 0.7559, 0.7043, 0.7544, 0.6757, 0.6708, 0.7224, 0.7365,
        0.7312, 0.7235, 0.7115, 0.7159, 0.7103, 0.7491, 0.7138, 0.7539, 0.7289,
        0.7110, 0.7085, 0.6879, 0.7108, 0.7428, 0.7503, 0.0078, 0.7597, 0.7386,
        0.7086, 0.7125, 0.7562, 0.7139, 0.7386, 0.7389, 0.7346, 0.7100, 0.6835,
        0.7223, 0.7034, 0.6780, 0.7546, 0.7832, 0.7268, 0.7496, 0.7124, 0.7295,
        0.7647, 0.6936, 0.6811, 0.7473, 0.6988, 0.7371, 0.7226, 0.7107, 0.7127,
        0.7457, 0.7175, 0.6882, 0.7442, 0.7159, 0.7015, 0.0018, 0.8057, 0.7292,
        0.7006, 0.0720, 0.7487, 0.6759, 0.0077, 0.7126, 0.7267, 0.6959, 0.7214,
        0.6537, 0.7233, 0.7289, 0.7273, 0.7226, 0.7074, 0.7235, 0.7553, 0.7065,
        0.7263, 0.7435, 0.7247, 0.7367, 0.7473, 0.6756, 0.7343, 0.7586, 0.7160,
        0.7726, 0.7292, 0.7011, 0.7104, 0.0087, 0.6769, 0.7274, 0.7384, 0.7305,
        0.7311, 0.7231, 0.7215, 0.7551, 0.7366, 0.7153, 0.7165, 0.7423, 0.6808,
        0.7053, 0.7304, 0.7586, 0.7499, 0.7102, 0.7168, 0.7330, 0.7136, 0.7024,
        0.7740, 0.7467, 0.7026, 0.6824, 0.7201, 0.6950, 0.7246, 0.7256, 0.7402,
        0.7517, 0.6969, 0.7106, 0.6960, 0.7443, 0.7206, 0.6833, 0.7310, 0.7101,
        0.7243, 0.7320, 0.7373, 0.7325, 0.7348, 0.7528, 0.7257, 0.7427, 0.7118,
        0.7172, 0.7295, 0.6717, 0.7166, 0.7400, 0.7344, 0.7316, 0.7334, 0.7352,
        0.7393, 0.7494, 0.7540, 0.6914, 0.6296, 0.7342, 0.7027, 0.7401, 0.7416,
        0.7104, 0.6981, 0.5376, 0.7295, 0.7168, 0.7220, 0.7398, 0.7458, 0.7602,
        0.0009, 0.7629, 0.7364, 0.7084, 0.6732, 0.7023, 0.7384, 0.7227, 0.7194,
        0.7711, 0.7408, 0.6940, 0.7182, 0.6868, 0.7205, 0.6988, 0.7150, 0.7453,
        0.7310, 0.7505, 0.7073, 0.7750, 0.7382, 0.7208, 0.7036, 0.7686, 0.7716,
        0.7207, 0.7009, 0.7425, 0.7154, 0.7606, 0.7319, 0.7310, 0.6908, 0.7196,
        0.6887, 0.6550, 0.6698, 0.7275, 0.7360, 0.7223, 0.6573, 0.7827, 0.7009,
        0.7430, 0.7202, 0.7463, 0.7238, 0.7118, 0.7255, 0.7900, 0.7007, 0.7179,
        0.6899, 0.6967, 0.7101, 0.7209, 0.6693, 0.7331, 0.7336, 0.7323, 0.7217,
        0.7054, 0.7266, 0.7459, 0.7216, 0.7064, 0.7174, 0.0046, 0.7406, 0.7064,
        0.7145, 0.7169, 0.7247, 0.7740, 0.7067, 0.7286, 0.7313, 0.6983, 0.7223,
        0.7307, 0.6936, 0.7146, 0.7052, 0.7430, 0.7269, 0.7200, 0.7591, 0.7314,
        0.7223, 0.7287, 0.7106, 0.7304, 0.7056, 0.7287, 0.7206, 0.7233, 0.7162,
        0.6929, 0.7543, 0.7449, 0.7345, 0.7113, 0.7412, 0.7170, 0.7206, 0.7409,
        0.7444, 0.7267, 0.0025, 0.7328, 0.7755, 0.7264, 0.7337, 0.7461, 0.7309,
        0.7184, 0.7700, 0.4453, 0.7104, 0.7447, 0.7676, 0.7582, 0.7241, 0.7345,
        0.7371, 0.7027, 0.7550, 0.6853, 0.6965, 0.7476, 0.7112, 0.7699],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-6.5346e-02, -1.6492e-01, -1.0601e-01,  ...,  4.3543e-02,
         -1.6171e-02, -1.9290e-02],
        [-5.5310e-06, -6.4851e-33, -9.3887e-44,  ...,  2.0300e-04,
          2.2330e-17, -3.1007e-10],
        [ 1.9217e-03, -2.1623e-05, -3.2827e-14,  ..., -1.1190e-12,
         -4.0910e-07,  5.6187e-06],
        ...,
        [ 9.0468e-25,  3.8367e-13,  3.0095e-04,  ..., -1.9134e-04,
         -1.5374e-11, -6.1163e-04],
        [-7.7245e-03,  9.3610e-04,  6.5235e-04,  ..., -1.7495e-03,
         -3.8198e-03, -7.4694e-03],
        [ 6.7694e-02, -7.8379e-02, -5.5214e-02,  ...,  2.1290e-02,
          2.0287e-02, -1.8083e-01]], device='cuda:1', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.3648, 0.2459, 0.5453,  ..., 0.2306, 0.4097, 0.1223],
        [1.0000, 1.0000, 1.0000,  ..., 1.0001, 1.0000, 1.0000],
        [1.0009, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9995],
        [0.9969, 0.9968, 1.0002,  ..., 0.9951, 0.9988, 0.9380],
        [0.9136, 0.7869, 0.9720,  ..., 0.7739, 0.9549, 0.4915]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 0.1998, -0.0723, -0.0677, -0.0818, -0.0606, -0.0686, -0.0772, -0.0811,
        -0.0696, -0.0644, -0.0714, -0.0622, -0.0643, -0.0613, -0.0128, -0.0865,
        -0.0666, -0.0629, -0.1151, -0.0631, -0.0768,  0.6286, -0.0443, -0.0681,
        -0.1022, -0.0620, -0.0631, -0.0933, -0.0689, -0.0617, -0.0947, -0.0889,
        -0.0645, -0.0644, -0.0451, -0.0609, -0.0639, -0.0803, -0.0716, -0.1215,
        -0.0629, -0.0757, -0.0619, -0.2291, -0.0714, -0.0722, -0.0799, -0.0642,
        -0.0681, -0.0627, -0.0815, -0.0785, -0.0666, -0.0657, -0.0699, -0.0645,
        -0.0800, -0.0641, -0.0677, -0.0669, -0.0600, -0.0623, -0.0794, -0.0603,
        -0.0622, -0.0619, -0.0594, -0.0591, -0.0621, -0.0695, -0.0724, -0.0633,
        -0.0655, -0.0729, -0.0657, -0.0646, -0.0629, -0.0733, -0.0629,  0.0956,
        -0.0617, -0.0620, -0.0668, -0.0626, -0.3589, -0.0735, -0.0628, -0.0730,
        -0.0672, -0.3065, -0.0586, -0.0685, -0.0690, -0.0634, -0.0863, -0.0627,
        -0.0669, -0.0761, -0.0751, -0.0606, -0.0686, -0.0685, -0.0689, -0.0622,
        -0.0695, -0.0621, -0.0625, -0.0652, -0.0669,  0.1739, -0.0666, -0.0879,
        -0.1488, -0.0631, -0.0707, -0.2801, -0.0636, -0.0645, -0.0722, -0.0627,
        -0.0805, -0.0670, -0.0636, -0.1001, -0.0906, -0.0654, -0.0627, -0.0613,
        -0.0679, -0.0839, -0.0627, -0.0633, -0.0776, -0.0704, -0.0673, -0.0750,
        -0.0615,  0.6543, -0.0674, -0.0681, -0.0672, -0.0729, -0.0591, -0.0650,
        -0.0641, -0.0725, -0.0646, -0.0614, -0.0583, -0.4500, -0.0655, -0.0634,
        -0.0706, -0.0630, -0.0580, -0.0648, -0.0964, -0.0623, -0.0677, -0.0706,
        -0.0637, -0.0804,  0.2727, -0.0759, -0.0635, -0.0644, -0.0600, -0.0743,
        -0.0979, -0.0650, -0.0621, -0.0606, -0.0648, -0.0608, -0.0617, -0.0613,
        -0.0668, -0.0607, -0.0629, -0.0737, -0.0702, -0.0678, -0.0723, -0.0711,
        -0.0631, -0.0700, -0.0777, -0.0609, -0.0681, -0.0795, -0.0641, -0.0620,
        -0.0896, -0.0764, -0.0607, -0.0595, -0.0641, -0.0648, -0.0617, -0.0596,
        -0.0742, -0.0777, -0.0690, -0.0688, -0.0634, -0.0764, -0.0643, -0.0648,
        -0.0610, -0.0710, -0.0681, -0.0620, -0.0609, -0.0614, -0.0602, -0.0631,
        -0.0659,  0.6850, -0.0769, -0.0708, -0.0635, -0.0619,  0.2318, -0.0701,
        -0.0624, -0.0914, -0.0609, -0.0925, -0.0670, -0.0629, -0.0639, -0.0650,
        -0.0619, -0.0874, -0.0597, -0.0610, -0.0625,  0.0428, -0.0689, -0.0599,
        -0.0634, -0.0716, -0.0857, -0.0607, -0.0735, -0.0699, -0.0704, -0.0665,
        -0.0769, -0.0866, -0.0829, -0.0638, -0.0675, -0.0622, -0.0620, -0.0655,
        -0.1131, -0.0726, -0.0672, -0.0768, -0.0787, -0.0683, -0.0611,  0.1339,
         0.2749, -0.0596, -0.0643, -0.0610, -0.0701, -0.0647, -0.0600, -0.0611,
        -0.0620, -0.2514, -0.0636, -0.0707, -0.0662, -0.0787, -0.0704, -0.0682,
        -0.0661, -0.0648, -0.0756, -0.0675, -0.0660, -0.0649, -0.0669, -0.0629,
        -0.0640, -0.0647, -0.0623, -0.0627, -0.0622, -0.0663, -0.0647, -0.1568,
        -0.0301, -0.0654, -0.0645, -0.0633, -0.0709, -0.0622, -0.0924, -0.0627,
        -0.0812, -0.0618, -0.0861, -0.0615, -0.0715, -0.0668, -0.0752, -0.0606,
        -0.0812, -0.0611, -0.1030, -0.0683, -0.0644, -0.0670, -0.0674, -0.0620,
        -0.0695, -0.0807, -0.0719, -0.0609, -0.0602, -0.0638, -0.0634, -0.1067,
        -0.0759, -0.0615, -0.0774, -0.0627, -0.1058, -0.0608, -0.0624, -0.0635,
        -0.0601, -0.0649, -0.0611, -0.0709, -0.0650, -0.0644, -0.0620, -0.0646,
        -0.0602, -0.0718, -0.0650, -0.0671, -0.0615, -0.0625, -0.0714, -0.0660,
        -0.0600, -0.0631,  0.5027, -0.0604, -0.0619, -0.0651, -0.1102, -0.0731,
        -0.0624, -0.0701, -0.0614, -0.0726, -0.0703, -0.0634, -0.0686, -0.0697,
        -0.3382, -0.0706, -0.0665, -0.0665, -0.0666, -0.0685, -0.0601, -0.0645,
        -0.0671, -0.0643, -0.0596, -0.0604, -0.0638, -0.0701,  0.3213, -0.0619,
        -0.1086, -0.0650, -0.0970, -0.0685, -0.0625, -0.0629, -0.0738, -0.0623,
        -0.0621, -0.0633, -0.0708, -0.0720, -0.1633, -0.0673, -0.0775, -0.0620,
        -0.0687, -0.0699, -0.3826, -0.0676, -0.0641, -0.0766, -0.0510, -0.0617,
        -0.6460, -0.0722, -0.0800, -0.0659, -0.0620, -0.0624, -0.0939, -0.0619,
        -0.0595, -0.0601, -0.0695, -0.0653, -0.0683, -0.0648, -0.0775, -0.0829,
        -0.0629, -0.0699, -0.0676, -0.0739, -0.0605, -0.0594, -0.0597, -0.0883,
        -0.0616, -0.0672, -0.0639, -0.0852, -0.0645, -0.0782, -0.0675, -0.0688,
         0.1670, -0.0615, -0.0652, -0.0647, -0.0678, -0.0732, -0.1571,  0.0968,
        -0.0619,  0.5789, -0.0646, -0.0602, -0.0671, -0.1772, -0.0614,  0.5741,
        -0.1025,  0.3329,  0.0805,  0.0518, -0.0739, -0.0672, -0.0615, -0.0627,
        -0.0738, -0.0633, -0.0860, -0.4203, -0.0725, -0.0639, -0.0713, -0.0697,
        -0.0635, -0.0628, -0.0624, -0.0664, -0.0604, -0.0639, -0.0955, -0.0687,
        -0.0663, -0.0622, -0.0617, -0.0637, -0.0636, -0.0666, -0.0653, -0.0679,
        -0.0661, -0.0783, -0.0662, -0.0643, -0.2907, -0.0617, -0.0700, -0.0611,
        -0.2127, -0.0667, -0.0684, -0.0607, -0.0612, -0.0649,  0.0291, -0.0643,
        -0.0642, -0.0820, -0.0628, -0.0631, -0.0656, -0.0656,  0.0486, -0.0961],
       device='cuda:1', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0095, 0.9894, 0.9910, 0.9860, 0.9931, 0.9915, 0.9875, 0.9883, 0.9909,
        0.9919, 0.9896, 0.9937, 0.9916, 0.9929, 0.8548, 0.9855, 0.9922, 0.9925,
        0.9659, 0.9927, 0.9884, 0.0211, 0.7891, 0.9912, 0.9820, 0.9934, 0.9914,
        0.9836, 0.9909, 0.9938, 0.9818, 0.9842, 0.9924, 0.9922, 0.0131, 0.9933,
        0.9918, 0.9872, 0.9896, 0.9642, 0.9924, 0.9888, 0.9928, 0.0119, 0.9904,
        0.9906, 0.9870, 0.9925, 0.9902, 0.9925, 0.9858, 0.9877, 0.9910, 0.9916,
        0.9900, 0.9925, 0.9859, 0.9934, 0.9920, 0.9916, 0.9935, 0.9930, 0.9864,
        0.9933, 0.9921, 0.9935, 0.9948, 0.9936, 0.9926, 0.9906, 0.9897, 0.9931,
        0.9920, 0.9891, 0.9910, 0.9924, 0.9922, 0.9900, 0.9923, 0.4465, 0.9923,
        0.9937, 0.9909, 0.9927, 0.3941, 0.9886, 0.9926, 0.9897, 0.9913, 0.0504,
        0.9934, 0.9908, 0.9907, 0.9930, 0.9856, 0.9920, 0.9919, 0.9904, 0.9892,
        0.9940, 0.9911, 0.9902, 0.9903, 0.9934, 0.9897, 0.9927, 0.9930, 0.9918,
        0.9911, 0.0160, 0.9923, 0.9845, 0.0809, 0.9923, 0.9903, 0.0132, 0.9933,
        0.9907, 0.9899, 0.9935, 0.9875, 0.9917, 0.9921, 0.9793, 0.9842, 0.9922,
        0.9935, 0.9935, 0.9916, 0.9869, 0.9934, 0.9922, 0.9885, 0.9891, 0.9907,
        0.9877, 0.9930, 0.0170, 0.9913, 0.9909, 0.9911, 0.9893, 0.9935, 0.9917,
        0.9922, 0.9893, 0.9916, 0.9936, 0.9938, 0.0149, 0.9920, 0.9920, 0.9900,
        0.9928, 0.9941, 0.9922, 0.9809, 0.9921, 0.9908, 0.9899, 0.9932, 0.9870,
        0.0045, 0.9889, 0.9923, 0.9927, 0.9936, 0.9891, 0.9812, 0.9919, 0.9929,
        0.9935, 0.9903, 0.9924, 0.9930, 0.9933, 0.9911, 0.9937, 0.9925, 0.9897,
        0.9907, 0.9911, 0.9901, 0.9908, 0.9930, 0.9916, 0.9873, 0.9928, 0.9914,
        0.9887, 0.9918, 0.9935, 0.9848, 0.9883, 0.9929, 0.9935, 0.9925, 0.9922,
        0.9935, 0.9938, 0.9883, 0.9877, 0.9900, 0.9919, 0.9914, 0.9884, 0.9927,
        0.9917, 0.9937, 0.9903, 0.9914, 0.9928, 0.9931, 0.9932, 0.9930, 0.9928,
        0.9922, 0.0277, 0.9881, 0.9896, 0.9923, 0.9920, 0.0081, 0.9898, 0.9927,
        0.9827, 0.9930, 0.9816, 0.9914, 0.9925, 0.9922, 0.9923, 0.9934, 0.9845,
        0.9940, 0.9929, 0.9924, 0.0121, 0.9912, 0.9942, 0.9939, 0.9900, 0.9863,
        0.9938, 0.9881, 0.9901, 0.9896, 0.9915, 0.9860, 0.9862, 0.9867, 0.9918,
        0.9921, 0.9924, 0.9937, 0.9917, 0.8159, 0.9894, 0.9912, 0.9889, 0.9876,
        0.9918, 0.9922, 0.0613, 0.0089, 0.9936, 0.9916, 0.9929, 0.9898, 0.9925,
        0.9936, 0.9925, 0.9923, 0.3472, 0.9927, 0.9907, 0.9906, 0.9874, 0.9902,
        0.9913, 0.9909, 0.9911, 0.0182, 0.9923, 0.9909, 0.9915, 0.9917, 0.9920,
        0.9928, 0.9921, 0.9936, 0.9920, 0.9936, 0.9906, 0.9924, 0.9412, 0.9414,
        0.9919, 0.9920, 0.9927, 0.9895, 0.9924, 0.9825, 0.9926, 0.9873, 0.9934,
        0.9853, 0.9932, 0.9909, 0.9905, 0.9886, 0.9940, 0.0139, 0.9937, 0.9805,
        0.9910, 0.9919, 0.9912, 0.9912, 0.9930, 0.9904, 0.9840, 0.9899, 0.9932,
        0.9928, 0.9935, 0.9922, 0.9830, 0.9885, 0.9930, 0.9878, 0.9925, 0.9812,
        0.9938, 0.9930, 0.9919, 0.9931, 0.9927, 0.9923, 0.9912, 0.9924, 0.9912,
        0.9927, 0.9915, 0.9936, 0.9891, 0.9917, 0.9914, 0.9930, 0.9926, 0.9895,
        0.9908, 0.9928, 0.9927, 0.0353, 0.9936, 0.9924, 0.9910, 0.9756, 0.9891,
        0.9923, 0.9911, 0.9926, 0.9902, 0.9910, 0.9918, 0.9906, 0.9906, 0.0188,
        0.9910, 0.9912, 0.9912, 0.9916, 0.9908, 0.9944, 0.9924, 0.9917, 0.9914,
        0.9933, 0.9930, 0.9925, 0.9893, 0.0063, 0.9925, 0.9798, 0.9932, 0.9809,
        0.9917, 0.9926, 0.9934, 0.9891, 0.9940, 0.9932, 0.9927, 0.9908, 0.9895,
        0.9774, 0.9911, 0.9889, 0.9935, 0.9902, 0.9906, 0.0177, 0.9916, 0.9928,
        0.9878, 0.9250, 0.9938, 0.0370, 0.9888, 0.9869, 0.9910, 0.9940, 0.9932,
        0.9831, 0.9931, 0.9941, 0.9929, 0.9905, 0.9915, 0.9912, 0.9913, 0.9888,
        0.9823, 0.9928, 0.9916, 0.9910, 0.9884, 0.9938, 0.9939, 0.9934, 0.9862,
        0.9932, 0.9911, 0.9924, 0.9865, 0.9927, 0.9873, 0.9910, 0.9920, 0.0300,
        0.9921, 0.9926, 0.9917, 0.9909, 0.9907, 0.1932, 0.0110, 0.9932, 0.0050,
        0.9930, 0.9933, 0.9906, 0.4199, 0.9924, 0.0233, 0.9779, 0.0128, 0.0041,
        0.0217, 0.9893, 0.9915, 0.9932, 0.9922, 0.9879, 0.9920, 0.9862, 0.3992,
        0.9899, 0.9922, 0.9897, 0.9903, 0.9927, 0.9925, 0.9929, 0.9899, 0.9937,
        0.9917, 0.9807, 0.9910, 0.9921, 0.9936, 0.9924, 0.9935, 0.9919, 0.9922,
        0.9919, 0.9914, 0.9918, 0.9882, 0.9920, 0.9928, 0.0081, 0.9933, 0.9901,
        0.9928, 0.0192, 0.9929, 0.9895, 0.9932, 0.9929, 0.9920, 0.7973, 0.9911,
        0.9928, 0.9864, 0.9929, 0.9923, 0.9923, 0.9922, 0.4333, 0.0452],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[ 3.0558e-01, -1.0780e-03,  3.1283e-04,  2.1700e-03,  1.4557e-04,
          2.1068e-04, -1.0325e-04, -8.6206e-04, -1.3862e-04, -3.1864e-04,
          3.8681e-04, -1.9276e-05,  1.3087e-03,  1.0429e-05, -3.9979e-03,
         -2.1277e-03, -8.2530e-05, -8.8513e-05, -1.1407e-03,  3.2391e-04,
         -1.1230e-03,  1.4330e-01, -8.9619e-04,  4.7062e-04,  1.1089e-03,
          8.4468e-04, -2.3395e-04,  5.6433e-04, -5.8400e-04, -1.4639e-04,
          1.4429e-03, -2.6300e-04,  5.1757e-04, -8.1158e-04, -2.2632e-01,
         -4.5175e-06,  1.5340e-04,  7.1228e-05, -8.3448e-04, -1.6328e-04,
          2.0542e-05,  1.5012e-03,  4.9740e-04, -2.5035e-01, -6.7895e-04,
          8.1613e-04, -7.4758e-04, -1.1090e-03, -1.2980e-03,  3.2995e-04,
         -2.1393e-05,  2.2552e-04, -1.1901e-04,  1.7161e-03, -9.4120e-05,
         -3.8853e-04, -1.9848e-03,  5.6834e-04, -2.2255e-04,  1.4069e-03,
          2.6985e-04,  6.0889e-04,  1.8896e-03, -5.6562e-04,  4.2863e-04,
         -1.4574e-03, -2.7443e-04,  3.9977e-04, -3.7388e-04,  7.5647e-04,
         -1.5983e-03, -7.8319e-04, -5.6133e-05, -1.3204e-04, -5.1179e-04,
         -1.2670e-04, -5.9412e-04, -6.9656e-04,  3.4882e-04,  2.3813e-04,
          1.2574e-04,  7.3191e-04, -5.9044e-04,  2.2057e-05, -2.1821e-04,
          4.2694e-05,  2.6746e-04,  4.8628e-04, -1.0422e-04,  5.6681e-02,
         -6.7296e-04, -2.3175e-04,  7.8085e-04,  1.0678e-04,  1.6726e-03,
         -6.7760e-04,  6.9785e-04, -3.0536e-04,  1.6396e-03, -2.8577e-04,
         -5.3782e-04, -1.9658e-04, -7.6856e-04,  5.7085e-04,  1.6904e-04,
          9.5101e-05, -1.2892e-04, -4.8178e-04, -2.8220e-04,  1.8593e-01,
         -1.9245e-03,  3.5302e-04,  2.8337e-02, -6.0651e-05, -2.9151e-04,
          2.2414e-01,  8.8144e-04,  9.0427e-04,  1.5564e-03, -4.3102e-04,
         -2.1376e-04, -3.0058e-04,  3.5113e-04, -3.5966e-06, -8.2502e-04,
         -1.0826e-03,  9.7470e-05, -1.1212e-04, -8.0116e-04,  2.7670e-04,
          8.9078e-04,  2.1342e-04,  4.6106e-04,  3.3410e-04,  9.7757e-04,
          1.6890e-03,  2.5762e-04, -1.7350e-01, -1.5438e-03,  5.0118e-05,
          2.3740e-04, -3.0417e-05, -3.2059e-04,  4.3471e-04,  9.9298e-05,
         -7.7533e-04,  6.3568e-04,  6.4653e-04,  7.0451e-04,  2.0117e-01,
         -8.7495e-04, -8.9083e-04, -5.0911e-04,  5.9226e-04, -1.2479e-04,
         -8.3274e-04,  9.4433e-04,  1.0064e-03, -4.5116e-04, -2.4400e-03,
          3.4400e-04,  1.5933e-03, -6.6408e-01,  7.7947e-04, -3.9051e-04,
         -7.0898e-04,  1.1309e-03,  1.0842e-05,  1.8125e-03, -1.9706e-04,
         -6.2117e-04,  2.0499e-05, -2.1347e-04, -4.2033e-04,  3.3143e-04,
          1.3957e-03,  3.1454e-04, -2.9467e-04,  1.0591e-04, -2.2634e-04,
         -5.5238e-04,  2.6395e-04,  7.9999e-04, -2.2402e-03, -9.6158e-04,
         -9.3694e-04,  7.7470e-04,  5.4576e-04,  6.7462e-05,  1.3403e-03,
          2.5344e-04, -2.0184e-04, -2.4124e-03,  6.2682e-03, -2.1669e-04,
          6.4027e-04,  1.1717e-03, -4.2543e-04, -2.9644e-06,  4.1692e-04,
          4.7262e-04,  3.0312e-04, -9.2985e-04,  5.3516e-04, -1.0580e-05,
          4.7968e-04, -2.2289e-04,  9.6683e-04,  9.3986e-04,  2.3566e-04,
         -4.7613e-04, -4.8754e-04, -2.8544e-04,  4.8547e-04,  2.5727e-05,
         -3.0235e-04,  1.1114e-04, -1.0597e-01, -2.0841e-04, -8.3016e-04,
          4.3861e-04,  9.4309e-06,  3.7119e-01,  6.0505e-04,  8.6902e-04,
         -1.6953e-03,  8.6585e-04,  2.4631e-04,  2.0493e-04, -4.3219e-04,
         -3.5564e-04, -1.0351e-04,  3.4789e-04, -1.3620e-03, -9.7213e-04,
          1.0527e-03, -5.4656e-04, -2.4631e-01, -9.0987e-04, -4.2157e-04,
          1.2840e-04, -2.2319e-03, -1.7383e-03,  1.9018e-04,  2.6039e-03,
          4.2395e-04,  4.7841e-04, -2.1582e-04,  1.5523e-03, -5.2829e-04,
         -5.9531e-04, -1.1151e-04, -6.8782e-04,  1.8612e-06, -1.2351e-04,
          6.4203e-06,  2.0753e-04,  8.1986e-04,  8.3686e-04, -8.8123e-04,
         -4.9831e-04, -3.0380e-04,  2.4728e-04, -4.8892e-02,  3.3504e-01,
          5.1742e-04,  4.0458e-04, -1.1085e-03,  7.0319e-04,  8.1728e-05,
         -9.2737e-04,  1.8554e-04,  1.2285e-03, -4.5794e-04, -8.4874e-04,
         -7.2822e-04,  1.6265e-03, -1.3182e-03, -8.1105e-05,  1.0182e-03,
         -9.6205e-05, -1.5761e-03, -1.6482e-01,  1.0443e-04,  2.9027e-04,
          4.8929e-04,  5.2643e-05,  2.4806e-04,  8.0141e-04, -1.9891e-05,
         -8.2370e-04,  1.6612e-04,  1.2786e-03,  8.3226e-04, -1.1853e-03,
          1.2736e-03, -6.5189e-04,  5.7156e-04,  1.0868e-03, -4.9504e-04,
          5.1975e-04, -2.2602e-04, -1.2949e-03,  6.5355e-04, -5.8893e-04,
          7.9302e-04,  1.0888e-05, -1.2226e-04,  4.0500e-04,  6.8548e-04,
          6.9967e-04,  1.0439e-03,  2.1565e-01,  3.0678e-04, -1.1142e-03,
          9.7721e-05,  1.0212e-03, -5.9129e-04, -1.5701e-03, -1.1972e-04,
          7.7704e-04,  2.7321e-04,  1.4661e-03,  2.0869e-04, -4.3190e-04,
          9.8861e-05,  4.5931e-05, -9.2194e-03, -9.1012e-04,  2.3037e-04,
         -6.3808e-04, -1.3479e-04,  8.9779e-05,  1.0127e-03,  8.0455e-04,
         -2.3450e-04,  4.7978e-04, -2.4759e-05, -2.6061e-04, -1.3207e-03,
          6.0342e-04, -1.1744e-04,  8.2196e-04,  3.6710e-04, -1.1792e-04,
         -5.2188e-04,  5.2878e-04, -2.9853e-04,  7.3097e-04,  4.5769e-05,
         -4.2990e-04,  4.3460e-04,  1.7784e-03,  1.1389e-03,  8.1036e-02,
         -1.3649e-04, -3.4704e-04,  6.7321e-05,  5.9494e-03, -6.9773e-04,
         -2.8849e-05, -1.3132e-03, -8.1372e-04,  1.2358e-03, -6.7184e-04,
         -8.3135e-05,  1.4968e-04, -9.2863e-05,  1.4874e-01,  3.6978e-04,
          3.2813e-04, -1.4394e-03, -1.0177e-03,  1.0321e-03,  3.4915e-04,
          8.8974e-04,  5.0624e-04, -6.5769e-04,  5.4022e-04,  1.2498e-04,
          5.0689e-04, -4.4549e-04, -4.7457e-01,  3.0642e-04, -2.3852e-03,
         -6.6895e-04,  2.4288e-03, -6.3051e-04, -2.3205e-04, -2.4178e-04,
          2.1297e-03, -3.9913e-04, -7.7912e-04, -2.7622e-05, -1.1214e-03,
          5.9158e-05, -8.0180e-05, -3.6418e-04, -6.9277e-04, -2.2495e-04,
          1.5913e-05,  6.1757e-04, -1.6661e-01,  5.8926e-04,  6.7863e-04,
          4.7704e-04, -1.9605e-03,  3.9041e-04,  7.9805e-02,  6.9489e-04,
          2.7231e-03,  5.7449e-04, -7.8749e-04, -7.9531e-04, -1.5236e-03,
          5.1032e-04, -3.1888e-04, -9.7434e-04, -1.0803e-03,  1.3718e-05,
          6.3534e-04,  5.9719e-04,  1.7343e-03, -3.3000e-04,  1.0464e-03,
         -1.1235e-03,  8.6495e-04,  1.1654e-04, -5.3549e-04,  8.6856e-05,
          7.9827e-04,  5.0285e-04, -7.7087e-04,  5.2237e-04, -5.7428e-04,
          3.7970e-03, -3.7056e-04,  6.2631e-05, -1.7822e-03, -3.7815e-04,
          9.4662e-02,  2.1170e-04,  6.0017e-04,  1.1153e-03,  2.2059e-04,
         -2.0993e-03,  2.4978e-03,  2.7058e-01, -3.7602e-04, -5.9683e-01,
         -9.9023e-05, -4.5837e-04, -1.0560e-04, -2.6858e-03, -1.0395e-04,
          1.2357e-01,  2.3332e-03, -2.2919e-01,  7.3342e-01, -1.3589e-01,
         -6.0023e-04, -3.6469e-04,  5.0822e-04, -1.5876e-05,  5.5303e-04,
          2.5716e-04,  2.0748e-03, -5.0015e-03, -1.0685e-03, -1.0572e-04,
         -1.8862e-03,  9.6405e-04,  2.9964e-04,  5.8719e-04, -1.3489e-04,
          2.7232e-05,  6.9995e-04, -1.3371e-04, -1.7782e-05, -1.1017e-03,
          2.1493e-04,  8.1271e-04, -1.3074e-04, -9.6664e-04, -1.1912e-03,
         -3.1913e-04,  3.2753e-04,  6.9253e-04,  1.2344e-05,  2.6060e-03,
          3.5762e-04,  4.5882e-04,  3.6915e-01, -3.9723e-05,  7.9834e-04,
         -6.2894e-04, -1.5041e-01, -6.3331e-04, -4.1265e-04, -2.7349e-05,
         -9.6057e-04,  1.8188e-03,  3.1603e-04,  4.7209e-04,  2.3757e-04,
         -1.1165e-03, -9.0139e-04,  6.3613e-04,  6.5558e-04,  7.4256e-04,
         -2.7654e-02,  6.5723e-02]], device='cuda:1', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[0.0021, 0.0143, 0.0053, 0.0260, 0.0037, 0.0068, 0.0149, 0.0085, 0.0086,
         0.0044, 0.0118, 0.0035, 0.0052, 0.0033, 0.0181, 0.0097, 0.0066, 0.0035,
         0.0091, 0.0041, 0.0134, 0.0006, 0.0017, 0.0074, 0.0047, 0.0033, 0.0052,
         0.0474, 0.0084, 0.0036, 0.0197, 0.0536, 0.0041, 0.0047, 0.0003, 0.0039,
         0.0037, 0.0130, 0.0118, 0.0202, 0.0053, 0.0157, 0.0037, 0.0003, 0.0111,
         0.0097, 0.0141, 0.0048, 0.0081, 0.0037, 0.0135, 0.0059, 0.0051, 0.0046,
         0.0072, 0.0052, 0.0144, 0.0044, 0.0066, 0.0068, 0.0034, 0.0044, 0.0217,
         0.0041, 0.0035, 0.0038, 0.0035, 0.0035, 0.0050, 0.0062, 0.0125, 0.0044,
         0.0053, 0.0090, 0.0059, 0.0046, 0.0042, 0.0112, 0.0037, 0.0003, 0.0034,
         0.0042, 0.0056, 0.0039, 0.0015, 0.0093, 0.0038, 0.0086, 0.0061, 0.0003,
         0.0035, 0.0084, 0.0095, 0.0053, 0.0154, 0.0044, 0.0077, 0.0048, 0.0086,
         0.0034, 0.0076, 0.0080, 0.0067, 0.0039, 0.0086, 0.0035, 0.0035, 0.0063,
         0.0055, 0.0006, 0.0063, 0.0178, 0.0016, 0.0039, 0.0068, 0.0004, 0.0042,
         0.0050, 0.0104, 0.0051, 0.0133, 0.0055, 0.0040, 0.0218, 0.0157, 0.0056,
         0.0037, 0.0033, 0.0065, 0.0171, 0.0037, 0.0044, 0.0124, 0.0061, 0.0062,
         0.0122, 0.0041, 0.0004, 0.0081, 0.0056, 0.0072, 0.0099, 0.0037, 0.0054,
         0.0063, 0.0097, 0.0049, 0.0033, 0.0032, 0.0002, 0.0050, 0.0049, 0.0088,
         0.0034, 0.0033, 0.0049, 0.0178, 0.0036, 0.0069, 0.0092, 0.0049, 0.0093,
         0.0002, 0.0127, 0.0039, 0.0051, 0.0036, 0.0122, 0.0171, 0.0035, 0.0041,
         0.0034, 0.0042, 0.0036, 0.0040, 0.0041, 0.0054, 0.0034, 0.0039, 0.0116,
         0.0083, 0.0036, 0.0108, 0.0091, 0.0055, 0.0145, 0.0176, 0.0034, 0.0069,
         0.0078, 0.0044, 0.0043, 0.0187, 0.0201, 0.0037, 0.0034, 0.0050, 0.0048,
         0.0038, 0.0038, 0.0111, 0.0118, 0.0069, 0.0094, 0.0046, 0.0139, 0.0045,
         0.0063, 0.0037, 0.0082, 0.0065, 0.0035, 0.0033, 0.0036, 0.0033, 0.0039,
         0.0052, 0.0004, 0.0167, 0.0108, 0.0057, 0.0033, 0.0003, 0.0071, 0.0041,
         0.0231, 0.0036, 0.0090, 0.0058, 0.0040, 0.0057, 0.0046, 0.0036, 0.0169,
         0.0036, 0.0033, 0.0037, 0.0002, 0.0062, 0.0033, 0.0044, 0.0105, 0.0127,
         0.0041, 0.0126, 0.0091, 0.0089, 0.0058, 0.0209, 0.0124, 0.0144, 0.0048,
         0.0067, 0.0035, 0.0043, 0.0044, 0.0016, 0.0109, 0.0072, 0.0211, 0.0182,
         0.0089, 0.0039, 0.0003, 0.0006, 0.0032, 0.0066, 0.0045, 0.0068, 0.0050,
         0.0033, 0.0032, 0.0042, 0.0009, 0.0040, 0.0105, 0.0067, 0.0170, 0.0102,
         0.0081, 0.0067, 0.0058, 0.0007, 0.0080, 0.0064, 0.0045, 0.0070, 0.0037,
         0.0049, 0.0053, 0.0038, 0.0046, 0.0050, 0.0058, 0.0049, 0.0049, 0.0032,
         0.0060, 0.0050, 0.0036, 0.0088, 0.0044, 0.0145, 0.0042, 0.0133, 0.0041,
         0.0154, 0.0034, 0.0092, 0.0072, 0.0168, 0.0033, 0.0002, 0.0036, 0.0125,
         0.0058, 0.0048, 0.0056, 0.0065, 0.0034, 0.0095, 0.0097, 0.0120, 0.0033,
         0.0035, 0.0040, 0.0037, 0.1105, 0.0116, 0.0035, 0.0144, 0.0040, 0.0141,
         0.0038, 0.0060, 0.0038, 0.0032, 0.0041, 0.0035, 0.0084, 0.0068, 0.0043,
         0.0039, 0.0048, 0.0035, 0.0107, 0.0048, 0.0051, 0.0033, 0.0038, 0.0093,
         0.0066, 0.0045, 0.0036, 0.0004, 0.0031, 0.0042, 0.0057, 0.0155, 0.0110,
         0.0043, 0.0107, 0.0034, 0.0084, 0.0110, 0.0038, 0.0059, 0.0075, 0.0019,
         0.0087, 0.0057, 0.0055, 0.0074, 0.0067, 0.0035, 0.0038, 0.0095, 0.0050,
         0.0036, 0.0041, 0.0041, 0.0088, 0.0007, 0.0043, 0.0119, 0.0028, 0.0120,
         0.0061, 0.0038, 0.0040, 0.0146, 0.0050, 0.0036, 0.0055, 0.0109, 0.0067,
         0.0064, 0.0063, 0.0164, 0.0037, 0.0058, 0.0114, 0.0012, 0.0062, 0.0051,
         0.0150, 0.0057, 0.0034, 0.0002, 0.0079, 0.0167, 0.0066, 0.0037, 0.0032,
         0.0191, 0.0046, 0.0034, 0.0036, 0.0082, 0.0069, 0.0044, 0.0053, 0.0267,
         0.0118, 0.0035, 0.0088, 0.0073, 0.0184, 0.0040, 0.0031, 0.0033, 0.0152,
         0.0043, 0.0060, 0.0049, 0.0425, 0.0037, 0.0143, 0.0061, 0.0039, 0.0008,
         0.0037, 0.0054, 0.0055, 0.0068, 0.0094, 0.0005, 0.0003, 0.0041, 0.0005,
         0.0052, 0.0033, 0.0068, 0.0078, 0.0032, 0.0005, 0.0215, 0.0006, 0.0005,
         0.0002, 0.0099, 0.0074, 0.0037, 0.0035, 0.0101, 0.0042, 0.0224, 0.0039,
         0.0077, 0.0049, 0.0090, 0.0082, 0.0048, 0.0038, 0.0037, 0.0070, 0.0035,
         0.0051, 0.0231, 0.0093, 0.0065, 0.0037, 0.0038, 0.0052, 0.0036, 0.0083,
         0.0063, 0.0090, 0.0067, 0.0165, 0.0054, 0.0049, 0.0003, 0.0037, 0.0072,
         0.0041, 0.0008, 0.0059, 0.0067, 0.0044, 0.0034, 0.0081, 0.0015, 0.0055,
         0.0049, 0.0111, 0.0051, 0.0042, 0.0052, 0.0058, 0.0063, 0.0002]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.2055], device='cuda:1', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0030], device='cuda:1', grad_fn=<AddBackward0>)
Using device: cuda:1
===== Training profile tensin-3x512-s03 - 2 =====
[0:00:01.749669] epoch: 0 | elbo: 58928.1514453125 | train_rmse: 0.3275 | val_rmse: 0.5489 | val_ll: -0.9439
[0:01:36.325025] epoch: 50 | elbo: 58208.6860546875 | train_rmse: 0.3242 | val_rmse: 0.5455 | val_ll: -0.9359
[0:03:11.868298] epoch: 100 | elbo: 57782.101406249996 | train_rmse: 0.3246 | val_rmse: 0.5442 | val_ll: -0.9301
[0:04:46.003323] epoch: 150 | elbo: 57422.745859375005 | train_rmse: 0.3251 | val_rmse: 0.5441 | val_ll: -0.9375
[0:06:19.415996] epoch: 200 | elbo: 56936.500976562485 | train_rmse: 0.3238 | val_rmse: 0.5418 | val_ll: -0.9367
[0:07:53.233997] epoch: 250 | elbo: 56496.2909375 | train_rmse: 0.3239 | val_rmse: 0.5394 | val_ll: -0.9237
[0:09:27.204470] epoch: 300 | elbo: 56021.31253906251 | train_rmse: 0.323 | val_rmse: 0.538 | val_ll: -0.9257
[0:11:02.000108] epoch: 350 | elbo: 55674.103984375004 | train_rmse: 0.3237 | val_rmse: 0.5372 | val_ll: -0.9224
[0:12:35.627122] epoch: 400 | elbo: 55077.930507812496 | train_rmse: 0.3236 | val_rmse: 0.5355 | val_ll: -0.9239
[0:14:07.585932] epoch: 450 | elbo: 54930.315585937504 | train_rmse: 0.3252 | val_rmse: 0.5371 | val_ll: -0.9329
[0:15:40.029922] epoch: 500 | elbo: 54579.3544921875 | train_rmse: 0.3222 | val_rmse: 0.534 | val_ll: -0.9228
[0:17:12.852107] epoch: 550 | elbo: 54290.652265625 | train_rmse: 0.3213 | val_rmse: 0.5307 | val_ll: -0.9181
[0:18:46.237055] epoch: 600 | elbo: 53910.33464843751 | train_rmse: 0.3223 | val_rmse: 0.531 | val_ll: -0.9267
[0:20:19.688686] epoch: 650 | elbo: 53493.4828125 | train_rmse: 0.3232 | val_rmse: 0.5275 | val_ll: -0.8991
[0:21:53.233842] epoch: 700 | elbo: 53193.721875 | train_rmse: 0.3221 | val_rmse: 0.5277 | val_ll: -0.9097
[0:23:29.249981] epoch: 750 | elbo: 52703.95769531249 | train_rmse: 0.3263 | val_rmse: 0.5315 | val_ll: -0.9392
[0:25:03.273144] epoch: 800 | elbo: 52531.498046875 | train_rmse: 0.321 | val_rmse: 0.5253 | val_ll: -0.9042
[0:26:35.954650] epoch: 850 | elbo: 52245.13171875001 | train_rmse: 0.3215 | val_rmse: 0.5225 | val_ll: -0.8941
[0:28:09.900196] epoch: 900 | elbo: 51934.3413671875 | train_rmse: 0.3215 | val_rmse: 0.5223 | val_ll: -0.9033
[0:29:45.312365] epoch: 950 | elbo: 51408.4733203125 | train_rmse: 0.3214 | val_rmse: 0.5211 | val_ll: -0.875
[0:31:18.943167] epoch: 1000 | elbo: 51081.2573828125 | train_rmse: 0.3223 | val_rmse: 0.523 | val_ll: -0.903
[0:32:53.153752] epoch: 1050 | elbo: 50859.40359375 | train_rmse: 0.3219 | val_rmse: 0.5216 | val_ll: -0.8993
[0:34:25.277817] epoch: 1100 | elbo: 50552.125468750004 | train_rmse: 0.3226 | val_rmse: 0.5195 | val_ll: -0.887
[0:35:58.834019] epoch: 1150 | elbo: 50370.7787109375 | train_rmse: 0.3221 | val_rmse: 0.5184 | val_ll: -0.8952
[0:37:30.530298] epoch: 1200 | elbo: 49970.29296874999 | train_rmse: 0.3228 | val_rmse: 0.5166 | val_ll: -0.8871
[0:39:02.680892] epoch: 1250 | elbo: 49876.809726562504 | train_rmse: 0.3227 | val_rmse: 0.5153 | val_ll: -0.8816
[0:40:34.246516] epoch: 1300 | elbo: 49524.2268359375 | train_rmse: 0.3226 | val_rmse: 0.5174 | val_ll: -0.8923
[0:42:09.245261] epoch: 1350 | elbo: 49121.4184375 | train_rmse: 0.3215 | val_rmse: 0.515 | val_ll: -0.877
[0:43:45.776930] epoch: 1400 | elbo: 49026.757929687505 | train_rmse: 0.3248 | val_rmse: 0.5158 | val_ll: -0.8849
[0:45:22.627245] epoch: 1450 | elbo: 48759.30218750001 | train_rmse: 0.3227 | val_rmse: 0.5123 | val_ll: -0.8699
[0:46:56.228227] epoch: 1500 | elbo: 48509.3196875 | train_rmse: 0.3221 | val_rmse: 0.5117 | val_ll: -0.8696
[0:48:29.923456] epoch: 1550 | elbo: 48352.264375000006 | train_rmse: 0.3222 | val_rmse: 0.5103 | val_ll: -0.8699
[0:50:04.767493] epoch: 1600 | elbo: 48121.602812499994 | train_rmse: 0.3249 | val_rmse: 0.5117 | val_ll: -0.8794
[0:51:38.923001] epoch: 1650 | elbo: 47644.04718750001 | train_rmse: 0.3229 | val_rmse: 0.5083 | val_ll: -0.8613
[0:53:12.297845] epoch: 1700 | elbo: 47379.11015625 | train_rmse: 0.3229 | val_rmse: 0.5072 | val_ll: -0.8696
[0:54:45.886957] epoch: 1750 | elbo: 47112.06078125 | train_rmse: 0.3231 | val_rmse: 0.5061 | val_ll: -0.8587
[0:56:18.905458] epoch: 1800 | elbo: 47016.65882812499 | train_rmse: 0.3219 | val_rmse: 0.5059 | val_ll: -0.8607
[0:57:51.737103] epoch: 1850 | elbo: 46828.4533203125 | train_rmse: 0.3247 | val_rmse: 0.504 | val_ll: -0.8558
[0:59:23.770694] epoch: 1900 | elbo: 46545.2933984375 | train_rmse: 0.3236 | val_rmse: 0.5034 | val_ll: -0.8552
[1:00:55.546219] epoch: 1950 | elbo: 46234.90421875 | train_rmse: 0.3235 | val_rmse: 0.5029 | val_ll: -0.8534
[1:02:27.788741] epoch: 2000 | elbo: 46116.143828125 | train_rmse: 0.324 | val_rmse: 0.502 | val_ll: -0.8408
[1:04:00.670228] epoch: 2050 | elbo: 46012.6950390625 | train_rmse: 0.3253 | val_rmse: 0.5035 | val_ll: -0.8488
[1:05:32.641992] epoch: 2100 | elbo: 45694.0869140625 | train_rmse: 0.3239 | val_rmse: 0.5012 | val_ll: -0.842
[1:07:04.202520] epoch: 2150 | elbo: 45428.697382812505 | train_rmse: 0.3232 | val_rmse: 0.5006 | val_ll: -0.8475
[1:08:37.350611] epoch: 2200 | elbo: 45354.94765624999 | train_rmse: 0.3239 | val_rmse: 0.4992 | val_ll: -0.8358
[1:10:09.726273] epoch: 2250 | elbo: 45392.0466796875 | train_rmse: 0.3237 | val_rmse: 0.4984 | val_ll: -0.8363
[1:11:42.203710] epoch: 2300 | elbo: 44895.550624999996 | train_rmse: 0.324 | val_rmse: 0.4983 | val_ll: -0.8367
[1:13:15.193685] epoch: 2350 | elbo: 44864.37761718751 | train_rmse: 0.3229 | val_rmse: 0.4973 | val_ll: -0.8392
[1:14:46.799809] epoch: 2400 | elbo: 44495.11046875 | train_rmse: 0.3274 | val_rmse: 0.5017 | val_ll: -0.8673
[1:16:18.956148] epoch: 2450 | elbo: 44449.1333984375 | train_rmse: 0.3234 | val_rmse: 0.4963 | val_ll: -0.8254
[1:17:50.696989] epoch: 2500 | elbo: 44156.2176953125 | train_rmse: 0.3256 | val_rmse: 0.4959 | val_ll: -0.8266
[1:19:23.592607] epoch: 2550 | elbo: 44150.137578125 | train_rmse: 0.3229 | val_rmse: 0.4959 | val_ll: -0.8345
[1:20:55.833315] epoch: 2600 | elbo: 43848.0161328125 | train_rmse: 0.3235 | val_rmse: 0.4949 | val_ll: -0.8225
[1:22:27.230079] epoch: 2650 | elbo: 43605.5403125 | train_rmse: 0.325 | val_rmse: 0.4959 | val_ll: -0.8398
[1:23:59.427998] epoch: 2700 | elbo: 43498.81457031249 | train_rmse: 0.3242 | val_rmse: 0.4936 | val_ll: -0.8326
[1:25:32.755952] epoch: 2750 | elbo: 43496.982265624996 | train_rmse: 0.3239 | val_rmse: 0.4924 | val_ll: -0.819
[1:27:05.782105] epoch: 2800 | elbo: 43295.2564453125 | train_rmse: 0.3262 | val_rmse: 0.4954 | val_ll: -0.841
[1:28:38.569019] epoch: 2850 | elbo: 42983.06640625 | train_rmse: 0.3235 | val_rmse: 0.4931 | val_ll: -0.8148
[1:30:10.087718] epoch: 2900 | elbo: 42922.224921875 | train_rmse: 0.3237 | val_rmse: 0.4907 | val_ll: -0.8127
[1:31:41.935532] epoch: 2950 | elbo: 42975.546640625 | train_rmse: 0.3253 | val_rmse: 0.49 | val_ll: -0.8295
[1:33:14.407582] epoch: 3000 | elbo: 42759.937109375 | train_rmse: 0.3273 | val_rmse: 0.4967 | val_ll: -0.847
[1:34:46.741736] epoch: 3050 | elbo: 42615.3428515625 | train_rmse: 0.3238 | val_rmse: 0.4898 | val_ll: -0.8121
[1:36:20.032089] epoch: 3100 | elbo: 42461.8690234375 | train_rmse: 0.3269 | val_rmse: 0.4897 | val_ll: -0.7945
[1:37:53.341869] epoch: 3150 | elbo: 42264.066796875006 | train_rmse: 0.3253 | val_rmse: 0.4892 | val_ll: -0.8067
[1:39:26.531751] epoch: 3200 | elbo: 41962.5364453125 | train_rmse: 0.3238 | val_rmse: 0.4884 | val_ll: -0.8064
[1:41:02.353174] epoch: 3250 | elbo: 41946.5805859375 | train_rmse: 0.3246 | val_rmse: 0.4909 | val_ll: -0.8091
[1:42:36.997832] epoch: 3300 | elbo: 41730.1390625 | train_rmse: 0.3219 | val_rmse: 0.4867 | val_ll: -0.7962
[1:44:10.581271] epoch: 3350 | elbo: 41898.5405859375 | train_rmse: 0.3263 | val_rmse: 0.4904 | val_ll: -0.8147
[1:45:45.017184] epoch: 3400 | elbo: 41561.6668359375 | train_rmse: 0.3225 | val_rmse: 0.4864 | val_ll: -0.7861
[1:47:20.141845] epoch: 3450 | elbo: 41402.5960546875 | train_rmse: 0.322 | val_rmse: 0.486 | val_ll: -0.7956
[1:48:55.392213] epoch: 3500 | elbo: 41155.98707031249 | train_rmse: 0.3216 | val_rmse: 0.4846 | val_ll: -0.7973
[1:50:30.161941] epoch: 3550 | elbo: 41097.012226562496 | train_rmse: 0.3239 | val_rmse: 0.4841 | val_ll: -0.7905
[1:52:03.894736] epoch: 3600 | elbo: 40938.4967578125 | train_rmse: 0.3222 | val_rmse: 0.484 | val_ll: -0.7814
[1:53:36.116528] epoch: 3650 | elbo: 40831.7491015625 | train_rmse: 0.3253 | val_rmse: 0.4846 | val_ll: -0.789
[1:55:08.282929] epoch: 3700 | elbo: 40739.5044140625 | train_rmse: 0.3249 | val_rmse: 0.4839 | val_ll: -0.785
[1:56:40.740671] epoch: 3750 | elbo: 40962.3521875 | train_rmse: 0.3232 | val_rmse: 0.4846 | val_ll: -0.781
[1:58:12.853623] epoch: 3800 | elbo: 40804.52328125 | train_rmse: 0.3221 | val_rmse: 0.4847 | val_ll: -0.778
[1:59:45.973546] epoch: 3850 | elbo: 40253.4269921875 | train_rmse: 0.323 | val_rmse: 0.485 | val_ll: -0.7931
[2:01:18.821590] epoch: 3900 | elbo: 40275.580195312505 | train_rmse: 0.3238 | val_rmse: 0.484 | val_ll: -0.7883
[2:02:51.660896] epoch: 3950 | elbo: 40171.8746484375 | train_rmse: 0.3232 | val_rmse: 0.4849 | val_ll: -0.7798
[2:04:27.180533] epoch: 4000 | elbo: 39894.1640234375 | train_rmse: 0.324 | val_rmse: 0.4834 | val_ll: -0.7762
[2:06:01.995248] epoch: 4050 | elbo: 39755.05765625001 | train_rmse: 0.3241 | val_rmse: 0.4848 | val_ll: -0.7828
[2:07:37.451440] epoch: 4100 | elbo: 39668.96546875 | train_rmse: 0.3237 | val_rmse: 0.4838 | val_ll: -0.772
[2:09:12.480080] epoch: 4150 | elbo: 39789.9873046875 | train_rmse: 0.3255 | val_rmse: 0.4856 | val_ll: -0.782
[2:10:48.170804] epoch: 4200 | elbo: 39563.7456640625 | train_rmse: 0.3234 | val_rmse: 0.4834 | val_ll: -0.7583
[2:12:20.652324] epoch: 4250 | elbo: 39427.63003906251 | train_rmse: 0.324 | val_rmse: 0.4837 | val_ll: -0.7816
[2:13:54.142445] epoch: 4300 | elbo: 39431.770937500005 | train_rmse: 0.3239 | val_rmse: 0.4822 | val_ll: -0.7719
[2:15:27.534631] epoch: 4350 | elbo: 39284.303867187504 | train_rmse: 0.323 | val_rmse: 0.4824 | val_ll: -0.7712
[2:17:02.626029] epoch: 4400 | elbo: 39128.98132812501 | train_rmse: 0.3246 | val_rmse: 0.483 | val_ll: -0.7745
[2:18:38.958272] epoch: 4450 | elbo: 39250.262617187494 | train_rmse: 0.3244 | val_rmse: 0.4832 | val_ll: -0.7768
[2:20:13.814612] epoch: 4500 | elbo: 38852.955624999995 | train_rmse: 0.3246 | val_rmse: 0.4826 | val_ll: -0.7658
[2:21:49.215499] epoch: 4550 | elbo: 38916.1421484375 | train_rmse: 0.3242 | val_rmse: 0.4824 | val_ll: -0.7806
[2:23:24.450891] epoch: 4600 | elbo: 38694.443085937506 | train_rmse: 0.3257 | val_rmse: 0.4823 | val_ll: -0.7721
[2:24:57.072832] epoch: 4650 | elbo: 38702.3080859375 | train_rmse: 0.3238 | val_rmse: 0.4817 | val_ll: -0.7696
[2:26:31.117442] epoch: 4700 | elbo: 38481.579648437495 | train_rmse: 0.3262 | val_rmse: 0.4842 | val_ll: -0.7975
[2:28:04.973487] epoch: 4750 | elbo: 38407.4157421875 | train_rmse: 0.3258 | val_rmse: 0.4843 | val_ll: -0.7753
[2:29:37.135879] epoch: 4800 | elbo: 38507.9583984375 | train_rmse: 0.33 | val_rmse: 0.4831 | val_ll: -0.7745
[2:31:10.249543] epoch: 4850 | elbo: 38142.33417968749 | train_rmse: 0.3249 | val_rmse: 0.4805 | val_ll: -0.7733
[2:32:43.125412] epoch: 4900 | elbo: 38113.996796875 | train_rmse: 0.3242 | val_rmse: 0.4802 | val_ll: -0.7808
[2:34:15.406003] epoch: 4950 | elbo: 37954.9158984375 | train_rmse: 0.3244 | val_rmse: 0.4805 | val_ll: -0.7733
[2:35:47.679705] epoch: 5000 | elbo: 37994.5227734375 | train_rmse: 0.3263 | val_rmse: 0.48 | val_ll: -0.7704
[2:37:20.739004] epoch: 5050 | elbo: 37933.38046875 | train_rmse: 0.3242 | val_rmse: 0.4801 | val_ll: -0.7802
[2:38:54.520998] epoch: 5100 | elbo: 37944.161289062504 | train_rmse: 0.325 | val_rmse: 0.4799 | val_ll: -0.7708
[2:40:28.371989] epoch: 5150 | elbo: 37666.0002734375 | train_rmse: 0.324 | val_rmse: 0.4791 | val_ll: -0.7762
[2:42:01.895346] epoch: 5200 | elbo: 37694.6219921875 | train_rmse: 0.3243 | val_rmse: 0.4785 | val_ll: -0.7757
[2:43:34.429690] epoch: 5250 | elbo: 37649.068125000005 | train_rmse: 0.3256 | val_rmse: 0.4803 | val_ll: -0.7794
[2:45:07.001831] epoch: 5300 | elbo: 37466.749140625 | train_rmse: 0.3247 | val_rmse: 0.4782 | val_ll: -0.7699
[2:46:40.517174] epoch: 5350 | elbo: 37485.382421874994 | train_rmse: 0.3244 | val_rmse: 0.4771 | val_ll: -0.7629
[2:48:16.143150] epoch: 5400 | elbo: 37334.7861328125 | train_rmse: 0.3246 | val_rmse: 0.4784 | val_ll: -0.7695
[2:49:50.513348] epoch: 5450 | elbo: 37525.4569921875 | train_rmse: 0.3259 | val_rmse: 0.4791 | val_ll: -0.7664
[2:51:24.763999] epoch: 5500 | elbo: 37515.85476562501 | train_rmse: 0.3254 | val_rmse: 0.4777 | val_ll: -0.765
[2:52:57.678979] epoch: 5550 | elbo: 37125.978242187506 | train_rmse: 0.3245 | val_rmse: 0.4777 | val_ll: -0.7756
[2:54:32.314877] epoch: 5600 | elbo: 36871.256953125 | train_rmse: 0.3247 | val_rmse: 0.4759 | val_ll: -0.7627
[2:56:05.507917] epoch: 5650 | elbo: 36953.30609375 | train_rmse: 0.3247 | val_rmse: 0.4761 | val_ll: -0.7643
[2:57:40.006665] epoch: 5700 | elbo: 36878.830546875004 | train_rmse: 0.3262 | val_rmse: 0.4761 | val_ll: -0.7676
[2:59:15.565261] epoch: 5750 | elbo: 36721.788515625005 | train_rmse: 0.324 | val_rmse: 0.4754 | val_ll: -0.761
[3:00:51.471467] epoch: 5800 | elbo: 36650.42882812499 | train_rmse: 0.3247 | val_rmse: 0.4759 | val_ll: -0.7647
[3:02:26.249373] epoch: 5850 | elbo: 36524.7637109375 | train_rmse: 0.3249 | val_rmse: 0.4748 | val_ll: -0.7544
[3:03:59.270716] epoch: 5900 | elbo: 36612.1630078125 | train_rmse: 0.3243 | val_rmse: 0.4758 | val_ll: -0.7663
[3:05:31.712059] epoch: 5950 | elbo: 36547.709921875 | train_rmse: 0.3245 | val_rmse: 0.4741 | val_ll: -0.7584
[3:07:05.393523] epoch: 6000 | elbo: 36376.17875 | train_rmse: 0.3236 | val_rmse: 0.4736 | val_ll: -0.7498
[3:08:37.825580] epoch: 6050 | elbo: 36488.906757812496 | train_rmse: 0.3243 | val_rmse: 0.4734 | val_ll: -0.7603
[3:10:13.329131] epoch: 6100 | elbo: 36278.7204296875 | train_rmse: 0.3229 | val_rmse: 0.4739 | val_ll: -0.7628
[3:11:48.147548] epoch: 6150 | elbo: 36186.791171875004 | train_rmse: 0.324 | val_rmse: 0.4727 | val_ll: -0.7467
[3:13:22.307979] epoch: 6200 | elbo: 36018.416484375004 | train_rmse: 0.3233 | val_rmse: 0.4727 | val_ll: -0.7605
[3:14:55.169414] epoch: 6250 | elbo: 36083.70398437501 | train_rmse: 0.3259 | val_rmse: 0.4762 | val_ll: -0.7668
[3:16:27.599396] epoch: 6300 | elbo: 35973.043359375 | train_rmse: 0.3232 | val_rmse: 0.4731 | val_ll: -0.7565
[3:18:02.524078] epoch: 6350 | elbo: 36018.95699218749 | train_rmse: 0.325 | val_rmse: 0.4748 | val_ll: -0.7442
[3:19:36.055019] epoch: 6400 | elbo: 35849.0559765625 | train_rmse: 0.3224 | val_rmse: 0.4721 | val_ll: -0.7538
[3:21:08.799865] epoch: 6450 | elbo: 35923.722382812506 | train_rmse: 0.3228 | val_rmse: 0.4724 | val_ll: -0.7552
[3:22:43.824104] epoch: 6500 | elbo: 35652.139921874994 | train_rmse: 0.3222 | val_rmse: 0.4706 | val_ll: -0.7462
[3:24:17.876646] epoch: 6550 | elbo: 35626.50203125 | train_rmse: 0.3221 | val_rmse: 0.4711 | val_ll: -0.7609
[3:25:51.807259] epoch: 6600 | elbo: 35838.167578125 | train_rmse: 0.3214 | val_rmse: 0.4704 | val_ll: -0.7537
[3:27:26.132422] epoch: 6650 | elbo: 35559.667812499996 | train_rmse: 0.3223 | val_rmse: 0.4696 | val_ll: -0.7466
[3:29:00.091419] epoch: 6700 | elbo: 35392.2892578125 | train_rmse: 0.3215 | val_rmse: 0.4695 | val_ll: -0.7486
[3:30:34.010726] epoch: 6750 | elbo: 35421.5686328125 | train_rmse: 0.322 | val_rmse: 0.4697 | val_ll: -0.7531
[3:32:07.324828] epoch: 6800 | elbo: 35375.201796875 | train_rmse: 0.3212 | val_rmse: 0.4682 | val_ll: -0.7408
[3:33:41.563197] epoch: 6850 | elbo: 35583.8339453125 | train_rmse: 0.3221 | val_rmse: 0.4693 | val_ll: -0.752
[3:35:14.960759] epoch: 6900 | elbo: 35407.50453124999 | train_rmse: 0.3218 | val_rmse: 0.4675 | val_ll: -0.7446
[3:36:47.521089] epoch: 6950 | elbo: 35394.9974609375 | train_rmse: 0.3218 | val_rmse: 0.4667 | val_ll: -0.7418
[3:38:20.372224] epoch: 7000 | elbo: 35160.296875 | train_rmse: 0.3204 | val_rmse: 0.4672 | val_ll: -0.744
[3:39:52.085716] epoch: 7050 | elbo: 35010.5861328125 | train_rmse: 0.3208 | val_rmse: 0.4671 | val_ll: -0.7317
[3:41:24.235107] epoch: 7100 | elbo: 35088.237539062495 | train_rmse: 0.3212 | val_rmse: 0.4665 | val_ll: -0.7456
[3:42:57.057140] epoch: 7150 | elbo: 35230.217578125 | train_rmse: 0.3204 | val_rmse: 0.4674 | val_ll: -0.7476
[3:44:29.402330] epoch: 7200 | elbo: 35123.755078125 | train_rmse: 0.3206 | val_rmse: 0.4667 | val_ll: -0.7307
[3:46:01.209061] epoch: 7250 | elbo: 34900.8551171875 | train_rmse: 0.3203 | val_rmse: 0.4662 | val_ll: -0.7463
[3:47:33.532892] epoch: 7300 | elbo: 34740.863984375 | train_rmse: 0.321 | val_rmse: 0.4662 | val_ll: -0.7465
[3:49:08.856371] epoch: 7350 | elbo: 35022.189570312505 | train_rmse: 0.3212 | val_rmse: 0.4662 | val_ll: -0.7369
[3:50:42.718913] epoch: 7400 | elbo: 34982.8929296875 | train_rmse: 0.3213 | val_rmse: 0.466 | val_ll: -0.7492
[3:52:17.314193] epoch: 7450 | elbo: 34910.475625 | train_rmse: 0.3252 | val_rmse: 0.4702 | val_ll: -0.7615
[3:53:50.043972] epoch: 7500 | elbo: 34518.087265625 | train_rmse: 0.3205 | val_rmse: 0.4649 | val_ll: -0.7425
[3:55:23.133058] epoch: 7550 | elbo: 34531.52960937501 | train_rmse: 0.3203 | val_rmse: 0.4658 | val_ll: -0.7425
[3:56:57.797308] epoch: 7600 | elbo: 34497.6233203125 | train_rmse: 0.3206 | val_rmse: 0.4651 | val_ll: -0.7471
[3:58:29.705505] epoch: 7650 | elbo: 34595.3603515625 | train_rmse: 0.3198 | val_rmse: 0.4657 | val_ll: -0.7469
[4:00:02.304831] epoch: 7700 | elbo: 34441.977890625 | train_rmse: 0.3198 | val_rmse: 0.4653 | val_ll: -0.7401
[4:01:35.934333] epoch: 7750 | elbo: 34360.201875 | train_rmse: 0.3203 | val_rmse: 0.4644 | val_ll: -0.7472
[4:03:09.329494] epoch: 7800 | elbo: 34349.325429687495 | train_rmse: 0.3219 | val_rmse: 0.4674 | val_ll: -0.7444
[4:04:41.897909] epoch: 7850 | elbo: 34262.947695312505 | train_rmse: 0.3214 | val_rmse: 0.4649 | val_ll: -0.7401
[4:06:15.908162] epoch: 7900 | elbo: 34205.7066015625 | train_rmse: 0.3207 | val_rmse: 0.4658 | val_ll: -0.7408
[4:07:50.270763] epoch: 7950 | elbo: 34065.39714843749 | train_rmse: 0.3203 | val_rmse: 0.4651 | val_ll: -0.7459
[4:09:23.581412] epoch: 8000 | elbo: 34107.12007812499 | train_rmse: 0.3214 | val_rmse: 0.4644 | val_ll: -0.737
[4:10:57.688567] epoch: 8050 | elbo: 34236.58007812501 | train_rmse: 0.3205 | val_rmse: 0.4636 | val_ll: -0.7358
[4:12:33.262173] epoch: 8100 | elbo: 34086.388789062505 | train_rmse: 0.3216 | val_rmse: 0.4648 | val_ll: -0.749
[4:14:05.545212] epoch: 8150 | elbo: 34070.569492187504 | train_rmse: 0.3194 | val_rmse: 0.4629 | val_ll: -0.7451
[4:15:39.981351] epoch: 8200 | elbo: 33833.13984375 | train_rmse: 0.3191 | val_rmse: 0.4633 | val_ll: -0.742
[4:17:13.699854] epoch: 8250 | elbo: 33990.01855468749 | train_rmse: 0.3188 | val_rmse: 0.463 | val_ll: -0.7351
[4:18:46.356922] epoch: 8300 | elbo: 33904.90515625 | train_rmse: 0.3205 | val_rmse: 0.4633 | val_ll: -0.7427
[4:20:19.727324] epoch: 8350 | elbo: 33769.0490234375 | train_rmse: 0.3197 | val_rmse: 0.4628 | val_ll: -0.7468
[4:21:54.295905] epoch: 8400 | elbo: 34080.873789062505 | train_rmse: 0.3229 | val_rmse: 0.4641 | val_ll: -0.7542
[4:23:26.848320] epoch: 8450 | elbo: 33730.168828125 | train_rmse: 0.3188 | val_rmse: 0.4628 | val_ll: -0.7351
[4:24:59.728252] epoch: 8500 | elbo: 33749.711093750004 | train_rmse: 0.3212 | val_rmse: 0.4652 | val_ll: -0.7513
[4:26:32.756058] epoch: 8550 | elbo: 33636.6017578125 | train_rmse: 0.3177 | val_rmse: 0.4617 | val_ll: -0.7329
[4:28:06.285261] epoch: 8600 | elbo: 33720.69562500001 | train_rmse: 0.3177 | val_rmse: 0.4625 | val_ll: -0.7428
[4:29:39.640531] epoch: 8650 | elbo: 33546.60857421874 | train_rmse: 0.3184 | val_rmse: 0.4629 | val_ll: -0.7478
[4:31:12.363663] epoch: 8700 | elbo: 33912.9348046875 | train_rmse: 0.319 | val_rmse: 0.4621 | val_ll: -0.7475
[4:32:45.753417] epoch: 8750 | elbo: 33325.70935546875 | train_rmse: 0.3186 | val_rmse: 0.4632 | val_ll: -0.7436
[4:34:19.921609] epoch: 8800 | elbo: 33382.818964843755 | train_rmse: 0.3179 | val_rmse: 0.4623 | val_ll: -0.7373
[4:35:54.463426] epoch: 8850 | elbo: 33431.55908203125 | train_rmse: 0.3175 | val_rmse: 0.4616 | val_ll: -0.7399
[4:37:27.520365] epoch: 8900 | elbo: 33254.779062500005 | train_rmse: 0.3176 | val_rmse: 0.4613 | val_ll: -0.7322
[4:39:01.737067] epoch: 8950 | elbo: 33295.365488281255 | train_rmse: 0.319 | val_rmse: 0.4636 | val_ll: -0.7512
[4:40:33.737395] epoch: 9000 | elbo: 33427.382558593745 | train_rmse: 0.3172 | val_rmse: 0.4612 | val_ll: -0.7419
[4:42:06.174661] epoch: 9050 | elbo: 33258.03445312501 | train_rmse: 0.3173 | val_rmse: 0.4615 | val_ll: -0.7388
[4:43:38.588261] epoch: 9100 | elbo: 33132.128085937504 | train_rmse: 0.3199 | val_rmse: 0.4616 | val_ll: -0.7432
[4:45:11.722782] epoch: 9150 | elbo: 32998.41015624999 | train_rmse: 0.3173 | val_rmse: 0.4606 | val_ll: -0.7399
[4:46:44.633351] epoch: 9200 | elbo: 33045.12505859375 | train_rmse: 0.3172 | val_rmse: 0.4606 | val_ll: -0.7345
[4:48:17.700402] epoch: 9250 | elbo: 33070.798222656245 | train_rmse: 0.318 | val_rmse: 0.4609 | val_ll: -0.7519
[4:49:50.538610] epoch: 9300 | elbo: 32853.337617187506 | train_rmse: 0.3173 | val_rmse: 0.4604 | val_ll: -0.7401
[4:51:23.834191] epoch: 9350 | elbo: 32863.92076171875 | train_rmse: 0.3159 | val_rmse: 0.4599 | val_ll: -0.7342
[4:52:57.671237] epoch: 9400 | elbo: 32893.57515625 | train_rmse: 0.3162 | val_rmse: 0.4603 | val_ll: -0.7392
[4:54:30.085137] epoch: 9450 | elbo: 33041.58800781249 | train_rmse: 0.317 | val_rmse: 0.4595 | val_ll: -0.728
[4:56:02.375095] epoch: 9500 | elbo: 33081.08724609375 | train_rmse: 0.3172 | val_rmse: 0.4616 | val_ll: -0.7422
[4:57:37.053769] epoch: 9550 | elbo: 33283.91541015626 | train_rmse: 0.3171 | val_rmse: 0.4595 | val_ll: -0.7257
[4:59:09.441555] epoch: 9600 | elbo: 32746.235234375003 | train_rmse: 0.3156 | val_rmse: 0.459 | val_ll: -0.7308
[5:00:41.852819] epoch: 9650 | elbo: 32740.953496093753 | train_rmse: 0.3157 | val_rmse: 0.459 | val_ll: -0.7361
[5:02:14.652696] epoch: 9700 | elbo: 32800.480390625 | train_rmse: 0.3155 | val_rmse: 0.4596 | val_ll: -0.7375
[5:03:48.139148] epoch: 9750 | elbo: 32627.886855468754 | train_rmse: 0.3169 | val_rmse: 0.4592 | val_ll: -0.7342
[5:05:21.033717] epoch: 9800 | elbo: 32811.202304687504 | train_rmse: 0.3163 | val_rmse: 0.459 | val_ll: -0.7347
[5:06:53.237314] epoch: 9850 | elbo: 32568.984062499996 | train_rmse: 0.3158 | val_rmse: 0.4589 | val_ll: -0.7443
[5:08:25.659202] epoch: 9900 | elbo: 32409.421738281246 | train_rmse: 0.3155 | val_rmse: 0.4588 | val_ll: -0.739
[5:09:58.456865] epoch: 9950 | elbo: 32416.035175781242 | train_rmse: 0.3151 | val_rmse: 0.4585 | val_ll: -0.7443
Training finished in 5:11:29.919880 seconds
Saved SVI model to experiments/sigma-over-underfit/models/tensin-3x512-s03/checkpoint_2.pt
File Size is 4.0595598220825195 MB
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:1 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.3 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.1250, -0.0118, -0.1032,  ...,  0.0798,  0.1046, -0.0770],
        [ 0.0031,  0.0045,  0.0033,  ...,  0.0004,  0.0044, -0.0083],
        [-0.0140, -0.0005,  0.0036,  ..., -0.0013, -0.0079, -0.0042],
        ...,
        [-0.0128,  0.0003,  0.0124,  ..., -0.0073, -0.0021, -0.0018],
        [-0.0139, -0.0007,  0.0131,  ..., -0.0076, -0.0066, -0.0032],
        [-0.0043,  0.0014,  0.0079,  ..., -0.0066,  0.0078, -0.0019]],
       device='cuda:1', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0160, 0.0200, 0.0161,  ..., 0.0173, 0.0160, 0.0153],
        [0.1200, 0.1164, 0.1166,  ..., 0.1184, 0.1162, 0.1175],
        [0.1494, 0.1492, 0.1487,  ..., 0.1486, 0.1483, 0.1492],
        ...,
        [0.1469, 0.1458, 0.1459,  ..., 0.1460, 0.1476, 0.1468],
        [0.1359, 0.1330, 0.1305,  ..., 0.1326, 0.1325, 0.1340],
        [0.1212, 0.1208, 0.1227,  ..., 0.1212, 0.1190, 0.1228]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-9.3340e-01, -1.9966e+00, -2.5444e+00, -2.2907e+00, -2.2179e+00,
        -2.4762e+00,  3.9172e-01, -2.0587e+00, -4.1452e-01, -2.3520e+00,
        -2.3741e+00, -2.4338e+00, -2.4310e+00, -1.0591e+00, -2.3760e+00,
        -2.1882e+00, -2.3874e+00, -2.3594e+00, -1.9116e+00, -5.4557e-01,
        -2.5525e+00, -1.2676e-01, -2.4622e+00, -2.4158e+00, -2.4500e+00,
        -2.2851e+00, -2.5119e+00, -7.2965e-01, -1.1929e-01, -2.6443e+00,
        -2.3717e+00, -2.4188e+00, -2.6187e+00, -2.4447e+00, -3.0602e-01,
        -2.4376e+00, -2.5143e-01, -2.4868e+00, -6.0779e-03, -1.7819e-01,
        -2.2017e+00, -2.2026e+00, -1.6057e+00, -2.4917e+00, -2.5708e+00,
        -3.7094e-01, -2.8655e-01, -8.1269e-01, -7.8546e-01, -2.4109e+00,
        -2.3788e+00,  3.1877e-01, -2.0093e+00, -6.4099e-01, -2.4665e+00,
        -5.4660e-01, -2.4062e+00, -2.3137e+00, -9.0447e-02, -6.6108e-01,
        -6.5634e-01, -4.3474e-01, -8.3237e-01, -2.4677e+00, -9.7520e-01,
         6.7047e-03, -2.0510e+00, -2.5842e+00, -2.5756e+00,  8.8533e-02,
        -2.4732e+00, -2.2896e+00, -2.0829e+00,  2.9559e+00, -2.3473e+00,
        -2.4462e+00, -2.3826e+00, -1.9063e+00, -2.5978e+00, -2.2364e+00,
        -7.7221e-01, -2.4393e+00, -5.6399e-01, -2.3365e+00, -6.7406e-01,
        -2.3794e+00, -2.5900e+00, -2.2336e+00, -1.8326e-01, -1.8497e-01,
        -1.0097e+00, -2.4541e+00, -1.8487e+00, -2.0942e+00, -2.3217e+00,
        -2.2160e+00, -2.2192e+00,  1.1353e-01, -2.5530e+00, -2.5001e+00,
        -2.4218e+00, -4.4670e-01, -2.5637e+00, -2.3768e+00, -1.0918e+00,
        -9.1503e-01, -2.6703e+00, -2.3137e+00, -1.5723e+00, -1.6916e-01,
        -2.4504e+00, -1.0279e+00, -1.7241e+00, -2.4006e+00, -7.3812e-01,
         1.0171e-02, -1.1147e+00, -6.8961e-01, -6.8721e-01, -1.6169e-01,
        -2.6359e+00, -2.3619e+00, -2.4881e+00, -2.4575e-01, -2.3716e+00,
        -2.1287e+00, -2.3588e+00, -2.1949e+00, -5.4638e-01,  1.0617e+00,
        -2.2073e+00, -2.2282e+00, -7.0141e-02, -7.5133e-01, -1.4653e+00,
        -1.8748e+00, -2.4573e+00, -2.2501e+00, -2.4140e+00, -1.7342e+00,
        -1.9643e+00,  5.2162e-01, -1.1485e+00, -4.8286e-01, -2.5451e+00,
        -7.5881e-01, -1.9375e+00, -2.3573e+00, -9.0079e-01, -2.6497e+00,
        -7.1657e-01, -4.6138e-02, -1.2029e+00, -2.3825e+00, -1.2600e+00,
        -2.3761e+00, -1.2732e+00, -2.6020e+00, -7.9465e-01, -3.2606e-01,
        -8.2672e-01, -2.4711e+00,  5.8622e-02, -2.1945e+00, -2.1006e+00,
        -1.9075e+00, -1.1338e+00,  1.8646e-01, -2.5437e+00, -8.4845e-01,
         1.3349e-02, -2.5087e+00, -2.1303e+00, -2.6195e+00, -2.6030e+00,
         2.5023e+00, -1.0772e+00, -4.1336e-01, -2.3543e+00, -6.0147e-01,
        -2.2783e+00, -2.3136e+00, -2.3727e+00, -9.6563e-01, -2.4892e+00,
        -5.9163e-01, -2.3240e+00, -5.8708e-01, -9.2704e-01, -1.8451e+00,
        -8.9648e-01,  4.5692e+00, -2.6243e+00, -2.3297e+00, -5.1123e-01,
        -1.9635e+00, -8.8575e-01,  1.4742e-01, -1.0187e+00, -2.1434e+00,
        -2.6645e+00, -2.4491e+00, -2.3554e+00, -1.1097e+00, -1.9866e+00,
        -1.2116e+00, -2.3838e+00, -2.4568e+00,  2.2174e-03, -1.9120e-01,
         1.9562e-01, -2.5535e+00, -1.1271e+00, -2.3543e+00, -2.5415e+00,
        -2.5880e+00, -3.6741e-01,  3.4220e-01, -2.4950e+00, -2.4480e+00,
        -3.0364e-01, -2.2894e+00, -3.9171e-01, -1.9496e+00,  1.8992e+00,
        -5.7438e-01,  2.2603e-01, -5.4637e-01, -2.3591e+00, -2.1660e-02,
        -2.4258e+00, -1.1847e+00, -1.5721e+00, -6.2009e-01, -2.4527e+00,
        -5.1758e-01, -1.4292e-01, -2.3803e+00,  1.5996e+00, -1.8514e+00,
        -2.2647e+00, -1.7845e-01, -8.6886e-01, -4.2290e-01, -2.5679e+00,
        -2.5031e+00, -2.4442e+00,  5.3523e-01, -2.5617e+00,  5.1508e-03,
        -2.3656e+00, -4.1977e-01, -8.4339e-01, -2.3345e+00, -1.4536e+00,
        -2.3130e+00, -2.1205e+00, -8.3369e-01, -2.3677e+00, -2.4955e+00,
        -2.6434e+00,  2.8388e-01, -4.3850e-01, -2.4024e+00, -1.9407e+00,
        -2.4329e+00, -3.8758e-01, -2.1152e+00, -2.4118e-02, -2.7151e-01,
        -4.7998e-01, -2.3510e+00, -2.4927e+00,  5.0217e-02, -2.6759e+00,
        -5.2423e-01, -2.5633e+00, -2.6995e+00,  1.6138e-02, -2.3704e+00,
        -9.5315e-01, -9.1250e-02, -2.5076e+00, -2.3620e+00, -2.3389e+00,
        -2.4219e+00,  4.3328e-02,  1.6493e-01, -2.4742e+00, -1.0645e+00,
        -2.1950e+00, -2.4374e+00, -1.1209e-01, -2.5594e+00, -1.9896e+00,
        -1.7337e+00, -1.4143e+00, -1.5120e+00, -2.2386e+00, -2.6217e+00,
        -5.6172e-01, -2.3826e+00, -2.4544e+00, -2.6380e+00, -2.5019e+00,
        -7.1676e-01,  4.4317e-02,  1.8576e-01, -2.4065e+00, -2.1825e+00,
        -1.0336e+00, -2.0097e+00, -6.9358e-01, -2.4001e+00, -2.4526e+00,
        -1.2385e+00,  6.2471e-01, -1.6187e+00, -9.5262e-01, -2.1194e+00,
        -5.3533e-01, -1.4742e-01, -2.2962e+00, -2.2972e+00, -2.2329e+00,
        -2.5310e+00,  2.7391e-01, -7.9798e-01, -4.6794e-01, -2.4576e+00,
        -2.5091e+00, -1.9810e+00, -5.8194e-01,  1.2112e-01, -2.5684e+00,
        -3.4907e-01, -2.5243e+00,  3.0830e+00, -1.8532e+00, -5.9982e-01,
        -2.5249e+00, -4.7109e-01, -1.0589e+00, -1.8513e+00, -2.2535e+00,
        -2.5686e+00, -2.3505e+00,  4.1177e-02,  1.2529e-01, -1.1387e+00,
        -2.3984e+00, -2.4827e+00, -2.3105e+00, -2.0320e+00, -2.5058e+00,
        -2.2560e+00,  1.2768e+00, -9.9852e-01, -2.2817e+00, -2.6015e+00,
        -2.5228e+00, -1.2110e+00, -2.1748e+00, -4.8466e-01, -9.9372e-01,
         3.8088e-01, -2.4182e+00, -2.4654e+00, -2.1377e+00, -2.1263e+00,
        -2.5510e+00, -2.3770e+00, -2.6165e+00, -2.1460e+00, -2.5245e+00,
        -2.3538e+00, -2.5875e+00, -4.8142e-01, -2.4417e+00, -2.2472e+00,
        -1.9065e+00, -2.3613e+00, -2.4751e+00, -4.8895e-01, -2.0336e+00,
         2.0731e-01, -1.9848e+00, -1.3756e+00, -6.9548e-01, -2.3861e+00,
        -2.5614e+00, -2.0294e+00, -7.3412e-01, -1.4134e+00, -1.0826e+00,
        -6.1844e-01, -1.7961e+00, -9.2241e-03, -2.3576e+00, -2.4305e+00,
        -2.5301e+00, -2.6123e+00, -2.4853e+00,  2.4285e-01, -2.2796e+00,
        -2.4605e+00, -2.4059e+00, -2.4151e+00, -6.7882e-01, -1.8837e+00,
        -2.0384e-01, -4.1868e-01, -1.8411e-01, -2.1473e-01, -2.1721e+00,
        -6.7214e-01, -8.8913e-01, -3.8119e-01, -6.2985e-01, -2.3922e+00,
        -2.3014e+00, -9.0631e-02, -2.5538e+00, -1.9764e+00, -2.3555e+00,
        -2.5354e+00, -2.3386e+00, -1.2116e+00, -2.4200e+00,  5.7726e+00,
        -2.5112e+00, -2.3390e+00, -2.3373e+00, -2.4434e+00, -2.5223e+00,
        -2.4100e+00, -9.1288e-01, -5.5178e-01, -9.0421e-01, -9.3128e-01,
        -2.7037e+00, -1.3374e+00,  8.6135e-02, -2.6021e+00, -2.2302e+00,
        -2.3813e+00, -2.1485e+00, -2.5461e+00, -6.9566e-01, -2.5909e+00,
        -9.7218e-01, -2.0236e-01, -2.3798e+00, -8.5138e-01, -1.3755e+00,
        -2.6019e+00, -3.5183e-01, -2.4112e+00, -2.4611e+00, -7.7262e-01,
        -2.1604e+00, -6.0532e-01, -1.0771e-01, -2.6996e+00, -2.3608e+00,
        -2.3931e+00, -2.4408e+00, -2.6332e+00, -7.9412e-01, -1.1012e+00,
        -1.8642e+00, -2.6942e+00, -2.5203e+00, -2.4992e+00, -8.2608e-02,
        -2.4396e+00, -1.3025e-02, -2.4238e+00, -2.4617e+00, -2.4218e+00,
        -2.3968e+00, -2.4637e+00,  2.0947e-01, -8.9725e-01, -2.4860e+00,
        -1.3583e-01,  2.9140e+00, -2.4806e+00, -2.4270e+00, -2.1565e+00,
        -2.5918e+00, -6.8081e-01, -6.7050e-01, -1.6146e-01, -2.0717e+00,
        -2.3858e+00, -2.4744e+00,  1.5655e-01, -2.3669e-01, -6.2651e-01,
        -2.4024e+00, -7.8208e-01, -2.3785e+00, -2.3671e+00, -2.3351e+00,
        -2.8544e-01, -4.5014e-01, -2.4515e+00, -2.6026e+00, -2.3731e+00,
        -2.2915e+00, -2.0616e+00], device='cuda:1', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0217, 0.1637, 0.2037, 0.1843, 0.1798, 0.1981, 0.0041, 0.1839, 0.0022,
        0.1890, 0.1933, 0.1972, 0.2002, 0.0011, 0.1906, 0.1921, 0.1932, 0.1900,
        0.1590, 0.0036, 0.2066, 0.0023, 0.1969, 0.1957, 0.1973, 0.1859, 0.2240,
        0.0013, 0.0078, 0.2209, 0.1924, 0.1987, 0.2089, 0.2053, 0.0022, 0.1945,
        0.0034, 0.1998, 0.0010, 0.0023, 0.1772, 0.1899, 0.1377, 0.1989, 0.2051,
        0.0021, 0.0013, 0.0014, 0.0017, 0.1987, 0.1936, 0.0010, 0.1741, 0.0037,
        0.2024, 0.0107, 0.1943, 0.1860, 0.0294, 0.0016, 0.0111, 0.0018, 0.0100,
        0.1995, 0.0155, 0.0013, 0.1845, 0.2129, 0.2145, 0.0047, 0.1995, 0.1862,
        0.1710, 0.0406, 0.1930, 0.2018, 0.1995, 0.1597, 0.2078, 0.1824, 0.0015,
        0.1969, 0.0019, 0.1879, 0.0021, 0.1929, 0.2118, 0.1847, 0.0068, 0.0097,
        0.0012, 0.1978, 0.1565, 0.1752, 0.1884, 0.1826, 0.1797, 0.0014, 0.2090,
        0.2008, 0.1971, 0.0014, 0.2132, 0.2111, 0.0173, 0.0019, 0.2163, 0.1857,
        0.1443, 0.0074, 0.1962, 0.0298, 0.1438, 0.1918, 0.0148, 0.0015, 0.0017,
        0.0051, 0.0025, 0.0016, 0.2134, 0.1964, 0.2005, 0.0365, 0.1898, 0.1753,
        0.1917, 0.1820, 0.0015, 0.0016, 0.1788, 0.1882, 0.0016, 0.0033, 0.0033,
        0.1546, 0.1990, 0.1820, 0.1941, 0.1570, 0.1756, 0.0018, 0.0014, 0.0014,
        0.2053, 0.0079, 0.1656, 0.1907, 0.0027, 0.2145, 0.0131, 0.0013, 0.1379,
        0.1946, 0.0016, 0.1969, 0.0236, 0.2204, 0.0011, 0.0147, 0.0037, 0.2037,
        0.0063, 0.1778, 0.1810, 0.1617, 0.0017, 0.0034, 0.2091, 0.0016, 0.0046,
        0.2079, 0.1730, 0.2100, 0.2096, 0.0035, 0.0019, 0.0021, 0.1905, 0.0013,
        0.1836, 0.1872, 0.1936, 0.0010, 0.2006, 0.0015, 0.1903, 0.0133, 0.0013,
        0.1577, 0.0088, 0.0466, 0.2114, 0.1913, 0.0028, 0.1668, 0.0015, 0.0014,
        0.0014, 0.1756, 0.2150, 0.1990, 0.1898, 0.0018, 0.1616, 0.0023, 0.1943,
        0.1986, 0.0027, 0.0066, 0.0907, 0.2077, 0.0521, 0.1907, 0.2041, 0.2061,
        0.0089, 0.0098, 0.2012, 0.1987, 0.0016, 0.1842, 0.0026, 0.1684, 0.0180,
        0.0019, 0.0023, 0.0034, 0.1892, 0.0066, 0.1942, 0.0061, 0.0540, 0.0026,
        0.1984, 0.0017, 0.0039, 0.1996, 0.0338, 0.1521, 0.1870, 0.0016, 0.0069,
        0.0057, 0.2079, 0.2029, 0.1989, 0.0010, 0.2123, 0.0012, 0.1928, 0.0067,
        0.0064, 0.1973, 0.0031, 0.1872, 0.1767, 0.0013, 0.1899, 0.2036, 0.2142,
        0.0012, 0.0013, 0.1924, 0.1602, 0.1960, 0.0023, 0.1714, 0.0074, 0.0016,
        0.0014, 0.1882, 0.1996, 0.0008, 0.2143, 0.0014, 0.2035, 0.2181, 0.0017,
        0.1909, 0.0024, 0.0019, 0.2105, 0.1887, 0.1893, 0.1944, 0.0041, 0.0035,
        0.1984, 0.0015, 0.1797, 0.1955, 0.0075, 0.2072, 0.1626, 0.1535, 0.1532,
        0.1389, 0.1830, 0.2125, 0.0013, 0.1921, 0.1967, 0.2143, 0.2005, 0.0037,
        0.0010, 0.0040, 0.1967, 0.1782, 0.0281, 0.1791, 0.0020, 0.1951, 0.1987,
        0.0010, 0.0100, 0.1651, 0.0012, 0.1774, 0.0015, 0.0013, 0.1959, 0.1875,
        0.1874, 0.2092, 0.0756, 0.0176, 0.0024, 0.1976, 0.2047, 0.1749, 0.0028,
        0.0013, 0.2065, 0.0016, 0.2114, 0.0249, 0.1531, 0.0078, 0.2079, 0.0018,
        0.0012, 0.1635, 0.1815, 0.2134, 0.1904, 0.0018, 0.0026, 0.0348, 0.1962,
        0.2027, 0.1937, 0.1760, 0.2008, 0.1822, 0.0021, 0.0056, 0.1954, 0.2139,
        0.2028, 0.0017, 0.1818, 0.0171, 0.0009, 0.0007, 0.1952, 0.1995, 0.1744,
        0.1727, 0.2131, 0.1917, 0.2080, 0.1785, 0.2049, 0.1901, 0.2136, 0.0052,
        0.1950, 0.1889, 0.1557, 0.1897, 0.1989, 0.0021, 0.1686, 0.0019, 0.1673,
        0.1320, 0.0059, 0.1930, 0.2054, 0.1997, 0.0014, 0.1361, 0.0019, 0.0023,
        0.1495, 0.0014, 0.1920, 0.1943, 0.2026, 0.2167, 0.1998, 0.0012, 0.1881,
        0.1965, 0.1923, 0.1941, 0.0015, 0.1590, 0.0021, 0.0026, 0.0067, 0.0011,
        0.1771, 0.0815, 0.0114, 0.0023, 0.0016, 0.1990, 0.1872, 0.0079, 0.2073,
        0.1797, 0.1917, 0.2020, 0.1895, 0.0015, 0.1947, 0.0753, 0.2034, 0.1883,
        0.1943, 0.1968, 0.2013, 0.1935, 0.0181, 0.0016, 0.0060, 0.0025, 0.2171,
        0.1351, 0.0039, 0.2081, 0.1806, 0.1913, 0.1740, 0.2070, 0.0029, 0.2104,
        0.0016, 0.0052, 0.1920, 0.0024, 0.0036, 0.2103, 0.0019, 0.1923, 0.1967,
        0.0035, 0.1841, 0.0335, 0.0019, 0.2179, 0.1887, 0.1921, 0.1989, 0.2109,
        0.0015, 0.0010, 0.1528, 0.2197, 0.2046, 0.2036, 0.0013, 0.1980, 0.0011,
        0.1925, 0.1980, 0.1957, 0.1987, 0.1979, 0.0015, 0.1059, 0.2000, 0.0185,
        0.0033, 0.2017, 0.1957, 0.1748, 0.2078, 0.0027, 0.0010, 0.0268, 0.1745,
        0.1916, 0.1985, 0.0013, 0.0083, 0.0015, 0.1937, 0.0013, 0.1916, 0.1932,
        0.1876, 0.0056, 0.0022, 0.1985, 0.2092, 0.1991, 0.1835, 0.1685],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-2.9313e-03,  3.4407e-06, -3.9350e-07,  ...,  4.1300e-08,
          1.0717e-08,  2.2086e-08],
        [-1.4251e-30,  4.4875e-07,  4.7711e-08,  ...,  2.2217e-08,
         -6.8273e-07, -1.3247e-06],
        [-5.4059e-04,  3.0731e-07, -2.8722e-06,  ..., -4.9045e-44,
          5.9112e-08, -2.8984e-10],
        ...,
        [-5.0615e-08,  9.0983e-07,  2.1859e-08,  ...,  1.7680e-06,
          1.7757e-06, -3.9198e-06],
        [-1.4013e-44, -8.1925e-10,  9.1424e-08,  ...,  7.4234e-07,
          3.6444e-06,  4.2055e-06],
        [-2.2421e-44,  1.6132e-06, -5.3979e-09,  ...,  6.7592e-09,
          1.0184e-06,  4.7109e-08]], device='cuda:1', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.9987, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.5569, -0.5832, -0.6208, -0.6480, -0.6288, -0.5266, -0.6315, -0.5839,
        -0.5694, -0.5764, -0.6281, -0.6380, -0.5754, -0.6007, -0.5943, -0.5831,
        -0.5234, -0.5681, -0.5352, -0.6114, -0.5528, -0.5904, -0.6063, -0.5965,
        -0.5007, -0.6123, -0.4750, -0.5831, -0.6049, -0.6121, -0.6591, -0.5205,
        -0.6243, -0.6530, -0.5427, -0.6587, -0.6244, -0.6204, -0.5378, -0.5946,
        -0.5946, -0.5927, -0.5747, -0.6080, -0.6208, -0.5354, -0.5498, -0.5831,
        -0.6216, -0.6323, -0.6632, -0.5286, -0.6429, -0.6266, -0.6356, -0.5503,
        -0.6878, -0.6526, -0.5811, -0.6722, -0.6541, -0.6118, -0.5309, -0.5908,
        -0.6030, -0.5625, -0.5940, -0.5791, -0.5593, -0.6307, -0.6179, -0.5959,
        -0.5898, -0.5809, -0.5335, -0.6718, -0.4948, -0.5784, -0.6405, -0.6074,
        -0.5649, -0.6191, -0.6992, -0.5703, -0.5256, -0.5932, -0.5755, -0.5958,
        -0.6198, -0.5959, -0.5166, -0.5644, -0.5345, -0.6411, -0.6820, -0.6266,
        -0.6294, -0.4843, -0.5642, -0.6266, -0.6200, -0.6116, -0.5300, -0.5858,
        -0.6205, -0.6057, -0.5843, -0.4878, -0.5951, -0.6576, -0.6061, -0.5688,
        -0.5952, -0.4980, -0.6441, -0.6480, -0.5559, -0.6728, -0.6495, -0.5998,
        -0.6205, -0.6121, -0.5713, -0.4357, -0.6111, -0.5239, -0.5290, -0.5558,
        -0.5614, -0.5963, -0.5187, -0.6149, -0.6453, -0.5682, -0.6115, -0.5604,
        -0.5956, -0.5265, -0.6052, -0.5806, -0.5864, -0.5532, -0.5438, -0.6085,
        -0.6224, -0.6379, -0.5406, -0.4834, -0.5371, -0.4823, -0.6588, -0.5796,
        -0.6134, -0.6375, -0.5696, -0.6036, -0.6357, -0.6883, -0.6040, -0.6074,
        -0.5758, -0.4237, -0.5749, -0.5523, -0.5986, -0.6181, -0.5548, -0.6203,
         1.3435, -0.6433, -0.6176, -0.5582, -0.5659, -0.5833, -0.5927, -0.5592,
        -0.5115, -0.6254, -0.6783, -0.5077, -0.4820, -0.6371, -0.6119, -0.6656,
        -0.5326, -0.5901, -0.6075, -0.5648, -0.6539, -0.6046, -0.6026, -0.5853,
        -0.5835, -0.6015, -0.6082, -0.5953, -0.5694, -0.6084, -0.5715, -0.6415,
        -0.6733, -0.6221, -0.5468, -0.5609, -0.5501, -0.6351, -0.6138, -0.5702,
        -0.5846, -0.5587, -0.6081, -0.5535, -0.6129, -0.6066, -0.5508, -0.5760,
        -0.5711, -0.5631, -0.6234, -0.5967, -0.5967, -0.5682, -0.5619, -0.5401,
        -0.5870, -0.6132, -0.6742, -0.5289, -0.6044, -0.5444, -0.6008, -0.4508,
        -0.5745, -0.5912, -0.6138, -0.5695, -0.5229, -0.5874, -0.5681, -0.5912,
        -0.5411, -0.6385, -0.6345, -0.5698, -0.6507, -0.6446, -0.5842, -0.5471,
        -0.6263, -0.5981, -0.6087, -0.5664, -0.5146, -0.6320, -0.7255, -0.5627,
        -0.6216, -0.5767, -0.5883, -0.6075, -0.5814, -0.6093, -0.6447, -0.6606,
        -0.5375, -0.5917, -0.6104,  0.6737, -0.4612, -0.5481, -0.6299, -0.4661,
        -0.5899, -0.6643, -0.0056, -0.6208, -0.6057, -0.5876, -0.6596, -0.5469,
        -0.6415, -0.5828, -0.5672, -0.6238, -0.6082, -0.6294, -0.5945, -0.5909,
        -0.5845, -0.5797, -0.5485, -0.5815, -0.5792, -0.6409, -0.5652, -0.5605,
        -0.6176, -0.5019, -0.5477, -0.6343, -0.6013, -0.5328, -0.6284, -0.6205,
        -0.5589, -0.6191, -0.5936, -0.6237, -0.6474, -0.5491, -0.5859, -0.6435,
        -0.6421, -0.5910, -0.6396, -0.5607, -0.6233, -0.5650, -0.5542, -0.6077,
        -0.5669, -0.5709, -0.6469, -0.6388, -0.5213, -0.5733, -0.6580, -0.6427,
        -0.6389, -0.6249, -0.5818, -0.6254, -0.6246, -0.5816, -0.6661, -0.6145,
        -0.5952, -0.6101, -0.6181, -0.6255, -0.5665, -0.6194, -0.6000, -0.6446,
        -0.5994, -0.5968, -0.6305, -0.5051, -0.6353, -0.5534, -0.6129, -0.6239,
        -0.6087, -0.6747, -0.5888, -0.6381, -0.6046, -0.5652, -0.5530, -0.6028,
        -0.5843, -0.5947, -0.5293, -0.5348, -0.6230, -0.5852, -0.6190, -0.5681,
        -0.5875, -0.6216, -0.6007, -0.4988, -0.5700, -0.5537, -0.6387, -0.6153,
        -0.5820, -0.5704, -0.1726, -0.5272, -0.5989, -0.6178, -0.6375, -0.6262,
        -0.5935, -0.5717, -0.6430, -0.5436, -0.5972, -0.5844, -0.5832, -0.6655,
        -0.6122, -0.6001, -0.6447, -0.5562, -0.6470, -0.5184, -0.5879, -0.5313,
        -0.5745, -0.5630, -0.6073, -0.5639, -0.5626, -0.6012, -0.6408, -0.5917,
        -0.5963, -0.4749, -0.6022, -0.5885, -0.6308, -0.5742, -0.6543, -0.6681,
        -0.6248, -0.5772, -0.5730, -0.5960, -0.6035, -0.5312, -0.5890, -0.5998,
        -0.7029, -0.5727, -0.5609, -0.6036, -0.5496, -0.4903, -0.6193, -0.6330,
        -0.6017, -0.6655, -0.6195, -0.5821, -0.5986, -0.6153, -0.5608, -0.6008,
        -0.5907, -0.6368, -0.5596, -0.5937, -0.5759, -0.6417, -0.6118, -0.4578,
        -0.5637, -0.5780, -0.6085, -0.6497, -0.6108, -0.5528, -0.6092, -0.6024,
        -0.5725, -0.6703, -0.6284, -0.5936, -0.6458, -0.5932, -0.6011, -0.5732,
        -0.5238, -0.5906, -0.5708, -0.6033, -0.6062, -0.5863, -0.5895, -0.6630,
        -0.6333, -0.6457, -0.5622, -0.5948, -0.5801, -0.5850, -0.5970, -0.5847,
        -0.5976, -0.5768, -0.5770, -0.6348, -0.5815, -0.5998, -0.5697, -0.5663,
        -0.4503, -0.6434, -0.5434, -0.5529, -0.6540, -0.5722, -0.5871, -0.6145,
        -0.5279, -0.4633, -0.6011, -0.5985, -0.5567, -0.5566, -0.5964, -0.6288,
        -0.6113, -0.5872, -0.5702, -0.5538, -0.6580, -0.5769, -0.5925, -0.5732],
       device='cuda:1', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([8.7711e-01, 8.7400e-01, 8.7331e-01, 8.6782e-01, 8.7097e-01, 8.8025e-01,
        8.6100e-01, 8.7563e-01, 8.7906e-01, 8.7595e-01, 8.7189e-01, 8.7346e-01,
        8.7195e-01, 8.6489e-01, 8.6789e-01, 8.7501e-01, 8.8110e-01, 8.7888e-01,
        8.7580e-01, 8.6580e-01, 8.7971e-01, 8.7469e-01, 8.7055e-01, 8.7221e-01,
        8.8340e-01, 8.7037e-01, 8.9093e-01, 8.7244e-01, 8.6088e-01, 8.7318e-01,
        8.6273e-01, 8.7931e-01, 8.6805e-01, 8.7227e-01, 8.7361e-01, 8.6509e-01,
        8.6762e-01, 8.6668e-01, 8.7383e-01, 8.6085e-01, 8.7413e-01, 8.7044e-01,
        8.7535e-01, 8.6491e-01, 8.8045e-01, 8.8008e-01, 8.7293e-01, 8.7001e-01,
        8.7828e-01, 8.7122e-01, 8.6445e-01, 8.7871e-01, 8.6722e-01, 8.6770e-01,
        8.6449e-01, 8.8128e-01, 8.6481e-01, 8.6067e-01, 8.6935e-01, 8.6588e-01,
        8.6877e-01, 8.7184e-01, 8.8478e-01, 8.6313e-01, 8.6175e-01, 8.7465e-01,
        8.7280e-01, 8.7518e-01, 8.7243e-01, 8.7413e-01, 8.6911e-01, 8.7055e-01,
        8.6930e-01, 8.7254e-01, 8.8338e-01, 8.6745e-01, 8.8105e-01, 8.7227e-01,
        8.6239e-01, 8.7377e-01, 8.7699e-01, 8.6499e-01, 8.6244e-01, 8.7021e-01,
        8.8128e-01, 8.7363e-01, 8.7212e-01, 8.7681e-01, 8.6655e-01, 8.7736e-01,
        8.7522e-01, 8.7779e-01, 8.8128e-01, 8.6483e-01, 8.5743e-01, 8.6768e-01,
        8.6508e-01, 8.8639e-01, 8.6886e-01, 8.6230e-01, 8.7430e-01, 8.7338e-01,
        8.8194e-01, 8.7473e-01, 8.7259e-01, 8.6421e-01, 8.7869e-01, 8.8095e-01,
        8.7401e-01, 8.6958e-01, 8.7791e-01, 8.6979e-01, 8.6885e-01, 8.8403e-01,
        8.6280e-01, 8.6777e-01, 8.7889e-01, 8.6171e-01, 8.5962e-01, 8.7658e-01,
        8.6515e-01, 8.6864e-01, 8.7059e-01, 8.8419e-01, 8.6376e-01, 8.7412e-01,
        8.8038e-01, 8.7100e-01, 8.7392e-01, 8.7006e-01, 8.8131e-01, 8.6925e-01,
        8.6796e-01, 8.7595e-01, 8.6866e-01, 8.7955e-01, 8.7964e-01, 8.7911e-01,
        8.7651e-01, 8.7533e-01, 8.7595e-01, 8.7643e-01, 8.7703e-01, 8.6611e-01,
        8.7397e-01, 8.6581e-01, 8.8175e-01, 8.8944e-01, 8.8594e-01, 8.9172e-01,
        8.6865e-01, 8.7346e-01, 8.6756e-01, 8.7216e-01, 8.7108e-01, 8.7899e-01,
        8.5795e-01, 8.6752e-01, 8.7893e-01, 8.6939e-01, 8.7123e-01, 8.6544e-01,
        8.7992e-01, 8.7160e-01, 8.7146e-01, 8.7909e-01, 8.8209e-01, 8.7540e-01,
        8.8530e-01, 8.5974e-01, 8.7536e-01, 8.7946e-01, 8.7707e-01, 8.7781e-01,
        8.7403e-01, 8.6979e-01, 8.7858e-01, 8.7112e-01, 8.6464e-01, 8.7651e-01,
        8.6714e-01, 8.7183e-01, 8.6385e-01, 8.5495e-01, 8.7903e-01, 8.7383e-01,
        8.7176e-01, 8.7454e-01, 8.7004e-01, 8.7005e-01, 8.7329e-01, 8.7260e-01,
        8.6604e-01, 8.8202e-01, 8.6905e-01, 8.6917e-01, 8.7641e-01, 8.6926e-01,
        8.7506e-01, 8.7243e-01, 8.6484e-01, 8.7304e-01, 8.7833e-01, 8.7454e-01,
        8.6734e-01, 8.7250e-01, 8.7338e-01, 8.7818e-01, 8.7498e-01, 8.7178e-01,
        8.7074e-01, 8.8005e-01, 8.6496e-01, 8.7088e-01, 8.8058e-01, 8.7726e-01,
        8.7677e-01, 8.8100e-01, 8.6821e-01, 8.6456e-01, 8.7112e-01, 8.7635e-01,
        8.7485e-01, 8.6842e-01, 8.7646e-01, 8.7000e-01, 8.6250e-01, 8.7024e-01,
        8.6625e-01, 8.8220e-01, 8.6217e-01, 8.6589e-01, 8.7233e-01, 8.7700e-01,
        8.6637e-01, 8.6957e-01, 8.7845e-01, 8.7464e-01, 8.7806e-01, 8.7046e-01,
        8.7229e-01, 8.5846e-01, 8.6148e-01, 8.7264e-01, 8.7065e-01, 8.6994e-01,
        8.7151e-01, 8.8289e-01, 8.7142e-01, 8.7191e-01, 8.7049e-01, 8.7389e-01,
        8.7846e-01, 8.7331e-01, 8.5721e-01, 8.7417e-01, 8.7699e-01, 8.6440e-01,
        8.6879e-01, 8.7236e-01, 8.6765e-01, 8.7567e-01, 8.6590e-01, 8.6914e-01,
        8.7718e-01, 8.7896e-01, 8.6689e-01, 1.4854e-03, 8.9063e-01, 8.7560e-01,
        8.6611e-01, 8.8233e-01, 8.7386e-01, 8.6379e-01, 1.2765e-02, 8.7541e-01,
        8.7097e-01, 8.6342e-01, 8.5932e-01, 8.7153e-01, 8.6704e-01, 8.7169e-01,
        8.7045e-01, 8.7691e-01, 8.6583e-01, 8.7224e-01, 8.7133e-01, 8.6995e-01,
        8.7924e-01, 8.7690e-01, 8.7161e-01, 8.7867e-01, 8.7395e-01, 8.6193e-01,
        8.6983e-01, 8.7494e-01, 8.5930e-01, 8.8586e-01, 8.7803e-01, 8.6653e-01,
        8.6480e-01, 1.6066e-02, 8.6474e-01, 8.7211e-01, 8.8086e-01, 8.7392e-01,
        8.6584e-01, 8.5912e-01, 8.6508e-01, 8.8420e-01, 8.6879e-01, 8.6559e-01,
        8.7163e-01, 8.7365e-01, 8.5988e-01, 8.6848e-01, 8.7898e-01, 8.7308e-01,
        8.7716e-01, 8.6783e-01, 8.6820e-01, 8.7347e-01, 8.6689e-01, 8.7381e-01,
        8.7857e-01, 8.6822e-01, 8.6388e-01, 8.6893e-01, 8.7281e-01, 8.7080e-01,
        8.8321e-01, 8.6730e-01, 8.7001e-01, 8.7220e-01, 8.6150e-01, 8.6392e-01,
        8.6921e-01, 8.6528e-01, 8.7285e-01, 8.7077e-01, 8.7113e-01, 8.6993e-01,
        8.6661e-01, 8.6651e-01, 8.6562e-01, 8.7471e-01, 8.7608e-01, 8.8067e-01,
        8.7292e-01, 8.7623e-01, 8.6254e-01, 8.6781e-01, 8.7858e-01, 8.6744e-01,
        8.7124e-01, 8.6762e-01, 8.7533e-01, 8.7838e-01, 8.7430e-01, 8.8177e-01,
        8.7371e-01, 8.6995e-01, 8.7796e-01, 8.6690e-01, 8.5800e-01, 8.7253e-01,
        8.6630e-01, 8.7035e-01, 8.6859e-01, 8.7396e-01, 8.7234e-01, 8.7590e-01,
        8.7743e-01, 8.7330e-01, 8.6783e-01, 8.6803e-01, 8.7214e-01, 8.8045e-01,
        8.2297e-04, 8.7843e-01, 8.6841e-01, 8.7536e-01, 8.7161e-01, 8.5737e-01,
        8.6989e-01, 8.8090e-01, 8.6573e-01, 8.7814e-01, 8.7697e-01, 8.7141e-01,
        8.7666e-01, 8.5071e-01, 8.6567e-01, 8.6686e-01, 8.6088e-01, 8.8362e-01,
        8.6515e-01, 8.8154e-01, 8.6619e-01, 8.7893e-01, 8.7377e-01, 8.7580e-01,
        8.7476e-01, 8.8090e-01, 8.7187e-01, 8.7265e-01, 8.6621e-01, 8.7363e-01,
        8.6306e-01, 8.8387e-01, 8.7185e-01, 8.7604e-01, 8.6537e-01, 8.6805e-01,
        8.7384e-01, 8.6256e-01, 8.5975e-01, 8.6950e-01, 8.7099e-01, 8.6600e-01,
        8.6309e-01, 8.7908e-01, 8.6205e-01, 8.7427e-01, 8.6093e-01, 8.7874e-01,
        8.7923e-01, 8.7662e-01, 8.7769e-01, 8.9301e-01, 8.7101e-01, 8.6889e-01,
        8.6406e-01, 8.5550e-01, 8.6553e-01, 8.7409e-01, 8.6432e-01, 8.6977e-01,
        8.7712e-01, 8.7726e-01, 8.7115e-01, 8.6847e-01, 8.7041e-01, 8.7251e-01,
        8.7192e-01, 8.6264e-01, 8.7188e-01, 2.1679e-01, 8.7922e-01, 8.6815e-01,
        8.7531e-01, 8.5525e-01, 8.7624e-01, 8.7252e-01, 8.6816e-01, 8.7029e-01,
        8.7781e-01, 8.6406e-01, 8.6509e-01, 8.7363e-01, 8.6419e-01, 8.6663e-01,
        8.6472e-01, 8.7161e-01, 8.8213e-01, 8.6949e-01, 8.7679e-01, 8.6955e-01,
        8.7520e-01, 8.7606e-01, 8.6611e-01, 8.6896e-01, 8.6966e-01, 8.6960e-01,
        8.7206e-01, 8.7368e-01, 8.6759e-01, 8.5850e-01, 8.7194e-01, 8.7135e-01,
        8.7835e-01, 8.7358e-01, 8.7252e-01, 8.7169e-01, 8.7634e-01, 8.6754e-01,
        8.7440e-01, 8.8652e-01, 5.3002e-03, 8.6944e-01, 8.7839e-01, 8.7937e-01,
        8.6873e-01, 8.7645e-01, 8.8239e-01, 8.7279e-01, 8.7778e-01, 8.8606e-01,
        8.6903e-01, 8.7334e-01, 8.6952e-01, 8.7674e-01, 8.7029e-01, 8.6723e-01,
        8.6352e-01, 8.7249e-01, 8.7463e-01, 8.7217e-01, 8.7039e-01, 8.7345e-01,
        8.6527e-01, 8.7652e-01], device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[ 1.0717e-08, -1.1707e-07,  3.8358e-08,  ...,  5.9889e-07,
         -1.8517e-06, -5.6186e-09],
        [-6.3709e-07,  6.8554e-06, -4.9600e-08,  ...,  1.8292e-08,
         -1.3196e-06,  6.3194e-08],
        [-3.8378e-07, -2.6051e-07, -1.5862e-08,  ..., -3.3773e-08,
         -5.5713e-07,  1.6728e-07],
        ...,
        [-1.8304e-06,  6.8941e-08,  3.3737e-08,  ..., -1.5204e-06,
          1.8929e-07,  3.2983e-08],
        [-1.4770e-06, -3.2618e-08,  2.0209e-10,  ...,  3.0977e-06,
          1.1230e-07,  7.7554e-08],
        [-6.6244e-05,  1.0432e-03,  3.7420e-03,  ..., -6.2272e-03,
         -6.7892e-03, -8.9768e-04]], device='cuda:1', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        ...,
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],
        [1.0010, 0.9994, 0.9988,  ..., 0.9983, 0.9940, 1.0007]],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-0.0191, -0.0165, -0.0156, -0.0115, -0.0177, -0.0181, -0.0108, -0.0213,
        -0.0145, -0.0186, -0.0146, -0.0146, -0.0132, -0.0183, -0.0049, -0.0130,
        -0.0151, -0.0131, -0.0154, -0.0123, -0.0200, -0.0223, -0.0166, -0.0117,
        -0.0117, -0.0139, -0.0138, -0.0122, -0.0164, -0.0146, -0.0134, -0.0189,
        -0.0165, -0.0154, -0.0029, -0.0097, -0.0211, -0.0112, -0.0112, -0.0165,
        -0.0106, -0.0239, -0.0141, -0.1841, -0.0133, -0.0157, -0.0208, -0.0115,
        -0.0084, -0.0201, -0.0103, -0.0147, -0.0168, -0.0118, -0.0100, -0.0134,
        -0.0101, -0.0124, -0.0142, -0.0120, -0.0126, -0.0090, -0.0086, -0.0070,
        -0.0185, -0.0143, -0.0180, -0.0131, -0.0148, -0.0161, -0.0134, -0.0176,
        -0.0123, -0.0199, -0.0121, -0.0155, -0.0151, -0.0131, -0.0144, -0.0148,
        -0.0158, -0.0181, -0.0190, -0.0123, -0.0148, -0.0170, -0.0180, -0.0127,
        -0.0164, -0.2249, -0.0131, -0.0180, -0.0109, -0.0164, -0.0126, -0.0078,
        -0.0231, -0.0198, -0.0070, -0.0101, -0.0187, -0.0153, -0.0117, -0.0132,
        -0.0125, -0.0111, -0.0176, -0.0132, -0.0128,  0.1251, -0.0229, -0.0101,
        -0.0168, -0.0133, -0.0149, -0.3200, -0.0090, -0.0125, -0.0159, -0.0152,
        -0.0173, -0.0136, -0.0100, -0.0203, -0.0132, -0.0163, -0.0125, -0.0067,
        -0.0135, -0.0154, -0.0114, -0.0184, -0.0169, -0.0136, -0.0069, -0.0081,
        -0.0151,  0.6769, -0.0151, -0.0117, -0.0181, -0.0176, -0.0188, -0.0133,
        -0.0202, -0.0170, -0.0191, -0.0082, -0.0170, -0.4850, -0.0171, -0.0130,
        -0.0130, -0.0145, -0.0166, -0.0162, -0.0109, -0.0183, -0.0150, -0.0189,
        -0.0151, -0.0126,  0.3138, -0.0198, -0.0085, -0.0124, -0.0169, -0.0187,
        -0.0098, -0.0117, -0.0120, -0.0159, -0.0144, -0.0186, -0.0170, -0.0193,
        -0.0166, -0.0180, -0.0164, -0.0109, -0.0123, -0.0095, -0.0139, -0.0094,
        -0.0127, -0.0211, -0.0114, -0.0179, -0.0142, -0.0173, -0.0132, -0.0174,
        -0.0198, -0.0099, -0.0149, -0.0127, -0.0124, -0.0106, -0.0168, -0.0148,
        -0.0130, -0.0104, -0.0165, -0.0160, -0.0144, -0.0218, -0.0159, -0.0149,
        -0.0136, -0.0139, -0.0103, -0.0164, -0.0152, -0.0151, -0.0157, -0.0204,
        -0.0176,  0.6842, -0.0092, -0.0167, -0.0100, -0.0158,  0.1857, -0.0131,
        -0.0129, -0.0117, -0.0135, -0.0127, -0.0090, -0.0209, -0.0168, -0.0158,
        -0.0159, -0.0152, -0.0151, -0.0155, -0.0132,  0.0837, -0.0203, -0.0161,
        -0.0241, -0.0140, -0.0173, -0.0139, -0.0053, -0.0184, -0.0162, -0.0142,
        -0.0095, -0.0115, -0.0180, -0.0074, -0.0116, -0.0112, -0.0164, -0.0174,
        -0.0151, -0.0147, -0.0164, -0.0070, -0.0173, -0.0166, -0.0117,  0.1080,
         0.2338, -0.0115, -0.0144, -0.0144, -0.0145, -0.0179, -0.0142, -0.0237,
        -0.0207, -0.0181, -0.0131, -0.0166, -0.0134, -0.0170, -0.0130, -0.0137,
        -0.0162, -0.0128, -0.0283, -0.0139, -0.0208, -0.0182, -0.0147, -0.0175,
        -0.0174, -0.0137, -0.0157, -0.0166, -0.0150, -0.0133, -0.0129, -0.0098,
        -0.0114, -0.0195, -0.0070, -0.0156, -0.0215, -0.0164, -0.0115, -0.0184,
        -0.0190, -0.0113, -0.0144, -0.0137, -0.0171, -0.0150, -0.0142, -0.0130,
        -0.1236, -0.0130, -0.0141, -0.0119, -0.0165, -0.0134, -0.0173, -0.0118,
        -0.0200, -0.0201, -0.0092, -0.0113, -0.0151, -0.0161, -0.0195, -0.0165,
        -0.0161, -0.0123, -0.0137, -0.0148, -0.0213, -0.0182, -0.0121, -0.0153,
        -0.0145, -0.0180, -0.0161, -0.0178, -0.0165, -0.0089, -0.0121, -0.0205,
        -0.0163, -0.0207, -0.0141, -0.0110, -0.0093, -0.0114, -0.0124, -0.0137,
        -0.0134, -0.0155,  0.4108, -0.0204, -0.0154, -0.0144, -0.0165, -0.0122,
        -0.0137, -0.0136, -0.0181, -0.0263, -0.0165, -0.0129, -0.0093, -0.0128,
        -0.0194, -0.0129, -0.0133, -0.0156, -0.0140, -0.0070, -0.0135, -0.0191,
        -0.0085, -0.0095, -0.0176, -0.0201, -0.0111, -0.0192,  0.3547, -0.0139,
        -0.0141, -0.0124, -0.0065, -0.0111, -0.0102, -0.0126, -0.0165, -0.0224,
        -0.0145, -0.0178, -0.0174, -0.0078, -0.0152, -0.0134, -0.0113, -0.0183,
        -0.0111, -0.0098, -0.0196, -0.0225, -0.0139, -0.0115, -0.0210, -0.0150,
        -0.6581, -0.0142, -0.0149, -0.0130, -0.0145, -0.0120, -0.0197, -0.0223,
        -0.0161, -0.0176, -0.0149, -0.0170, -0.0085, -0.0102, -0.0140, -0.0231,
        -0.0155, -0.0124, -0.0116, -0.0094, -0.0080, -0.0052, -0.0212, -0.0075,
        -0.0108, -0.0135, -0.0210, -0.0114, -0.0165, -0.0109, -0.0074, -0.0179,
         0.1030, -0.0129, -0.0116, -0.0156, -0.0183, -0.0205, -0.0111,  0.0516,
        -0.0211,  0.6427, -0.0162, -0.0130, -0.0119, -0.0180, -0.0091,  0.5036,
        -0.0178,  0.4197,  0.0345,  0.0905, -0.0144, -0.0133, -0.0137, -0.0110,
        -0.0187, -0.0155, -0.0089, -0.0184, -0.0173, -0.0188, -0.0157, -0.0144,
        -0.0210, -0.0153, -0.0167, -0.0153, -0.0159, -0.0098, -0.0078, -0.0056,
        -0.0135, -0.0137, -0.0135, -0.0136, -0.0135, -0.0118, -0.0163, -0.0173,
        -0.0182, -0.0134, -0.0182, -0.0165, -0.3313, -0.0095, -0.0189, -0.0165,
        -0.0237, -0.0171, -0.0160, -0.0114, -0.0140, -0.0122, -0.0161, -0.0109,
        -0.0150, -0.0179, -0.0181, -0.0153, -0.0125, -0.0129, -0.0130, -0.1353],
       device='cuda:1', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.9963, 0.9950, 0.9980, 0.9995, 0.9959, 0.9971, 0.9974, 0.9953, 0.9990,
        0.9989, 0.9995, 0.9989, 0.9983, 0.9966, 0.9996, 0.9994, 0.9964, 0.9986,
        0.9995, 0.9962, 0.9947, 0.9946, 0.9971, 0.9974, 0.9988, 0.9987, 0.9992,
        0.9975, 0.9982, 0.9984, 0.9972, 0.9963, 0.9980, 0.9957, 0.0146, 0.9998,
        0.9980, 0.9974, 0.9989, 0.9995, 0.9990, 0.9970, 0.9991, 0.0139, 0.9968,
        0.9951, 0.9971, 1.0000, 0.9999, 0.9987, 0.9965, 0.9989, 0.9980, 0.9984,
        0.9996, 0.9978, 0.9993, 0.9972, 0.9980, 0.9979, 0.9995, 0.9991, 0.9999,
        0.9999, 0.9982, 0.9982, 0.9965, 0.9993, 0.9995, 0.9985, 0.9977, 0.9979,
        0.9999, 0.9974, 0.9986, 0.9990, 0.9955, 0.9961, 0.9966, 0.9992, 0.9982,
        0.9980, 0.9945, 0.9981, 0.9988, 0.9955, 0.9982, 0.9992, 0.9961, 0.9429,
        0.9984, 0.9966, 0.9985, 0.9950, 0.9997, 1.0001, 0.9960, 0.9970, 0.9989,
        0.9996, 0.9945, 0.9988, 0.9998, 0.9951, 0.9998, 0.9997, 0.9989, 0.9983,
        0.9996, 0.0137, 0.9935, 0.9999, 0.9981, 0.9985, 0.9988, 0.0173, 0.9999,
        0.9998, 0.9976, 0.9975, 0.9981, 0.9991, 0.9999, 0.9976, 0.9989, 0.9981,
        0.9986, 0.9990, 0.9994, 0.9958, 0.9994, 0.9971, 0.9983, 0.9973, 0.9993,
        0.9999, 0.9992, 0.0206, 0.9983, 0.9971, 0.9958, 0.9931, 0.9989, 0.9989,
        0.9976, 0.9968, 0.9950, 0.9995, 0.9961, 0.0154, 0.9977, 0.9990, 0.9953,
        0.9982, 0.9976, 0.9983, 0.9999, 0.9949, 0.9961, 0.9960, 0.9959, 0.9996,
        0.0045, 0.9954, 0.9998, 0.9993, 0.9990, 0.9987, 0.9972, 0.9998, 0.9999,
        0.9989, 0.9981, 0.9974, 0.9982, 0.9953, 0.9995, 0.9971, 0.9963, 0.9985,
        0.9998, 0.9972, 0.9980, 0.9998, 0.9979, 0.9945, 0.9980, 0.9986, 0.9978,
        0.9983, 0.9990, 0.9962, 0.9951, 0.9970, 0.9990, 0.9991, 0.9990, 0.9996,
        0.9977, 0.9989, 0.9990, 0.9997, 0.9976, 0.9982, 0.9986, 0.9957, 0.9988,
        0.9959, 0.9990, 0.9956, 0.9986, 0.9983, 0.9980, 0.9989, 0.9990, 0.9969,
        0.9980, 0.0494, 0.9993, 0.9987, 0.9996, 0.9987, 0.0082, 0.9995, 0.9974,
        0.9983, 0.9988, 0.9999, 0.9998, 0.9950, 0.9973, 0.9984, 0.9951, 0.9996,
        0.9963, 0.9994, 0.9990, 0.0134, 0.9961, 0.9972, 0.9941, 0.9983, 0.9932,
        0.9990, 1.0000, 0.9923, 0.9994, 0.9977, 0.9990, 0.9993, 0.9974, 0.9994,
        0.9977, 0.9992, 0.9971, 0.9994, 0.9983, 0.9970, 0.9965, 1.0000, 0.9969,
        0.9985, 0.9994, 0.2017, 0.0096, 0.9994, 0.9967, 0.9988, 0.9984, 0.9959,
        0.9981, 0.9953, 0.9983, 0.9957, 0.9972, 0.9955, 0.9994, 0.9967, 0.9995,
        0.9985, 0.9955, 0.9996, 0.0328, 0.9970, 0.9940, 0.9965, 0.9994, 0.9964,
        0.9963, 0.9995, 0.9964, 0.9969, 0.9980, 1.0000, 0.9990, 0.9998, 0.9972,
        0.9969, 0.9995, 0.9990, 0.9958, 0.9984, 0.9988, 0.9971, 0.9963, 0.9979,
        0.9952, 0.9995, 0.9963, 0.9965, 0.9978, 0.9989, 0.0142, 0.9991, 0.9980,
        0.9987, 0.9979, 0.9986, 0.9949, 0.9965, 0.9963, 0.9958, 1.0001, 0.9985,
        0.9997, 0.9994, 0.9964, 0.9998, 0.9959, 0.9961, 0.9996, 0.9969, 0.9991,
        0.9975, 0.9973, 0.9984, 0.9989, 0.9973, 0.9986, 0.9962, 0.9953, 0.9973,
        0.9972, 0.9971, 0.9947, 0.9931, 0.9998, 0.9968, 0.9994, 0.9999, 0.9984,
        0.9967, 0.9986, 0.9994, 0.0924, 0.9977, 0.9954, 0.9995, 0.9982, 0.9947,
        0.9949, 0.9993, 0.9969, 0.9954, 0.9959, 0.9984, 0.9983, 0.9973, 0.9976,
        0.9992, 0.9992, 0.9992, 0.9974, 1.0008, 0.9972, 0.9965, 0.9979, 0.9996,
        0.9992, 0.9963, 0.9999, 0.9963, 0.0070, 0.9994, 0.9986, 0.9975, 1.0000,
        0.9998, 0.9970, 0.9994, 0.9983, 0.9968, 0.9978, 0.9989, 0.9965, 0.9989,
        0.9979, 0.9990, 0.9997, 0.9977, 0.9998, 0.9998, 0.9487, 0.9968, 0.9983,
        0.9981, 0.9981, 0.9971, 0.0407, 0.9987, 0.9978, 0.9994, 0.9991, 0.9987,
        0.9959, 0.9941, 0.9961, 0.9966, 0.9983, 0.9969, 0.9998, 0.9989, 0.9991,
        0.9930, 0.9988, 0.9990, 0.9977, 0.9999, 0.9999, 1.0000, 0.9944, 0.9998,
        0.9990, 0.9986, 0.9926, 0.9998, 0.9985, 0.9991, 0.9985, 0.9983, 0.0763,
        0.9975, 0.9999, 0.9991, 0.9966, 0.9967, 0.9990, 0.0114, 0.9965, 0.0051,
        0.9988, 0.9968, 0.9994, 0.9965, 0.9980, 0.0360, 0.9966, 0.0809, 0.0040,
        0.0271, 0.9959, 0.9981, 0.9993, 0.9985, 0.9978, 0.9991, 0.9999, 0.9963,
        0.9942, 0.9927, 0.9979, 0.9953, 0.9969, 0.9974, 0.9992, 0.9978, 0.9948,
        0.9993, 1.0000, 0.9999, 0.9995, 0.9996, 0.9998, 0.9969, 0.9986, 0.9986,
        0.9965, 0.9995, 0.9973, 0.9979, 0.9972, 0.9981, 0.0083, 0.9989, 0.9968,
        0.9977, 0.9944, 0.9973, 0.9976, 0.9975, 0.9987, 0.9997, 0.9993, 0.9974,
        0.9990, 0.9937, 0.9984, 0.9987, 0.9989, 0.9978, 0.9989, 0.0517],
       device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[-8.0619e-03,  7.9982e-03, -2.0417e-03, -1.7993e-03,  7.0230e-03,
          7.5461e-03,  3.0303e-03, -3.4272e-02,  2.0613e-02,  9.3559e-03,
          1.0807e-02,  2.7562e-03, -6.3857e-03, -6.6563e-03, -7.4782e-03,
          2.9778e-03,  4.0290e-03,  3.4826e-03, -1.3642e-02, -1.8281e-02,
          6.9451e-03, -3.5429e-03, -8.0489e-03, -9.6338e-03,  3.6038e-03,
         -7.0198e-03,  1.5014e-02, -8.6829e-03, -3.8240e-03, -1.4515e-04,
         -5.6842e-03,  1.0712e-02, -6.9875e-03, -1.4813e-03, -2.0554e-01,
          6.3334e-03,  2.0759e-03, -1.0586e-02,  5.0862e-03,  1.8646e-03,
         -3.8015e-03, -1.3223e-02,  1.1531e-02, -2.1538e-01,  6.5869e-03,
         -2.2033e-03,  1.9029e-02,  1.3200e-02, -3.4323e-03,  5.9043e-03,
         -1.4989e-03,  8.4258e-03,  2.6555e-04,  4.5044e-03,  5.1391e-03,
         -8.7839e-03, -1.0435e-02, -1.4750e-02, -1.4600e-02,  1.9584e-02,
         -9.5900e-03, -1.0574e-02, -4.3328e-04,  1.6574e-02, -7.2220e-03,
          1.7993e-02, -1.0984e-02,  7.3476e-03, -3.7127e-03, -4.0367e-03,
          2.4389e-03, -1.4316e-02,  6.3766e-03, -3.5474e-03, -1.7245e-02,
         -8.2981e-03, -5.5602e-03, -8.8042e-03,  2.1150e-02, -6.9966e-03,
          7.1014e-04,  8.8207e-04, -8.5785e-03, -7.2421e-03, -1.9776e-04,
         -1.2019e-02, -7.6749e-03,  5.0248e-03,  3.3762e-03,  1.4640e-05,
         -1.0862e-03, -8.7613e-03, -5.7949e-03, -3.2410e-03,  1.8213e-03,
          7.3641e-03, -1.6435e-02, -8.6022e-03,  1.4500e-03,  1.4514e-02,
          2.9981e-03, -3.0125e-03, -2.7529e-03, -4.2700e-03, -4.7462e-04,
         -6.9495e-03, -1.5989e-02, -2.4582e-03, -1.1718e-02,  2.2076e-01,
          1.5848e-02, -1.5014e-03,  4.3776e-03,  1.5788e-02, -1.0734e-02,
          1.7290e-01, -6.0369e-03,  2.5627e-03,  1.4618e-03, -1.6107e-02,
          8.4679e-03, -2.1428e-02,  4.2136e-03,  1.1573e-02, -7.8716e-03,
         -9.8052e-03,  8.8893e-03, -2.7032e-03,  1.8493e-03,  1.9294e-02,
         -5.7797e-03, -1.6213e-02,  3.6469e-03,  9.3902e-03,  1.8787e-03,
          1.4625e-02,  5.7904e-03, -1.4427e-01, -3.1477e-02,  4.8038e-04,
          5.5293e-03,  1.2046e-02, -2.2682e-03,  2.4432e-03,  5.7981e-03,
         -2.3879e-02, -2.2700e-02,  1.0059e-02, -2.5296e-04,  1.9427e-01,
         -7.4425e-03,  1.9867e-02,  6.3377e-03,  1.7928e-02,  9.2215e-03,
         -1.1597e-02, -7.0083e-03,  1.0853e-03,  7.4991e-03,  8.6150e-03,
          1.1184e-02,  2.5791e-03, -6.5927e-01, -1.1872e-02,  4.1443e-04,
         -1.1305e-03,  1.3362e-02, -1.4633e-03, -1.9242e-03, -8.4178e-03,
          4.5374e-04, -2.0055e-02,  8.3405e-03,  1.1149e-02,  2.5502e-03,
          6.9618e-03, -9.0620e-03, -1.7261e-02, -2.5661e-02, -4.3409e-03,
          1.3244e-02,  1.4236e-02,  1.1140e-03, -7.0099e-03,  4.8838e-04,
          6.4744e-03,  4.5854e-03, -8.6338e-04,  1.4760e-02, -1.9002e-02,
          1.5968e-02, -4.1305e-03, -3.0735e-05,  3.8031e-04,  2.5997e-03,
         -1.1172e-02, -1.2686e-02, -1.4192e-02,  1.8684e-02, -1.4946e-03,
          3.7271e-03,  3.4769e-03, -1.1520e-02, -4.3592e-04, -7.5347e-03,
          2.7908e-03, -1.1362e-04, -8.6465e-03,  5.8950e-04, -9.7077e-04,
         -9.5197e-03, -9.1005e-03, -3.0228e-03,  1.0870e-02,  1.2827e-02,
          3.8663e-03, -3.8430e-03, -5.9870e-02, -5.0423e-03,  5.4799e-03,
          2.7712e-02,  6.8419e-03,  3.6733e-01,  1.7031e-02,  1.6194e-03,
         -2.0462e-03, -1.7875e-02,  4.7324e-03,  2.1351e-02, -6.1250e-03,
          3.9543e-03,  1.9057e-02,  3.5700e-03, -1.2205e-03,  6.8137e-03,
         -1.6910e-02, -6.6180e-03, -2.2394e-01,  2.2936e-03,  1.5988e-03,
          5.9186e-03,  1.0639e-02, -2.0403e-02, -4.7466e-03,  6.2892e-04,
          9.1682e-03,  1.3173e-02, -8.2938e-03, -1.7702e-02, -4.6405e-03,
          2.3597e-03, -4.8022e-03, -2.1865e-03,  1.9156e-02,  8.4683e-03,
         -1.4708e-03,  6.4565e-03, -1.6746e-02,  1.1763e-03, -1.2568e-02,
         -3.9562e-03,  5.0492e-03,  9.8873e-03, -1.3053e-02,  3.1372e-01,
         -9.2417e-03,  2.0014e-02,  2.8188e-03, -5.3917e-03, -1.4982e-02,
         -9.5450e-03, -1.1238e-02,  1.4961e-03,  4.2867e-03, -1.2945e-03,
          1.2950e-02,  5.1934e-03, -1.1598e-02,  1.1985e-03, -9.4611e-03,
         -5.5322e-03,  1.2851e-02, -9.0572e-02,  1.4500e-03,  1.3025e-02,
          1.2596e-02,  4.1420e-03,  1.0602e-02, -2.3329e-02, -2.2352e-04,
         -4.5758e-04, -5.3831e-03,  2.1862e-02, -2.1364e-02,  1.4424e-04,
         -1.8323e-02,  1.6110e-02,  1.2129e-02,  5.5349e-03, -1.7382e-03,
         -1.2891e-02,  1.6129e-03, -1.0785e-02,  7.1967e-03,  6.5601e-03,
          3.8173e-03,  3.6890e-03,  4.6280e-03,  1.3989e-02, -5.7923e-03,
         -3.4740e-03, -1.7035e-03,  2.1153e-01,  1.7720e-02,  5.3951e-03,
          8.9753e-03,  2.4842e-02,  1.2574e-03,  7.4322e-03,  5.2874e-03,
          3.9566e-03, -4.0838e-03,  1.3904e-02, -2.9848e-03, -9.0914e-03,
          6.6288e-03, -2.2857e-03, -5.2422e-03, -5.9222e-03,  1.4724e-02,
          3.2028e-03,  1.5760e-03,  7.1452e-03, -1.3339e-02,  8.0336e-03,
         -8.0278e-03, -1.1737e-03, -4.5126e-03,  4.0675e-03,  1.2183e-03,
         -4.0640e-03,  8.9583e-03,  1.5451e-02, -5.8441e-03,  3.6909e-03,
         -1.1177e-02, -1.3521e-03,  5.6611e-03, -4.9524e-03, -9.0471e-03,
         -7.6001e-06, -1.1790e-03,  3.3146e-04, -6.9882e-03,  3.1174e-02,
         -1.2084e-02,  1.3755e-02, -4.6736e-03,  3.3715e-03,  1.5980e-03,
         -1.3632e-02,  2.8391e-03,  4.3693e-03,  4.0854e-04,  2.2981e-03,
          1.3091e-03,  5.7691e-03, -2.8179e-03,  7.5350e-03,  1.6590e-02,
         -1.2855e-02,  1.1008e-02, -1.8207e-02, -3.1512e-03,  1.2681e-02,
          1.6749e-02,  6.7422e-03,  1.2100e-02, -1.6027e-02,  7.5070e-03,
          1.7796e-02,  8.5320e-03, -4.2699e-01, -1.0584e-03,  1.1225e-02,
          8.3565e-03,  4.6717e-03, -2.2548e-03,  2.5039e-02, -5.5515e-03,
          2.0025e-03, -6.0640e-03, -2.1023e-03, -1.1542e-03, -6.2415e-03,
          1.1490e-02,  6.7861e-03,  7.5941e-03,  1.1880e-02,  8.9868e-03,
         -7.0818e-03, -5.1054e-03, -1.4053e-03,  1.1840e-02, -9.6585e-03,
          2.4874e-04, -1.1347e-02, -1.0576e-02,  7.2952e-02, -2.1763e-02,
          6.9505e-03,  1.5609e-02,  1.1076e-02,  8.6155e-04,  4.5483e-04,
         -4.4872e-03, -1.2069e-02, -3.4694e-04, -1.9739e-03,  4.1053e-03,
          1.1889e-02, -1.8120e-03,  5.7140e-03,  2.4161e-03,  6.7003e-03,
         -5.5225e-03,  1.0307e-02,  1.8335e-02, -1.9420e-02, -2.7099e-03,
         -1.0308e-02,  8.4283e-03, -7.2765e-03, -4.7582e-03,  1.0559e-02,
         -2.8969e-03, -1.3159e-02,  6.0013e-04,  1.0893e-02, -2.4501e-03,
          3.8459e-02, -2.6300e-03, -7.4910e-04,  1.2731e-02,  1.2306e-02,
          2.0951e-02, -1.9941e-03,  2.6388e-01,  7.7657e-04, -5.8707e-01,
          3.1755e-03, -6.5637e-03, -1.0951e-02,  4.0660e-03, -1.4133e-02,
          8.3816e-02,  5.5576e-03, -3.0701e-02,  7.5225e-01, -1.1015e-01,
         -7.2187e-03,  4.6713e-03, -2.6837e-02, -5.0810e-03,  1.3588e-02,
          6.8163e-03,  1.4237e-02, -2.4165e-03,  3.6592e-03,  1.8817e-03,
          6.0439e-03, -1.4881e-03,  5.8202e-03,  7.9364e-04, -2.2202e-03,
          7.1775e-03,  4.8835e-03,  6.8303e-03, -3.0105e-05,  1.2270e-02,
          8.6048e-03,  1.6750e-02,  2.1169e-03,  1.2270e-02,  1.6982e-03,
          1.4658e-02, -7.2557e-03,  7.5987e-03,  8.6385e-03,  2.0800e-03,
         -1.0979e-02, -1.0202e-02,  3.6431e-01,  3.7601e-04, -2.1251e-03,
          6.7502e-03, -8.7792e-03,  3.7820e-03, -1.7355e-03,  4.3677e-03,
         -3.6622e-03, -6.9089e-03, -9.2847e-03,  4.8441e-03,  3.1306e-03,
          8.2124e-03, -5.0439e-03,  4.3659e-03,  1.5336e-02, -2.0313e-02,
         -9.1546e-03,  5.7710e-02]], device='cuda:1', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[1.7902e-01, 4.6943e-01, 3.4998e-01, 4.9025e-01, 2.4387e-01, 3.6048e-01,
         3.9817e-01, 4.1538e-01, 3.4461e-01, 3.3631e-01, 5.0273e-01, 3.1833e-01,
         3.1056e-01, 3.1426e-01, 5.8545e-01, 4.0656e-01, 3.0717e-01, 3.0937e-01,
         4.5570e-01, 3.2901e-01, 5.0770e-01, 4.3199e-02, 3.1780e-01, 3.8195e-01,
         3.5228e-01, 2.2993e-01, 3.5971e-01, 5.0284e-01, 4.2805e-01, 3.2045e-01,
         4.7305e-01, 5.0021e-01, 3.8240e-01, 3.0553e-01, 7.6045e-05, 2.6488e-01,
         3.1861e-01, 3.8541e-01, 4.2990e-01, 5.3685e-01, 2.9195e-01, 4.0648e-01,
         2.5480e-01, 1.1090e-04, 3.1581e-01, 4.0159e-01, 4.2595e-01, 3.3772e-01,
         3.7797e-01, 2.9832e-01, 4.8456e-01, 3.5243e-01, 3.5773e-01, 2.5245e-01,
         3.5833e-01, 3.1604e-01, 4.8367e-01, 3.0353e-01, 2.9476e-01, 3.1491e-01,
         2.7643e-01, 3.6906e-01, 4.0667e-01, 3.1653e-01, 2.5871e-01, 2.7077e-01,
         2.7440e-01, 3.3126e-01, 3.0845e-01, 3.1291e-01, 4.2607e-01, 2.5536e-01,
         3.1815e-01, 3.5486e-01, 3.3431e-01, 2.5730e-01, 2.2514e-01, 4.7252e-01,
         2.5958e-01, 1.9253e-01, 2.7041e-01, 3.3677e-01, 3.4417e-01, 3.0104e-01,
         3.6069e-01, 3.9444e-01, 3.0206e-01, 3.4580e-01, 2.9510e-01, 1.5639e-04,
         2.7331e-01, 3.2646e-01, 3.2440e-01, 2.6552e-01, 4.3058e-01, 3.5792e-01,
         2.7831e-01, 4.0531e-01, 3.2259e-01, 2.3578e-01, 4.4208e-01, 3.0881e-01,
         3.2058e-01, 2.9295e-01, 3.9581e-01, 2.8359e-01, 2.3566e-01, 3.2286e-01,
         3.3847e-01, 4.4287e-04, 3.6993e-01, 4.7155e-01, 4.1471e-01, 2.8992e-01,
         3.4742e-01, 1.3961e-04, 3.2981e-01, 2.6262e-01, 4.5272e-01, 3.3648e-01,
         3.4203e-01, 3.4960e-01, 3.8262e-01, 4.7503e-01, 4.6954e-01, 3.6399e-01,
         3.6313e-01, 3.1519e-01, 3.2262e-01, 4.2215e-01, 3.2056e-01, 3.5336e-01,
         3.8724e-01, 3.7891e-01, 3.8374e-01, 3.7708e-01, 3.2681e-01, 1.0063e-04,
         3.3021e-01, 3.5304e-01, 3.1808e-01, 4.5087e-01, 2.2740e-01, 3.1564e-01,
         3.3429e-01, 3.4331e-01, 3.0097e-01, 2.9855e-01, 3.4303e-01, 7.2951e-05,
         3.5005e-01, 2.7423e-01, 3.8635e-01, 3.1780e-01, 3.1619e-01, 3.1334e-01,
         4.4079e-01, 2.7726e-01, 3.3171e-01, 3.7048e-01, 3.1849e-01, 3.1632e-01,
         5.1249e-05, 4.3354e-01, 3.3087e-01, 3.5560e-01, 3.2263e-01, 4.2774e-01,
         4.1738e-01, 3.4129e-01, 3.0035e-01, 3.4612e-01, 2.9203e-01, 2.6784e-01,
         3.1188e-01, 3.1162e-01, 3.0091e-01, 2.6214e-01, 2.4874e-01, 3.6224e-01,
         3.1162e-01, 3.5234e-01, 4.0495e-01, 4.3574e-01, 3.2502e-01, 4.8615e-01,
         4.4490e-01, 2.8808e-01, 3.3365e-01, 3.5339e-01, 3.1178e-01, 3.4807e-01,
         4.7715e-01, 4.1876e-01, 2.5543e-01, 2.9425e-01, 2.8567e-01, 3.1504e-01,
         2.5729e-01, 2.4999e-01, 4.2825e-01, 4.1036e-01, 4.0189e-01, 3.7527e-01,
         2.9405e-01, 4.3773e-01, 3.2007e-01, 3.1460e-01, 2.5992e-01, 3.0086e-01,
         3.2362e-01, 2.4469e-01, 2.2878e-01, 2.3106e-01, 2.8807e-01, 3.0629e-01,
         2.9889e-01, 1.4089e-04, 3.9861e-01, 3.9953e-01, 4.2103e-01, 2.6741e-01,
         6.5642e-05, 2.9796e-01, 2.9321e-01, 5.4760e-01, 3.2850e-01, 4.2384e-01,
         3.5508e-01, 3.6211e-01, 3.7098e-01, 2.5897e-01, 3.2517e-01, 5.1024e-01,
         3.2845e-01, 3.1919e-01, 2.8770e-01, 6.4821e-05, 2.6072e-01, 2.7933e-01,
         2.8939e-01, 4.7823e-01, 5.4929e-01, 3.4398e-01, 4.1189e-01, 4.6484e-01,
         3.4454e-01, 3.3440e-01, 4.2217e-01, 4.4295e-01, 4.7685e-01, 2.7667e-01,
         3.4859e-01, 2.3342e-01, 3.1586e-01, 2.7895e-01, 3.5078e-01, 4.2126e-01,
         3.9818e-01, 5.1214e-01, 4.5692e-01, 3.7256e-01, 2.4160e-01, 2.3332e-04,
         7.0230e-04, 2.6360e-01, 3.5798e-01, 3.3097e-01, 2.8525e-01, 3.1310e-01,
         2.2211e-01, 2.9943e-01, 2.7011e-01, 3.1560e-01, 3.1305e-01, 3.5070e-01,
         3.1271e-01, 5.8275e-01, 4.0426e-01, 3.2681e-01, 3.6420e-01, 3.4003e-01,
         4.5963e-04, 2.5337e-01, 3.6966e-01, 3.3293e-01, 3.8925e-01, 3.2505e-01,
         3.4945e-01, 3.0457e-01, 2.9117e-01, 3.1326e-01, 2.5928e-01, 3.2029e-01,
         2.5975e-01, 3.7601e-01, 2.9208e-01, 3.0139e-01, 3.3528e-01, 3.1330e-01,
         4.9704e-01, 2.8791e-01, 5.0912e-01, 3.2933e-01, 4.2920e-01, 3.3158e-01,
         5.0310e-01, 2.6802e-01, 3.7275e-01, 3.1827e-01, 4.7329e-01, 3.4411e-01,
         6.0157e-05, 2.6672e-01, 4.3756e-01, 3.3287e-01, 2.4445e-01, 3.3872e-01,
         3.2937e-01, 2.8658e-01, 3.6037e-01, 5.3318e-01, 3.8746e-01, 2.9788e-01,
         2.5305e-01, 2.6189e-01, 3.3073e-01, 5.7483e-01, 4.2829e-01, 2.8462e-01,
         4.6088e-01, 2.8627e-01, 5.1574e-01, 2.6056e-01, 3.5253e-01, 2.9138e-01,
         2.5721e-01, 3.2761e-01, 2.7712e-01, 3.3049e-01, 3.8126e-01, 3.2644e-01,
         2.8532e-01, 2.9386e-01, 2.6496e-01, 3.6495e-01, 3.0935e-01, 3.0128e-01,
         2.5070e-01, 3.4502e-01, 3.7557e-01, 3.4445e-01, 3.2953e-01, 2.9226e-01,
         2.0127e-04, 2.5161e-01, 3.4533e-01, 3.5598e-01, 4.8087e-01, 4.2931e-01,
         3.0811e-01, 3.3536e-01, 2.9849e-01, 3.3905e-01, 3.1796e-01, 2.8468e-01,
         3.6468e-01, 3.6337e-01, 2.3204e-01, 4.0090e-01, 3.1698e-01, 2.7833e-01,
         3.3340e-01, 3.1143e-01, 2.6055e-01, 2.9746e-01, 4.0080e-01, 3.2966e-01,
         2.8136e-01, 2.7942e-01, 2.5919e-01, 3.4369e-01, 4.8635e-04, 3.1809e-01,
         4.5111e-01, 2.7803e-01, 3.8939e-01, 3.0771e-01, 3.1823e-01, 2.7223e-01,
         3.5801e-01, 3.5033e-01, 2.7441e-01, 3.3377e-01, 3.9002e-01, 2.9807e-01,
         4.2058e-01, 3.4430e-01, 3.5586e-01, 3.1472e-01, 3.6139e-01, 3.9407e-01,
         8.0354e-03, 3.7185e-01, 3.4107e-01, 4.1260e-01, 4.7686e-01, 2.4961e-01,
         8.7520e-05, 3.1629e-01, 5.1005e-01, 3.1216e-01, 2.7388e-01, 3.2364e-01,
         4.6653e-01, 3.0171e-01, 2.7561e-01, 2.1646e-01, 4.4969e-01, 3.4692e-01,
         3.2715e-01, 3.7338e-01, 4.1064e-01, 4.4426e-01, 2.8144e-01, 2.9504e-01,
         4.6779e-01, 4.2422e-01, 2.7803e-01, 3.0255e-01, 2.9388e-01, 4.4472e-01,
         3.0203e-01, 3.8895e-01, 3.5254e-01, 4.5285e-01, 3.0222e-01, 3.4070e-01,
         3.0700e-01, 2.6378e-01, 1.8164e-04, 2.6744e-01, 3.2909e-01, 3.2926e-01,
         2.7601e-01, 4.5309e-01, 1.6153e-01, 6.9617e-05, 2.8079e-01, 4.8407e-04,
         3.2208e-01, 3.7769e-01, 3.0015e-01, 5.5464e-01, 2.7386e-01, 1.0783e-04,
         5.6931e-01, 1.1174e-03, 3.7229e-04, 5.4182e-05, 3.9694e-01, 4.3249e-01,
         3.1621e-01, 2.9978e-01, 3.6547e-01, 2.7701e-01, 4.9622e-01, 4.2809e-01,
         3.3913e-01, 3.0844e-01, 4.0325e-01, 3.4179e-01, 3.3315e-01, 2.4925e-01,
         2.5771e-01, 3.4935e-01, 3.0162e-01, 3.1175e-01, 5.8346e-01, 3.8522e-01,
         3.1290e-01, 2.9989e-01, 3.4619e-01, 3.0761e-01, 2.6470e-01, 3.5125e-01,
         3.2279e-01, 3.4206e-01, 3.0458e-01, 3.8544e-01, 3.6002e-01, 3.7368e-01,
         8.3023e-05, 3.2630e-01, 4.1397e-01, 3.1190e-01, 2.6850e-01, 3.1457e-01,
         3.8188e-01, 2.9357e-01, 2.7613e-01, 3.7483e-01, 3.1100e-01, 2.8836e-01,
         2.8088e-01, 4.5910e-01, 2.7218e-01, 2.9493e-01, 3.5210e-01, 3.0113e-01,
         4.1110e-01, 4.8408e-05]], device='cuda:1', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.2477], device='cuda:1', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0030], device='cuda:1', grad_fn=<AddBackward0>)
Using device: cuda:1
===== Training profile tensin-3x512-s03 - 3 =====
[0:00:01.708104] epoch: 0 | elbo: 32997.229511718746 | train_rmse: 0.3279 | val_rmse: 0.4676 | val_ll: -0.7685
[0:01:34.045122] epoch: 50 | elbo: 32343.790878906246 | train_rmse: 0.3147 | val_rmse: 0.4582 | val_ll: -0.7463
[0:03:07.370794] epoch: 100 | elbo: 32627.130878906253 | train_rmse: 0.3151 | val_rmse: 0.4581 | val_ll: -0.7411
[0:04:41.179498] epoch: 150 | elbo: 32285.179746093752 | train_rmse: 0.3146 | val_rmse: 0.4587 | val_ll: -0.7414
[0:06:13.355453] epoch: 200 | elbo: 32599.18140625 | train_rmse: 0.3155 | val_rmse: 0.4588 | val_ll: -0.731
[0:07:46.056064] epoch: 250 | elbo: 32138.3940234375 | train_rmse: 0.3148 | val_rmse: 0.4588 | val_ll: -0.7438
[0:09:19.531142] epoch: 300 | elbo: 32356.471171875 | train_rmse: 0.3148 | val_rmse: 0.4588 | val_ll: -0.7454
[0:10:52.919857] epoch: 350 | elbo: 32137.947832031256 | train_rmse: 0.3175 | val_rmse: 0.4606 | val_ll: -0.7544
[0:12:26.179557] epoch: 400 | elbo: 32324.03546875 | train_rmse: 0.3153 | val_rmse: 0.4595 | val_ll: -0.7401
[0:13:58.053759] epoch: 450 | elbo: 31996.09080078125 | train_rmse: 0.3148 | val_rmse: 0.4585 | val_ll: -0.7404
[0:15:31.300661] epoch: 500 | elbo: 32406.621367187494 | train_rmse: 0.3152 | val_rmse: 0.4576 | val_ll: -0.7437
[0:17:04.030342] epoch: 550 | elbo: 32085.848007812503 | train_rmse: 0.3148 | val_rmse: 0.4572 | val_ll: -0.7372
[0:18:36.454328] epoch: 600 | elbo: 32119.906640625 | train_rmse: 0.3155 | val_rmse: 0.4595 | val_ll: -0.7443
[0:20:09.233335] epoch: 650 | elbo: 31955.609707031253 | train_rmse: 0.3145 | val_rmse: 0.4584 | val_ll: -0.7398
[0:21:44.218362] epoch: 700 | elbo: 31937.6437890625 | train_rmse: 0.3149 | val_rmse: 0.4586 | val_ll: -0.7291
[0:23:18.305573] epoch: 750 | elbo: 31941.445683593753 | train_rmse: 0.3149 | val_rmse: 0.4588 | val_ll: -0.7345
[0:24:51.186540] epoch: 800 | elbo: 31980.5315234375 | train_rmse: 0.3154 | val_rmse: 0.4581 | val_ll: -0.7461
[0:26:23.461422] epoch: 850 | elbo: 31946.698007812498 | train_rmse: 0.3148 | val_rmse: 0.459 | val_ll: -0.7444
[0:27:55.923220] epoch: 900 | elbo: 31798.049179687503 | train_rmse: 0.3143 | val_rmse: 0.4575 | val_ll: -0.7294
[0:29:28.802771] epoch: 950 | elbo: 31870.072519531248 | train_rmse: 0.3152 | val_rmse: 0.4592 | val_ll: -0.7419
[0:31:02.410122] epoch: 1000 | elbo: 31778.360078125 | train_rmse: 0.3137 | val_rmse: 0.4578 | val_ll: -0.7372
[0:32:35.208786] epoch: 1050 | elbo: 31828.443906249995 | train_rmse: 0.3131 | val_rmse: 0.4576 | val_ll: -0.7355
[0:34:08.300413] epoch: 1100 | elbo: 31690.42353515625 | train_rmse: 0.3146 | val_rmse: 0.458 | val_ll: -0.7362
[0:35:41.115764] epoch: 1150 | elbo: 31654.5114453125 | train_rmse: 0.3127 | val_rmse: 0.4573 | val_ll: -0.7452
[0:37:15.024220] epoch: 1200 | elbo: 31852.469960937502 | train_rmse: 0.3131 | val_rmse: 0.4574 | val_ll: -0.7409
[0:38:50.414001] epoch: 1250 | elbo: 31974.077929687504 | train_rmse: 0.3129 | val_rmse: 0.4566 | val_ll: -0.7318
[0:40:23.794761] epoch: 1300 | elbo: 31621.814199218752 | train_rmse: 0.3139 | val_rmse: 0.4568 | val_ll: -0.7342
[0:41:57.199828] epoch: 1350 | elbo: 31641.080507812498 | train_rmse: 0.3131 | val_rmse: 0.4574 | val_ll: -0.7343
[0:43:30.340936] epoch: 1400 | elbo: 31608.79658203125 | train_rmse: 0.3123 | val_rmse: 0.4567 | val_ll: -0.7412
[0:45:02.735259] epoch: 1450 | elbo: 31423.136874999997 | train_rmse: 0.3138 | val_rmse: 0.4565 | val_ll: -0.7315
[0:46:34.714616] epoch: 1500 | elbo: 31604.238652343745 | train_rmse: 0.3138 | val_rmse: 0.4574 | val_ll: -0.7369
[0:48:07.739292] epoch: 1550 | elbo: 31815.4859375 | train_rmse: 0.3169 | val_rmse: 0.4576 | val_ll: -0.7404
[0:49:40.327817] epoch: 1600 | elbo: 31513.52228515625 | train_rmse: 0.3125 | val_rmse: 0.4558 | val_ll: -0.7307
[0:51:16.652953] epoch: 1650 | elbo: 31561.649707031254 | train_rmse: 0.3132 | val_rmse: 0.456 | val_ll: -0.7315
[0:52:52.210003] epoch: 1700 | elbo: 31487.726894531246 | train_rmse: 0.3133 | val_rmse: 0.4566 | val_ll: -0.7326
[0:54:26.205928] epoch: 1750 | elbo: 31467.309296875 | train_rmse: 0.3127 | val_rmse: 0.4563 | val_ll: -0.7449
[0:56:02.534458] epoch: 1800 | elbo: 31600.023847656248 | train_rmse: 0.3128 | val_rmse: 0.4556 | val_ll: -0.7357
[0:57:37.751753] epoch: 1850 | elbo: 31431.229355468753 | train_rmse: 0.3121 | val_rmse: 0.4554 | val_ll: -0.7274
[0:59:12.647979] epoch: 1900 | elbo: 31241.86302734375 | train_rmse: 0.3123 | val_rmse: 0.4561 | val_ll: -0.7338
[1:00:46.850365] epoch: 1950 | elbo: 31309.052968750002 | train_rmse: 0.3131 | val_rmse: 0.4567 | val_ll: -0.7353
[1:02:19.981761] epoch: 2000 | elbo: 31286.061601562495 | train_rmse: 0.3122 | val_rmse: 0.455 | val_ll: -0.7279
[1:03:54.043524] epoch: 2050 | elbo: 31293.132187499996 | train_rmse: 0.312 | val_rmse: 0.4557 | val_ll: -0.7247
[1:05:27.762596] epoch: 2100 | elbo: 31474.074765625002 | train_rmse: 0.3132 | val_rmse: 0.4554 | val_ll: -0.7342
[1:07:02.181793] epoch: 2150 | elbo: 31399.38447265625 | train_rmse: 0.3121 | val_rmse: 0.4552 | val_ll: -0.7372
[1:08:36.384188] epoch: 2200 | elbo: 31187.530546875 | train_rmse: 0.3131 | val_rmse: 0.455 | val_ll: -0.7326
[1:10:10.429465] epoch: 2250 | elbo: 31238.440566406247 | train_rmse: 0.3144 | val_rmse: 0.4568 | val_ll: -0.7474
[1:11:45.218950] epoch: 2300 | elbo: 31101.299687500003 | train_rmse: 0.3132 | val_rmse: 0.4545 | val_ll: -0.7349
[1:13:19.042917] epoch: 2350 | elbo: 31153.358300781256 | train_rmse: 0.3126 | val_rmse: 0.4556 | val_ll: -0.7303
[1:14:51.822785] epoch: 2400 | elbo: 31497.579375 | train_rmse: 0.3128 | val_rmse: 0.4546 | val_ll: -0.7354
[1:16:25.054802] epoch: 2450 | elbo: 31038.811640624997 | train_rmse: 0.3135 | val_rmse: 0.4554 | val_ll: -0.7318
[1:17:58.749900] epoch: 2500 | elbo: 30909.799433593747 | train_rmse: 0.3134 | val_rmse: 0.4555 | val_ll: -0.7346
[1:19:33.540930] epoch: 2550 | elbo: 31265.365175781255 | train_rmse: 0.3136 | val_rmse: 0.4556 | val_ll: -0.7426
[1:21:06.732967] epoch: 2600 | elbo: 31147.967343750002 | train_rmse: 0.313 | val_rmse: 0.454 | val_ll: -0.7392
[1:22:38.628587] epoch: 2650 | elbo: 31199.957929687498 | train_rmse: 0.3128 | val_rmse: 0.4542 | val_ll: -0.7268
[1:24:10.024397] epoch: 2700 | elbo: 31090.066738281246 | train_rmse: 0.3132 | val_rmse: 0.4547 | val_ll: -0.7253
[1:25:45.291767] epoch: 2750 | elbo: 31083.565957031253 | train_rmse: 0.3123 | val_rmse: 0.4541 | val_ll: -0.7334
[1:27:18.790186] epoch: 2800 | elbo: 30921.15578125 | train_rmse: 0.3132 | val_rmse: 0.4541 | val_ll: -0.7338
[1:28:54.131785] epoch: 2850 | elbo: 30972.133554687498 | train_rmse: 0.3134 | val_rmse: 0.4543 | val_ll: -0.7288
[1:30:27.589588] epoch: 2900 | elbo: 31035.021875 | train_rmse: 0.3131 | val_rmse: 0.4546 | val_ll: -0.7388
[1:32:00.957844] epoch: 2950 | elbo: 31037.228828125 | train_rmse: 0.3128 | val_rmse: 0.454 | val_ll: -0.7283
[1:33:35.134390] epoch: 3000 | elbo: 30928.002011718752 | train_rmse: 0.3126 | val_rmse: 0.4539 | val_ll: -0.7262
[1:35:09.552680] epoch: 3050 | elbo: 30785.83546875 | train_rmse: 0.3127 | val_rmse: 0.4534 | val_ll: -0.7235
[1:36:43.081711] epoch: 3100 | elbo: 30803.139277343747 | train_rmse: 0.3131 | val_rmse: 0.4533 | val_ll: -0.732
[1:38:15.267908] epoch: 3150 | elbo: 30810.50126953125 | train_rmse: 0.3131 | val_rmse: 0.4535 | val_ll: -0.7344
[1:39:48.174938] epoch: 3200 | elbo: 30800.077714843752 | train_rmse: 0.3118 | val_rmse: 0.4537 | val_ll: -0.7358
[1:41:20.923224] epoch: 3250 | elbo: 30881.314902343747 | train_rmse: 0.3139 | val_rmse: 0.4548 | val_ll: -0.7322
[1:42:54.648590] epoch: 3300 | elbo: 30828.791171874997 | train_rmse: 0.3128 | val_rmse: 0.4534 | val_ll: -0.7369
[1:44:28.787137] epoch: 3350 | elbo: 30761.03490234375 | train_rmse: 0.3124 | val_rmse: 0.4542 | val_ll: -0.7228
[1:46:02.377662] epoch: 3400 | elbo: 30748.675234375005 | train_rmse: 0.3129 | val_rmse: 0.4542 | val_ll: -0.7291
[1:47:36.030623] epoch: 3450 | elbo: 31226.52119140625 | train_rmse: 0.3132 | val_rmse: 0.4531 | val_ll: -0.7282
[1:49:09.453729] epoch: 3500 | elbo: 30690.48490234375 | train_rmse: 0.312 | val_rmse: 0.4533 | val_ll: -0.7241
[1:50:42.725918] epoch: 3550 | elbo: 30679.210136718757 | train_rmse: 0.3146 | val_rmse: 0.4546 | val_ll: -0.7343
[1:52:16.364579] epoch: 3600 | elbo: 30659.947343750002 | train_rmse: 0.3127 | val_rmse: 0.4537 | val_ll: -0.7299
[1:53:48.962614] epoch: 3650 | elbo: 30778.4232421875 | train_rmse: 0.3135 | val_rmse: 0.4535 | val_ll: -0.7315
[1:55:21.106352] epoch: 3700 | elbo: 30793.54580078125 | train_rmse: 0.3134 | val_rmse: 0.4534 | val_ll: -0.7304
[1:56:55.421944] epoch: 3750 | elbo: 30572.076582031248 | train_rmse: 0.3131 | val_rmse: 0.4531 | val_ll: -0.7348
[1:58:30.576978] epoch: 3800 | elbo: 30457.582851562503 | train_rmse: 0.3135 | val_rmse: 0.4531 | val_ll: -0.7386
[2:00:03.119012] epoch: 3850 | elbo: 31175.331386718746 | train_rmse: 0.3143 | val_rmse: 0.4548 | val_ll: -0.7294
[2:01:37.520747] epoch: 3900 | elbo: 30508.733125 | train_rmse: 0.3127 | val_rmse: 0.4519 | val_ll: -0.721
[2:03:12.148769] epoch: 3950 | elbo: 30922.32564453125 | train_rmse: 0.3144 | val_rmse: 0.4549 | val_ll: -0.7299
[2:04:46.505368] epoch: 4000 | elbo: 30720.560136718745 | train_rmse: 0.3127 | val_rmse: 0.4527 | val_ll: -0.7261
[2:06:22.269973] epoch: 4050 | elbo: 30506.516796875 | train_rmse: 0.3159 | val_rmse: 0.4534 | val_ll: -0.7233
[2:07:56.808697] epoch: 4100 | elbo: 30426.67921875 | train_rmse: 0.3147 | val_rmse: 0.4526 | val_ll: -0.7279
[2:09:32.838481] epoch: 4150 | elbo: 30690.746386718743 | train_rmse: 0.314 | val_rmse: 0.4522 | val_ll: -0.7331
[2:11:09.237526] epoch: 4200 | elbo: 30467.840078125 | train_rmse: 0.3124 | val_rmse: 0.4523 | val_ll: -0.7267
[2:12:43.970700] epoch: 4250 | elbo: 30374.34287109375 | train_rmse: 0.313 | val_rmse: 0.4513 | val_ll: -0.7236
[2:14:19.924771] epoch: 4300 | elbo: 30752.4966015625 | train_rmse: 0.3184 | val_rmse: 0.4578 | val_ll: -0.7398
[2:15:54.058388] epoch: 4350 | elbo: 30528.975468749995 | train_rmse: 0.317 | val_rmse: 0.4532 | val_ll: -0.7346
[2:17:29.036416] epoch: 4400 | elbo: 30382.06447265625 | train_rmse: 0.3136 | val_rmse: 0.4534 | val_ll: -0.7307
[2:19:03.633294] epoch: 4450 | elbo: 30489.900390625 | train_rmse: 0.3153 | val_rmse: 0.4546 | val_ll: -0.7383
[2:20:36.618260] epoch: 4500 | elbo: 30461.441484375006 | train_rmse: 0.3137 | val_rmse: 0.4524 | val_ll: -0.7318
[2:22:09.780203] epoch: 4550 | elbo: 30285.098769531247 | train_rmse: 0.3133 | val_rmse: 0.4526 | val_ll: -0.7223
[2:23:43.266111] epoch: 4600 | elbo: 30554.907773437502 | train_rmse: 0.3143 | val_rmse: 0.4512 | val_ll: -0.7243
[2:25:15.366739] epoch: 4650 | elbo: 30199.684394531254 | train_rmse: 0.3155 | val_rmse: 0.4529 | val_ll: -0.7224
[2:26:48.281677] epoch: 4700 | elbo: 30346.494667968753 | train_rmse: 0.3139 | val_rmse: 0.4517 | val_ll: -0.7144
[2:28:21.737936] epoch: 4750 | elbo: 30365.486699218753 | train_rmse: 0.3131 | val_rmse: 0.452 | val_ll: -0.7245
[2:29:56.807173] epoch: 4800 | elbo: 30636.148496093745 | train_rmse: 0.3184 | val_rmse: 0.4573 | val_ll: -0.7391
[2:31:31.792925] epoch: 4850 | elbo: 30874.487226562505 | train_rmse: 0.3137 | val_rmse: 0.4527 | val_ll: -0.72
[2:33:05.528440] epoch: 4900 | elbo: 30256.819472656247 | train_rmse: 0.3135 | val_rmse: 0.4522 | val_ll: -0.7288
[2:34:37.755753] epoch: 4950 | elbo: 30078.828671874995 | train_rmse: 0.3146 | val_rmse: 0.4531 | val_ll: -0.7295
[2:36:11.102203] epoch: 5000 | elbo: 30415.357871093747 | train_rmse: 0.3159 | val_rmse: 0.4534 | val_ll: -0.7222
[2:37:45.671628] epoch: 5050 | elbo: 30671.21064453125 | train_rmse: 0.3143 | val_rmse: 0.4524 | val_ll: -0.717
[2:39:17.551416] epoch: 5100 | elbo: 30243.99482421875 | train_rmse: 0.3145 | val_rmse: 0.4525 | val_ll: -0.7302
[2:40:50.542278] epoch: 5150 | elbo: 30316.986093749998 | train_rmse: 0.3156 | val_rmse: 0.4527 | val_ll: -0.7287
[2:42:22.540212] epoch: 5200 | elbo: 30189.16494140625 | train_rmse: 0.3136 | val_rmse: 0.4522 | val_ll: -0.7274
[2:43:55.823517] epoch: 5250 | elbo: 30440.4477734375 | train_rmse: 0.3184 | val_rmse: 0.4572 | val_ll: -0.738
[2:45:28.613744] epoch: 5300 | elbo: 30138.075664062497 | train_rmse: 0.3132 | val_rmse: 0.4522 | val_ll: -0.7332
[2:47:03.095961] epoch: 5350 | elbo: 29913.2795703125 | train_rmse: 0.3136 | val_rmse: 0.4517 | val_ll: -0.7239
[2:48:36.478295] epoch: 5400 | elbo: 30222.916972656247 | train_rmse: 0.3152 | val_rmse: 0.4531 | val_ll: -0.7302
[2:50:09.237339] epoch: 5450 | elbo: 29938.211777343746 | train_rmse: 0.3139 | val_rmse: 0.4521 | val_ll: -0.7292
[2:51:42.517106] epoch: 5500 | elbo: 30040.539277343756 | train_rmse: 0.3162 | val_rmse: 0.4542 | val_ll: -0.7399
[2:53:15.779399] epoch: 5550 | elbo: 30027.433945312496 | train_rmse: 0.3165 | val_rmse: 0.4518 | val_ll: -0.7426
[2:54:48.497658] epoch: 5600 | elbo: 29969.217812500003 | train_rmse: 0.3139 | val_rmse: 0.4509 | val_ll: -0.7285
[2:56:20.338849] epoch: 5650 | elbo: 30122.6896875 | train_rmse: 0.3146 | val_rmse: 0.4515 | val_ll: -0.7319
[2:57:52.567636] epoch: 5700 | elbo: 29917.066757812496 | train_rmse: 0.3145 | val_rmse: 0.4511 | val_ll: -0.7263
[2:59:24.846301] epoch: 5750 | elbo: 30145.587460937495 | train_rmse: 0.3141 | val_rmse: 0.4518 | val_ll: -0.728
[3:00:57.119164] epoch: 5800 | elbo: 30107.49728515625 | train_rmse: 0.3146 | val_rmse: 0.4506 | val_ll: -0.7223
[3:02:30.401372] epoch: 5850 | elbo: 30339.31845703125 | train_rmse: 0.3139 | val_rmse: 0.4511 | val_ll: -0.7264
[3:04:03.501073] epoch: 5900 | elbo: 29838.558515625005 | train_rmse: 0.3147 | val_rmse: 0.4504 | val_ll: -0.7249
[3:05:37.695623] epoch: 5950 | elbo: 29767.820449218747 | train_rmse: 0.3142 | val_rmse: 0.4509 | val_ll: -0.7228
[3:07:11.696192] epoch: 6000 | elbo: 30061.039101562503 | train_rmse: 0.3135 | val_rmse: 0.4518 | val_ll: -0.7174
[3:08:45.447100] epoch: 6050 | elbo: 29789.702031249995 | train_rmse: 0.3139 | val_rmse: 0.4504 | val_ll: -0.725
[3:10:19.885210] epoch: 6100 | elbo: 29928.169023437502 | train_rmse: 0.3146 | val_rmse: 0.4521 | val_ll: -0.7288
[3:11:54.753614] epoch: 6150 | elbo: 30185.168300781253 | train_rmse: 0.3138 | val_rmse: 0.4496 | val_ll: -0.7189
[3:13:28.521095] epoch: 6200 | elbo: 29830.174355468753 | train_rmse: 0.3139 | val_rmse: 0.4502 | val_ll: -0.7353
[3:15:03.225300] epoch: 6250 | elbo: 29621.456308593748 | train_rmse: 0.314 | val_rmse: 0.4498 | val_ll: -0.7225
[3:16:37.329132] epoch: 6300 | elbo: 29571.93314453125 | train_rmse: 0.3151 | val_rmse: 0.4503 | val_ll: -0.7267
[3:18:10.100308] epoch: 6350 | elbo: 29613.0517578125 | train_rmse: 0.3142 | val_rmse: 0.4509 | val_ll: -0.7221
[3:19:42.577163] epoch: 6400 | elbo: 29602.267226562504 | train_rmse: 0.3142 | val_rmse: 0.4491 | val_ll: -0.7226
[3:21:14.481318] epoch: 6450 | elbo: 30040.087128906256 | train_rmse: 0.3137 | val_rmse: 0.4505 | val_ll: -0.7177
[3:22:46.668344] epoch: 6500 | elbo: 29574.761601562503 | train_rmse: 0.3131 | val_rmse: 0.4491 | val_ll: -0.7279
[3:24:19.026327] epoch: 6550 | elbo: 29818.576230468745 | train_rmse: 0.3134 | val_rmse: 0.4504 | val_ll: -0.727
[3:25:51.280780] epoch: 6600 | elbo: 29599.70732421875 | train_rmse: 0.314 | val_rmse: 0.4491 | val_ll: -0.7233
[3:27:24.840856] epoch: 6650 | elbo: 29703.718007812502 | train_rmse: 0.3122 | val_rmse: 0.4489 | val_ll: -0.7117
[3:28:56.657988] epoch: 6700 | elbo: 29510.9596484375 | train_rmse: 0.3126 | val_rmse: 0.4485 | val_ll: -0.7159
[3:30:31.314660] epoch: 6750 | elbo: 29612.167363281245 | train_rmse: 0.3132 | val_rmse: 0.4496 | val_ll: -0.7151
[3:32:06.585907] epoch: 6800 | elbo: 30021.423632812504 | train_rmse: 0.3155 | val_rmse: 0.4529 | val_ll: -0.7283
[3:33:41.893705] epoch: 6850 | elbo: 29853.042519531253 | train_rmse: 0.3137 | val_rmse: 0.4492 | val_ll: -0.7203
[3:35:15.663444] epoch: 6900 | elbo: 29463.73171875 | train_rmse: 0.3151 | val_rmse: 0.4489 | val_ll: -0.7186
[3:36:48.128934] epoch: 6950 | elbo: 29457.90408203125 | train_rmse: 0.3131 | val_rmse: 0.4493 | val_ll: -0.7122
[3:38:21.252299] epoch: 7000 | elbo: 29496.956054687507 | train_rmse: 0.3129 | val_rmse: 0.4484 | val_ll: -0.7183
[3:39:54.683413] epoch: 7050 | elbo: 29406.535039062503 | train_rmse: 0.3121 | val_rmse: 0.4493 | val_ll: -0.7164
[3:41:27.302133] epoch: 7100 | elbo: 29377.46798828125 | train_rmse: 0.3126 | val_rmse: 0.4482 | val_ll: -0.7148
[3:43:02.289488] epoch: 7150 | elbo: 29276.24453125 | train_rmse: 0.3126 | val_rmse: 0.4484 | val_ll: -0.716
[3:44:36.414455] epoch: 7200 | elbo: 29705.224355468752 | train_rmse: 0.3132 | val_rmse: 0.4472 | val_ll: -0.7108
[3:46:09.420538] epoch: 7250 | elbo: 29557.7944140625 | train_rmse: 0.3123 | val_rmse: 0.4479 | val_ll: -0.7079
[3:47:42.573192] epoch: 7300 | elbo: 29327.2578125 | train_rmse: 0.3144 | val_rmse: 0.4501 | val_ll: -0.718
[3:49:14.491194] epoch: 7350 | elbo: 29398.486386718752 | train_rmse: 0.3142 | val_rmse: 0.4497 | val_ll: -0.7048
[3:50:47.780014] epoch: 7400 | elbo: 29769.1015234375 | train_rmse: 0.3134 | val_rmse: 0.4475 | val_ll: -0.7103
[3:52:23.321143] epoch: 7450 | elbo: 29419.744980468746 | train_rmse: 0.3109 | val_rmse: 0.4465 | val_ll: -0.7077
[3:53:56.339573] epoch: 7500 | elbo: 29512.346855468746 | train_rmse: 0.3118 | val_rmse: 0.447 | val_ll: -0.7166
[3:55:30.740766] epoch: 7550 | elbo: 30218.331972656255 | train_rmse: 0.3116 | val_rmse: 0.4485 | val_ll: -0.7149
[3:57:04.958381] epoch: 7600 | elbo: 29616.93705078125 | train_rmse: 0.3196 | val_rmse: 0.4501 | val_ll: -0.7241
[3:58:37.181468] epoch: 7650 | elbo: 29430.8983984375 | train_rmse: 0.312 | val_rmse: 0.4471 | val_ll: -0.7057
[4:00:10.967776] epoch: 7700 | elbo: 29267.28875 | train_rmse: 0.3126 | val_rmse: 0.4466 | val_ll: -0.7021
[4:01:44.096822] epoch: 7750 | elbo: 29406.15083984375 | train_rmse: 0.3127 | val_rmse: 0.4461 | val_ll: -0.7127
[4:03:18.558931] epoch: 7800 | elbo: 29256.5416015625 | train_rmse: 0.3119 | val_rmse: 0.4471 | val_ll: -0.7072
[4:04:53.975579] epoch: 7850 | elbo: 29197.780136718753 | train_rmse: 0.3125 | val_rmse: 0.4478 | val_ll: -0.7127
[4:06:28.786001] epoch: 7900 | elbo: 29652.467167968753 | train_rmse: 0.3115 | val_rmse: 0.446 | val_ll: -0.7089
[4:08:02.367341] epoch: 7950 | elbo: 29176.478769531248 | train_rmse: 0.3127 | val_rmse: 0.4465 | val_ll: -0.7091
[4:09:35.189135] epoch: 8000 | elbo: 29195.837324218755 | train_rmse: 0.3127 | val_rmse: 0.4489 | val_ll: -0.706
[4:11:07.314999] epoch: 8050 | elbo: 29501.516660156245 | train_rmse: 0.3118 | val_rmse: 0.4471 | val_ll: -0.7085
[4:12:39.790563] epoch: 8100 | elbo: 29324.52109375 | train_rmse: 0.3111 | val_rmse: 0.4466 | val_ll: -0.7086
[4:14:12.843752] epoch: 8150 | elbo: 29442.653984375 | train_rmse: 0.3112 | val_rmse: 0.4459 | val_ll: -0.6999
[4:15:44.462698] epoch: 8200 | elbo: 29159.546953125002 | train_rmse: 0.3113 | val_rmse: 0.4456 | val_ll: -0.6995
[4:17:16.673964] epoch: 8250 | elbo: 29169.541171874997 | train_rmse: 0.3138 | val_rmse: 0.4468 | val_ll: -0.7134
[4:18:49.004123] epoch: 8300 | elbo: 29712.176601562503 | train_rmse: 0.312 | val_rmse: 0.4461 | val_ll: -0.7109
[4:20:21.566334] epoch: 8350 | elbo: 29162.732285156246 | train_rmse: 0.3112 | val_rmse: 0.4459 | val_ll: -0.7015
[4:21:53.720956] epoch: 8400 | elbo: 29156.75224609375 | train_rmse: 0.3129 | val_rmse: 0.4483 | val_ll: -0.7118
[4:23:26.623168] epoch: 8450 | elbo: 29148.209492187492 | train_rmse: 0.3113 | val_rmse: 0.4445 | val_ll: -0.7008
[4:24:59.414503] epoch: 8500 | elbo: 29085.1958203125 | train_rmse: 0.3111 | val_rmse: 0.4458 | val_ll: -0.6972
[4:26:32.934299] epoch: 8550 | elbo: 29403.625742187498 | train_rmse: 0.311 | val_rmse: 0.4457 | val_ll: -0.7034
[4:28:05.302262] epoch: 8600 | elbo: 29950.14359375 | train_rmse: 0.3135 | val_rmse: 0.4457 | val_ll: -0.6951
[4:29:38.243656] epoch: 8650 | elbo: 29545.921777343756 | train_rmse: 0.3108 | val_rmse: 0.4443 | val_ll: -0.6957
[4:31:10.716822] epoch: 8700 | elbo: 29058.359687500004 | train_rmse: 0.3115 | val_rmse: 0.4448 | val_ll: -0.7007
[4:32:43.347827] epoch: 8750 | elbo: 28979.158320312497 | train_rmse: 0.3136 | val_rmse: 0.4484 | val_ll: -0.7042
[4:34:17.357493] epoch: 8800 | elbo: 28992.038925781253 | train_rmse: 0.3104 | val_rmse: 0.4446 | val_ll: -0.7004
[4:35:51.176835] epoch: 8850 | elbo: 29053.92189453125 | train_rmse: 0.3114 | val_rmse: 0.4447 | val_ll: -0.6985
[4:37:25.380147] epoch: 8900 | elbo: 29042.094140625006 | train_rmse: 0.313 | val_rmse: 0.4468 | val_ll: -0.7023
[4:38:58.541844] epoch: 8950 | elbo: 29084.74814453125 | train_rmse: 0.3131 | val_rmse: 0.4449 | val_ll: -0.7018
[4:40:32.299053] epoch: 9000 | elbo: 29101.59013671875 | train_rmse: 0.3108 | val_rmse: 0.444 | val_ll: -0.6931
[4:42:07.091287] epoch: 9050 | elbo: 29303.950117187505 | train_rmse: 0.3112 | val_rmse: 0.4438 | val_ll: -0.6952
[4:43:40.940334] epoch: 9100 | elbo: 28970.592968749996 | train_rmse: 0.3124 | val_rmse: 0.4435 | val_ll: -0.6907
[4:45:15.681544] epoch: 9150 | elbo: 28968.9205859375 | train_rmse: 0.3111 | val_rmse: 0.4445 | val_ll: -0.7008
[4:46:49.677958] epoch: 9200 | elbo: 28886.306269531247 | train_rmse: 0.3121 | val_rmse: 0.4436 | val_ll: -0.6985
[4:48:24.413704] epoch: 9250 | elbo: 28810.268437499995 | train_rmse: 0.3112 | val_rmse: 0.4441 | val_ll: -0.6965
[4:49:57.981319] epoch: 9300 | elbo: 29490.998906250003 | train_rmse: 0.3112 | val_rmse: 0.4428 | val_ll: -0.6943
[4:51:32.685404] epoch: 9350 | elbo: 29675.66916015624 | train_rmse: 0.3117 | val_rmse: 0.4434 | val_ll: -0.6845
[4:53:04.187829] epoch: 9400 | elbo: 28880.2255859375 | train_rmse: 0.3115 | val_rmse: 0.4451 | val_ll: -0.6906
[4:54:37.081028] epoch: 9450 | elbo: 29131.38787109375 | train_rmse: 0.3102 | val_rmse: 0.4435 | val_ll: -0.6874
[4:56:10.109683] epoch: 9500 | elbo: 28763.431875000002 | train_rmse: 0.3107 | val_rmse: 0.4443 | val_ll: -0.6915
[4:57:45.138531] epoch: 9550 | elbo: 29189.408515624993 | train_rmse: 0.3105 | val_rmse: 0.4427 | val_ll: -0.6823
[4:59:17.512141] epoch: 9600 | elbo: 28747.088027343747 | train_rmse: 0.3112 | val_rmse: 0.4429 | val_ll: -0.6789
[5:00:50.863681] epoch: 9650 | elbo: 28914.282109375003 | train_rmse: 0.3122 | val_rmse: 0.4445 | val_ll: -0.6891
[5:02:23.059926] epoch: 9700 | elbo: 28945.84619140625 | train_rmse: 0.3119 | val_rmse: 0.443 | val_ll: -0.694
[5:03:57.447008] epoch: 9750 | elbo: 28943.9258203125 | train_rmse: 0.3113 | val_rmse: 0.4432 | val_ll: -0.6926
[5:05:32.157715] epoch: 9800 | elbo: 29062.980644531257 | train_rmse: 0.3111 | val_rmse: 0.4429 | val_ll: -0.6864
[5:07:06.296680] epoch: 9850 | elbo: 28838.376953125 | train_rmse: 0.3125 | val_rmse: 0.4424 | val_ll: -0.6849
[5:08:41.039080] epoch: 9900 | elbo: 28941.12794921875 | train_rmse: 0.3106 | val_rmse: 0.4417 | val_ll: -0.6855
[5:10:13.740879] epoch: 9950 | elbo: 28881.952109375 | train_rmse: 0.3113 | val_rmse: 0.4423 | val_ll: -0.6787
