Start time: 2023-07-06 14:05:22.971480
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:3 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.001 TRAIN_SIZE: 20000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.1835,  0.1500, -0.3897,  ...,  0.3757, -0.0117,  0.2174],
        [ 0.1416,  0.1143, -0.9134,  ..., -0.1834,  0.9524,  0.0090],
        [ 0.1579, -0.1001, -0.0361,  ...,  0.3049,  0.0464, -0.4223],
        ...,
        [-0.3368,  0.1723, -0.0942,  ...,  0.1908, -0.1991,  0.4137],
        [-0.3908,  0.1906, -0.1231,  ..., -0.3708,  0.4022,  0.7265],
        [ 0.0302, -0.0700,  0.4324,  ...,  0.1134, -0.2829, -0.6205]],
       device='cuda:3', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        ...,
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-1.7823e-01,  5.0294e-01, -5.8265e-01,  1.3545e-01, -1.3441e-01,
         7.4781e-02, -4.0722e-01, -3.0516e-02,  6.6712e-01, -5.9947e-02,
         6.5102e-01, -5.5580e-01,  4.0616e-01,  5.3144e-01,  3.7917e-01,
        -3.8019e-02, -3.0921e-01, -1.0750e-01, -2.1006e-02, -7.8506e-02,
        -2.1181e-01,  3.8762e-01, -5.6436e-02, -8.5820e-02,  9.6550e-02,
        -1.6387e-01, -1.8363e-01,  1.4983e-01,  3.3374e-01,  2.3622e-01,
         2.6034e-01, -1.3371e-01, -4.1936e-01, -8.8920e-02,  4.0596e-03,
        -5.7847e-01,  4.8493e-02, -1.3689e-01,  2.4492e-01,  4.4066e-04,
        -1.0737e-01, -1.4807e-02, -1.1360e-02, -1.3731e-01, -1.3159e-02,
         3.7334e-01, -2.6897e-01,  1.5575e-01, -1.3909e-01,  1.5582e-01,
        -5.6248e-02,  2.3335e-01, -8.2304e-02,  3.9618e-01,  1.7103e-01,
         2.5977e-02,  6.3141e-02,  1.0835e-01, -3.9917e-01,  1.7405e-01,
         3.7544e-01, -2.5445e-01, -6.6561e-02, -4.5028e-02,  1.7191e-01,
         2.1605e-01, -4.3711e-01, -1.2801e-01,  1.9165e-01,  4.8792e-01,
        -5.8266e-01, -3.8354e-01,  4.4461e-02,  5.3406e-01,  5.3733e-01,
         2.1702e-02,  4.6778e-01,  1.0149e-01,  1.6570e-01,  5.7628e-02,
         3.6372e-02,  6.5085e-02, -4.9143e-01, -5.6108e-01,  2.7023e-01,
        -7.7419e-02,  1.2483e-01,  6.0247e-02,  1.4922e-01, -1.7925e-01,
        -5.3547e-01,  7.9142e-01,  3.5786e-01,  1.8893e-02,  4.7649e-01,
         1.1538e-01, -7.1701e-01,  8.9792e-02, -6.7074e-01,  1.1091e-01,
         1.3316e-01, -1.1757e-01, -6.5029e-01, -1.2018e-01, -3.0556e-01,
         1.8410e-01, -5.9650e-01, -2.5510e-01, -5.9243e-01,  3.6801e-01,
        -3.7799e-01,  1.4594e-01, -5.2876e-01,  2.5049e-01, -3.8332e-01,
         3.4906e-01, -2.2868e-01, -1.3695e-01, -1.2946e-01, -3.4877e-02,
         3.8910e-01, -2.2079e-01, -5.8160e-01, -1.0523e-01,  6.1920e-01,
        -5.5133e-01, -1.3600e-01, -4.2393e-01,  9.0929e-03,  3.2082e-01,
         2.6637e-02, -1.5651e-01, -2.0505e-01,  6.5848e-01,  4.0317e-01,
         5.6654e-01, -3.4408e-01,  3.6509e-01,  2.7423e-01, -4.4687e-01,
         1.2560e-01,  3.4853e-01,  5.3357e-02,  6.9369e-03, -5.7181e-01,
        -1.6457e-01,  4.5253e-01, -2.2041e-01,  4.8412e-01, -1.3008e-01,
        -1.1653e-01, -6.2746e-02, -1.8840e-01, -2.0879e-01, -2.0041e-01,
        -4.6775e-02,  1.0068e-01, -1.4608e-01,  2.0044e-01,  6.8071e-01,
         2.7867e-01,  3.0461e-01,  2.6189e-01, -1.2851e-01,  2.6575e-01,
         7.6081e-02,  1.1064e-01,  5.6273e-01, -3.6695e-01,  6.9775e-02,
        -1.5413e-01,  2.0223e-01, -5.5553e-01, -2.8271e-01,  2.9218e-01,
        -4.1837e-01,  1.5541e-01,  3.4703e-01, -1.6147e-01,  2.6572e-01,
         2.0150e-01, -2.0140e-01,  5.8032e-02, -3.0731e-01, -3.2231e-01,
        -2.9253e-01,  3.9034e-01, -3.0781e-01,  2.1896e-01, -1.4347e-01,
        -2.0774e-01, -1.9129e-02, -7.8975e-02,  6.1907e-02,  2.0551e-01,
         7.5416e-01, -3.6009e-01, -3.9572e-01, -1.6817e-02, -2.9420e-01,
         2.0326e-01, -1.0500e-01,  2.6212e-01, -5.6098e-02,  3.0379e-01,
        -7.1649e-01,  2.5382e-01,  6.4169e-02, -2.4733e-02, -3.3224e-01,
         1.7891e-01, -1.1408e-01,  4.3750e-01,  6.1905e-01, -3.2717e-01,
         2.8304e-01,  5.8473e-02,  4.1732e-02, -4.1793e-01,  3.1945e-02,
         5.2301e-01, -4.7642e-01,  3.0189e-01, -1.0101e-02, -3.8748e-01,
         1.9294e-02, -1.9180e-01,  1.3966e-01,  6.0038e-01,  6.9607e-01,
        -1.7796e-01, -2.8247e-01,  1.5151e-02,  1.1559e-01, -2.4949e-01,
         2.4764e-01, -1.8608e-01, -1.8710e-01,  2.1646e-01, -5.1569e-01,
        -3.3553e-02,  2.0670e-01,  4.7244e-01, -5.4790e-02,  3.4801e-01,
         4.7386e-01,  7.4456e-02, -3.4960e-02,  1.3684e-02, -1.9884e-02,
         4.7523e-01,  3.1636e-01,  2.8267e-01, -6.1449e-01,  2.6332e-01,
        -4.5052e-01,  3.3751e-01, -1.7049e-01, -3.5623e-01, -7.4953e-01,
         2.8320e-02, -1.2254e-01,  5.1217e-01, -3.9512e-01, -4.9254e-02,
         2.5675e-01,  1.4440e-01,  8.9358e-02,  1.6988e-01,  2.8435e-01,
        -1.3521e-01,  2.9503e-01, -1.1815e-01, -3.0179e-01,  1.8413e-02,
         7.8128e-02,  1.9856e-01, -9.7554e-02, -6.9772e-01,  5.0231e-01,
         5.4229e-01,  3.0727e-01,  7.2248e-01, -2.5087e-01,  5.6369e-02,
         3.8292e-01, -4.4517e-02, -6.4815e-02, -6.2449e-02, -1.6913e-01,
         1.1095e-01,  2.5666e-01,  2.5901e-02, -3.2260e-01, -3.3425e-01,
         7.6750e-02, -4.5165e-01, -5.5429e-01,  1.4742e-01, -3.9479e-01,
        -6.6243e-02,  4.2477e-01,  3.3378e-01, -2.2579e-01,  1.1150e-01,
         2.1304e-01, -4.4511e-01,  6.6918e-01, -2.6922e-01, -1.6936e-01,
        -1.3958e-01, -3.6294e-01, -3.5499e-01,  3.1946e-01, -3.3084e-01,
        -8.8676e-02, -2.9500e-01, -3.7574e-02, -3.6834e-01, -2.2618e-01,
         1.5549e-01, -7.3923e-01, -1.8274e-01, -2.2317e-01, -5.4449e-01,
         2.2825e-01,  6.1362e-01, -6.3564e-01,  2.0346e-01, -2.5278e-01,
        -1.8720e-01,  6.0705e-01,  7.6132e-02, -2.4135e-01, -1.2162e-01,
         3.7631e-01,  1.3138e-01,  1.2385e-01,  7.0855e-01,  2.7663e-01,
        -3.2371e-03,  4.4839e-01,  1.0249e-01, -2.4125e-01,  1.8269e-01,
        -1.8524e-01,  6.7911e-02,  1.8687e-01, -3.3226e-01,  2.7731e-01,
        -6.1685e-02, -6.4994e-01, -2.7982e-01,  5.6103e-01,  1.2059e-01,
         1.0904e-01, -2.7520e-01, -2.3984e-01, -2.0639e-01,  9.0588e-02,
         3.5252e-01,  8.6946e-03, -2.7902e-01, -2.1374e-01,  2.9388e-01,
         4.4775e-02, -3.5772e-01,  8.5682e-02,  9.1087e-02, -2.2566e-01,
         4.2784e-01,  2.5430e-01, -1.4201e-02, -4.1120e-01,  1.5796e-01,
        -1.0250e-01,  1.9909e-03,  2.4264e-01, -3.5435e-01,  9.4867e-02,
        -2.6800e-01, -2.3458e-02,  4.8180e-01,  4.3842e-01, -4.5777e-01,
        -5.3493e-01,  2.5293e-01, -1.7988e-01, -3.4911e-01,  3.7370e-01,
         7.1683e-02,  1.4495e-01, -3.0386e-01,  2.9548e-01,  1.9367e-01,
        -6.5042e-02, -3.4505e-01, -2.1409e-01,  5.1218e-01,  1.3953e-01,
        -1.9668e-01, -2.3720e-01, -2.8874e-01, -4.9342e-01,  1.4879e-01,
        -1.6595e-01, -4.6694e-02, -7.1695e-02,  1.4346e-01,  6.3961e-01,
         9.5283e-02,  2.3878e-01, -6.3243e-02,  1.9631e-03, -3.1289e-01,
         3.4219e-01,  1.4886e-01, -6.9626e-02, -3.6541e-01, -2.1746e-01,
        -2.1174e-02, -6.0332e-02,  2.1608e-01,  1.0489e-01,  4.5518e-01,
        -2.5834e-01, -2.4532e-01, -5.9218e-01, -4.0317e-01, -1.4641e-01,
         1.7277e-01, -2.5228e-01, -1.4424e-01,  8.9234e-02,  2.8347e-01,
         5.5036e-01, -1.1967e-01,  1.0841e-02,  1.4467e-01, -3.9940e-01,
        -8.6467e-02,  1.7574e-01,  1.5059e-01,  1.5513e-01, -2.8242e-02,
         2.0815e-01,  1.6971e-01, -1.3287e-01,  3.4010e-01,  1.1939e-01,
        -2.7150e-02, -4.6605e-01, -1.4778e-01,  1.9036e-01, -3.3692e-01,
         1.9554e-02, -9.2924e-03,  3.9853e-02,  1.3913e-01,  2.6812e-01,
        -8.9671e-02,  5.4844e-01,  8.7221e-02,  2.6845e-01,  4.7170e-02,
         6.9888e-01, -1.1909e-01, -2.0987e-01, -3.6266e-01,  1.2260e-01,
        -4.1352e-01, -2.1393e-01, -2.5586e-01,  3.3608e-01, -1.7097e-01,
        -2.1286e-01,  5.1641e-01,  4.9373e-01, -1.8716e-01,  5.7447e-01,
         1.9947e-01,  4.8937e-02,  2.3979e-01,  3.8402e-02, -2.4368e-01,
         2.6271e-01, -7.3902e-02, -8.6151e-01,  3.6611e-01,  6.4215e-01,
         3.8981e-01, -1.4609e-01,  4.2651e-01,  5.7394e-03, -3.9965e-01,
         2.3539e-02,  3.8235e-01, -6.5336e-02, -9.7194e-02,  3.0629e-01,
         9.4631e-01,  3.5458e-01,  5.3717e-01,  1.3678e-01, -6.8224e-01,
        -9.7560e-02,  4.5199e-02, -4.2750e-01, -1.9899e-01, -4.2412e-01,
         2.3418e-02,  2.3419e-01], device='cuda:3', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1477, -0.0802,  0.0741,  ..., -0.2312, -0.2779,  0.0872],
        [ 0.0822, -0.2462, -0.3271,  ..., -0.0950,  0.1078,  0.6187],
        [ 0.3417,  0.1422, -0.1896,  ..., -0.0823,  0.2866,  0.0515],
        ...,
        [ 0.5165,  0.3619, -0.5512,  ...,  0.5884,  0.1077, -0.6799],
        [-0.1900,  0.3652,  0.2294,  ..., -0.0391,  0.1073,  0.3460],
        [-0.6463,  0.2119,  0.2474,  ..., -0.1109,  0.2080, -0.3046]],
       device='cuda:3', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        ...,
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-1.7304e-02, -7.4371e-01,  4.8193e-01, -5.7575e-01, -1.4242e-01,
         9.0947e-02, -2.8964e-01,  3.3412e-01,  5.8711e-01, -4.6437e-01,
         5.2750e-01, -4.8782e-02,  3.2854e-01, -5.6577e-01, -9.4810e-02,
        -7.4957e-01, -8.9866e-02, -7.3507e-01,  1.0449e-01,  6.8090e-02,
        -6.3063e-01, -1.0982e-01,  5.9980e-01, -4.0583e-01,  2.5301e-01,
        -4.0862e-01,  2.2811e-01, -1.4941e-01, -4.2616e-01,  1.9282e-01,
        -2.6936e-01, -3.7600e-02, -3.2158e-01,  4.3963e-01, -2.3129e-01,
        -2.4136e-01,  9.2466e-02, -1.9063e-01,  2.4385e-01,  5.5556e-02,
        -4.9878e-02, -2.6260e-02,  1.9846e-01,  1.1003e-01,  1.4934e-01,
        -1.1645e-01,  5.2751e-01, -3.7193e-01,  3.3210e-01, -4.8605e-01,
         7.3373e-02,  2.1925e-01,  1.8722e-01, -3.3734e-01,  2.8190e-02,
        -3.4027e-01,  1.9335e-01,  1.8271e-01,  3.5423e-01,  4.2895e-01,
        -4.2429e-02,  3.7924e-01,  1.4194e-02, -8.8860e-02,  3.6483e-01,
         2.6001e-02,  3.1037e-01,  4.8087e-02,  4.9200e-02, -1.2883e-01,
        -1.4996e-01,  3.0200e-01,  2.5841e-01, -3.9988e-02, -1.3636e-02,
        -1.0147e-02, -2.5595e-01, -5.8228e-02,  3.4675e-01,  1.4846e-01,
        -1.1782e-01, -4.1372e-01, -1.9294e-01, -3.8107e-01,  5.3011e-01,
         4.3008e-01,  3.6775e-01,  3.1596e-01, -7.3831e-01,  6.5674e-01,
         1.7591e-01, -2.2728e-01,  2.1152e-01, -7.6497e-02, -7.9657e-01,
        -3.5461e-01, -2.9145e-01,  3.2658e-01,  3.0738e-01, -4.1942e-01,
         1.4252e-01, -2.0938e-01,  2.4464e-01, -2.5563e-01,  5.5339e-02,
         2.6314e-02,  4.4030e-01,  2.9581e-01, -1.9017e-01,  4.9778e-01,
        -3.2663e-01, -3.4342e-01, -4.7281e-02,  3.4712e-01,  2.2902e-01,
         2.6529e-01,  4.6699e-01, -6.6848e-02,  6.7599e-01,  1.1846e-01,
         1.4533e-01,  2.3124e-01,  1.3194e-01,  2.9267e-01,  1.2637e-01,
         7.5708e-01, -5.3138e-01, -3.7185e-01, -1.7431e-01, -2.8134e-01,
         2.5362e-02, -1.7231e-01,  2.3303e-01,  3.5459e-01, -1.0472e-01,
        -1.2351e-01, -1.2743e-02, -8.9211e-02,  7.6929e-02,  4.1401e-01,
         2.8890e-03,  4.9753e-01,  4.9041e-04,  3.3155e-01, -6.3475e-01,
         2.4530e-01, -7.0779e-02,  1.4113e-01, -3.1092e-02,  2.9879e-01,
        -7.7595e-01,  4.5914e-02,  4.6827e-01,  1.3830e-01, -6.2100e-01,
         1.0129e-01, -8.7237e-01, -2.7344e-01, -2.4382e-01,  1.9606e-02,
        -2.5932e-02, -1.3276e-01, -6.0999e-02, -2.2408e-01, -1.2813e-01,
         4.5391e-01,  3.5208e-01,  4.4332e-01, -4.3483e-03,  2.1009e-01,
        -1.1654e-01, -3.2487e-01, -2.3709e-01, -1.2858e-01, -1.8470e-01,
        -4.2830e-01, -2.7153e-01,  1.8645e-01,  2.7568e-01,  4.6523e-01,
        -7.9293e-01, -4.0920e-01, -3.9577e-01,  1.4098e-01, -1.1561e-01,
         9.9970e-03,  2.0233e-02, -4.4423e-01,  5.0118e-01, -4.0907e-01,
         3.0411e-01,  3.3983e-01,  4.8734e-01, -1.0515e-01, -1.9493e-01,
         1.2675e-01, -1.3493e-01,  5.3422e-01, -6.8750e-01,  6.2515e-01,
        -3.3760e-01,  2.3088e-01, -1.8721e-01, -1.2790e-01,  4.6638e-01,
         5.6233e-01,  2.3614e-01, -3.6404e-01,  5.4913e-01, -2.8064e-01,
        -1.2519e-01,  2.2452e-02,  5.4213e-01, -1.6696e-02,  3.1207e-03,
         9.1642e-02,  8.9236e-02, -1.7071e-01,  2.0936e-01, -5.7836e-02,
        -3.4964e-01,  6.7236e-02,  2.9235e-01,  5.0427e-01, -2.0986e-01,
         1.6906e-01,  1.8073e-01,  2.1306e-01, -1.3690e-01,  1.0679e-01,
         1.7766e-01,  2.6185e-01,  1.9269e-01, -6.3151e-01,  7.1693e-02,
         3.0994e-01, -3.4274e-01, -1.1084e-01, -4.6322e-01,  4.6479e-01,
         5.1805e-01,  2.2691e-01,  1.4708e-01, -1.4369e-01,  2.3180e-01,
        -6.0684e-01, -3.4615e-01, -9.8005e-02, -1.4337e-02,  2.3830e-01,
        -2.7290e-01,  5.0760e-01, -4.4781e-02, -4.6378e-02, -3.9196e-01,
         3.4800e-01,  3.4621e-01, -5.9266e-02, -6.3678e-01, -2.7233e-01,
        -9.5937e-02, -3.8816e-02,  2.7815e-01,  1.0940e-02,  2.3537e-01,
        -2.3845e-02, -5.0769e-02,  6.3709e-02, -9.4922e-02,  6.9529e-01,
        -3.8007e-01, -3.6726e-01,  1.5587e-01, -1.4668e-01, -2.9612e-01,
        -2.0268e-01,  3.0045e-01,  5.0118e-01,  1.0445e-01, -4.3017e-01,
        -1.3133e-02, -4.7267e-01,  1.0596e-01,  3.4572e-01,  5.6988e-01,
        -3.9530e-01,  1.7752e-01, -1.8111e-01, -2.1140e-01,  5.0470e-01,
         3.6142e-01, -1.8509e-01, -2.0833e-01, -2.4364e-04,  2.7748e-01,
        -3.9475e-01, -1.4549e-01, -7.9127e-02,  5.1687e-01,  1.2715e-02,
         1.6593e-01, -1.5096e-01,  2.1595e-01, -3.0374e-01,  2.2438e-01,
         1.1915e+00, -1.7656e-01,  2.1653e-01, -4.3415e-01, -8.2995e-02,
         9.6594e-02,  7.7716e-03,  7.6154e-01,  2.1252e-01, -2.5723e-01,
        -2.2339e-02, -2.8503e-01, -6.7069e-01, -6.0677e-01,  2.8146e-01,
         3.1510e-01,  2.1616e-01,  2.0367e-01,  1.6425e-01, -2.1851e-01,
         1.5391e-01, -3.7612e-01, -3.1791e-01,  3.8760e-01, -4.1480e-01,
        -1.6114e-01, -2.5449e-01, -4.5605e-01, -9.8087e-02,  7.9350e-03,
        -4.3156e-01,  5.3155e-01,  1.1264e-01,  2.4581e-01,  1.2908e-01,
         2.5088e-02, -1.9425e-01,  1.8834e-01, -4.1726e-01,  1.9716e-01,
         2.2442e-02,  1.7662e-01, -3.2100e-02, -4.5868e-01,  3.1658e-02,
        -1.5456e-01, -4.6440e-01,  9.0809e-02,  2.2375e-01,  4.6116e-01,
         1.5411e-01, -2.9951e-01,  2.5007e-01, -5.0178e-01, -1.6389e-01,
        -2.7900e-01,  5.3930e-02,  6.6744e-02, -3.2384e-01,  9.5226e-02,
        -5.5849e-01,  4.2804e-01,  3.5015e-02, -1.1938e-01, -1.4371e-01,
        -4.8624e-01, -3.8124e-01,  8.9776e-01, -8.2407e-02,  2.7250e-01,
         1.1459e-01,  1.2936e-01,  2.5598e-01,  1.4239e-01,  6.7955e-02,
         4.4034e-02,  1.0860e-01, -4.9753e-01,  6.5137e-02, -1.1185e+00,
        -1.4087e-01,  3.1329e-01,  9.7088e-02, -1.0664e-01, -4.4843e-01,
         4.8064e-01, -3.4439e-01, -2.5299e-01,  4.7642e-01, -9.8902e-02,
         8.9830e-02, -1.5237e-01,  1.5722e-02, -3.6142e-01,  1.1983e-01,
         1.0132e-01,  8.7161e-02,  1.2674e-02,  2.4100e-01,  3.2719e-01,
         1.7056e-01, -3.3684e-02, -5.5951e-02, -1.2346e-01,  9.4406e-02,
        -1.3117e-02,  3.2713e-01, -7.7023e-02, -1.4104e-01,  3.4217e-01,
         1.7002e-01,  2.4150e-01,  3.5155e-01,  1.3955e-01,  2.2147e-01,
        -2.7118e-01,  1.1829e-02,  2.2552e-02,  2.1026e-02,  2.3369e-02,
         5.2346e-01,  2.8261e-02, -2.6856e-01, -3.8788e-01, -3.6790e-01,
        -2.5159e-01,  1.4458e-01,  2.6003e-01, -6.3867e-02,  7.0197e-02,
        -1.3925e-01,  3.9376e-03, -5.1241e-02,  5.1188e-01,  1.5008e-01,
         4.6375e-01, -7.4895e-02, -3.1592e-02,  2.6546e-01,  1.5406e-01,
         6.1295e-01, -5.8630e-01, -1.0401e-01,  5.7565e-01,  3.0136e-01,
         2.0434e-01,  7.4667e-02,  5.2513e-01,  1.7537e-01, -2.3197e-01,
         3.9474e-01, -3.0074e-01, -1.3381e-01,  1.9023e-01, -1.5827e-01,
        -1.7900e-01,  3.2330e-01,  2.9406e-01, -5.1262e-01, -4.6590e-01,
         2.8354e-01, -5.7071e-01,  1.6701e-01,  3.4173e-01, -1.7136e-02,
         2.9176e-01, -1.0677e+00, -2.3840e-01, -2.3391e-01,  3.9123e-01,
         8.9054e-03,  1.6719e-01, -7.3138e-02,  3.1225e-01, -1.4398e-01,
         4.2196e-01, -4.3965e-01, -1.7356e-01,  5.5655e-01,  2.6707e-01,
        -8.8729e-02, -2.0944e-01,  1.4981e-01,  2.7648e-01,  3.8481e-01,
        -1.4325e-01,  4.7390e-01,  4.9885e-01, -1.0093e-01, -2.6477e-02,
         8.3113e-01,  1.4151e-01, -4.6067e-01, -2.1335e-01, -1.3827e-01,
        -1.9782e-01,  3.5200e-01,  4.1190e-02, -3.3886e-01, -3.2043e-01,
         1.8600e-01,  9.3158e-02, -1.0647e-01,  2.4555e-01,  2.5628e-01,
        -2.3356e-01,  2.1775e-01], device='cuda:3', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.2788,  0.4712, -0.3558,  ...,  0.0852,  0.0957,  0.2607],
        [-0.1484, -0.2478,  0.0234,  ...,  0.0594,  0.5778,  0.1728],
        [-0.1258, -0.0337,  0.0160,  ...,  0.0029,  0.5243,  0.3411],
        ...,
        [-0.1701,  0.2458, -0.2898,  ..., -0.4678, -0.1546, -0.2172],
        [-0.3523,  0.2947, -0.2264,  ..., -0.2111,  0.4472, -0.2963],
        [-0.0756, -0.2880,  0.2236,  ..., -0.1032,  0.3290,  0.5057]],
       device='cuda:3', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        ...,
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010],
        [0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0010, 0.0010]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6095e-01, -2.2319e-01,  1.1103e-01, -2.3292e-01,  6.2166e-06,
        -1.5685e-01, -8.2054e-01, -3.1450e-01, -3.6688e-01, -2.4558e-01,
        -1.1119e-01,  3.1010e-01,  4.9447e-01,  5.8524e-02, -2.0650e-01,
        -3.1385e-02, -5.9704e-03,  5.4822e-01,  2.6223e-01,  5.0320e-01,
        -9.8184e-02, -7.3414e-02, -7.1630e-01, -9.9733e-02,  2.7497e-03,
         2.6648e-01,  2.8489e-01, -4.2355e-01, -2.4236e-01,  2.1705e-01,
        -5.7162e-01, -1.0769e-01,  7.3977e-02,  3.0081e-02, -4.1515e-01,
        -4.7219e-01,  2.1122e-01,  4.0944e-01, -1.2078e-01,  1.7836e-01,
         1.0095e+00,  4.1193e-01,  3.1660e-01,  3.8632e-01, -2.9621e-01,
        -2.5063e-01, -9.8408e-02, -5.0758e-01,  2.3404e-01, -1.0582e-01,
         2.3921e-01, -3.2840e-01, -5.1646e-01,  1.3984e-01,  1.0987e-01,
         2.7113e-01,  4.0299e-01, -5.9984e-01,  2.9657e-01, -1.2630e-01,
        -8.4542e-02, -2.4016e-01, -6.3478e-01, -7.7057e-01,  8.2179e-03,
         1.3906e-02, -4.2099e-01, -4.7980e-01, -3.7086e-02,  6.4549e-02,
         8.9574e-02, -3.0327e-01,  6.2960e-02, -2.8747e-01,  4.0641e-01,
         2.7348e-01,  2.7422e-01, -4.6330e-02,  8.4286e-03,  5.7026e-03,
        -1.4329e-01, -1.0820e-01,  2.9727e-01, -1.3750e-01, -3.8899e-01,
        -3.9868e-01, -1.2433e-01,  8.4280e-02, -2.8246e-01,  3.9499e-01,
         4.8705e-02, -9.7446e-02,  2.1331e-01,  8.5474e-01, -7.5803e-03,
         1.6337e-01,  1.3875e-01, -2.0394e-01, -2.9539e-01, -8.0152e-02,
         4.4473e-01,  1.4239e-01,  4.7914e-02, -5.3566e-01,  8.5798e-02,
        -3.1580e-01, -1.6829e-01,  4.4699e-01,  2.0898e-02, -1.8093e-01,
        -3.4566e-01, -7.8551e-01, -4.5781e-01, -1.2586e-01,  3.2521e-01,
         1.1650e-01,  3.1360e-01,  1.7587e-01,  8.2772e-02,  2.6422e-01,
        -4.4335e-01, -5.5629e-02,  1.6321e-01, -5.7374e-02,  1.6760e-01,
        -6.6611e-01, -4.6156e-01, -2.9009e-01, -3.4648e-01,  2.5283e-02,
        -6.7641e-01, -1.5736e-01, -1.8773e-01, -3.5351e-02, -2.5819e-01,
        -9.7859e-02,  1.2043e-01,  7.5777e-01, -3.3025e-01,  1.6098e-01,
        -4.6054e-01, -6.1820e-01, -1.3218e-01,  3.4195e-01, -9.2533e-02,
         9.5837e-02, -2.7968e-01,  1.9094e-01,  6.6922e-01, -2.3402e-02,
         4.0333e-01, -3.1954e-01, -1.2860e-01, -1.4148e-01,  8.7744e-03,
        -4.2406e-01, -1.1015e-01, -2.4406e-01,  6.5505e-02, -2.3290e-01,
         1.0324e-01, -2.6453e-01,  1.2824e-01,  3.1069e-01,  2.4128e-02,
         2.8166e-01,  2.7599e-02,  3.8887e-01, -3.8325e-01,  2.3562e-02,
        -2.0804e-01, -3.0988e-01,  3.6945e-01, -1.4312e-02,  3.7924e-01,
        -3.8777e-03, -4.3181e-02, -3.5745e-01, -3.6740e-02,  3.6153e-01,
         3.5577e-01, -1.3028e-01,  3.1018e-01, -2.7162e-02, -6.7490e-01,
        -3.9503e-01,  1.4966e-01, -2.2544e-02, -6.3600e-03,  2.3867e-01,
         5.6061e-02,  6.8928e-01,  5.2488e-01, -1.1731e-01, -7.3006e-02,
         2.1148e-01,  5.1771e-01, -4.9143e-01, -2.4439e-01, -3.5833e-01,
        -8.8262e-03,  4.1680e-03, -8.3199e-02, -6.7976e-01, -3.4704e-01,
         3.3973e-01,  2.3142e-01,  2.7203e-01, -1.9648e-01,  7.6699e-02,
         1.2691e-02, -2.2158e-01, -2.5528e-01, -1.4726e-01, -3.7426e-02,
         9.5982e-02, -3.5082e-01, -4.1665e-01, -1.3127e-01, -1.8484e-01,
        -4.5049e-01,  1.1465e-01, -3.9104e-01,  1.3391e-01, -1.2047e-01,
         2.3434e-02,  7.0874e-01, -8.7442e-01, -1.0756e-01,  1.1349e-01,
        -4.0147e-01, -4.4457e-01, -1.1596e-01, -4.8422e-01,  1.4537e-02,
         6.7207e-02,  1.1576e-01,  2.3008e-01, -1.6026e-01, -9.9004e-02,
        -4.3666e-01,  5.7932e-01,  2.2296e-02, -1.1045e-01,  2.0033e-01,
         1.7564e-02,  2.1350e-01, -6.8457e-01,  5.1056e-01,  9.6357e-02,
        -3.4199e-01,  7.5500e-01,  8.1252e-01,  1.0723e-03,  3.4866e-01,
        -6.2976e-01, -9.1457e-02, -4.6801e-01,  7.6142e-02,  4.0938e-01,
        -1.1158e-01,  2.2524e-01,  5.3375e-01, -9.1241e-02,  6.9274e-01,
        -2.3365e-01,  1.3238e-01,  1.7037e-01, -3.2833e-02, -1.4657e-01,
         2.9560e-01,  4.0805e-02,  2.3687e-01,  2.2300e-01, -4.3533e-01,
        -1.1714e-01, -9.1614e-02, -1.3742e-01,  3.0131e-01,  6.4255e-02,
         1.6029e-01, -1.1636e-01, -1.7743e-01,  7.8712e-02, -1.9594e-01,
        -1.2638e-01, -5.5543e-03, -3.8076e-02, -4.5537e-01,  3.5469e-01,
         3.0777e-01,  1.9096e-01, -1.9797e-01,  1.2205e-01, -6.0301e-01,
         2.1815e-01, -4.1466e-01,  2.8104e-01,  2.2488e-01,  2.1195e-01,
         6.2088e-01, -3.6156e-02,  3.3130e-01,  4.1718e-01,  1.6908e-01,
         2.7611e-01, -2.8473e-01, -4.3983e-01, -3.5778e-01, -2.0570e-01,
         3.3422e-01,  7.7666e-02,  2.7353e-01, -2.2929e-01,  1.2507e-01,
        -2.6608e-01,  2.7955e-01, -6.9187e-02,  2.0744e-01,  6.2913e-01,
        -4.7449e-02, -7.7963e-02, -4.2061e-02,  1.1904e-01,  2.7021e-01,
         2.4009e-01, -2.0361e-01, -5.1800e-01,  4.2063e-01,  7.7571e-02,
        -3.7487e-01, -1.0334e-01, -2.8719e-01,  1.6545e-01,  6.4722e-01,
         2.0639e-01,  6.9956e-02, -1.2094e-01,  1.9477e-01, -2.2353e-01,
        -2.8806e-01, -5.5906e-02, -4.3698e-01,  2.4996e-01, -3.2311e-01,
        -7.6289e-03,  3.2002e-01,  5.8519e-02, -3.7117e-01,  1.3092e-01,
        -3.5996e-01, -3.2252e-01,  8.4701e-02, -1.9236e-01, -1.1628e-01,
        -1.6623e-01,  1.3229e-01,  3.9279e-02, -3.4568e-01,  2.8687e-01,
         1.0365e-04,  3.4252e-01, -4.1116e-01,  1.1817e-01, -6.7696e-02,
        -9.3700e-02, -3.3893e-02, -5.0696e-01,  4.5188e-01,  5.5303e-02,
        -2.3416e-01,  2.4825e-01,  4.7991e-01, -8.8871e-02,  1.6317e-01,
        -4.9000e-02,  6.1902e-02,  3.1430e-01, -4.0040e-01,  2.2202e-01,
         5.3194e-01, -6.7501e-01, -4.1207e-02, -2.2033e-01, -5.4734e-01,
        -3.7292e-01, -2.1342e-02, -2.9803e-01,  3.5128e-01,  4.5594e-01,
         1.0907e-01, -3.4712e-01, -1.0442e-01,  4.1210e-02, -1.7895e-01,
         2.1669e-01, -5.9524e-01, -4.8311e-01,  1.7753e-01,  1.1325e-01,
        -3.0399e-01,  1.1610e-01,  5.3027e-01, -3.3722e-01,  2.2936e-02,
        -4.5464e-01, -2.3298e-01, -3.1652e-02,  4.0282e-01,  4.0792e-01,
         4.4835e-02,  1.8021e-01,  2.1295e-01, -1.3891e-01,  2.5734e-01,
        -3.9804e-01,  3.5444e-02, -2.1211e-01, -2.1667e-01, -8.0195e-01,
         4.3522e-01, -6.2927e-03,  3.0222e-01, -6.3260e-02, -1.7499e-01,
        -4.2224e-01, -2.0100e-02, -1.7991e-01, -2.7192e-01, -9.7680e-02,
         6.7486e-01, -1.2820e-01,  1.9117e-01, -2.1720e-01,  4.1311e-01,
        -3.9284e-01,  2.2391e-01, -4.3222e-02, -1.8916e-01, -1.3163e-01,
        -1.2097e-01, -2.3999e-02, -7.0565e-02,  3.2697e-01,  2.1047e-01,
         3.0648e-01,  1.0702e-01,  2.5062e-01, -1.3994e-01, -1.4183e-01,
        -3.1237e-01,  1.4234e-01, -3.8979e-01, -7.1782e-01,  4.5149e-01,
         6.1514e-02,  2.9931e-01,  4.6365e-01,  1.8079e-01,  3.4996e-02,
        -1.5266e-01, -3.2438e-02,  1.8433e-03,  3.0434e-01, -7.2029e-02,
        -1.1054e-01, -2.6927e-01, -1.2103e-01,  1.8328e-01, -2.9967e-01,
        -2.0482e-01,  5.9222e-01,  2.7238e-01, -9.4106e-02, -1.2597e-01,
        -1.4423e-03, -3.7636e-02,  2.0951e-01, -1.2990e-01, -6.9982e-02,
        -2.9833e-01,  1.3813e-01,  4.8790e-01,  4.4359e-01, -5.4234e-01,
        -2.2232e-02, -4.4852e-01, -1.9540e-01, -8.1884e-02,  2.7409e-02,
         1.1340e-01,  2.6882e-01, -1.5397e-01, -3.6771e-01,  9.0708e-02,
        -3.3893e-01, -5.7200e-01,  1.9194e-01, -2.6039e-01, -2.3555e-01,
         2.3619e-01, -1.7284e-01,  1.8537e-01,  2.6478e-01,  1.6624e-01,
        -3.5986e-01,  2.2010e-01, -1.9533e-01, -5.0837e-03, -2.3669e-01,
        -4.9835e-01, -2.9042e-01], device='cuda:3', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[-5.2712e-01, -7.0956e-02, -2.6507e-01,  4.9107e-02, -4.6840e-01,
          3.7766e-01,  6.5125e-02, -6.4483e-01, -5.1418e-02, -7.3347e-02,
          7.7586e-02,  3.6846e-01,  1.6299e-02, -5.1360e-01,  3.3335e-01,
          1.9600e-01,  1.6399e-01, -6.0144e-02,  6.7448e-02, -2.3993e-01,
          2.1158e-01, -3.7435e-01,  2.1212e-01, -4.3648e-01,  2.1342e-01,
          1.3655e-01, -9.4448e-02,  5.1290e-01, -1.8700e-01, -2.4868e-01,
          3.9635e-01, -2.9689e-02, -1.3566e-01,  2.0276e-01,  3.1499e-01,
          4.3862e-01,  2.3528e-01, -2.1874e-01, -1.7627e-01,  5.4695e-01,
         -3.4564e-01,  1.8596e-01,  5.6978e-01, -3.5967e-01, -7.6621e-01,
          2.8281e-01,  3.2153e-01,  1.0333e-01, -2.2576e-04, -2.4523e-01,
          1.8515e-01, -1.9156e-01,  8.5216e-02, -1.1440e-01,  1.3664e-01,
         -2.8783e-02, -8.3767e-02,  4.5932e-02,  2.4333e-01,  3.7524e-01,
         -1.2266e-01,  3.2491e-02,  1.3177e-01,  4.0853e-02,  5.8741e-01,
          3.3160e-01, -4.4519e-01,  1.3297e-01,  4.8815e-01, -5.8482e-02,
          1.1656e-01, -4.6473e-02,  2.9471e-01,  4.1302e-01, -2.4435e-01,
          3.3516e-01,  4.5440e-01,  7.7628e-01,  2.5442e-01, -1.7041e-01,
         -8.1643e-02, -3.6171e-01,  1.5283e+00,  7.2549e-01, -1.5073e-01,
          7.3865e-02, -1.3814e-01,  1.1274e-01, -3.3980e-01, -2.6665e-01,
         -5.2510e-01, -5.6240e-02,  5.1630e-01, -4.2648e-01,  1.3107e-01,
         -1.1780e-01, -2.7782e-01, -6.5778e-01, -3.1632e-01, -3.5595e-01,
          1.7769e-01, -4.5943e-02,  2.4815e-02, -6.7668e-02, -1.6603e-01,
          3.1003e-01,  1.5384e-01,  3.5805e-02, -3.7982e-01,  7.7088e-02,
          2.1380e-01, -5.6650e-02, -8.0646e-02,  1.5003e-01, -2.3201e-01,
          1.8431e-01, -5.9380e-02, -1.7364e-01, -4.0408e-02, -4.3388e-01,
          2.1807e-01, -3.2008e-01, -1.8544e-01,  8.6832e-02, -2.9531e-01,
         -9.8302e-01, -1.9996e-01, -7.4661e-01, -2.0033e-01, -2.7294e-01,
         -6.1487e-02, -1.7734e-01, -1.2285e-01,  5.6673e-01, -5.4452e-01,
         -3.3242e-01,  2.0020e-01, -1.1325e-01, -5.1715e-01,  6.1956e-02,
         -1.3762e-01,  3.8813e-01, -2.9038e-01,  1.9499e-01, -5.6554e-01,
         -1.2507e-01,  3.2748e-01,  4.8523e-01,  8.5386e-02,  8.2318e-02,
          4.4197e-01, -8.1378e-01, -1.3064e-02,  1.4536e-01, -2.8817e-01,
         -1.6783e-01,  2.0350e-01, -1.6474e-01,  4.7377e-01, -1.5639e-01,
         -5.9547e-01,  7.8079e-01, -5.5068e-04, -7.5995e-02, -1.3074e-01,
         -2.7026e-02,  6.6253e-01, -1.3055e-01,  7.0458e-02, -5.0365e-02,
         -1.0988e-01, -2.5686e-01, -1.2837e-01, -4.2178e-01,  5.5756e-02,
         -2.7145e-01,  1.8831e-01, -4.4553e-01, -3.6702e-01,  4.6499e-01,
          2.9777e-01, -1.1107e-01,  2.1238e-01, -1.9493e-01,  4.2616e-01,
          5.0402e-01, -2.9285e-02, -2.6706e-01,  5.1945e-01, -3.2715e-01,
         -2.3235e-01,  4.3050e-01, -2.1709e-02, -4.5890e-02,  2.3844e-01,
         -6.0082e-02,  2.2823e-01, -7.4333e-02, -1.5916e-01, -5.1889e-01,
          1.5099e-01, -8.5818e-02,  1.8366e-01,  4.6666e-02, -1.5220e-01,
          4.9711e-01,  1.0973e-04, -2.0585e-01,  1.9420e-01, -4.5441e-01,
         -2.8119e-01,  7.2892e-02,  9.6605e-01, -1.3841e-01, -1.4538e-01,
          3.8296e-01,  3.8431e-01,  1.2155e-01,  3.5919e-01,  1.9235e-01,
          1.0766e-03,  2.1961e-02,  2.2306e-03,  1.6756e-01,  5.4918e-02,
         -3.5872e-01, -1.0786e-01, -1.2222e-01, -6.2915e-02, -1.0956e-01,
         -6.4043e-01,  4.1374e-02,  5.0744e-01, -4.7316e-01, -1.3198e-01,
          7.2067e-02, -2.1112e-01, -1.5420e-01, -2.3405e-01,  2.1890e-01,
         -1.1361e+00, -2.1137e-02,  9.2953e-02,  5.6192e-01,  4.5833e-01,
          1.8510e-01,  7.0186e-02, -1.7859e-01,  1.6492e-01,  2.7981e-01,
          5.7287e-02, -4.3227e-01,  5.2469e-01, -4.9466e-01,  1.4758e-01,
          5.1209e-01,  1.2855e-03,  3.9800e-01,  1.3482e-01,  7.1103e-01,
          2.2576e-01, -2.8789e-01, -2.4486e-01,  4.8815e-01, -2.4586e-02,
          2.4282e-01,  4.7912e-01,  2.0262e-01, -9.1052e-02,  5.9088e-01,
         -2.5107e-01, -2.2955e-01,  5.8053e-02,  2.4642e-01, -1.5015e-01,
          1.0118e-01,  2.9097e-01,  2.5908e-01, -4.6526e-02,  6.6680e-01,
         -3.1072e-02, -3.3750e-01,  3.1587e-01, -3.7881e-01, -3.5255e-01,
          1.7037e-01,  6.2696e-01, -2.2562e-01,  2.4479e-01,  1.6097e-01,
          4.1574e-01, -1.2871e-01,  7.0741e-01, -2.3545e-01,  9.4130e-02,
         -2.2288e-02,  8.4580e-01, -4.8738e-02,  3.3806e-01, -3.7344e-02,
         -7.0104e-01,  3.6496e-01, -1.1175e+00, -9.0129e-02,  1.1532e-01,
          5.3138e-01, -5.5376e-01,  3.4933e-02, -4.0697e-01, -2.3859e-01,
         -4.8752e-01,  4.7204e-01,  2.8419e-01,  4.6566e-01,  2.6587e-01,
          2.5850e-02,  2.7889e-01,  2.2883e-01,  3.9242e-02,  2.2063e-03,
         -2.0491e-01,  3.1594e-01, -6.9179e-03, -6.0164e-01, -4.0042e-02,
          6.8635e-01,  1.6406e-03, -2.2828e-01,  4.2744e-01, -3.8224e-01,
         -2.5402e-01,  2.9738e-02, -5.3204e-01, -4.5358e-01, -1.1041e-01,
         -4.3004e-01, -1.2864e-01,  9.8366e-02, -4.3952e-02, -2.5774e-01,
         -5.5484e-01,  1.1773e-01, -1.4732e-01,  2.0359e-01,  3.2019e-01,
         -3.5849e-01,  1.3771e-01,  7.0644e-02,  4.1491e-01,  2.5033e-01,
         -1.9692e-01,  4.2207e-01,  1.2325e-01,  3.2770e-02,  5.7602e-01,
         -4.6690e-01,  5.8665e-01,  1.3835e-01,  3.4721e-01,  8.9881e-02,
         -3.2446e-01,  5.6094e-01,  1.5568e-03, -8.8423e-01,  5.9764e-01,
          4.3544e-01,  9.6777e-02, -1.0247e-01, -3.0208e-01,  2.9380e-01,
          2.3795e-01,  5.3790e-01,  3.7809e-01, -7.9441e-02, -5.9762e-02,
         -7.7931e-01,  3.6725e-01, -1.7359e-01,  1.5960e-02, -2.1635e-01,
         -3.2070e-01, -1.0234e-01, -5.3817e-01,  2.5714e-01, -2.8024e-01,
          1.4045e-01, -9.6703e-02, -3.6307e-02, -4.0841e-01, -7.2556e-02,
         -2.9512e-01,  9.1372e-01, -1.3275e-01,  1.8243e-01,  1.1152e-01,
          5.0070e-02,  2.7225e-01, -2.7956e-01, -2.0305e-01,  8.9187e-02,
         -1.7559e-01, -9.8309e-01,  8.5700e-02, -9.3020e-02,  1.4150e-01,
          1.9997e-01,  2.4721e-02,  4.7068e-03,  1.4871e-01,  4.3829e-01,
         -4.1308e-01,  2.8249e-02, -7.1723e-01, -2.7475e-01,  1.2950e-02,
          1.9841e-02, -3.1568e-01, -5.0631e-01,  7.3525e-01,  4.2387e-01,
          9.4272e-02, -2.7129e-01,  1.0417e-01, -2.1489e-02,  2.2844e-01,
          6.6069e-02,  1.1213e-02,  4.0196e-01, -5.2389e-01, -2.3472e-01,
          3.5398e-01, -1.2385e-01,  3.5512e-01, -1.2373e-01, -3.5143e-01,
         -5.5630e-01,  3.6632e-01,  4.8838e-01,  2.8989e-01,  3.2981e-02,
          1.0680e-01,  5.6363e-01,  1.9797e-01, -1.5344e-01, -6.2795e-02,
         -1.9439e-01, -2.0567e-01, -1.5660e-01, -5.5940e-02, -9.2299e-01,
         -2.2475e-02, -5.7838e-02,  2.2665e-02, -5.3552e-02,  3.1344e-02,
         -1.3055e-01, -3.8965e-01,  2.8472e-01,  2.4042e-01, -6.3908e-02,
         -1.4635e-01,  4.7678e-01,  4.9551e-01, -5.9208e-01,  3.3777e-01,
         -3.8621e-01, -1.5966e-01, -1.7552e-01,  1.0183e-01,  1.1439e-01,
         -1.6734e-01, -6.1672e-01, -2.5923e-03, -1.8086e-01,  2.3559e-01,
         -4.4792e-01,  2.7403e-01,  2.3048e-01, -4.2347e-02,  1.5858e-01,
          2.6751e-01, -1.8064e-01, -5.6363e-01, -3.3554e-01, -7.2343e-02,
         -4.8078e-02, -2.2422e-01,  5.0196e-01, -1.8251e-01,  6.9536e-02,
         -2.4182e-01,  4.0072e-01,  6.3814e-02,  2.3845e-01, -1.3936e-02,
          3.1882e-01,  5.6725e-02,  7.8200e-01,  3.4071e-02,  3.1432e-01,
         -1.4857e-01,  7.4095e-01, -1.8018e-01,  2.5577e-01, -1.9243e-01,
         -1.5961e-01,  4.8432e-01,  3.5048e-02, -1.4308e-01, -2.2501e-03,
          1.1579e-01,  8.7587e-02]], device='cuda:3', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 512]) tensor([[0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,
         0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010]],
       device='cuda:3', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([-0.0549], device='cuda:3', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0010], device='cuda:3', grad_fn=<AddBackward0>)
likelihood_guide.likelihood._scale.loc torch.Size([]) Parameter containing:
tensor(0.7517, device='cuda:3', requires_grad=True)
likelihood_guide.likelihood._scale.scale torch.Size([]) tensor(0.0010, device='cuda:3', grad_fn=<AddBackward0>)
Using device: cuda:3
===== Training profile tensin-3x512-sl - 2 =====
[0:00:03.601063] epoch: 0 | elbo: 23561219.624999996 | train_rmse: 71.4709 | val_rmse: 71.2563 | val_ll: -436.9171
[0:03:08.073503] epoch: 50 | elbo: 3896421.0387500003 | train_rmse: 15.0823 | val_rmse: 18.1657 | val_ll: -31.277
[0:06:13.155629] epoch: 100 | elbo: 3584574.827499999 | train_rmse: 10.2884 | val_rmse: 14.7071 | val_ll: -20.2811
[0:09:16.689515] epoch: 150 | elbo: 3436276.5387499994 | train_rmse: 7.7972 | val_rmse: 13.1312 | val_ll: -16.0227
[0:12:20.412760] epoch: 200 | elbo: 3332266.9524999997 | train_rmse: 6.1054 | val_rmse: 12.2103 | val_ll: -13.6406
[0:15:21.091557] epoch: 250 | elbo: 3251988.0175 | train_rmse: 4.8486 | val_rmse: 11.5554 | val_ll: -11.9613
[0:18:23.254430] epoch: 300 | elbo: 3181605.0162499994 | train_rmse: 3.7774 | val_rmse: 11.0296 | val_ll: -10.6818
[0:21:24.601882] epoch: 350 | elbo: 3118458.08625 | train_rmse: 2.9637 | val_rmse: 10.6853 | val_ll: -9.7626
[0:24:25.586047] epoch: 400 | elbo: 3059280.8012500005 | train_rmse: 2.3087 | val_rmse: 10.3955 | val_ll: -9.0229
[0:27:27.537301] epoch: 450 | elbo: 3002988.79875 | train_rmse: 1.7803 | val_rmse: 10.208 | val_ll: -8.5054
[0:30:32.466742] epoch: 500 | elbo: 2948266.4762500003 | train_rmse: 1.402 | val_rmse: 10.0411 | val_ll: -8.0447
[0:33:35.484358] epoch: 550 | elbo: 2894553.784999999 | train_rmse: 1.0716 | val_rmse: 9.8813 | val_ll: -7.5895
[0:36:41.501362] epoch: 600 | elbo: 2841620.454999999 | train_rmse: 0.8848 | val_rmse: 9.7125 | val_ll: -7.2934
[0:39:45.522084] epoch: 650 | elbo: 2789272.3724999996 | train_rmse: 0.7469 | val_rmse: 9.5497 | val_ll: -6.8641
[0:42:48.359413] epoch: 700 | elbo: 2737219.4862499996 | train_rmse: 0.7499 | val_rmse: 9.3512 | val_ll: -6.506
[0:45:53.400814] epoch: 750 | elbo: 2685530.4212499997 | train_rmse: 0.6468 | val_rmse: 9.1085 | val_ll: -5.9918
[0:48:55.033584] epoch: 800 | elbo: 2633768.57 | train_rmse: 0.5527 | val_rmse: 8.849 | val_ll: -5.4458
[0:52:03.290154] epoch: 850 | elbo: 2581887.9562500003 | train_rmse: 0.6203 | val_rmse: 8.5926 | val_ll: -4.9579
[0:55:07.919841] epoch: 900 | elbo: 2529799.46 | train_rmse: 0.6371 | val_rmse: 8.2977 | val_ll: -4.5719
[0:58:06.128117] epoch: 950 | elbo: 2477654.5475000003 | train_rmse: 0.7973 | val_rmse: 7.9535 | val_ll: -4.2042
[1:01:05.449532] epoch: 1000 | elbo: 2425229.72875 | train_rmse: 0.7253 | val_rmse: 7.6049 | val_ll: -3.9537
[1:04:02.976246] epoch: 1050 | elbo: 2372712.545 | train_rmse: 0.7583 | val_rmse: 7.2303 | val_ll: -3.7181
[1:07:00.672660] epoch: 1100 | elbo: 2320049.0925 | train_rmse: 0.8755 | val_rmse: 6.8233 | val_ll: -3.5513
[1:10:03.258025] epoch: 1150 | elbo: 2267058.705 | train_rmse: 0.8859 | val_rmse: 6.3773 | val_ll: -3.383
[1:13:00.574350] epoch: 1200 | elbo: 2213897.46125 | train_rmse: 0.9388 | val_rmse: 5.954 | val_ll: -3.2491
[1:16:02.866472] epoch: 1250 | elbo: 2160418.1275000004 | train_rmse: 1.0091 | val_rmse: 5.4816 | val_ll: -3.1212
[1:19:09.143365] epoch: 1300 | elbo: 2106634.5524999998 | train_rmse: 1.0698 | val_rmse: 5.039 | val_ll: -3.0242
[1:22:14.794420] epoch: 1350 | elbo: 2052587.9306249998 | train_rmse: 1.0743 | val_rmse: 4.5803 | val_ll: -2.9186
[1:25:19.704899] epoch: 1400 | elbo: 1998210.3975000002 | train_rmse: 1.1044 | val_rmse: 4.1557 | val_ll: -2.8244
[1:28:24.049406] epoch: 1450 | elbo: 1943506.6956250002 | train_rmse: 1.1024 | val_rmse: 3.7557 | val_ll: -2.7355
[1:31:25.339082] epoch: 1500 | elbo: 1888703.901875 | train_rmse: 1.0846 | val_rmse: 3.4041 | val_ll: -2.6354
[1:34:31.130771] epoch: 1550 | elbo: 1834021.4368750001 | train_rmse: 1.0433 | val_rmse: 3.096 | val_ll: -2.536
[1:37:31.846473] epoch: 1600 | elbo: 1779371.3493750002 | train_rmse: 0.9968 | val_rmse: 2.8035 | val_ll: -2.4399
[1:40:33.139583] epoch: 1650 | elbo: 1724966.919375 | train_rmse: 0.9413 | val_rmse: 2.5665 | val_ll: -2.3604
[1:43:35.473194] epoch: 1700 | elbo: 1670844.2037499999 | train_rmse: 0.9129 | val_rmse: 2.3522 | val_ll: -2.2854
[1:46:35.032831] epoch: 1750 | elbo: 1616893.9693750003 | train_rmse: 0.8817 | val_rmse: 2.1473 | val_ll: -2.2199
[1:49:35.765521] epoch: 1800 | elbo: 1563004.9906249999 | train_rmse: 0.8547 | val_rmse: 1.9906 | val_ll: -2.1617
[1:52:38.420799] epoch: 1850 | elbo: 1509443.3675000002 | train_rmse: 0.8243 | val_rmse: 1.8439 | val_ll: -2.1222
[1:55:44.297609] epoch: 1900 | elbo: 1456008.94375 | train_rmse: 0.8172 | val_rmse: 1.7269 | val_ll: -2.0825
[1:58:50.577644] epoch: 1950 | elbo: 1402620.15625 | train_rmse: 0.8122 | val_rmse: 1.6194 | val_ll: -2.0597
[2:01:51.870691] epoch: 2000 | elbo: 1349345.1975000002 | train_rmse: 0.8199 | val_rmse: 1.5326 | val_ll: -2.038
[2:04:54.308184] epoch: 2050 | elbo: 1296098.3962500002 | train_rmse: 0.8189 | val_rmse: 1.4498 | val_ll: -2.0192
[2:07:57.050197] epoch: 2100 | elbo: 1242784.34125 | train_rmse: 0.814 | val_rmse: 1.3851 | val_ll: -2.0
[2:10:59.501628] epoch: 2150 | elbo: 1189503.490625 | train_rmse: 0.8187 | val_rmse: 1.3108 | val_ll: -1.9819
[2:14:02.758168] epoch: 2200 | elbo: 1136353.4287500004 | train_rmse: 0.8295 | val_rmse: 1.2555 | val_ll: -1.9657
[2:17:06.858194] epoch: 2250 | elbo: 1083120.5087500003 | train_rmse: 0.8268 | val_rmse: 1.1947 | val_ll: -1.9472
[2:20:10.358484] epoch: 2300 | elbo: 1029953.9937500004 | train_rmse: 0.8269 | val_rmse: 1.1448 | val_ll: -1.9262
[2:23:07.567487] epoch: 2350 | elbo: 976921.945625 | train_rmse: 0.8142 | val_rmse: 1.0838 | val_ll: -1.9015
[2:26:06.736699] epoch: 2400 | elbo: 924095.9806249999 | train_rmse: 0.807 | val_rmse: 1.0302 | val_ll: -1.8775
[2:29:09.153242] epoch: 2450 | elbo: 871536.6146874999 | train_rmse: 0.7987 | val_rmse: 0.9852 | val_ll: -1.8552
[2:32:11.461881] epoch: 2500 | elbo: 819178.0603124999 | train_rmse: 0.7929 | val_rmse: 0.9493 | val_ll: -1.833
[2:35:15.003951] epoch: 2550 | elbo: 767237.29875 | train_rmse: 0.7762 | val_rmse: 0.9103 | val_ll: -1.8111
[2:38:16.567568] epoch: 2600 | elbo: 715681.4196875 | train_rmse: 0.7694 | val_rmse: 0.8841 | val_ll: -1.7933
[2:41:19.966334] epoch: 2650 | elbo: 664640.0078125001 | train_rmse: 0.7565 | val_rmse: 0.8656 | val_ll: -1.7737
[2:44:23.537551] epoch: 2700 | elbo: 614225.3346875 | train_rmse: 0.7478 | val_rmse: 0.842 | val_ll: -1.7597
[2:47:26.925621] epoch: 2750 | elbo: 564562.3803125 | train_rmse: 0.7514 | val_rmse: 0.8289 | val_ll: -1.7447
[2:50:30.737423] epoch: 2800 | elbo: 515743.62703125004 | train_rmse: 0.7409 | val_rmse: 0.8165 | val_ll: -1.7314
[2:53:38.576434] epoch: 2850 | elbo: 467933.81578125 | train_rmse: 0.7427 | val_rmse: 0.8092 | val_ll: -1.7124
[2:56:42.947894] epoch: 2900 | elbo: 421184.36890624993 | train_rmse: 0.7347 | val_rmse: 0.7954 | val_ll: -1.697
[2:59:44.379400] epoch: 2950 | elbo: 375595.02625 | train_rmse: 0.7211 | val_rmse: 0.7802 | val_ll: -1.6739
[3:02:44.765976] epoch: 3000 | elbo: 331780.38421875 | train_rmse: 0.7035 | val_rmse: 0.7609 | val_ll: -1.6487
[3:05:46.170638] epoch: 3050 | elbo: 289692.76812500006 | train_rmse: 0.6929 | val_rmse: 0.7449 | val_ll: -1.6166
[3:08:49.932455] epoch: 3100 | elbo: 249763.59921875 | train_rmse: 0.6684 | val_rmse: 0.716 | val_ll: -1.5765
[3:11:51.498265] epoch: 3150 | elbo: 212375.47781250003 | train_rmse: 0.6498 | val_rmse: 0.6903 | val_ll: -1.5296
[3:14:51.165327] epoch: 3200 | elbo: 177898.74117187504 | train_rmse: 0.6236 | val_rmse: 0.6637 | val_ll: -1.4754
[3:17:50.799639] epoch: 3250 | elbo: 147039.55375000002 | train_rmse: 0.5876 | val_rmse: 0.6277 | val_ll: -1.4133
[3:20:51.375399] epoch: 3300 | elbo: 119995.92421874998 | train_rmse: 0.5579 | val_rmse: 0.6005 | val_ll: -1.3437
[3:23:55.623406] epoch: 3350 | elbo: 97797.53117187499 | train_rmse: 0.5298 | val_rmse: 0.5646 | val_ll: -1.2737
[3:26:57.864683] epoch: 3400 | elbo: 80482.02582031248 | train_rmse: 0.5026 | val_rmse: 0.5369 | val_ll: -1.2034
[3:30:01.130851] epoch: 3450 | elbo: 68553.7241015625 | train_rmse: 0.4802 | val_rmse: 0.5156 | val_ll: -1.1363
[3:33:01.825683] epoch: 3500 | elbo: 61235.09734374999 | train_rmse: 0.4528 | val_rmse: 0.4888 | val_ll: -1.0693
[3:36:02.735097] epoch: 3550 | elbo: 57378.742656250004 | train_rmse: 0.4355 | val_rmse: 0.4722 | val_ll: -1.0057
[3:39:04.267445] epoch: 3600 | elbo: 55458.29531249999 | train_rmse: 0.4206 | val_rmse: 0.4565 | val_ll: -0.9496
[3:42:07.279291] epoch: 3650 | elbo: 53894.973671875 | train_rmse: 0.4063 | val_rmse: 0.4438 | val_ll: -0.8954
[3:45:10.084888] epoch: 3700 | elbo: 52599.50484375 | train_rmse: 0.3943 | val_rmse: 0.4344 | val_ll: -0.8487
[3:48:10.546630] epoch: 3750 | elbo: 51558.665468750005 | train_rmse: 0.385 | val_rmse: 0.4259 | val_ll: -0.8062
[3:51:13.600730] epoch: 3800 | elbo: 50588.85857421876 | train_rmse: 0.3791 | val_rmse: 0.4204 | val_ll: -0.7694
[3:54:19.115448] epoch: 3850 | elbo: 49775.494882812505 | train_rmse: 0.3718 | val_rmse: 0.4146 | val_ll: -0.7366
[3:57:20.427128] epoch: 3900 | elbo: 48955.11115234374 | train_rmse: 0.3673 | val_rmse: 0.4101 | val_ll: -0.7082
[4:00:20.040786] epoch: 3950 | elbo: 48290.844062499986 | train_rmse: 0.3635 | val_rmse: 0.4066 | val_ll: -0.6795
[4:03:20.901059] epoch: 4000 | elbo: 47630.66232421876 | train_rmse: 0.3586 | val_rmse: 0.4028 | val_ll: -0.6593
[4:06:22.409104] epoch: 4050 | elbo: 47170.39359375 | train_rmse: 0.354 | val_rmse: 0.4015 | val_ll: -0.6385
[4:09:23.687198] epoch: 4100 | elbo: 46683.1456640625 | train_rmse: 0.3512 | val_rmse: 0.3989 | val_ll: -0.6283
[4:12:25.977301] epoch: 4150 | elbo: 46210.86654296876 | train_rmse: 0.3485 | val_rmse: 0.3974 | val_ll: -0.6106
[4:15:27.216912] epoch: 4200 | elbo: 45828.966113281254 | train_rmse: 0.3459 | val_rmse: 0.3954 | val_ll: -0.5953
[4:18:29.441389] epoch: 4250 | elbo: 45425.589023437504 | train_rmse: 0.3443 | val_rmse: 0.3948 | val_ll: -0.5837
[4:21:30.949566] epoch: 4300 | elbo: 45129.43072265625 | train_rmse: 0.3413 | val_rmse: 0.3933 | val_ll: -0.5737
[4:24:32.913260] epoch: 4350 | elbo: 44796.31572265625 | train_rmse: 0.3393 | val_rmse: 0.3922 | val_ll: -0.5673
[4:27:36.103168] epoch: 4400 | elbo: 44481.63412109374 | train_rmse: 0.3378 | val_rmse: 0.3917 | val_ll: -0.5583
[4:30:38.900344] epoch: 4450 | elbo: 44203.173574218745 | train_rmse: 0.3362 | val_rmse: 0.3915 | val_ll: -0.5548
[4:33:35.222229] epoch: 4500 | elbo: 43923.5579296875 | train_rmse: 0.3362 | val_rmse: 0.391 | val_ll: -0.5524
[4:36:32.500175] epoch: 4550 | elbo: 43628.92414062501 | train_rmse: 0.3337 | val_rmse: 0.3893 | val_ll: -0.5441
[4:39:36.637495] epoch: 4600 | elbo: 43406.37816406251 | train_rmse: 0.3332 | val_rmse: 0.3897 | val_ll: -0.5385
[4:42:43.757246] epoch: 4650 | elbo: 43214.6217578125 | train_rmse: 0.3326 | val_rmse: 0.3897 | val_ll: -0.5382
[4:45:47.048138] epoch: 4700 | elbo: 42913.5496875 | train_rmse: 0.3305 | val_rmse: 0.3883 | val_ll: -0.5284
[4:48:48.250294] epoch: 4750 | elbo: 42773.449160156255 | train_rmse: 0.3298 | val_rmse: 0.3878 | val_ll: -0.5314
[4:51:50.343064] epoch: 4800 | elbo: 42581.660625 | train_rmse: 0.3287 | val_rmse: 0.3866 | val_ll: -0.5261
[4:54:52.694676] epoch: 4850 | elbo: 42349.97167968751 | train_rmse: 0.3274 | val_rmse: 0.3867 | val_ll: -0.5213
[4:57:54.307932] epoch: 4900 | elbo: 42216.62029296874 | train_rmse: 0.3276 | val_rmse: 0.3871 | val_ll: -0.525
[5:01:00.437354] epoch: 4950 | elbo: 42051.36228515624 | train_rmse: 0.3267 | val_rmse: 0.3851 | val_ll: -0.5162
[5:04:00.473233] epoch: 5000 | elbo: 41811.62576171876 | train_rmse: 0.3258 | val_rmse: 0.3844 | val_ll: -0.5132
[5:06:59.929555] epoch: 5050 | elbo: 41692.12216796876 | train_rmse: 0.3251 | val_rmse: 0.3846 | val_ll: -0.5119
[5:09:59.652594] epoch: 5100 | elbo: 41564.30037109375 | train_rmse: 0.3242 | val_rmse: 0.3835 | val_ll: -0.507
[5:13:00.083049] epoch: 5150 | elbo: 41419.47953125001 | train_rmse: 0.3241 | val_rmse: 0.3836 | val_ll: -0.509
[5:16:00.699342] epoch: 5200 | elbo: 41277.75386718749 | train_rmse: 0.3238 | val_rmse: 0.3825 | val_ll: -0.5029
[5:19:02.287165] epoch: 5250 | elbo: 41143.8612109375 | train_rmse: 0.3229 | val_rmse: 0.3816 | val_ll: -0.5001
[5:22:03.055306] epoch: 5300 | elbo: 41016.99306640625 | train_rmse: 0.3225 | val_rmse: 0.3817 | val_ll: -0.5008
[5:25:03.262507] epoch: 5350 | elbo: 40859.00300781249 | train_rmse: 0.3211 | val_rmse: 0.3815 | val_ll: -0.5031
[5:28:03.871450] epoch: 5400 | elbo: 40818.175351562495 | train_rmse: 0.3221 | val_rmse: 0.381 | val_ll: -0.4954
[5:31:03.244606] epoch: 5450 | elbo: 40617.30892578125 | train_rmse: 0.3213 | val_rmse: 0.3802 | val_ll: -0.4945
[5:34:04.284432] epoch: 5500 | elbo: 40468.05822265625 | train_rmse: 0.3207 | val_rmse: 0.3807 | val_ll: -0.4938
[5:37:06.480218] epoch: 5550 | elbo: 40435.148066406255 | train_rmse: 0.3204 | val_rmse: 0.3792 | val_ll: -0.4873
[5:40:06.120622] epoch: 5600 | elbo: 40353.724355468745 | train_rmse: 0.3211 | val_rmse: 0.3796 | val_ll: -0.4853
[5:43:06.465625] epoch: 5650 | elbo: 40209.13681640625 | train_rmse: 0.3204 | val_rmse: 0.3801 | val_ll: -0.4872
[5:46:06.983022] epoch: 5700 | elbo: 40087.8099609375 | train_rmse: 0.3201 | val_rmse: 0.3791 | val_ll: -0.4874
[5:49:07.889862] epoch: 5750 | elbo: 40044.40017578126 | train_rmse: 0.3201 | val_rmse: 0.3785 | val_ll: -0.4843
[5:52:15.355487] epoch: 5800 | elbo: 39955.682285156254 | train_rmse: 0.3205 | val_rmse: 0.379 | val_ll: -0.4804
[5:55:18.686232] epoch: 5850 | elbo: 39791.0366796875 | train_rmse: 0.3194 | val_rmse: 0.3781 | val_ll: -0.4805
[5:58:24.071661] epoch: 5900 | elbo: 39663.28837890625 | train_rmse: 0.3196 | val_rmse: 0.3786 | val_ll: -0.4821
[6:01:29.065086] epoch: 5950 | elbo: 39702.78533203125 | train_rmse: 0.3207 | val_rmse: 0.3778 | val_ll: -0.4797
[6:04:31.153545] epoch: 6000 | elbo: 39590.37568359375 | train_rmse: 0.3209 | val_rmse: 0.3786 | val_ll: -0.4783
[6:07:32.413083] epoch: 6050 | elbo: 39428.55755859374 | train_rmse: 0.3195 | val_rmse: 0.3775 | val_ll: -0.4793
[6:10:34.042865] epoch: 6100 | elbo: 39368.41154296875 | train_rmse: 0.3187 | val_rmse: 0.3778 | val_ll: -0.4777
[6:13:36.074281] epoch: 6150 | elbo: 39283.5454296875 | train_rmse: 0.3186 | val_rmse: 0.3771 | val_ll: -0.4768
[6:16:37.815556] epoch: 6200 | elbo: 39192.18509765624 | train_rmse: 0.3183 | val_rmse: 0.3775 | val_ll: -0.4793
[6:19:39.666630] epoch: 6250 | elbo: 39173.0362109375 | train_rmse: 0.3182 | val_rmse: 0.3773 | val_ll: -0.4757
[6:22:40.448595] epoch: 6300 | elbo: 39005.08855468751 | train_rmse: 0.318 | val_rmse: 0.377 | val_ll: -0.4754
[6:25:41.819041] epoch: 6350 | elbo: 38953.07976562501 | train_rmse: 0.3179 | val_rmse: 0.3774 | val_ll: -0.4762
[6:28:43.167915] epoch: 6400 | elbo: 38890.7619140625 | train_rmse: 0.3205 | val_rmse: 0.3773 | val_ll: -0.4738
[6:31:44.388148] epoch: 6450 | elbo: 38831.35669921875 | train_rmse: 0.3187 | val_rmse: 0.3772 | val_ll: -0.4704
[6:34:46.563855] epoch: 6500 | elbo: 38746.663808593745 | train_rmse: 0.3178 | val_rmse: 0.377 | val_ll: -0.4703
[6:37:48.952407] epoch: 6550 | elbo: 38649.21109375 | train_rmse: 0.3173 | val_rmse: 0.3762 | val_ll: -0.4704
[6:40:53.208977] epoch: 6600 | elbo: 38616.07671875 | train_rmse: 0.3173 | val_rmse: 0.3762 | val_ll: -0.4667
[6:43:56.586767] epoch: 6650 | elbo: 38564.105488281246 | train_rmse: 0.3175 | val_rmse: 0.3759 | val_ll: -0.466
[6:46:57.666216] epoch: 6700 | elbo: 38503.709726562505 | train_rmse: 0.317 | val_rmse: 0.3766 | val_ll: -0.4678
[6:49:58.169264] epoch: 6750 | elbo: 38451.954433593746 | train_rmse: 0.3177 | val_rmse: 0.3759 | val_ll: -0.4654
[6:52:58.265184] epoch: 6800 | elbo: 38328.480937500004 | train_rmse: 0.3172 | val_rmse: 0.3763 | val_ll: -0.466
[6:55:59.182824] epoch: 6850 | elbo: 38320.905019531245 | train_rmse: 0.3168 | val_rmse: 0.3767 | val_ll: -0.4654
[6:58:59.770610] epoch: 6900 | elbo: 38126.17376953125 | train_rmse: 0.317 | val_rmse: 0.3766 | val_ll: -0.4661
[7:02:02.103416] epoch: 6950 | elbo: 38070.235605468755 | train_rmse: 0.3178 | val_rmse: 0.3764 | val_ll: -0.4654
[7:05:03.696921] epoch: 7000 | elbo: 38032.69677734375 | train_rmse: 0.3175 | val_rmse: 0.3766 | val_ll: -0.465
[7:08:08.438892] epoch: 7050 | elbo: 37951.5230859375 | train_rmse: 0.317 | val_rmse: 0.3765 | val_ll: -0.4657
[7:11:10.752949] epoch: 7100 | elbo: 38033.90822265625 | train_rmse: 0.3174 | val_rmse: 0.3765 | val_ll: -0.4662
[7:14:16.007947] epoch: 7150 | elbo: 37872.24443359375 | train_rmse: 0.3161 | val_rmse: 0.3765 | val_ll: -0.4696
[7:17:18.364230] epoch: 7200 | elbo: 37813.71292968751 | train_rmse: 0.3196 | val_rmse: 0.3772 | val_ll: -0.4669
[7:20:18.341791] epoch: 7250 | elbo: 37728.66068359375 | train_rmse: 0.3168 | val_rmse: 0.3762 | val_ll: -0.4631
[7:23:18.954735] epoch: 7300 | elbo: 37627.82363281251 | train_rmse: 0.3161 | val_rmse: 0.3757 | val_ll: -0.4656
[7:26:18.898197] epoch: 7350 | elbo: 37574.604375 | train_rmse: 0.3165 | val_rmse: 0.376 | val_ll: -0.465
[7:29:21.531392] epoch: 7400 | elbo: 37541.83931640624 | train_rmse: 0.3161 | val_rmse: 0.3762 | val_ll: -0.4623
[7:32:20.951011] epoch: 7450 | elbo: 37486.1716796875 | train_rmse: 0.3158 | val_rmse: 0.3757 | val_ll: -0.46
[7:35:26.821729] epoch: 7500 | elbo: 37400.057363281245 | train_rmse: 0.3154 | val_rmse: 0.3759 | val_ll: -0.4614
[7:38:31.168795] epoch: 7550 | elbo: 37317.31986328125 | train_rmse: 0.3157 | val_rmse: 0.3755 | val_ll: -0.4645
[7:41:32.352232] epoch: 7600 | elbo: 37274.36966796874 | train_rmse: 0.3159 | val_rmse: 0.3754 | val_ll: -0.462
[7:44:35.255236] epoch: 7650 | elbo: 37181.88449218749 | train_rmse: 0.3148 | val_rmse: 0.3754 | val_ll: -0.4612
[7:47:39.336397] epoch: 7700 | elbo: 37127.9446875 | train_rmse: 0.3154 | val_rmse: 0.3755 | val_ll: -0.4625
[7:50:44.760954] epoch: 7750 | elbo: 37058.19806640626 | train_rmse: 0.3156 | val_rmse: 0.3757 | val_ll: -0.4572
[7:53:48.153630] epoch: 7800 | elbo: 37023.74765625 | train_rmse: 0.3155 | val_rmse: 0.376 | val_ll: -0.4636
[7:56:50.888923] epoch: 7850 | elbo: 37002.80925781251 | train_rmse: 0.3147 | val_rmse: 0.3753 | val_ll: -0.4644
[7:59:55.864082] epoch: 7900 | elbo: 36862.29734375 | train_rmse: 0.3148 | val_rmse: 0.376 | val_ll: -0.4619
[8:02:58.989653] epoch: 7950 | elbo: 36798.95880859376 | train_rmse: 0.3143 | val_rmse: 0.3755 | val_ll: -0.4583
[8:06:03.468949] epoch: 8000 | elbo: 36682.649667968755 | train_rmse: 0.3144 | val_rmse: 0.3754 | val_ll: -0.4577
[8:09:02.475907] epoch: 8050 | elbo: 36692.977441406256 | train_rmse: 0.3143 | val_rmse: 0.3754 | val_ll: -0.458
[8:12:03.757811] epoch: 8100 | elbo: 36574.138730468745 | train_rmse: 0.3148 | val_rmse: 0.3761 | val_ll: -0.4625
[8:15:01.763182] epoch: 8150 | elbo: 36583.07031249999 | train_rmse: 0.315 | val_rmse: 0.3754 | val_ll: -0.4575
[8:18:05.489145] epoch: 8200 | elbo: 36506.568789062505 | train_rmse: 0.3146 | val_rmse: 0.3756 | val_ll: -0.4595
[8:21:11.587305] epoch: 8250 | elbo: 36512.13052734375 | train_rmse: 0.3147 | val_rmse: 0.3763 | val_ll: -0.4602
[8:24:17.164078] epoch: 8300 | elbo: 36384.50177734375 | train_rmse: 0.3132 | val_rmse: 0.3757 | val_ll: -0.4581
[8:27:23.614239] epoch: 8350 | elbo: 36573.238203124994 | train_rmse: 0.3138 | val_rmse: 0.3761 | val_ll: -0.4593
[8:30:25.042615] epoch: 8400 | elbo: 36367.299531250006 | train_rmse: 0.3139 | val_rmse: 0.3756 | val_ll: -0.4557
[8:33:25.953577] epoch: 8450 | elbo: 36307.79238281249 | train_rmse: 0.3138 | val_rmse: 0.3752 | val_ll: -0.4545
[8:36:28.098749] epoch: 8500 | elbo: 36170.234765625 | train_rmse: 0.3141 | val_rmse: 0.3759 | val_ll: -0.4602
[8:39:28.403503] epoch: 8550 | elbo: 36130.48462890625 | train_rmse: 0.3139 | val_rmse: 0.3748 | val_ll: -0.4547
[8:42:26.014943] epoch: 8600 | elbo: 36209.16802734376 | train_rmse: 0.3131 | val_rmse: 0.3745 | val_ll: -0.4558
[8:45:22.272711] epoch: 8650 | elbo: 36087.02919921875 | train_rmse: 0.3131 | val_rmse: 0.3751 | val_ll: -0.4556
[8:48:19.158622] epoch: 8700 | elbo: 35977.955312499995 | train_rmse: 0.3128 | val_rmse: 0.3741 | val_ll: -0.4537
[8:51:15.897022] epoch: 8750 | elbo: 35944.50708984375 | train_rmse: 0.3131 | val_rmse: 0.3747 | val_ll: -0.4533
[8:54:12.691872] epoch: 8800 | elbo: 35858.28703125 | train_rmse: 0.3131 | val_rmse: 0.3743 | val_ll: -0.4511
[8:57:17.195666] epoch: 8850 | elbo: 35867.13244140624 | train_rmse: 0.313 | val_rmse: 0.3758 | val_ll: -0.4611
[9:00:15.260570] epoch: 8900 | elbo: 35717.955820312505 | train_rmse: 0.3131 | val_rmse: 0.3743 | val_ll: -0.4552
[9:03:14.077637] epoch: 8950 | elbo: 35697.4164453125 | train_rmse: 0.3132 | val_rmse: 0.3744 | val_ll: -0.4518
[9:06:11.229654] epoch: 9000 | elbo: 35653.79376953126 | train_rmse: 0.3135 | val_rmse: 0.3748 | val_ll: -0.4545
[9:09:11.288038] epoch: 9050 | elbo: 35692.39714843751 | train_rmse: 0.3119 | val_rmse: 0.3745 | val_ll: -0.4491
[9:12:13.790686] epoch: 9100 | elbo: 35650.178945312495 | train_rmse: 0.3121 | val_rmse: 0.3749 | val_ll: -0.4511
[9:15:21.284492] epoch: 9150 | elbo: 35442.23857421875 | train_rmse: 0.3124 | val_rmse: 0.3747 | val_ll: -0.4522
[9:18:19.899135] epoch: 9200 | elbo: 35471.966328124996 | train_rmse: 0.3109 | val_rmse: 0.3738 | val_ll: -0.4501
[9:21:24.476652] epoch: 9250 | elbo: 35450.9655859375 | train_rmse: 0.3122 | val_rmse: 0.3741 | val_ll: -0.4533
[9:24:29.443591] epoch: 9300 | elbo: 35371.2551953125 | train_rmse: 0.3113 | val_rmse: 0.3734 | val_ll: -0.4487
[9:27:34.542770] epoch: 9350 | elbo: 35337.14662109375 | train_rmse: 0.3123 | val_rmse: 0.3736 | val_ll: -0.4511
[9:30:41.855109] epoch: 9400 | elbo: 35299.5715234375 | train_rmse: 0.3112 | val_rmse: 0.3741 | val_ll: -0.4508
[9:33:48.112259] epoch: 9450 | elbo: 35215.23109375 | train_rmse: 0.3115 | val_rmse: 0.3738 | val_ll: -0.4522
[9:36:54.346004] epoch: 9500 | elbo: 35266.075488281254 | train_rmse: 0.3118 | val_rmse: 0.3731 | val_ll: -0.4509
[9:39:58.679315] epoch: 9550 | elbo: 35197.04683593749 | train_rmse: 0.3115 | val_rmse: 0.3748 | val_ll: -0.4524
[9:43:04.456371] epoch: 9600 | elbo: 35251.4935546875 | train_rmse: 0.3112 | val_rmse: 0.3735 | val_ll: -0.4493
[9:46:10.212826] epoch: 9650 | elbo: 35033.88068359376 | train_rmse: 0.3111 | val_rmse: 0.3726 | val_ll: -0.4499
[9:49:12.530648] epoch: 9700 | elbo: 35036.64333984374 | train_rmse: 0.3108 | val_rmse: 0.3734 | val_ll: -0.4472
[9:52:10.293329] epoch: 9750 | elbo: 34939.624492187504 | train_rmse: 0.3105 | val_rmse: 0.3731 | val_ll: -0.4443
[9:55:08.270488] epoch: 9800 | elbo: 34973.91171875 | train_rmse: 0.3111 | val_rmse: 0.3739 | val_ll: -0.4479
[9:58:12.935603] epoch: 9850 | elbo: 34973.941875000004 | train_rmse: 0.3121 | val_rmse: 0.3741 | val_ll: -0.4476
[10:01:16.978537] epoch: 9900 | elbo: 34789.50207031249 | train_rmse: 0.3104 | val_rmse: 0.3736 | val_ll: -0.4488
[10:04:15.473911] epoch: 9950 | elbo: 34842.44712890625 | train_rmse: 0.3104 | val_rmse: 0.3732 | val_ll: -0.4473
Training finished in 10:07:10.116228 seconds
Saved SVI model to tests/sigma-over-underfit-test/models/tensin-3x512-sl/checkpoint_2.pt
File Size is 4.060103416442871 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:3 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.001 TRAIN_SIZE: 20000
Loaded SVI model from tests/sigma-over-underfit-test/models/tensin-3x512-sl/checkpoint_1.pt
using device: cuda:3
====== evaluating profile tensin-3x512-sl - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:03:57.481730
using device: cuda:3
====== evaluating profile tensin-3x512-sl - 2 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:03:56.642701
End time: 2023-07-07 00:20:31.110182
Total time: 10:15:08.138700
