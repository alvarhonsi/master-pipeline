Start time: 2023-07-13 18:37:50.486034
torch.Size([512, 10]) torch.Size([512, 1])
Sequential(
  (0): Linear(in_features=10, out_features=32, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=32, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([32, 10]) Parameter containing:
tensor([[-5.5142e-01,  8.5681e-02,  3.1892e-01, -3.3605e-01, -1.4858e-01,
         -5.7075e-01,  3.5857e-01, -1.9340e-01, -1.8532e-01, -1.6177e-01],
        [ 3.8332e-02, -1.9438e-01,  1.2553e-01, -1.2589e-01,  7.9512e-04,
          5.7514e-01, -2.0025e-01, -4.5613e-01, -1.9231e-01, -4.7369e-01],
        [ 2.9716e-01, -2.2968e-01, -2.1045e-02, -3.5908e-01, -5.0528e-01,
         -4.2698e-01,  1.2663e-01, -6.5499e-01,  1.4395e-01, -1.9253e-01],
        [ 1.5245e-01,  4.4850e-01, -1.3850e-01, -2.6037e-01, -1.4976e-01,
          7.7012e-01,  3.6604e-01,  1.7714e-01, -3.6415e-01,  9.8990e-02],
        [-1.5596e-01, -3.8342e-01,  4.3503e-01,  4.5410e-01,  5.2749e-01,
          2.8341e-02, -4.1569e-02,  1.7842e-01,  7.0336e-01, -8.3323e-01],
        [-1.3372e-01,  4.9412e-01,  7.3430e-01,  1.3358e-01,  1.5141e-02,
         -3.1930e-02, -1.4298e-02, -1.6196e-01,  3.3128e-01,  2.6664e-02],
        [-7.6773e-02, -6.2920e-01, -1.5249e-01,  3.9511e-01,  2.4321e-01,
          2.3987e-01,  3.4565e-02, -8.4096e-02,  6.7610e-02, -1.8412e-01],
        [ 3.3930e-01, -2.2197e-01, -2.3018e-01,  5.9775e-02, -5.6596e-02,
          3.2725e-02,  2.0001e-01,  4.9074e-02, -4.4274e-01,  4.3198e-01],
        [-4.1431e-01, -3.4353e-01, -4.9136e-01,  4.8169e-01, -3.0876e-01,
         -8.3704e-01,  8.9714e-02,  2.7553e-02, -4.2415e-01,  3.6556e-01],
        [-1.6551e-01, -1.7064e-01,  3.6931e-01,  2.9095e-02,  1.9979e-01,
         -4.2136e-02, -6.6224e-02,  6.2676e-02,  4.7105e-01,  2.1859e-02],
        [-2.9572e-01,  8.4258e-02, -8.2112e-01,  2.0197e-01,  1.9748e-01,
         -2.4747e-01,  2.9106e-02, -3.7135e-02, -6.7245e-02,  1.5315e-01],
        [ 4.6592e-02,  1.6261e-02, -4.1748e-01, -4.7403e-02, -4.3116e-02,
         -4.2857e-01, -2.3313e-01, -4.3111e-01,  6.2486e-01, -1.6060e-01],
        [-1.7293e-01, -4.4928e-01, -7.9363e-02, -1.3228e-01,  1.7361e-01,
         -5.8955e-01,  4.6016e-01, -3.7743e-01,  9.2456e-01, -1.9997e-01],
        [-1.4909e-01,  2.1921e-01,  5.3138e-01,  2.4284e-01, -4.9837e-01,
          2.5411e-01,  3.7422e-02, -1.4605e-01, -3.7842e-01, -5.7877e-02],
        [-2.0021e-01, -5.0584e-01,  2.3754e-01,  6.0868e-01, -1.1237e-01,
         -5.6584e-01,  2.1581e-01,  2.7248e-01, -4.2361e-01,  1.9445e-01],
        [-2.2149e-01,  4.9537e-01, -1.9626e-01, -2.0777e-01,  2.3804e-01,
         -3.4426e-01, -2.4816e-01, -2.0019e-01, -7.5520e-02,  1.2040e-01],
        [ 6.1222e-01, -1.9591e-01, -1.0940e-02,  2.1773e-01, -5.7162e-01,
          4.2480e-01,  3.9900e-01, -8.9534e-02,  2.7988e-01,  1.4513e-02],
        [ 9.7702e-02,  2.1367e-01, -2.6274e-01, -3.1204e-01,  9.6581e-02,
         -2.3121e-01, -1.7422e-01,  7.2158e-02,  6.0300e-01, -9.9684e-02],
        [ 1.0238e-01, -9.5928e-02,  3.7432e-01, -4.5404e-01,  2.6277e-01,
          1.0177e-01,  2.2451e-01, -7.1458e-02,  1.8969e-02, -1.7005e-01],
        [-3.6223e-01,  6.5892e-03, -3.0457e-01,  3.4776e-01,  5.6112e-01,
         -4.1052e-01, -1.7447e-01,  3.8428e-01,  1.9063e-01,  5.7045e-01],
        [-3.0001e-01,  1.2583e-01,  5.9245e-01,  7.9715e-02, -4.4453e-01,
         -3.8409e-01, -2.2779e-01,  3.4126e-01, -5.0477e-02,  6.2342e-02],
        [ 2.2019e-01,  1.8072e-01,  3.7352e-01,  2.4820e-01,  5.6163e-01,
          2.7965e-01,  8.6330e-01, -2.8767e-02,  3.5331e-01,  5.6934e-01],
        [ 3.8419e-01, -5.8357e-02,  2.2903e-01, -3.8701e-01,  6.5822e-01,
         -4.2104e-01, -1.7983e-01, -1.4397e-01, -1.4837e-01, -1.8972e-01],
        [ 3.4402e-01, -5.7361e-01,  5.7444e-01,  6.8498e-01, -2.8730e-01,
         -1.3613e-01, -1.2907e-01, -6.1928e-02,  8.4676e-01, -2.6694e-01],
        [-3.3861e-01,  1.6175e-01, -1.8822e-01,  5.0557e-01, -2.6744e-01,
          2.0970e-01, -1.9030e-01, -8.0071e-02, -1.2268e-01,  3.0979e-01],
        [ 3.4491e-01,  1.2814e-02,  8.4838e-02,  3.5555e-01,  1.5062e-01,
          1.1761e-01, -1.7386e-01,  1.7021e-01, -2.7723e-01, -1.4332e-02],
        [ 4.3432e-01,  1.9002e-01,  1.0530e-01,  8.4044e-02, -1.6970e-01,
         -1.1539e-01,  3.4469e-01,  1.4286e-01,  3.9048e-01, -1.0699e-01],
        [ 3.5935e-01, -6.6119e-01,  2.5428e-01,  1.4791e-02,  4.8288e-02,
          1.1856e-01,  1.1768e-01,  2.4425e-01, -1.2550e-01,  2.5049e-02],
        [-1.0179e-01,  1.8463e-01, -6.5572e-02, -4.3555e-01, -2.3188e-01,
          5.6224e-01,  7.5410e-01, -2.5643e-01, -5.5778e-01, -3.3385e-01],
        [-2.1390e-01, -5.2845e-01, -1.4216e-01,  2.9815e-01, -1.4292e-01,
          2.4130e-01,  2.4642e-01,  3.6785e-02,  3.8068e-01,  5.2703e-01],
        [ 1.9046e-01, -1.4839e-01,  8.1872e-01, -6.7136e-02, -6.4112e-02,
          2.2017e-01, -2.2088e-02,  6.7365e-02, -1.9169e-01,  2.9744e-01],
        [-3.7688e-01, -3.9695e-01, -9.5540e-02,  8.5447e-02,  7.4726e-02,
          5.5080e-01, -4.0264e-01, -3.9826e-02, -6.7396e-01,  3.7387e-01]],
       device='cuda:4', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([32, 10]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100]], device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([32]) Parameter containing:
tensor([ 0.2264, -0.1691, -0.2504, -0.0776,  0.4583, -0.4760,  0.6759, -0.2942,
        -0.1746,  0.3058,  0.1391, -0.4266, -0.3273, -0.1464, -0.0685, -0.3139,
        -0.4432,  0.1569, -0.0759, -0.0548, -0.2151,  0.4739, -0.0980, -0.4764,
        -0.2526,  0.2466,  0.0351,  0.7939, -0.4439, -0.3148, -0.2111,  0.5130],
       device='cuda:4', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([32]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([32, 32]) Parameter containing:
tensor([[ 4.1945e-02,  3.1477e-01, -3.8754e-01,  ...,  4.0299e-01,
         -2.8880e-01,  3.4031e-01],
        [ 1.6691e-01, -1.4391e-01,  1.1394e-01,  ...,  2.1201e-01,
         -2.2699e-01, -2.0432e-01],
        [ 2.3534e-01,  7.1147e-02,  2.1887e-01,  ..., -1.8904e-01,
          1.9282e-01,  1.4627e-02],
        ...,
        [-6.4732e-01,  2.5422e-01, -1.9986e-04,  ..., -4.8290e-02,
          3.5098e-01, -3.9358e-02],
        [ 5.9262e-01, -1.1084e-01,  5.8215e-02,  ...,  8.0488e-01,
          3.9570e-01, -2.8327e-01],
        [ 9.7493e-02,  4.2820e-01, -1.2281e-01,  ...,  2.8988e-01,
          2.8355e-01, -1.0181e-01]], device='cuda:4', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([32, 32]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([32]) Parameter containing:
tensor([-5.3518e-02,  5.3620e-05,  2.7577e-01,  1.9199e-01,  2.6251e-02,
         4.3386e-01, -2.0907e-01, -4.4727e-01, -2.2992e-01, -4.2580e-01,
        -1.1461e-01,  9.2058e-02, -3.4645e-01, -1.4232e-01, -1.5309e-01,
        -2.9888e-01, -3.6484e-01,  2.0163e-01,  2.4830e-01, -2.9638e-01,
        -1.7196e-02, -8.4347e-02, -1.2331e-01,  4.6525e-02,  1.8436e-02,
        -3.3158e-01, -4.6650e-01,  8.9137e-02, -3.5054e-01,  2.0933e-01,
         3.9919e-01,  2.6540e-01], device='cuda:4', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([32]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([32, 32]) Parameter containing:
tensor([[-0.1601, -0.5175, -0.1358,  ...,  0.0036, -0.0339, -0.7062],
        [ 0.3402,  0.6666, -0.3241,  ..., -0.3217, -0.2848,  0.2695],
        [ 0.2391,  0.4923, -0.0696,  ..., -0.1233,  0.1126,  0.1973],
        ...,
        [-0.1694, -0.1365,  0.0275,  ...,  0.2205,  0.2793,  0.1422],
        [-0.0244,  0.5909, -0.0306,  ...,  0.0564, -0.4211,  0.2759],
        [ 0.1532, -0.3155,  0.0166,  ..., -0.2012,  0.2033, -0.1645]],
       device='cuda:4', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([32, 32]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([32]) Parameter containing:
tensor([ 0.1541, -0.0457, -0.4797, -0.2269, -0.0586,  0.6290, -0.1372,  0.6788,
         0.9245, -0.2886,  0.0213,  0.5774,  0.0053, -0.2517,  0.3737, -0.0052,
         0.4722,  0.1448, -0.0071, -0.0275,  0.0257,  0.0416,  0.1423,  0.3590,
        -0.0584, -0.2885, -0.1995,  0.3073,  0.2282, -0.5327, -0.6584, -0.2578],
       device='cuda:4', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([32]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 32]) Parameter containing:
tensor([[ 0.1087, -0.2445, -0.3111,  0.1799, -0.1087,  0.0493, -0.4476,  0.0198,
          0.1073,  0.1231, -0.1353,  0.4768, -0.3033,  0.0064, -0.0225,  0.0532,
          0.0895, -0.3389, -0.3916,  0.2669, -0.2441, -0.1752,  0.5808, -0.4495,
          0.3029, -0.1804, -0.4695,  0.1045,  0.0781,  0.5768,  0.3028,  0.1526]],
       device='cuda:4', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 32]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100]], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([0.2879], device='cuda:4', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:4', grad_fn=<AddBackward0>)
Using device: cuda:4
===== Training profile sineasy10-3x32-s05 - 1 =====
[0:00:01.817250] epoch: 0 | elbo: 1239850.6687500002 | train_rmse: 7.6918 | val_rmse: 7.7244 | val_ll: -95.6264
[0:01:39.865681] epoch: 50 | elbo: 344083.933125 | train_rmse: 4.0231 | val_rmse: 4.0762 | val_ll: -23.1594
[0:03:18.245767] epoch: 100 | elbo: 107145.464921875 | train_rmse: 2.1844 | val_rmse: 2.318 | val_ll: -6.5989
[0:04:57.271193] epoch: 150 | elbo: 70785.345859375 | train_rmse: 1.7657 | val_rmse: 1.8882 | val_ll: -4.4708
[0:06:36.123772] epoch: 200 | elbo: 55931.072265625 | train_rmse: 1.5669 | val_rmse: 1.6921 | val_ll: -3.8158
[0:08:14.147134] epoch: 250 | elbo: 47488.100625 | train_rmse: 1.4376 | val_rmse: 1.5595 | val_ll: -3.4346
[0:09:53.329466] epoch: 300 | elbo: 41693.4735546875 | train_rmse: 1.3437 | val_rmse: 1.4622 | val_ll: -3.1782
[0:11:30.424984] epoch: 350 | elbo: 37214.411640625 | train_rmse: 1.2728 | val_rmse: 1.3871 | val_ll: -2.966
[0:13:08.992134] epoch: 400 | elbo: 34139.51939453125 | train_rmse: 1.214 | val_rmse: 1.3277 | val_ll: -2.7945
[0:14:45.643268] epoch: 450 | elbo: 31716.663749999996 | train_rmse: 1.169 | val_rmse: 1.2826 | val_ll: -2.6945
[0:16:23.004342] epoch: 500 | elbo: 29555.128593750007 | train_rmse: 1.1249 | val_rmse: 1.2404 | val_ll: -2.5789
[0:18:00.568583] epoch: 550 | elbo: 27944.590683593746 | train_rmse: 1.0924 | val_rmse: 1.2073 | val_ll: -2.5036
[0:19:38.514073] epoch: 600 | elbo: 26394.35201171875 | train_rmse: 1.0592 | val_rmse: 1.1752 | val_ll: -2.4424
[0:21:16.169542] epoch: 650 | elbo: 25267.03439453125 | train_rmse: 1.03 | val_rmse: 1.1464 | val_ll: -2.3614
[0:22:54.456227] epoch: 700 | elbo: 23994.373066406253 | train_rmse: 1.0021 | val_rmse: 1.1205 | val_ll: -2.3078
[0:24:31.692347] epoch: 750 | elbo: 22943.4121484375 | train_rmse: 0.9774 | val_rmse: 1.0962 | val_ll: -2.2301
[0:26:09.288777] epoch: 800 | elbo: 22043.943359375007 | train_rmse: 0.9549 | val_rmse: 1.0742 | val_ll: -2.1715
[0:27:49.049424] epoch: 850 | elbo: 21107.79982421875 | train_rmse: 0.9326 | val_rmse: 1.0539 | val_ll: -2.1182
[0:29:27.243443] epoch: 900 | elbo: 20418.347236328125 | train_rmse: 0.9136 | val_rmse: 1.0375 | val_ll: -2.0671
[0:31:04.954242] epoch: 950 | elbo: 19591.260634765626 | train_rmse: 0.8913 | val_rmse: 1.0195 | val_ll: -2.0118
[0:32:42.732894] epoch: 1000 | elbo: 18951.273164062506 | train_rmse: 0.8729 | val_rmse: 1.0016 | val_ll: -1.9645
[0:34:20.472198] epoch: 1050 | elbo: 18361.39845703125 | train_rmse: 0.8562 | val_rmse: 0.9871 | val_ll: -1.9273
[0:35:58.477387] epoch: 1100 | elbo: 17809.368535156253 | train_rmse: 0.8405 | val_rmse: 0.9718 | val_ll: -1.8846
[0:37:36.149578] epoch: 1150 | elbo: 17245.543291015623 | train_rmse: 0.825 | val_rmse: 0.9568 | val_ll: -1.8371
[0:39:13.778184] epoch: 1200 | elbo: 16842.851015625 | train_rmse: 0.8117 | val_rmse: 0.943 | val_ll: -1.795
[0:40:50.698142] epoch: 1250 | elbo: 16490.3066015625 | train_rmse: 0.8002 | val_rmse: 0.9305 | val_ll: -1.7605
[0:42:28.317065] epoch: 1300 | elbo: 16067.27119140625 | train_rmse: 0.7885 | val_rmse: 0.9192 | val_ll: -1.727
[0:44:06.330604] epoch: 1350 | elbo: 15707.059355468751 | train_rmse: 0.7764 | val_rmse: 0.9092 | val_ll: -1.6988
[0:45:43.911600] epoch: 1400 | elbo: 15379.802841796869 | train_rmse: 0.766 | val_rmse: 0.9011 | val_ll: -1.6747
[0:47:23.330260] epoch: 1450 | elbo: 15084.973515625 | train_rmse: 0.756 | val_rmse: 0.8938 | val_ll: -1.6587
[0:49:01.539586] epoch: 1500 | elbo: 14784.610195312498 | train_rmse: 0.7471 | val_rmse: 0.8877 | val_ll: -1.6446
[0:50:39.627885] epoch: 1550 | elbo: 14571.943046875002 | train_rmse: 0.7388 | val_rmse: 0.8818 | val_ll: -1.6387
[0:52:17.460878] epoch: 1600 | elbo: 14317.760175781248 | train_rmse: 0.732 | val_rmse: 0.8779 | val_ll: -1.622
[0:53:56.431193] epoch: 1650 | elbo: 14152.752167968752 | train_rmse: 0.7245 | val_rmse: 0.8732 | val_ll: -1.6175
[0:55:33.738296] epoch: 1700 | elbo: 13940.868046875 | train_rmse: 0.7185 | val_rmse: 0.869 | val_ll: -1.614
[0:57:11.744122] epoch: 1750 | elbo: 13850.177255859377 | train_rmse: 0.7128 | val_rmse: 0.8653 | val_ll: -1.6022
[0:58:51.038235] epoch: 1800 | elbo: 13666.565371093751 | train_rmse: 0.7069 | val_rmse: 0.8615 | val_ll: -1.5957
[1:00:29.398352] epoch: 1850 | elbo: 13568.639824218748 | train_rmse: 0.7032 | val_rmse: 0.8598 | val_ll: -1.5925
[1:02:08.162725] epoch: 1900 | elbo: 13391.5533203125 | train_rmse: 0.6974 | val_rmse: 0.8558 | val_ll: -1.5864
[1:03:46.345552] epoch: 1950 | elbo: 13279.484638671876 | train_rmse: 0.6926 | val_rmse: 0.853 | val_ll: -1.5815
[1:05:24.025274] epoch: 2000 | elbo: 13187.922109374998 | train_rmse: 0.6887 | val_rmse: 0.8501 | val_ll: -1.5781
[1:07:02.151540] epoch: 2050 | elbo: 13076.093164062499 | train_rmse: 0.6856 | val_rmse: 0.8487 | val_ll: -1.5748
[1:08:38.969036] epoch: 2100 | elbo: 12961.622880859377 | train_rmse: 0.682 | val_rmse: 0.8469 | val_ll: -1.577
[1:10:16.616930] epoch: 2150 | elbo: 12870.167763671876 | train_rmse: 0.6773 | val_rmse: 0.8427 | val_ll: -1.5702
[1:11:54.390062] epoch: 2200 | elbo: 12807.117529296873 | train_rmse: 0.6733 | val_rmse: 0.8407 | val_ll: -1.5642
[1:13:33.195680] epoch: 2250 | elbo: 12681.42923828125 | train_rmse: 0.67 | val_rmse: 0.8375 | val_ll: -1.5586
[1:15:09.951942] epoch: 2300 | elbo: 12607.034013671873 | train_rmse: 0.6669 | val_rmse: 0.8359 | val_ll: -1.556
[1:16:48.670753] epoch: 2350 | elbo: 12535.386904296874 | train_rmse: 0.6638 | val_rmse: 0.8341 | val_ll: -1.5525
[1:18:26.441720] epoch: 2400 | elbo: 12461.65396484375 | train_rmse: 0.6611 | val_rmse: 0.8319 | val_ll: -1.5453
[1:20:04.384138] epoch: 2450 | elbo: 12448.575429687498 | train_rmse: 0.6588 | val_rmse: 0.831 | val_ll: -1.5438
[1:21:42.406006] epoch: 2500 | elbo: 12313.05528320312 | train_rmse: 0.6553 | val_rmse: 0.829 | val_ll: -1.5376
[1:23:19.932346] epoch: 2550 | elbo: 12272.54439453125 | train_rmse: 0.6529 | val_rmse: 0.8279 | val_ll: -1.5342
[1:24:56.923940] epoch: 2600 | elbo: 12237.36685546875 | train_rmse: 0.6499 | val_rmse: 0.8255 | val_ll: -1.526
[1:26:33.886711] epoch: 2650 | elbo: 12143.074931640625 | train_rmse: 0.6475 | val_rmse: 0.825 | val_ll: -1.5277
[1:28:11.849829] epoch: 2700 | elbo: 12098.869140625002 | train_rmse: 0.6447 | val_rmse: 0.8227 | val_ll: -1.5193
[1:29:49.282739] epoch: 2750 | elbo: 12031.827207031249 | train_rmse: 0.6429 | val_rmse: 0.8227 | val_ll: -1.5208
[1:31:27.006540] epoch: 2800 | elbo: 11960.611689453126 | train_rmse: 0.6399 | val_rmse: 0.8219 | val_ll: -1.5143
[1:33:05.566615] epoch: 2850 | elbo: 11887.3173828125 | train_rmse: 0.6375 | val_rmse: 0.8201 | val_ll: -1.5155
[1:34:43.806597] epoch: 2900 | elbo: 11857.976289062499 | train_rmse: 0.6352 | val_rmse: 0.8194 | val_ll: -1.5149
[1:36:27.715160] epoch: 2950 | elbo: 11831.421552734373 | train_rmse: 0.633 | val_rmse: 0.8181 | val_ll: -1.5118
[1:38:08.963731] epoch: 3000 | elbo: 11750.461220703124 | train_rmse: 0.6317 | val_rmse: 0.818 | val_ll: -1.515
[1:39:50.188455] epoch: 3050 | elbo: 11807.52359375 | train_rmse: 0.6294 | val_rmse: 0.8168 | val_ll: -1.5092
[1:41:31.097471] epoch: 3100 | elbo: 11666.556591796876 | train_rmse: 0.6278 | val_rmse: 0.8162 | val_ll: -1.5109
[1:43:12.096089] epoch: 3150 | elbo: 11655.2921875 | train_rmse: 0.6259 | val_rmse: 0.8152 | val_ll: -1.5091
[1:44:52.895248] epoch: 3200 | elbo: 11609.874365234375 | train_rmse: 0.6252 | val_rmse: 0.816 | val_ll: -1.5072
[1:46:33.462875] epoch: 3250 | elbo: 11569.418476562503 | train_rmse: 0.6247 | val_rmse: 0.8149 | val_ll: -1.5114
[1:48:13.385103] epoch: 3300 | elbo: 11522.210146484374 | train_rmse: 0.6219 | val_rmse: 0.814 | val_ll: -1.5108
[1:49:53.221915] epoch: 3350 | elbo: 11502.7848046875 | train_rmse: 0.6208 | val_rmse: 0.8141 | val_ll: -1.5083
[1:51:33.331154] epoch: 3400 | elbo: 11441.413671875 | train_rmse: 0.6195 | val_rmse: 0.8123 | val_ll: -1.5012
[1:53:14.062778] epoch: 3450 | elbo: 11400.851328125 | train_rmse: 0.6179 | val_rmse: 0.8124 | val_ll: -1.5035
[1:54:54.227372] epoch: 3500 | elbo: 11394.715283203124 | train_rmse: 0.6173 | val_rmse: 0.8118 | val_ll: -1.5009
[1:56:34.265222] epoch: 3550 | elbo: 11364.053173828124 | train_rmse: 0.6162 | val_rmse: 0.8117 | val_ll: -1.5117
[1:58:16.160444] epoch: 3600 | elbo: 11328.765751953124 | train_rmse: 0.6153 | val_rmse: 0.8109 | val_ll: -1.5009
[1:59:56.396557] epoch: 3650 | elbo: 11260.239492187498 | train_rmse: 0.6139 | val_rmse: 0.8099 | val_ll: -1.5037
[2:01:38.744919] epoch: 3700 | elbo: 11340.993769531251 | train_rmse: 0.6137 | val_rmse: 0.8101 | val_ll: -1.4996
[2:03:19.654776] epoch: 3750 | elbo: 11269.227490234374 | train_rmse: 0.6118 | val_rmse: 0.8087 | val_ll: -1.4996
[2:04:59.645562] epoch: 3800 | elbo: 11197.904160156251 | train_rmse: 0.6105 | val_rmse: 0.8083 | val_ll: -1.4959
[2:06:40.525270] epoch: 3850 | elbo: 11247.714492187502 | train_rmse: 0.6106 | val_rmse: 0.8082 | val_ll: -1.4944
[2:08:21.312876] epoch: 3900 | elbo: 11164.850136718753 | train_rmse: 0.6091 | val_rmse: 0.808 | val_ll: -1.4961
[2:10:00.551352] epoch: 3950 | elbo: 11139.334892578123 | train_rmse: 0.6087 | val_rmse: 0.8073 | val_ll: -1.4903
[2:11:41.480303] epoch: 4000 | elbo: 11168.429404296876 | train_rmse: 0.6084 | val_rmse: 0.8074 | val_ll: -1.4932
[2:13:21.006562] epoch: 4050 | elbo: 11118.43796875 | train_rmse: 0.6072 | val_rmse: 0.8069 | val_ll: -1.4901
[2:15:00.904084] epoch: 4100 | elbo: 11070.716718750002 | train_rmse: 0.6054 | val_rmse: 0.8063 | val_ll: -1.4864
[2:16:41.768398] epoch: 4150 | elbo: 11045.433916015623 | train_rmse: 0.6036 | val_rmse: 0.8052 | val_ll: -1.4876
[2:18:20.826229] epoch: 4200 | elbo: 11038.62275390625 | train_rmse: 0.6034 | val_rmse: 0.8058 | val_ll: -1.4888
[2:20:00.974158] epoch: 4250 | elbo: 10978.28990234375 | train_rmse: 0.6019 | val_rmse: 0.8044 | val_ll: -1.4843
[2:21:42.277916] epoch: 4300 | elbo: 10991.104306640627 | train_rmse: 0.6033 | val_rmse: 0.8062 | val_ll: -1.493
[2:23:23.153511] epoch: 4350 | elbo: 10983.295986328125 | train_rmse: 0.6001 | val_rmse: 0.8043 | val_ll: -1.4875
[2:25:02.642150] epoch: 4400 | elbo: 11004.710703124998 | train_rmse: 0.6 | val_rmse: 0.8046 | val_ll: -1.4867
[2:26:43.183988] epoch: 4450 | elbo: 10941.377499999999 | train_rmse: 0.6005 | val_rmse: 0.8054 | val_ll: -1.4904
[2:28:24.091183] epoch: 4500 | elbo: 10848.218330078127 | train_rmse: 0.598 | val_rmse: 0.8033 | val_ll: -1.4794
[2:30:04.732406] epoch: 4550 | elbo: 10899.699443359374 | train_rmse: 0.5971 | val_rmse: 0.8028 | val_ll: -1.4766
[2:31:46.608906] epoch: 4600 | elbo: 10838.190488281249 | train_rmse: 0.5972 | val_rmse: 0.8033 | val_ll: -1.4796
[2:33:27.515587] epoch: 4650 | elbo: 10852.4359375 | train_rmse: 0.5964 | val_rmse: 0.8026 | val_ll: -1.4767
[2:35:09.008866] epoch: 4700 | elbo: 10810.972685546876 | train_rmse: 0.5961 | val_rmse: 0.8028 | val_ll: -1.4766
[2:36:49.159376] epoch: 4750 | elbo: 10785.3710546875 | train_rmse: 0.595 | val_rmse: 0.8027 | val_ll: -1.4739
[2:38:27.542152] epoch: 4800 | elbo: 10798.82685546875 | train_rmse: 0.5942 | val_rmse: 0.8016 | val_ll: -1.4733
[2:40:05.502910] epoch: 4850 | elbo: 10767.28046875 | train_rmse: 0.5938 | val_rmse: 0.8019 | val_ll: -1.4747
[2:41:45.441918] epoch: 4900 | elbo: 10742.092998046872 | train_rmse: 0.5932 | val_rmse: 0.802 | val_ll: -1.4741
[2:43:26.204095] epoch: 4950 | elbo: 10770.038144531249 | train_rmse: 0.5926 | val_rmse: 0.8013 | val_ll: -1.4716
[2:45:07.049919] epoch: 5000 | elbo: 10794.746220703124 | train_rmse: 0.5937 | val_rmse: 0.8023 | val_ll: -1.4736
[2:46:47.244853] epoch: 5050 | elbo: 10728.447373046873 | train_rmse: 0.5924 | val_rmse: 0.8015 | val_ll: -1.4731
[2:48:26.829698] epoch: 5100 | elbo: 10727.799770507812 | train_rmse: 0.5911 | val_rmse: 0.8013 | val_ll: -1.4693
[2:50:07.460713] epoch: 5150 | elbo: 10723.492138671876 | train_rmse: 0.591 | val_rmse: 0.8005 | val_ll: -1.4665
[2:51:49.852623] epoch: 5200 | elbo: 10667.334355468749 | train_rmse: 0.5898 | val_rmse: 0.8002 | val_ll: -1.4663
[2:53:28.148070] epoch: 5250 | elbo: 10660.0678125 | train_rmse: 0.5891 | val_rmse: 0.7998 | val_ll: -1.4637
[2:55:06.615554] epoch: 5300 | elbo: 10640.588232421873 | train_rmse: 0.5893 | val_rmse: 0.8001 | val_ll: -1.4657
[2:56:44.995626] epoch: 5350 | elbo: 10648.691845703124 | train_rmse: 0.5879 | val_rmse: 0.7998 | val_ll: -1.4648
[2:58:23.755164] epoch: 5400 | elbo: 10615.343720703124 | train_rmse: 0.5875 | val_rmse: 0.8002 | val_ll: -1.4611
[3:00:02.509280] epoch: 5450 | elbo: 10598.031376953126 | train_rmse: 0.5877 | val_rmse: 0.8003 | val_ll: -1.4663
[3:01:41.246067] epoch: 5500 | elbo: 10576.320634765625 | train_rmse: 0.586 | val_rmse: 0.7991 | val_ll: -1.4584
[3:03:20.131949] epoch: 5550 | elbo: 10579.130439453125 | train_rmse: 0.5859 | val_rmse: 0.7994 | val_ll: -1.4594
[3:04:57.894129] epoch: 5600 | elbo: 10570.4323828125 | train_rmse: 0.585 | val_rmse: 0.7989 | val_ll: -1.46
[3:06:37.556136] epoch: 5650 | elbo: 10545.003554687499 | train_rmse: 0.585 | val_rmse: 0.7987 | val_ll: -1.4585
[3:08:18.767525] epoch: 5700 | elbo: 10536.73177734375 | train_rmse: 0.5842 | val_rmse: 0.7982 | val_ll: -1.4593
[3:10:00.214242] epoch: 5750 | elbo: 10539.176147460936 | train_rmse: 0.5845 | val_rmse: 0.7987 | val_ll: -1.4613
[3:11:42.262175] epoch: 5800 | elbo: 10528.28486328125 | train_rmse: 0.5851 | val_rmse: 0.7996 | val_ll: -1.4635
[3:13:22.800278] epoch: 5850 | elbo: 10498.262089843749 | train_rmse: 0.5834 | val_rmse: 0.7984 | val_ll: -1.4615
[3:15:02.888791] epoch: 5900 | elbo: 10487.372744140626 | train_rmse: 0.5824 | val_rmse: 0.7984 | val_ll: -1.4583
[3:16:43.652036] epoch: 5950 | elbo: 10467.644140625 | train_rmse: 0.5821 | val_rmse: 0.7982 | val_ll: -1.4633
[3:18:25.894777] epoch: 6000 | elbo: 10515.796933593749 | train_rmse: 0.5838 | val_rmse: 0.7986 | val_ll: -1.4593
[3:20:06.812277] epoch: 6050 | elbo: 10473.976962890625 | train_rmse: 0.5816 | val_rmse: 0.7972 | val_ll: -1.4573
[3:21:47.428975] epoch: 6100 | elbo: 10443.1797265625 | train_rmse: 0.5804 | val_rmse: 0.797 | val_ll: -1.4564
[3:23:26.706870] epoch: 6150 | elbo: 10430.824394531252 | train_rmse: 0.5818 | val_rmse: 0.7975 | val_ll: -1.4552
[3:25:04.302284] epoch: 6200 | elbo: 10447.182333984372 | train_rmse: 0.5802 | val_rmse: 0.7971 | val_ll: -1.4563
[3:26:43.771522] epoch: 6250 | elbo: 10374.317695312502 | train_rmse: 0.5794 | val_rmse: 0.7966 | val_ll: -1.4559
[3:28:23.063067] epoch: 6300 | elbo: 10424.08216796875 | train_rmse: 0.5805 | val_rmse: 0.7976 | val_ll: -1.4611
[3:30:00.403658] epoch: 6350 | elbo: 10403.013935546875 | train_rmse: 0.5793 | val_rmse: 0.7963 | val_ll: -1.4607
[3:31:39.367630] epoch: 6400 | elbo: 10388.880927734377 | train_rmse: 0.5784 | val_rmse: 0.7962 | val_ll: -1.4583
[3:33:21.798287] epoch: 6450 | elbo: 10359.531181640625 | train_rmse: 0.5785 | val_rmse: 0.7964 | val_ll: -1.4576
[3:35:01.487117] epoch: 6500 | elbo: 10378.620039062498 | train_rmse: 0.5773 | val_rmse: 0.7954 | val_ll: -1.454
[3:36:42.143324] epoch: 6550 | elbo: 10349.9576953125 | train_rmse: 0.5768 | val_rmse: 0.7954 | val_ll: -1.4569
[3:38:21.604904] epoch: 6600 | elbo: 10327.088168945313 | train_rmse: 0.5763 | val_rmse: 0.7945 | val_ll: -1.4515
[3:40:01.361487] epoch: 6650 | elbo: 10316.231762695312 | train_rmse: 0.5762 | val_rmse: 0.7952 | val_ll: -1.4527
[3:41:42.507471] epoch: 6700 | elbo: 10352.46884765625 | train_rmse: 0.5754 | val_rmse: 0.7948 | val_ll: -1.4543
[3:43:22.507385] epoch: 6750 | elbo: 10295.569575195312 | train_rmse: 0.5748 | val_rmse: 0.7946 | val_ll: -1.4567
[3:45:02.230051] epoch: 6800 | elbo: 10315.600732421874 | train_rmse: 0.5756 | val_rmse: 0.7957 | val_ll: -1.4577
[3:46:42.192387] epoch: 6850 | elbo: 10277.083056640626 | train_rmse: 0.5739 | val_rmse: 0.7943 | val_ll: -1.4537
[3:48:21.427162] epoch: 6900 | elbo: 10288.71337890625 | train_rmse: 0.5737 | val_rmse: 0.794 | val_ll: -1.4549
[3:50:01.411572] epoch: 6950 | elbo: 10292.080659179686 | train_rmse: 0.5738 | val_rmse: 0.7947 | val_ll: -1.4561
[3:51:41.472753] epoch: 7000 | elbo: 10272.840429687498 | train_rmse: 0.5731 | val_rmse: 0.7942 | val_ll: -1.4543
[3:53:21.648966] epoch: 7050 | elbo: 10282.402353515625 | train_rmse: 0.5721 | val_rmse: 0.7934 | val_ll: -1.4519
[3:55:01.213728] epoch: 7100 | elbo: 10238.230390625 | train_rmse: 0.5725 | val_rmse: 0.7941 | val_ll: -1.4576
[3:56:43.189935] epoch: 7150 | elbo: 10244.150805664063 | train_rmse: 0.5717 | val_rmse: 0.7936 | val_ll: -1.455
[3:58:24.693287] epoch: 7200 | elbo: 10253.077001953126 | train_rmse: 0.5711 | val_rmse: 0.7938 | val_ll: -1.4533
[4:00:07.054224] epoch: 7250 | elbo: 10209.746425781252 | train_rmse: 0.5712 | val_rmse: 0.7929 | val_ll: -1.4563
[4:01:48.036930] epoch: 7300 | elbo: 10196.735283203127 | train_rmse: 0.5704 | val_rmse: 0.7925 | val_ll: -1.4529
[4:03:30.147683] epoch: 7350 | elbo: 10205.861572265629 | train_rmse: 0.5697 | val_rmse: 0.7924 | val_ll: -1.4497
[4:05:10.948822] epoch: 7400 | elbo: 10210.021123046874 | train_rmse: 0.5692 | val_rmse: 0.792 | val_ll: -1.451
[4:06:52.637132] epoch: 7450 | elbo: 10170.8089453125 | train_rmse: 0.5685 | val_rmse: 0.7917 | val_ll: -1.4477
[4:08:33.947539] epoch: 7500 | elbo: 10194.410185546873 | train_rmse: 0.5684 | val_rmse: 0.7921 | val_ll: -1.4518
[4:10:14.743146] epoch: 7550 | elbo: 10195.248906249999 | train_rmse: 0.5696 | val_rmse: 0.7928 | val_ll: -1.4531
[4:11:56.349392] epoch: 7600 | elbo: 10162.549921874997 | train_rmse: 0.567 | val_rmse: 0.7915 | val_ll: -1.4468
[4:13:37.480990] epoch: 7650 | elbo: 10147.473593749999 | train_rmse: 0.5669 | val_rmse: 0.7914 | val_ll: -1.4465
[4:15:18.850207] epoch: 7700 | elbo: 10163.987280273435 | train_rmse: 0.5668 | val_rmse: 0.7916 | val_ll: -1.4463
[4:16:59.193749] epoch: 7750 | elbo: 10119.950742187499 | train_rmse: 0.5663 | val_rmse: 0.7907 | val_ll: -1.4464
[4:18:40.552647] epoch: 7800 | elbo: 10146.954013671875 | train_rmse: 0.5657 | val_rmse: 0.7908 | val_ll: -1.447
[4:20:21.056482] epoch: 7850 | elbo: 10115.092631835938 | train_rmse: 0.5657 | val_rmse: 0.7905 | val_ll: -1.4454
[4:22:01.945154] epoch: 7900 | elbo: 10135.070092773436 | train_rmse: 0.5652 | val_rmse: 0.7904 | val_ll: -1.4451
[4:23:41.556584] epoch: 7950 | elbo: 10107.840395507812 | train_rmse: 0.5645 | val_rmse: 0.7901 | val_ll: -1.4425
[4:25:22.064820] epoch: 8000 | elbo: 10129.20708984375 | train_rmse: 0.5647 | val_rmse: 0.7902 | val_ll: -1.4395
[4:27:02.364745] epoch: 8050 | elbo: 10102.60529296875 | train_rmse: 0.5649 | val_rmse: 0.7907 | val_ll: -1.4423
[4:28:42.690448] epoch: 8100 | elbo: 10089.908896484374 | train_rmse: 0.5635 | val_rmse: 0.7898 | val_ll: -1.441
[4:30:24.170711] epoch: 8150 | elbo: 10070.311259765625 | train_rmse: 0.5638 | val_rmse: 0.7902 | val_ll: -1.443
[4:32:05.517193] epoch: 8200 | elbo: 10074.445615234374 | train_rmse: 0.563 | val_rmse: 0.7896 | val_ll: -1.4403
[4:33:45.363570] epoch: 8250 | elbo: 10061.119599609376 | train_rmse: 0.5631 | val_rmse: 0.7899 | val_ll: -1.4399
[4:35:24.741228] epoch: 8300 | elbo: 10067.713662109376 | train_rmse: 0.5624 | val_rmse: 0.7894 | val_ll: -1.4377
[4:37:04.903235] epoch: 8350 | elbo: 10029.893208007814 | train_rmse: 0.5623 | val_rmse: 0.7891 | val_ll: -1.4375
[4:38:45.747313] epoch: 8400 | elbo: 10046.17525878906 | train_rmse: 0.5616 | val_rmse: 0.7897 | val_ll: -1.4375
[4:40:25.483336] epoch: 8450 | elbo: 10055.97928222656 | train_rmse: 0.5615 | val_rmse: 0.7895 | val_ll: -1.4369
[4:42:05.030534] epoch: 8500 | elbo: 10014.072275390625 | train_rmse: 0.5616 | val_rmse: 0.7898 | val_ll: -1.4398
[4:43:43.524782] epoch: 8550 | elbo: 9989.282421875 | train_rmse: 0.5607 | val_rmse: 0.7889 | val_ll: -1.4397
[4:45:23.278860] epoch: 8600 | elbo: 10010.393964843748 | train_rmse: 0.5602 | val_rmse: 0.7895 | val_ll: -1.4406
[4:47:03.894054] epoch: 8650 | elbo: 10050.204321289062 | train_rmse: 0.5603 | val_rmse: 0.7887 | val_ll: -1.4363
[4:48:44.643091] epoch: 8700 | elbo: 9996.342958984374 | train_rmse: 0.5595 | val_rmse: 0.7891 | val_ll: -1.4376
[4:50:25.269323] epoch: 8750 | elbo: 10001.866083984374 | train_rmse: 0.559 | val_rmse: 0.7891 | val_ll: -1.4393
[4:52:07.039620] epoch: 8800 | elbo: 9974.600307617187 | train_rmse: 0.5594 | val_rmse: 0.7896 | val_ll: -1.4388
[4:53:48.498173] epoch: 8850 | elbo: 9997.903315429687 | train_rmse: 0.5584 | val_rmse: 0.7891 | val_ll: -1.439
[4:55:28.358577] epoch: 8900 | elbo: 9964.00843261719 | train_rmse: 0.5587 | val_rmse: 0.7897 | val_ll: -1.4409
[4:57:08.377061] epoch: 8950 | elbo: 9989.647456054687 | train_rmse: 0.5597 | val_rmse: 0.7896 | val_ll: -1.4381
[4:58:50.119078] epoch: 9000 | elbo: 9959.263842773435 | train_rmse: 0.559 | val_rmse: 0.7892 | val_ll: -1.4364
[5:00:31.309797] epoch: 9050 | elbo: 9972.011875 | train_rmse: 0.5577 | val_rmse: 0.7892 | val_ll: -1.4389
[5:02:11.852994] epoch: 9100 | elbo: 9951.949321289061 | train_rmse: 0.5583 | val_rmse: 0.7899 | val_ll: -1.442
[5:03:51.987433] epoch: 9150 | elbo: 9925.121997070313 | train_rmse: 0.5571 | val_rmse: 0.7889 | val_ll: -1.4398
[5:05:32.254518] epoch: 9200 | elbo: 9952.703447265627 | train_rmse: 0.5566 | val_rmse: 0.7889 | val_ll: -1.4392
[5:07:12.996916] epoch: 9250 | elbo: 9948.955351562501 | train_rmse: 0.5563 | val_rmse: 0.789 | val_ll: -1.4374
[5:08:53.468480] epoch: 9300 | elbo: 9927.918681640625 | train_rmse: 0.556 | val_rmse: 0.7886 | val_ll: -1.4375
[5:10:32.512946] epoch: 9350 | elbo: 9893.319848632813 | train_rmse: 0.5555 | val_rmse: 0.7891 | val_ll: -1.4398
[5:12:11.236558] epoch: 9400 | elbo: 9907.923081054685 | train_rmse: 0.5554 | val_rmse: 0.7889 | val_ll: -1.441
[5:13:47.987546] epoch: 9450 | elbo: 9888.201440429682 | train_rmse: 0.5558 | val_rmse: 0.7887 | val_ll: -1.4406
[5:15:25.741133] epoch: 9500 | elbo: 9880.055048828122 | train_rmse: 0.5548 | val_rmse: 0.7882 | val_ll: -1.4416
[5:17:03.736283] epoch: 9550 | elbo: 9883.911669921876 | train_rmse: 0.5549 | val_rmse: 0.7887 | val_ll: -1.443
[5:18:42.749929] epoch: 9600 | elbo: 9874.369794921875 | train_rmse: 0.5541 | val_rmse: 0.7882 | val_ll: -1.4426
[5:20:23.253017] epoch: 9650 | elbo: 9905.616699218746 | train_rmse: 0.5577 | val_rmse: 0.7903 | val_ll: -1.4467
[5:22:02.447712] epoch: 9700 | elbo: 9894.453920898437 | train_rmse: 0.5536 | val_rmse: 0.7884 | val_ll: -1.4419
[5:23:41.555320] epoch: 9750 | elbo: 9852.35592285156 | train_rmse: 0.5534 | val_rmse: 0.7888 | val_ll: -1.4456
[5:25:21.979046] epoch: 9800 | elbo: 9884.845166015624 | train_rmse: 0.5538 | val_rmse: 0.7884 | val_ll: -1.4451
[5:27:03.618456] epoch: 9850 | elbo: 9874.554516601562 | train_rmse: 0.5535 | val_rmse: 0.7892 | val_ll: -1.4473
[5:28:44.103258] epoch: 9900 | elbo: 9854.189204101562 | train_rmse: 0.553 | val_rmse: 0.7887 | val_ll: -1.4462
[5:30:24.054963] epoch: 9950 | elbo: 9831.192778320314 | train_rmse: 0.5524 | val_rmse: 0.7893 | val_ll: -1.448
Training finished in 5:32:00.682135 seconds
Saved SVI model to tests/dataset-tests/sineasy10-2/models/sineasy10-3x32-s05/checkpoint_1.pt
File Size is 0.02385425567626953 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=32, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=32, out_features=32, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=32, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-2/models/sineasy10-3x32-s05/checkpoint_1.pt
using device: cuda:4
====== evaluating profile sineasy10-3x32-s05 - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:02:41.935400
End time: 2023-07-14 00:12:36.999238
Total time: 5:34:46.513190
