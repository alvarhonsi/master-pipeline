Start time: 2023-07-13 18:37:52.519713
torch.Size([512, 10]) torch.Size([512, 1])
Sequential(
  (0): Linear(in_features=10, out_features=64, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=64, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:5 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([64, 10]) Parameter containing:
tensor([[-4.7055e-01,  7.7723e-02, -1.6497e-01,  2.9022e-01,  4.0675e-01,
         -6.3767e-01,  3.4722e-02, -2.5379e-01, -1.6284e-01, -1.3169e-01],
        [-3.0010e-02,  4.3855e-01, -3.2597e-01, -1.2589e-01,  7.9512e-04,
          1.7448e-02,  3.6343e-01, -5.7123e-01,  1.6345e-01, -1.8414e-01],
        [ 3.1695e-01, -6.2356e-01,  8.1755e-02, -4.6912e-02, -6.2956e-01,
         -2.4581e-01,  1.2663e-01, -6.5499e-01,  1.4395e-01,  1.4855e-01],
        [ 6.2489e-01,  1.4868e-01, -1.3850e-01,  1.3872e-01, -1.4899e-02,
          7.4168e-02,  8.0431e-01,  7.7854e-02, -4.1658e-01,  8.5354e-02],
        [-3.2553e-01, -2.8801e-01,  2.2567e-01,  4.9777e-01, -1.9911e-01,
          3.0144e-01,  4.9109e-01,  2.1879e-01,  3.8196e-01, -2.7133e-01],
        [-2.6464e-01,  3.3197e-01,  6.6391e-01,  1.1292e-01, -3.6744e-01,
          3.1399e-02, -5.2900e-01, -1.6196e-01, -1.1871e-01,  5.6776e-01],
        [-3.5215e-01, -4.5850e-01, -4.0317e-02, -6.4856e-02, -2.6739e-01,
          2.3870e-01,  3.4565e-02,  5.8130e-02,  4.6244e-02, -1.8412e-01],
        [-2.6768e-01, -1.8494e-01, -4.6563e-01, -4.2335e-03, -1.0570e-02,
         -3.3949e-01,  2.4935e-01,  1.6192e-01, -2.6230e-01,  3.2583e-01],
        [-4.5460e-01,  1.0758e-01, -3.4644e-01,  3.4720e-01, -3.0876e-01,
         -3.8987e-01, -2.7232e-01, -1.7942e-01, -4.2415e-01,  2.1751e-01],
        [-1.0320e-01,  2.4547e-01,  5.2267e-01,  2.9095e-02,  1.9979e-01,
         -7.2688e-01,  1.0948e-01,  2.4556e-01,  6.5424e-01,  4.7848e-01],
        [ 1.5250e-01, -7.0425e-02, -6.0195e-01,  2.0197e-01,  6.8910e-01,
         -1.2018e-01, -6.8540e-01, -2.3276e-01, -6.7245e-02,  1.8919e-01],
        [ 2.2447e-01, -1.4621e-01,  3.5565e-01, -4.7403e-02,  1.1430e-01,
         -2.6283e-01,  4.4440e-02,  5.9665e-02,  5.4886e-01, -2.3479e-01],
        [-1.7293e-01, -7.0461e-02,  4.6707e-02, -4.1755e-01,  5.2895e-01,
         -1.1480e+00,  5.5346e-01, -1.0503e-01,  6.2045e-01,  7.4054e-01],
        [ 3.4016e-01,  1.6372e-01,  4.2707e-01,  2.2733e-01, -1.9062e-01,
          6.2198e-01, -1.1577e-01,  3.5773e-01,  2.0379e-02,  8.6080e-02],
        [-2.0021e-01, -5.0584e-01,  4.0756e-01, -1.8915e-01,  2.4281e-01,
         -1.2715e-01, -4.6967e-01,  1.9521e-01, -3.7476e-02,  5.7203e-01],
        [-2.2308e-02,  2.4479e-01, -2.3901e-01,  1.0286e-01,  2.8861e-01,
          3.6789e-01,  2.9074e-01, -3.3024e-01,  8.7911e-02, -3.9278e-02],
        [ 4.6081e-01,  8.1543e-02, -2.1251e-02, -2.3650e-02, -4.4767e-01,
          4.3709e-02,  6.3788e-01, -8.9534e-02, -1.2931e-01,  1.3668e-01],
        [-2.7174e-01, -2.9845e-02, -5.2986e-01, -4.6696e-01,  6.8350e-02,
          4.2445e-02, -1.2541e-01, -2.6697e-01,  3.4649e-01,  2.8743e-01],
        [ 8.7727e-02,  2.9708e-02, -6.4822e-02,  5.5918e-04,  2.9807e-02,
          2.8522e-02, -5.2565e-01,  3.5966e-01,  1.8574e-01,  1.8100e-01],
        [ 1.9633e-01, -5.8569e-01,  1.6522e-01,  1.3453e-01,  6.7216e-02,
          3.6898e-02, -3.5932e-01,  7.2716e-01,  6.0719e-02,  1.8102e-01],
        [-3.3073e-01,  1.2583e-01,  7.2673e-02, -1.2259e-01, -3.2208e-01,
          3.8971e-01, -3.8227e-02, -1.0781e-01,  3.3844e-02, -4.2881e-01],
        [ 2.2019e-01,  1.1385e-01,  5.7513e-01,  1.2715e-01,  5.6163e-01,
          8.4635e-03,  5.5919e-01,  4.8742e-01, -4.4357e-01,  4.5096e-01],
        [ 1.2366e-01,  2.7154e-01,  1.0904e-01,  3.3570e-01, -3.3076e-01,
         -1.2107e-01, -1.9123e-01,  9.0323e-02, -1.4837e-01,  1.9734e-01],
        [ 1.1489e-01, -1.4256e-01, -1.9283e-01, -2.5468e-01,  7.2670e-03,
         -9.0235e-02, -2.8353e-01, -4.0975e-01,  6.8734e-01, -3.6060e-02],
        [-3.2620e-01, -5.4657e-02, -1.9716e-01, -9.5264e-02, -1.5810e-01,
          8.6093e-04, -5.1286e-01, -8.0071e-02, -1.8692e-01,  4.7331e-01],
        [ 3.9340e-01, -9.3469e-02,  3.6525e-02, -2.8275e-01,  1.5062e-01,
         -6.5582e-02,  1.0046e-01,  1.6680e-01, -1.8953e-02,  1.7426e-01],
        [ 6.3864e-01,  2.9036e-01,  1.0530e-01,  9.4452e-01,  8.1687e-02,
         -1.1539e-01, -8.3923e-02,  2.2832e-01, -1.2635e-01, -4.9067e-02],
        [ 4.5155e-01, -7.0149e-01,  2.3566e-01, -7.0121e-02, -1.5753e-01,
         -5.4134e-01,  1.1768e-01, -2.0553e-01, -5.9172e-01, -4.6763e-01],
        [-1.4463e-01, -3.1150e-01, -6.5572e-02, -2.0690e-01, -2.3188e-01,
          5.6224e-01,  1.5760e-01, -2.5643e-01, -1.5759e-01, -3.0634e-01],
        [ 2.0284e-02, -1.7682e-01,  3.3379e-01,  3.9092e-01,  8.3983e-02,
         -2.9468e-02,  3.8338e-01,  3.6785e-02,  1.1529e-01,  3.5039e-01],
        [ 2.1169e-01,  3.4075e-02,  7.9850e-01, -1.0215e-02, -8.9201e-01,
          1.1984e-01,  3.2902e-01,  4.1593e-02, -8.3403e-02,  2.9744e-01],
        [-2.4175e-01, -3.7341e-01,  6.7016e-02,  5.2825e-01,  1.0347e-01,
          3.1218e-01, -4.1974e-01,  6.2904e-02, -2.7857e-01, -4.2696e-01],
        [ 1.8259e-01,  1.6987e-01,  8.1017e-02, -2.0498e-01, -2.9518e-01,
          9.1440e-02,  1.8976e-01, -4.8851e-01, -1.8532e-01,  2.9227e-01],
        [ 3.8332e-02,  1.9199e-01,  2.4806e-01,  1.9221e-01, -1.1880e-02,
          5.4946e-01, -3.3886e-01,  1.0163e-01, -3.0657e-01, -2.5788e-01],
        [ 2.7279e-01, -8.5053e-02, -1.2595e-01,  1.0949e-01, -7.1094e-01,
          3.4666e-01, -1.2573e-01, -6.8453e-01,  6.7378e-01, -8.1546e-02],
        [-4.6892e-01,  5.1411e-01,  1.3826e-01, -2.6037e-01,  3.5660e-01,
         -4.1309e-01,  4.4923e-01, -2.2559e-02, -6.6013e-02,  2.4147e-01],
        [-1.5596e-01,  6.3018e-02,  4.0857e-02,  4.3003e-01,  2.5483e-01,
          2.8341e-02, -4.8716e-01,  1.7842e-01,  7.0336e-01, -3.5481e-01],
        [ 9.1534e-01,  3.3286e-01,  9.4466e-03, -2.0642e-02,  3.9541e-01,
         -3.0142e-01,  2.6832e-01, -4.4809e-01,  3.0506e-01, -6.0669e-02],
        [ 1.1221e+00, -5.0513e-01, -2.4732e-01,  4.2398e-01,  3.6156e-01,
          5.1578e-01, -9.2662e-02, -5.3793e-02,  1.2676e-01, -2.1782e-01],
        [ 4.7096e-01, -1.5104e-01, -4.0421e-01,  1.8556e-01, -1.6791e-01,
          2.7225e-01,  3.6496e-02, -3.2677e-01, -2.1865e-01,  9.3141e-02],
        [-3.1351e-01,  3.1152e-01, -1.1552e-01,  1.2268e-01,  3.8512e-01,
         -7.7743e-01,  3.2079e-01,  8.9004e-02, -2.6407e-01, -2.2627e-01],
        [-1.6551e-01,  3.2956e-02, -1.7244e-01,  3.5130e-01, -6.9090e-01,
         -4.2136e-02, -7.0815e-02,  1.5812e-01, -5.7917e-01,  2.1859e-02],
        [-1.3480e-01, -2.7344e-01,  1.5578e-01,  1.8889e-01,  3.6480e-02,
         -3.0663e-01,  6.0058e-02, -1.1259e-01, -4.3025e-01, -2.5973e-01],
        [-1.8228e-01, -1.9581e-01, -1.3230e-01,  2.7668e-01,  2.5016e-02,
         -8.5224e-02, -1.4377e-01,  5.3520e-02, -1.3793e-01, -5.9823e-01],
        [ 1.0911e-01, -4.1173e-01,  5.1627e-02,  5.9348e-02,  3.8757e-01,
         -2.6078e-01, -1.4575e-02, -9.6783e-03,  5.3949e-02, -1.9142e-01],
        [ 1.5354e-02,  2.9561e-01,  4.8469e-01,  2.4284e-01, -4.9837e-01,
         -1.5400e-01,  2.8287e-04, -1.8389e-01, -3.7842e-01, -5.7877e-02],
        [-8.0447e-02,  2.6170e-01,  1.5233e-02,  1.9321e-01, -2.0762e-01,
         -1.2824e-01,  8.6922e-01, -1.7427e-01, -4.0889e-01,  1.9445e-01],
        [ 1.5706e-01,  5.3139e-01, -4.9887e-01, -1.5147e-01,  2.3804e-01,
         -3.4981e-01, -5.0034e-01, -7.9992e-02, -1.7987e-01,  3.4954e-01],
        [-2.5057e-01, -4.8308e-01,  9.2099e-02,  3.2849e-01, -1.5960e-01,
          2.4578e-01,  8.0088e-01,  2.8865e-01,  3.4843e-01, -1.6070e-01],
        [ 4.3949e-01,  2.1367e-01, -6.7850e-02,  1.4890e-01,  3.4425e-01,
          3.3842e-01, -3.8336e-01, -6.4357e-02,  3.8212e-01, -9.9684e-02],
        [ 9.3874e-02, -3.2927e-01,  6.6029e-01, -3.6743e-01,  2.2830e-01,
          2.0790e-01, -4.4684e-01,  2.2561e-01, -1.0770e-01, -3.0601e-01],
        [ 9.4951e-02, -6.9054e-01,  3.4416e-01, -4.1010e-01,  2.8382e-01,
          3.2856e-01, -1.2949e-02,  2.2625e-01,  1.2284e-01, -1.3453e-01],
        [-2.3934e-01,  6.3379e-01, -3.6480e-01,  2.2670e-01, -3.8683e-01,
          3.1415e-01, -2.8419e-03, -4.6589e-01, -4.8894e-01, -1.0469e-01],
        [-3.1749e-01,  1.8072e-01,  1.2850e-01, -6.4739e-01,  2.4058e-01,
         -1.4818e-01,  5.9296e-01,  9.9719e-02,  2.0376e-04,  4.6957e-01],
        [-2.0029e-01, -1.2951e-01,  7.6434e-02, -4.8001e-01,  5.7891e-01,
         -4.2104e-01,  4.0953e-01, -9.7740e-02,  1.5081e-01, -1.8972e-01],
        [ 2.3468e-02,  2.1525e-01,  1.4024e-01,  1.6309e-01, -2.0056e-01,
          3.0826e-01,  4.5833e-02, -2.7352e-02,  5.0540e-01, -4.8088e-01],
        [-1.5245e-02,  3.4255e-01,  1.5213e-01,  4.6244e-01, -1.8597e-01,
          1.4993e-01,  5.4041e-01, -9.0786e-02, -3.8168e-01, -4.2785e-01],
        [-2.9248e-01,  9.8655e-02, -2.4950e-01,  2.8295e-01,  4.1721e-02,
          2.7030e-01, -2.7259e-01,  2.9396e-01, -3.0851e-01, -3.3154e-01],
        [ 3.1878e-01, -6.6445e-01,  8.3464e-03, -2.5108e-01,  1.4633e-01,
         -5.2418e-02,  3.2567e-01, -8.1577e-02,  4.2284e-01, -4.8407e-01],
        [ 2.2996e-01, -3.9155e-01, -5.4097e-01,  1.4292e-01,  8.0524e-01,
          6.5129e-01,  2.1910e-01,  2.4208e-01, -1.2550e-01,  1.2273e-01],
        [-3.5255e-01, -1.4831e-01,  4.1050e-01, -3.3719e-02, -5.5132e-01,
          4.5431e-01,  4.8116e-01, -4.5448e-01, -4.8611e-02, -3.1892e-02],
        [-2.1390e-01, -2.8309e-01, -2.2985e-01,  6.3419e-02, -1.5726e-01,
          2.4130e-01,  2.4796e-01, -6.3984e-01,  1.2752e-02, -2.4208e-01],
        [ 1.4973e-01, -6.1791e-01,  5.5873e-01,  1.1944e-02,  1.7122e-01,
         -1.9318e-01, -3.8412e-01,  2.0646e-01,  6.5372e-01,  6.1538e-03],
        [-3.7688e-01, -2.7155e-01, -2.5154e-01, -1.5856e-01, -2.6760e-01,
          2.8676e-01, -3.5385e-01, -1.3218e-01, -1.8669e-01,  3.2991e-01]],
       device='cuda:5', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([64, 10]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100],
        [0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100]], device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([64]) Parameter containing:
tensor([ 0.2392, -0.0379, -0.3687, -0.8119, -0.5445,  0.3098,  0.0717, -0.5639,
        -0.3441,  0.1958,  0.1668, -0.2536, -0.1787, -0.0566, -0.0685,  0.3006,
        -0.4432,  0.0651,  0.1327, -0.0253, -0.1644, -0.0976, -0.0980, -0.0627,
         0.1629,  0.0829, -0.2601,  0.5188,  0.4388,  0.2773,  0.1520,  0.4767,
        -0.1585, -0.1049,  0.0123,  0.5252,  0.6004,  0.1246,  0.6866, -0.0283,
         0.2444,  0.1440, -0.5015, -0.3497, -0.1861, -0.2324, -0.0632, -0.4926,
        -0.3574,  0.3461,  0.1615,  0.4207, -0.2287,  0.3060,  0.0553, -0.4309,
        -0.3899,  0.0430, -0.2281, -0.1939, -0.4402, -0.5449,  0.3130, -0.1966],
       device='cuda:5', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([64]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100], device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([64, 64]) Parameter containing:
tensor([[-0.6811,  0.3161, -0.3270,  ...,  0.1441, -0.2110, -0.0239],
        [ 0.2203, -0.3347,  0.6441,  ...,  0.2032,  0.0659,  0.5272],
        [ 0.7038, -0.1670,  0.4566,  ..., -0.7412,  0.2807, -0.2283],
        ...,
        [ 0.1048,  0.1075,  0.0584,  ..., -0.1556,  0.2848,  0.2082],
        [-0.6225,  0.2305, -0.0193,  ...,  0.0437,  0.1458,  0.0625],
        [-0.2267,  0.0859,  0.5048,  ...,  0.2608, -0.1071, -0.0373]],
       device='cuda:5', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([64, 64]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([64]) Parameter containing:
tensor([ 0.3025,  0.1352,  0.3700,  0.1716, -0.0192,  0.1063, -0.5706, -0.4473,
         0.2992, -0.1567, -0.1146,  0.0921, -0.5697,  0.1017,  0.0600,  0.0913,
        -0.1491,  0.6061,  0.4346, -0.4054,  0.0227,  0.0837,  0.2729,  0.1709,
        -0.4732, -0.3316, -0.1909,  0.0864, -0.2754,  0.2093,  0.0051, -0.0323,
        -0.1965, -0.1512,  0.2758,  0.4396,  0.0263, -0.0469,  0.0548, -0.4210,
        -0.1051, -0.0760,  0.1387,  0.3863, -0.1109, -0.1161, -0.4074,  0.2749,
        -0.1825, -0.2378,  0.0911, -0.1028, -0.1763,  0.2711, -0.1233, -0.1339,
         0.0184, -0.5446,  0.1913,  0.0891,  0.2523, -0.3813, -0.2715,  0.4334],
       device='cuda:5', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([64]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100], device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([64, 64]) Parameter containing:
tensor([[-0.0744, -0.5360,  0.3588,  ..., -0.3374,  0.0014, -0.0660],
        [ 0.1757,  0.7729,  0.1265,  ...,  0.3470,  0.5208,  0.0036],
        [ 0.3038, -0.0595, -0.3465,  ..., -0.1033,  0.0408,  0.1115],
        ...,
        [-0.1124, -0.4488,  0.0415,  ..., -0.5524,  0.4044,  0.0128],
        [ 0.2510,  0.1914, -0.4134,  ...,  0.2205,  0.0806,  0.2155],
        [ 0.1556, -0.4279,  0.1402,  ..., -0.5084, -0.0691,  0.5523]],
       device='cuda:5', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([64, 64]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([64]) Parameter containing:
tensor([-0.2210,  0.1183, -0.1472, -0.3089,  0.1225,  0.5399, -0.1065,  0.2722,
         0.0994, -0.2531,  0.0213,  0.4683,  0.2405, -0.2473,  0.2104, -0.5272,
        -0.3478,  0.1336, -0.3497, -0.0275,  0.2957,  0.5428, -0.0084, -0.1596,
        -0.1397, -0.4866,  0.0664,  0.3073, -0.0928, -0.2963, -0.4377,  0.3151,
         0.1783,  0.0243, -0.5760,  0.0975, -0.3268, -0.0503, -0.2340,  0.2785,
         0.1606,  0.1490, -0.0595, -0.2117,  0.0053,  0.4937,  0.0638, -0.4911,
         0.3041, -0.0662,  0.0253,  0.6752, -0.4164, -0.8839,  0.6171,  0.2710,
         0.1959,  0.1598,  0.5000,  0.3911,  0.4414, -0.4382,  0.0797, -0.2578],
       device='cuda:5', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([64]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100], device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 64]) Parameter containing:
tensor([[ 3.1297e-01, -1.3180e-01,  4.1689e-01,  1.1147e-01, -2.3095e-01,
         -2.8177e-01, -2.7794e-01, -5.4138e-01,  1.0726e-01,  4.0789e-01,
          8.4096e-02, -2.6012e-02, -1.2065e-01,  6.3524e-03, -2.2531e-02,
          4.0374e-01,  3.8021e-01, -1.4948e-01, -3.4344e-01,  3.0148e-01,
         -2.6721e-02, -8.3000e-02,  1.4736e-01, -2.3529e-01, -1.2065e-01,
          5.9608e-01, -3.3587e-01, -3.4515e-01,  9.5972e-02,  2.4205e-01,
         -8.7279e-03,  1.3463e-01,  3.2840e-02, -2.4451e-01,  1.4154e-01,
          2.0842e-02,  2.2589e-01, -7.3635e-02, -3.2984e-01,  5.1806e-01,
         -3.8147e-01, -7.9524e-02,  1.2603e-01, -3.9635e-04, -2.0608e-01,
         -1.4884e-01, -4.4520e-02, -4.7193e-02,  9.0008e-02, -3.3710e-01,
         -5.8034e-01,  2.6689e-01, -4.3677e-02, -1.7518e-01,  4.6406e-01,
          2.7115e-02,  5.3700e-01, -3.1759e-01, -1.4925e-02,  1.0452e-01,
         -1.2647e-02,  2.2349e-02, -9.3917e-02, -5.9171e-01]], device='cuda:5',
       requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 64]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100]], device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([0.2879], device='cuda:5', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:5', grad_fn=<AddBackward0>)
Using device: cuda:5
===== Training profile sineasy10-3x64-s05 - 1 =====
[0:00:01.928097] epoch: 0 | elbo: 2574101.1325 | train_rmse: 10.5452 | val_rmse: 10.4497 | val_ll: -79.4024
[0:01:43.403589] epoch: 50 | elbo: 203194.73468749996 | train_rmse: 3.0983 | val_rmse: 3.1018 | val_ll: -7.933
[0:03:25.376352] epoch: 100 | elbo: 104351.75679687501 | train_rmse: 2.1906 | val_rmse: 2.2347 | val_ll: -4.8746
[0:05:08.389570] epoch: 150 | elbo: 72350.07804687499 | train_rmse: 1.7958 | val_rmse: 1.8982 | val_ll: -4.0356
[0:06:49.802342] epoch: 200 | elbo: 56389.6709375 | train_rmse: 1.5523 | val_rmse: 1.7047 | val_ll: -3.5722
[0:08:31.338329] epoch: 250 | elbo: 46708.78765625 | train_rmse: 1.3917 | val_rmse: 1.5716 | val_ll: -3.2528
[0:10:13.431701] epoch: 300 | elbo: 40260.40703125 | train_rmse: 1.2718 | val_rmse: 1.4751 | val_ll: -3.0625
[0:11:54.787527] epoch: 350 | elbo: 35707.74982421876 | train_rmse: 1.1789 | val_rmse: 1.4016 | val_ll: -2.8938
[0:13:37.212108] epoch: 400 | elbo: 32082.604003906257 | train_rmse: 1.0988 | val_rmse: 1.3441 | val_ll: -2.7664
[0:15:19.484379] epoch: 450 | elbo: 29043.56859375 | train_rmse: 1.0318 | val_rmse: 1.2971 | val_ll: -2.6662
[0:17:01.213739] epoch: 500 | elbo: 26725.046132812502 | train_rmse: 0.9744 | val_rmse: 1.2572 | val_ll: -2.5645
[0:18:44.076665] epoch: 550 | elbo: 24789.909609375 | train_rmse: 0.9273 | val_rmse: 1.2247 | val_ll: -2.4808
[0:20:24.764519] epoch: 600 | elbo: 23346.90716796875 | train_rmse: 0.8876 | val_rmse: 1.1978 | val_ll: -2.4241
[0:22:05.266949] epoch: 650 | elbo: 21913.00404296875 | train_rmse: 0.8532 | val_rmse: 1.172 | val_ll: -2.3792
[0:23:46.957991] epoch: 700 | elbo: 20948.500478515627 | train_rmse: 0.8233 | val_rmse: 1.153 | val_ll: -2.3273
[0:25:28.207718] epoch: 750 | elbo: 20008.461005859375 | train_rmse: 0.7948 | val_rmse: 1.134 | val_ll: -2.3029
[0:27:10.820917] epoch: 800 | elbo: 19023.183935546873 | train_rmse: 0.7705 | val_rmse: 1.1184 | val_ll: -2.2808
[0:28:52.367656] epoch: 850 | elbo: 18266.05248046875 | train_rmse: 0.746 | val_rmse: 1.102 | val_ll: -2.2344
[0:30:34.399656] epoch: 900 | elbo: 17754.78302734375 | train_rmse: 0.7266 | val_rmse: 1.0893 | val_ll: -2.1995
[0:32:15.803601] epoch: 950 | elbo: 17103.654091796874 | train_rmse: 0.7076 | val_rmse: 1.0794 | val_ll: -2.1781
[0:33:58.469313] epoch: 1000 | elbo: 16593.168085937497 | train_rmse: 0.6905 | val_rmse: 1.0692 | val_ll: -2.1531
[0:35:40.668436] epoch: 1050 | elbo: 16191.860039062502 | train_rmse: 0.6737 | val_rmse: 1.0604 | val_ll: -2.1354
[0:37:22.016314] epoch: 1100 | elbo: 15787.899316406247 | train_rmse: 0.6593 | val_rmse: 1.0534 | val_ll: -2.1092
[0:39:03.425442] epoch: 1150 | elbo: 15412.491904296874 | train_rmse: 0.6466 | val_rmse: 1.0497 | val_ll: -2.0965
[0:40:45.937080] epoch: 1200 | elbo: 15186.302177734377 | train_rmse: 0.6366 | val_rmse: 1.0456 | val_ll: -2.0832
[0:42:27.364369] epoch: 1250 | elbo: 14773.983681640626 | train_rmse: 0.6211 | val_rmse: 1.0376 | val_ll: -2.0544
[0:44:08.487897] epoch: 1300 | elbo: 14507.307207031252 | train_rmse: 0.6107 | val_rmse: 1.0329 | val_ll: -2.0414
[0:45:53.274345] epoch: 1350 | elbo: 14254.257460937497 | train_rmse: 0.5997 | val_rmse: 1.0297 | val_ll: -2.0267
[0:47:39.245263] epoch: 1400 | elbo: 14021.10521484375 | train_rmse: 0.5901 | val_rmse: 1.0266 | val_ll: -2.0175
[0:49:25.372839] epoch: 1450 | elbo: 13793.023964843747 | train_rmse: 0.5793 | val_rmse: 1.0235 | val_ll: -2.0016
[0:51:10.470653] epoch: 1500 | elbo: 13618.202695312502 | train_rmse: 0.5712 | val_rmse: 1.0211 | val_ll: -1.9927
[0:52:57.365451] epoch: 1550 | elbo: 13439.627412109374 | train_rmse: 0.5633 | val_rmse: 1.0186 | val_ll: -1.9863
[0:54:43.539563] epoch: 1600 | elbo: 13239.950893554687 | train_rmse: 0.5548 | val_rmse: 1.0165 | val_ll: -1.9741
[0:56:28.464543] epoch: 1650 | elbo: 13052.027187500002 | train_rmse: 0.548 | val_rmse: 1.0145 | val_ll: -1.9619
[0:58:14.637641] epoch: 1700 | elbo: 12942.262758789064 | train_rmse: 0.5409 | val_rmse: 1.0136 | val_ll: -1.9653
[0:59:55.102019] epoch: 1750 | elbo: 12833.332021484373 | train_rmse: 0.5344 | val_rmse: 1.012 | val_ll: -1.9546
[1:01:36.207906] epoch: 1800 | elbo: 12670.436269531248 | train_rmse: 0.5291 | val_rmse: 1.0108 | val_ll: -1.9471
[1:03:17.360246] epoch: 1850 | elbo: 12506.569189453123 | train_rmse: 0.5221 | val_rmse: 1.0103 | val_ll: -1.9476
[1:05:01.859552] epoch: 1900 | elbo: 12420.547939453123 | train_rmse: 0.5163 | val_rmse: 1.0086 | val_ll: -1.9499
[1:06:50.062772] epoch: 1950 | elbo: 12307.769921875 | train_rmse: 0.5148 | val_rmse: 1.0076 | val_ll: -1.9462
[1:08:35.201705] epoch: 2000 | elbo: 12179.416337890625 | train_rmse: 0.5061 | val_rmse: 1.0066 | val_ll: -1.9452
[1:10:18.755574] epoch: 2050 | elbo: 12096.168583984374 | train_rmse: 0.5006 | val_rmse: 1.0053 | val_ll: -1.9391
[1:12:05.152069] epoch: 2100 | elbo: 11955.165717773436 | train_rmse: 0.4959 | val_rmse: 1.004 | val_ll: -1.9237
[1:13:50.527670] epoch: 2150 | elbo: 11925.661044921875 | train_rmse: 0.4917 | val_rmse: 1.0042 | val_ll: -1.9269
[1:15:37.086611] epoch: 2200 | elbo: 11797.557851562498 | train_rmse: 0.4864 | val_rmse: 1.0034 | val_ll: -1.926
[1:17:21.106325] epoch: 2250 | elbo: 11685.9135546875 | train_rmse: 0.4819 | val_rmse: 1.0036 | val_ll: -1.9222
[1:19:08.646037] epoch: 2300 | elbo: 11582.616123046875 | train_rmse: 0.4781 | val_rmse: 1.0027 | val_ll: -1.9208
[1:20:55.118214] epoch: 2350 | elbo: 11550.501728515625 | train_rmse: 0.474 | val_rmse: 1.0015 | val_ll: -1.9107
[1:22:41.976388] epoch: 2400 | elbo: 11430.822958984378 | train_rmse: 0.4699 | val_rmse: 1.0017 | val_ll: -1.9178
[1:24:26.608808] epoch: 2450 | elbo: 11371.00703125 | train_rmse: 0.4657 | val_rmse: 1.0011 | val_ll: -1.9116
[1:26:11.782595] epoch: 2500 | elbo: 11309.941206054687 | train_rmse: 0.4617 | val_rmse: 1.0007 | val_ll: -1.9143
[1:27:54.886669] epoch: 2550 | elbo: 11221.168847656249 | train_rmse: 0.4585 | val_rmse: 0.9999 | val_ll: -1.9111
[1:29:40.868035] epoch: 2600 | elbo: 11181.33238769531 | train_rmse: 0.455 | val_rmse: 1.0005 | val_ll: -1.9093
[1:31:25.716363] epoch: 2650 | elbo: 11098.509951171876 | train_rmse: 0.4506 | val_rmse: 0.9987 | val_ll: -1.9031
[1:33:10.653046] epoch: 2700 | elbo: 11060.9151171875 | train_rmse: 0.4471 | val_rmse: 0.9982 | val_ll: -1.9093
[1:34:54.625420] epoch: 2750 | elbo: 11009.450239257816 | train_rmse: 0.4439 | val_rmse: 0.9981 | val_ll: -1.9051
[1:36:39.633137] epoch: 2800 | elbo: 10954.945488281248 | train_rmse: 0.4403 | val_rmse: 0.9979 | val_ll: -1.9112
[1:38:23.170468] epoch: 2850 | elbo: 10881.806489257808 | train_rmse: 0.4371 | val_rmse: 0.9966 | val_ll: -1.899
[1:40:07.581095] epoch: 2900 | elbo: 10835.257612304687 | train_rmse: 0.4347 | val_rmse: 0.9969 | val_ll: -1.8968
[1:41:52.663507] epoch: 2950 | elbo: 10775.82748535156 | train_rmse: 0.433 | val_rmse: 0.9982 | val_ll: -1.9063
[1:43:36.644624] epoch: 3000 | elbo: 10754.771791992189 | train_rmse: 0.429 | val_rmse: 0.9974 | val_ll: -1.9072
[1:45:20.565573] epoch: 3050 | elbo: 10716.099067382813 | train_rmse: 0.4272 | val_rmse: 0.9979 | val_ll: -1.9111
[1:47:05.918774] epoch: 3100 | elbo: 10640.808535156251 | train_rmse: 0.4238 | val_rmse: 0.9964 | val_ll: -1.9003
[1:48:49.445962] epoch: 3150 | elbo: 10600.098662109373 | train_rmse: 0.4198 | val_rmse: 0.9974 | val_ll: -1.9025
[1:50:32.710619] epoch: 3200 | elbo: 10543.754682617187 | train_rmse: 0.4176 | val_rmse: 0.9967 | val_ll: -1.8985
[1:52:13.591902] epoch: 3250 | elbo: 10515.744804687502 | train_rmse: 0.4147 | val_rmse: 0.997 | val_ll: -1.9076
[1:53:55.211028] epoch: 3300 | elbo: 10487.502822265624 | train_rmse: 0.4127 | val_rmse: 0.9965 | val_ll: -1.901
[1:55:37.927521] epoch: 3350 | elbo: 10409.182670898435 | train_rmse: 0.4109 | val_rmse: 0.9964 | val_ll: -1.9015
[1:57:21.012862] epoch: 3400 | elbo: 10426.057617187498 | train_rmse: 0.4079 | val_rmse: 0.9968 | val_ll: -1.9113
[1:59:06.494196] epoch: 3450 | elbo: 10359.335859375 | train_rmse: 0.4057 | val_rmse: 0.9971 | val_ll: -1.9013
[2:00:51.885342] epoch: 3500 | elbo: 10291.09279296875 | train_rmse: 0.4026 | val_rmse: 0.9977 | val_ll: -1.9134
[2:02:38.773242] epoch: 3550 | elbo: 10301.773149414064 | train_rmse: 0.404 | val_rmse: 0.9984 | val_ll: -1.9064
[2:04:23.497929] epoch: 3600 | elbo: 10232.36705078125 | train_rmse: 0.3987 | val_rmse: 0.9979 | val_ll: -1.9051
[2:06:08.028427] epoch: 3650 | elbo: 10195.216147460937 | train_rmse: 0.3972 | val_rmse: 0.9992 | val_ll: -1.914
[2:07:51.883862] epoch: 3700 | elbo: 10157.210297851563 | train_rmse: 0.3939 | val_rmse: 0.9989 | val_ll: -1.9141
[2:09:36.498128] epoch: 3750 | elbo: 10149.06800292969 | train_rmse: 0.392 | val_rmse: 0.9989 | val_ll: -1.9164
[2:11:20.551638] epoch: 3800 | elbo: 10090.585703124998 | train_rmse: 0.3904 | val_rmse: 0.9999 | val_ll: -1.9165
[2:13:03.613559] epoch: 3850 | elbo: 10082.624853515623 | train_rmse: 0.3879 | val_rmse: 1.0008 | val_ll: -1.9189
[2:14:46.330744] epoch: 3900 | elbo: 10017.265771484375 | train_rmse: 0.3861 | val_rmse: 1.0004 | val_ll: -1.9155
[2:16:30.663860] epoch: 3950 | elbo: 9995.519365234375 | train_rmse: 0.3837 | val_rmse: 1.0002 | val_ll: -1.914
[2:18:13.310650] epoch: 4000 | elbo: 9985.369873046875 | train_rmse: 0.3816 | val_rmse: 1.0019 | val_ll: -1.9243
[2:19:55.726058] epoch: 4050 | elbo: 9928.151992187499 | train_rmse: 0.3801 | val_rmse: 1.0015 | val_ll: -1.917
[2:21:39.022769] epoch: 4100 | elbo: 9940.649624023437 | train_rmse: 0.3782 | val_rmse: 1.002 | val_ll: -1.9149
[2:23:21.894442] epoch: 4150 | elbo: 9869.200986328124 | train_rmse: 0.3766 | val_rmse: 1.0039 | val_ll: -1.923
[2:25:06.005824] epoch: 4200 | elbo: 9866.275654296875 | train_rmse: 0.3739 | val_rmse: 1.0022 | val_ll: -1.9184
[2:26:47.751665] epoch: 4250 | elbo: 9834.588525390627 | train_rmse: 0.3718 | val_rmse: 1.0034 | val_ll: -1.9287
[2:28:29.315548] epoch: 4300 | elbo: 9793.040146484378 | train_rmse: 0.37 | val_rmse: 1.0043 | val_ll: -1.925
[2:30:14.582055] epoch: 4350 | elbo: 9770.282324218751 | train_rmse: 0.3679 | val_rmse: 1.0042 | val_ll: -1.9243
[2:32:01.904471] epoch: 4400 | elbo: 9731.688471679689 | train_rmse: 0.3662 | val_rmse: 1.0061 | val_ll: -1.929
[2:33:49.053544] epoch: 4450 | elbo: 9730.659106445313 | train_rmse: 0.3648 | val_rmse: 1.0059 | val_ll: -1.9275
[2:35:30.037646] epoch: 4500 | elbo: 9702.660332031253 | train_rmse: 0.363 | val_rmse: 1.0065 | val_ll: -1.9397
[2:37:12.078343] epoch: 4550 | elbo: 9683.504970703125 | train_rmse: 0.362 | val_rmse: 1.0065 | val_ll: -1.9243
[2:38:52.747171] epoch: 4600 | elbo: 9654.860468750001 | train_rmse: 0.3609 | val_rmse: 1.0069 | val_ll: -1.9314
[2:40:35.021569] epoch: 4650 | elbo: 9625.531914062502 | train_rmse: 0.3583 | val_rmse: 1.0085 | val_ll: -1.9447
[2:42:17.846592] epoch: 4700 | elbo: 9609.453974609374 | train_rmse: 0.3573 | val_rmse: 1.0091 | val_ll: -1.9471
[2:43:59.200712] epoch: 4750 | elbo: 9568.1983984375 | train_rmse: 0.3552 | val_rmse: 1.0089 | val_ll: -1.9429
[2:45:40.265646] epoch: 4800 | elbo: 9537.886494140625 | train_rmse: 0.3538 | val_rmse: 1.0098 | val_ll: -1.9436
[2:47:21.392181] epoch: 4850 | elbo: 9556.710869140625 | train_rmse: 0.3535 | val_rmse: 1.0115 | val_ll: -1.9546
[2:49:02.645116] epoch: 4900 | elbo: 9528.324887695313 | train_rmse: 0.3507 | val_rmse: 1.0112 | val_ll: -1.9439
[2:50:44.217589] epoch: 4950 | elbo: 9479.06259765625 | train_rmse: 0.3494 | val_rmse: 1.0111 | val_ll: -1.9497
[2:52:25.249383] epoch: 5000 | elbo: 9470.556147460939 | train_rmse: 0.3468 | val_rmse: 1.0117 | val_ll: -1.9525
[2:54:06.829830] epoch: 5050 | elbo: 9448.229501953127 | train_rmse: 0.3475 | val_rmse: 1.0123 | val_ll: -1.9556
[2:55:49.093929] epoch: 5100 | elbo: 9428.561049804686 | train_rmse: 0.3448 | val_rmse: 1.0125 | val_ll: -1.953
[2:57:31.033152] epoch: 5150 | elbo: 9414.54473144531 | train_rmse: 0.3442 | val_rmse: 1.0131 | val_ll: -1.9619
[2:59:11.887446] epoch: 5200 | elbo: 9417.639970703125 | train_rmse: 0.3422 | val_rmse: 1.0134 | val_ll: -1.9535
[3:00:52.916612] epoch: 5250 | elbo: 9379.027373046873 | train_rmse: 0.341 | val_rmse: 1.0136 | val_ll: -1.9559
[3:02:35.094555] epoch: 5300 | elbo: 9380.705200195312 | train_rmse: 0.3399 | val_rmse: 1.0144 | val_ll: -1.9526
[3:04:16.227039] epoch: 5350 | elbo: 9349.687993164063 | train_rmse: 0.3382 | val_rmse: 1.0162 | val_ll: -1.9632
[3:05:58.426624] epoch: 5400 | elbo: 9314.719487304688 | train_rmse: 0.3367 | val_rmse: 1.0154 | val_ll: -1.9573
[3:07:40.919776] epoch: 5450 | elbo: 9309.491796875001 | train_rmse: 0.3359 | val_rmse: 1.0159 | val_ll: -1.9643
[3:09:21.288676] epoch: 5500 | elbo: 9298.295146484375 | train_rmse: 0.3341 | val_rmse: 1.0161 | val_ll: -1.9617
[3:11:01.859890] epoch: 5550 | elbo: 9286.246884765624 | train_rmse: 0.333 | val_rmse: 1.0167 | val_ll: -1.9696
[3:12:42.440368] epoch: 5600 | elbo: 9256.015537109375 | train_rmse: 0.3313 | val_rmse: 1.0171 | val_ll: -1.9683
[3:14:24.819118] epoch: 5650 | elbo: 9247.5019921875 | train_rmse: 0.3308 | val_rmse: 1.018 | val_ll: -1.9735
[3:16:06.473315] epoch: 5700 | elbo: 9225.732065429684 | train_rmse: 0.33 | val_rmse: 1.0176 | val_ll: -1.9692
[3:17:47.340660] epoch: 5750 | elbo: 9224.860249023437 | train_rmse: 0.3305 | val_rmse: 1.0181 | val_ll: -1.9693
[3:19:27.868369] epoch: 5800 | elbo: 9216.323124999999 | train_rmse: 0.3266 | val_rmse: 1.018 | val_ll: -1.969
[3:21:08.847527] epoch: 5850 | elbo: 9171.783432617187 | train_rmse: 0.3299 | val_rmse: 1.0194 | val_ll: -1.9732
[3:22:54.890019] epoch: 5900 | elbo: 9190.709423828124 | train_rmse: 0.3252 | val_rmse: 1.018 | val_ll: -1.9657
[3:24:41.039726] epoch: 5950 | elbo: 9169.058056640624 | train_rmse: 0.3259 | val_rmse: 1.0195 | val_ll: -1.9882
[3:26:27.057966] epoch: 6000 | elbo: 9174.361591796875 | train_rmse: 0.3229 | val_rmse: 1.0199 | val_ll: -1.9728
[3:28:12.781733] epoch: 6050 | elbo: 9142.830209960935 | train_rmse: 0.3223 | val_rmse: 1.0207 | val_ll: -1.9765
[3:29:55.818792] epoch: 6100 | elbo: 9129.227998046876 | train_rmse: 0.3212 | val_rmse: 1.0203 | val_ll: -1.9809
[3:31:38.422932] epoch: 6150 | elbo: 9117.560844726562 | train_rmse: 0.3206 | val_rmse: 1.0205 | val_ll: -1.9743
[3:33:22.040828] epoch: 6200 | elbo: 9073.174204101564 | train_rmse: 0.3177 | val_rmse: 1.0214 | val_ll: -1.9797
[3:35:07.750689] epoch: 6250 | elbo: 9072.394150390626 | train_rmse: 0.3172 | val_rmse: 1.0217 | val_ll: -1.9886
[3:36:52.143308] epoch: 6300 | elbo: 9065.737177734374 | train_rmse: 0.3185 | val_rmse: 1.0217 | val_ll: -1.9905
[3:38:35.924860] epoch: 6350 | elbo: 9058.4423046875 | train_rmse: 0.3153 | val_rmse: 1.0225 | val_ll: -1.9967
[3:40:19.648260] epoch: 6400 | elbo: 9051.154389648436 | train_rmse: 0.3143 | val_rmse: 1.0221 | val_ll: -1.9868
[3:42:04.389467] epoch: 6450 | elbo: 9010.990966796875 | train_rmse: 0.3133 | val_rmse: 1.0231 | val_ll: -1.9844
[3:43:48.714527] epoch: 6500 | elbo: 9009.31970703125 | train_rmse: 0.3125 | val_rmse: 1.022 | val_ll: -1.9913
[3:45:34.037254] epoch: 6550 | elbo: 9015.867236328124 | train_rmse: 0.3119 | val_rmse: 1.0229 | val_ll: -1.9902
[3:47:18.381644] epoch: 6600 | elbo: 9005.73703125 | train_rmse: 0.31 | val_rmse: 1.0235 | val_ll: -1.9895
[3:49:02.847381] epoch: 6650 | elbo: 8978.235126953126 | train_rmse: 0.3108 | val_rmse: 1.0248 | val_ll: -1.9986
[3:50:49.129748] epoch: 6700 | elbo: 8984.296914062501 | train_rmse: 0.3086 | val_rmse: 1.0243 | val_ll: -2.0028
[3:52:35.314749] epoch: 6750 | elbo: 8966.405668945314 | train_rmse: 0.3072 | val_rmse: 1.0249 | val_ll: -1.9955
[3:54:20.448225] epoch: 6800 | elbo: 8937.284052734376 | train_rmse: 0.3066 | val_rmse: 1.0253 | val_ll: -2.005
[3:56:06.345258] epoch: 6850 | elbo: 8943.41763671875 | train_rmse: 0.3054 | val_rmse: 1.025 | val_ll: -2.0024
[3:57:52.960266] epoch: 6900 | elbo: 8935.70111328125 | train_rmse: 0.3056 | val_rmse: 1.0258 | val_ll: -1.9996
[3:59:38.610528] epoch: 6950 | elbo: 8921.40658203125 | train_rmse: 0.3036 | val_rmse: 1.0254 | val_ll: -2.0018
[4:01:25.219124] epoch: 7000 | elbo: 8924.294770507811 | train_rmse: 0.3037 | val_rmse: 1.0272 | val_ll: -2.0058
[4:03:12.063092] epoch: 7050 | elbo: 8903.485429687498 | train_rmse: 0.3034 | val_rmse: 1.0259 | val_ll: -2.004
[4:04:58.443381] epoch: 7100 | elbo: 8901.899252929685 | train_rmse: 0.3003 | val_rmse: 1.0271 | val_ll: -2.0098
[4:06:44.295221] epoch: 7150 | elbo: 8906.390786132813 | train_rmse: 0.3002 | val_rmse: 1.0272 | val_ll: -2.0089
[4:08:28.603492] epoch: 7200 | elbo: 8865.720688476564 | train_rmse: 0.2994 | val_rmse: 1.0279 | val_ll: -2.0082
[4:10:13.647071] epoch: 7250 | elbo: 8843.997192382809 | train_rmse: 0.2989 | val_rmse: 1.0284 | val_ll: -2.0154
[4:11:57.524488] epoch: 7300 | elbo: 8856.090732421875 | train_rmse: 0.2982 | val_rmse: 1.0269 | val_ll: -2.0016
[4:13:41.332308] epoch: 7350 | elbo: 8851.533498535155 | train_rmse: 0.2984 | val_rmse: 1.0284 | val_ll: -2.0211
[4:15:26.569698] epoch: 7400 | elbo: 8844.742954101563 | train_rmse: 0.2975 | val_rmse: 1.029 | val_ll: -2.0163
[4:17:11.054096] epoch: 7450 | elbo: 8832.079438476565 | train_rmse: 0.2961 | val_rmse: 1.029 | val_ll: -2.0192
[4:18:54.813563] epoch: 7500 | elbo: 8813.38182128906 | train_rmse: 0.2957 | val_rmse: 1.0295 | val_ll: -2.0227
[4:20:41.377621] epoch: 7550 | elbo: 8812.987954101562 | train_rmse: 0.294 | val_rmse: 1.0287 | val_ll: -2.0089
[4:22:26.475179] epoch: 7600 | elbo: 8810.727309570313 | train_rmse: 0.2924 | val_rmse: 1.0289 | val_ll: -2.0179
[4:24:10.422321] epoch: 7650 | elbo: 8782.852072753909 | train_rmse: 0.2923 | val_rmse: 1.0298 | val_ll: -2.0243
[4:25:54.640369] epoch: 7700 | elbo: 8791.7028515625 | train_rmse: 0.2918 | val_rmse: 1.0291 | val_ll: -2.0262
[4:27:38.046956] epoch: 7750 | elbo: 8779.501013183592 | train_rmse: 0.2942 | val_rmse: 1.0306 | val_ll: -2.0273
[4:29:22.048808] epoch: 7800 | elbo: 8766.409948730468 | train_rmse: 0.2893 | val_rmse: 1.03 | val_ll: -2.026
[4:31:05.448393] epoch: 7850 | elbo: 8750.360107421875 | train_rmse: 0.2893 | val_rmse: 1.0291 | val_ll: -2.0207
[4:32:48.960420] epoch: 7900 | elbo: 8745.199736328123 | train_rmse: 0.2891 | val_rmse: 1.0313 | val_ll: -2.0262
[4:34:33.142009] epoch: 7950 | elbo: 8754.139379882812 | train_rmse: 0.2876 | val_rmse: 1.03 | val_ll: -2.0249
[4:36:18.744777] epoch: 8000 | elbo: 8724.051547851564 | train_rmse: 0.2877 | val_rmse: 1.0322 | val_ll: -2.0335
[4:38:02.722097] epoch: 8050 | elbo: 8737.98020019531 | train_rmse: 0.287 | val_rmse: 1.0306 | val_ll: -2.0287
[4:39:47.931657] epoch: 8100 | elbo: 8708.635175781248 | train_rmse: 0.2852 | val_rmse: 1.0313 | val_ll: -2.0333
[4:41:31.823594] epoch: 8150 | elbo: 8707.965961914062 | train_rmse: 0.285 | val_rmse: 1.0312 | val_ll: -2.033
[4:43:16.267004] epoch: 8200 | elbo: 8688.979101562498 | train_rmse: 0.2829 | val_rmse: 1.0309 | val_ll: -2.0268
[4:45:00.851415] epoch: 8250 | elbo: 8698.973054199221 | train_rmse: 0.2829 | val_rmse: 1.0306 | val_ll: -2.0322
[4:46:44.018492] epoch: 8300 | elbo: 8687.223662109374 | train_rmse: 0.2835 | val_rmse: 1.0301 | val_ll: -2.0279
[4:48:27.545403] epoch: 8350 | elbo: 8682.258889160154 | train_rmse: 0.2818 | val_rmse: 1.0313 | val_ll: -2.0402
[4:50:12.176623] epoch: 8400 | elbo: 8659.0731640625 | train_rmse: 0.2809 | val_rmse: 1.0316 | val_ll: -2.0388
[4:51:55.310190] epoch: 8450 | elbo: 8662.87506591797 | train_rmse: 0.281 | val_rmse: 1.0325 | val_ll: -2.0402
[4:53:38.776408] epoch: 8500 | elbo: 8644.07432861328 | train_rmse: 0.2801 | val_rmse: 1.0322 | val_ll: -2.0402
[4:55:22.494076] epoch: 8550 | elbo: 8655.77815673828 | train_rmse: 0.2796 | val_rmse: 1.0326 | val_ll: -2.0421
[4:57:05.545968] epoch: 8600 | elbo: 8638.436833496093 | train_rmse: 0.2789 | val_rmse: 1.0324 | val_ll: -2.0376
[4:58:49.063056] epoch: 8650 | elbo: 8614.976186523438 | train_rmse: 0.2779 | val_rmse: 1.0328 | val_ll: -2.0385
[5:00:33.257623] epoch: 8700 | elbo: 8619.984152832032 | train_rmse: 0.2763 | val_rmse: 1.0327 | val_ll: -2.0364
[5:02:15.651548] epoch: 8750 | elbo: 8623.691313476562 | train_rmse: 0.2762 | val_rmse: 1.0332 | val_ll: -2.0497
[5:03:59.036130] epoch: 8800 | elbo: 8597.380117187498 | train_rmse: 0.2754 | val_rmse: 1.033 | val_ll: -2.0427
[5:05:42.524889] epoch: 8850 | elbo: 8611.308803710937 | train_rmse: 0.2752 | val_rmse: 1.033 | val_ll: -2.046
[5:07:26.130119] epoch: 8900 | elbo: 8596.525634765625 | train_rmse: 0.2747 | val_rmse: 1.0341 | val_ll: -2.0453
[5:09:08.833253] epoch: 8950 | elbo: 8607.948166503907 | train_rmse: 0.273 | val_rmse: 1.0357 | val_ll: -2.0592
[5:10:52.955057] epoch: 9000 | elbo: 8578.491479492188 | train_rmse: 0.2723 | val_rmse: 1.0337 | val_ll: -2.0537
[5:12:37.390286] epoch: 9050 | elbo: 8577.030354003906 | train_rmse: 0.2723 | val_rmse: 1.0341 | val_ll: -2.052
[5:14:20.317078] epoch: 9100 | elbo: 8558.57366455078 | train_rmse: 0.2717 | val_rmse: 1.0348 | val_ll: -2.0535
[5:16:02.624073] epoch: 9150 | elbo: 8567.682888183592 | train_rmse: 0.2707 | val_rmse: 1.034 | val_ll: -2.0499
[5:17:43.717203] epoch: 9200 | elbo: 8552.59348144531 | train_rmse: 0.2721 | val_rmse: 1.036 | val_ll: -2.0502
[5:19:24.982351] epoch: 9250 | elbo: 8555.409228515624 | train_rmse: 0.2699 | val_rmse: 1.036 | val_ll: -2.0579
[5:21:05.956443] epoch: 9300 | elbo: 8550.847585449219 | train_rmse: 0.272 | val_rmse: 1.0356 | val_ll: -2.06
[5:22:47.720716] epoch: 9350 | elbo: 8518.899399414062 | train_rmse: 0.2689 | val_rmse: 1.0362 | val_ll: -2.0559
[5:24:28.169474] epoch: 9400 | elbo: 8525.103889160157 | train_rmse: 0.2684 | val_rmse: 1.0358 | val_ll: -2.0606
[5:26:08.969242] epoch: 9450 | elbo: 8535.717778320311 | train_rmse: 0.2676 | val_rmse: 1.0359 | val_ll: -2.057
[5:27:50.711959] epoch: 9500 | elbo: 8510.523847656248 | train_rmse: 0.2668 | val_rmse: 1.0368 | val_ll: -2.059
[5:29:33.207212] epoch: 9550 | elbo: 8519.99290283203 | train_rmse: 0.2693 | val_rmse: 1.0378 | val_ll: -2.0712
[5:31:14.593220] epoch: 9600 | elbo: 8521.975104980469 | train_rmse: 0.2649 | val_rmse: 1.0367 | val_ll: -2.0702
[5:32:55.664851] epoch: 9650 | elbo: 8505.138193359377 | train_rmse: 0.2648 | val_rmse: 1.0372 | val_ll: -2.0682
[5:34:38.880585] epoch: 9700 | elbo: 8493.720090332032 | train_rmse: 0.2633 | val_rmse: 1.0372 | val_ll: -2.0721
[5:36:19.318245] epoch: 9750 | elbo: 8487.94500732422 | train_rmse: 0.2639 | val_rmse: 1.0373 | val_ll: -2.0709
[5:38:00.028790] epoch: 9800 | elbo: 8492.802348632813 | train_rmse: 0.2633 | val_rmse: 1.0391 | val_ll: -2.0872
[5:39:39.557456] epoch: 9850 | elbo: 8495.887141113282 | train_rmse: 0.2642 | val_rmse: 1.0386 | val_ll: -2.0784
[5:41:19.101747] epoch: 9900 | elbo: 8478.783671875 | train_rmse: 0.2633 | val_rmse: 1.0385 | val_ll: -2.0699
[5:42:57.189266] epoch: 9950 | elbo: 8471.471249999999 | train_rmse: 0.2626 | val_rmse: 1.0393 | val_ll: -2.0763
Training finished in 5:44:34.516658 seconds
Saved SVI model to tests/dataset-tests/sineasy10-2/models/sineasy10-3x64-s05/checkpoint_1.pt
File Size is 0.07414722442626953 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=64, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=64, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:5 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-2/models/sineasy10-3x64-s05/checkpoint_1.pt
using device: cuda:5
====== evaluating profile sineasy10-3x64-s05 - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:02:45.125247
End time: 2023-07-14 00:25:15.936958
Total time: 5:47:23.417239
