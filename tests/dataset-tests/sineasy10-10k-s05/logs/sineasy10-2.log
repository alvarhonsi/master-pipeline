Start time: 2023-07-12 08:56:48.249218
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=256, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=256, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:5 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([256, 10]) Parameter containing:
tensor([[-0.1163,  0.4018,  0.1174,  ...,  0.1962, -0.0861,  0.0100],
        [-0.3942, -0.2028, -0.3912,  ..., -0.4561,  0.1634,  0.1020],
        [ 0.2218, -0.2297,  0.0523,  ..., -0.3976, -0.2303, -0.4965],
        ...,
        [-0.5577, -0.1740, -0.0326,  ...,  0.1035, -0.3807,  0.2022],
        [-0.3908,  0.0065,  0.4456,  ..., -0.1243,  0.4974,  0.2434],
        [-0.6199, -0.0700, -0.1370,  ...,  0.1902, -0.2829, -0.7796]],
       device='cuda:5', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([256, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([256]) Parameter containing:
tensor([ 1.2015e-01, -1.3464e-01, -5.8265e-01, -7.6361e-01, -8.2033e-02,
        -4.0784e-01,  8.0142e-01, -3.9512e-01,  3.7725e-01, -5.9947e-02,
         1.4789e-01,  2.3435e-03,  1.6988e-01,  4.3074e-01, -1.3521e-01,
         2.2016e-01,  3.3673e-02, -3.2105e-01,  3.0560e-01,  5.7298e-03,
         6.6584e-01,  3.8762e-01, -2.7014e-01,  4.3658e-01,  1.7679e-01,
        -3.1305e-01,  3.0661e-02, -4.5028e-01,  4.3709e-01,  2.3728e-01,
         9.4640e-02, -4.0185e-01, -6.2449e-02, -8.8920e-02, -6.1998e-01,
         1.0305e-01, -2.3913e-01, -1.3689e-01,  5.4501e-02,  4.4066e-04,
         6.5242e-03, -3.0216e-01,  2.3886e-02,  2.0551e-01,  2.2113e-01,
        -2.3240e-01, -2.6516e-01, -1.4044e-01, -1.3909e-01,  2.1304e-01,
        -2.4620e-01,  7.2318e-01, -2.6956e-01,  3.9618e-01, -9.6599e-02,
         6.4642e-02, -1.4083e-01,  4.2951e-02, -9.0624e-02,  1.7405e-01,
        -3.1639e-02, -5.4273e-01, -4.5534e-01, -1.8921e-01,  2.0208e-01,
        -4.3427e-01,  1.3501e-01, -4.9664e-01, -2.8068e-01,  4.8792e-01,
        -8.7809e-02, -4.1857e-01,  2.5816e-01, -1.8867e-01,  6.0535e-01,
         3.3127e-01,  3.7536e-01, -4.3918e-01,  8.2186e-02,  2.2328e-01,
        -7.5589e-02,  6.5085e-02,  4.4330e-01,  1.9975e-01, -6.7052e-02,
         7.1290e-02, -1.5709e-01, -1.1114e-01,  2.0305e-01,  8.2854e-02,
        -3.6506e-01,  7.9142e-01,  2.8795e-01,  4.2774e-01,  1.5496e-01,
         1.8785e-01, -7.4260e-01, -1.0377e-01, -6.7074e-01,  1.1091e-01,
         4.2344e-01, -1.1757e-01, -2.1504e-01, -8.5924e-03, -4.8672e-01,
         4.3739e-01, -1.5928e-01, -4.0625e-01, -5.8244e-01,  3.0463e-01,
        -8.8794e-01,  5.6478e-02, -3.2945e-01,  2.5049e-01,  6.0156e-02,
         3.5125e-01, -1.4453e-01, -1.7520e-01, -1.2946e-01,  1.6716e-01,
         3.0498e-02,  5.0899e-01, -4.7326e-01,  3.0799e-01, -1.7904e-01,
        -5.9175e-01, -1.4161e-01, -2.8293e-01,  1.0603e-01, -3.7866e-02,
        -2.5337e-02,  1.4611e-01, -2.5077e-01,  5.6260e-01,  3.8025e-02,
        -9.6771e-02, -3.0386e-01,  2.5917e-01,  2.7423e-01, -2.7433e-01,
        -7.4292e-02,  3.8356e-01,  1.4803e-01,  6.3038e-01, -3.0704e-01,
        -2.3720e-01, -1.3683e-01, -4.0517e-01,  3.0942e-01, -4.9396e-01,
         3.0425e-01,  3.4911e-01, -4.5182e-01,  3.9038e-01,  1.4767e-01,
         7.9546e-02, -3.1205e-01,  2.2139e-01,  2.2607e-02,  6.8616e-01,
         2.3429e-02, -1.0494e-01, -3.6224e-01, -3.3536e-01,  8.7071e-02,
        -6.0332e-02,  5.2959e-01,  4.5775e-02,  2.0214e-02,  4.6065e-02,
        -2.7345e-01, -5.4154e-01, -6.5330e-01,  6.7398e-02,  2.9218e-01,
        -4.1404e-01, -6.1859e-01,  3.1059e-01,  2.0478e-01,  5.2453e-01,
         3.9462e-01, -1.5229e-01,  1.4467e-01, -3.0731e-01,  1.4582e-01,
         6.9220e-03,  1.5059e-01, -1.2861e-02, -5.1037e-02, -1.4617e-01,
        -2.0774e-01, -1.9129e-02, -4.5915e-02,  1.6376e-01, -1.7609e-01,
         1.7616e-01, -2.2496e-01, -4.9939e-01, -2.3159e-01, -1.7218e-01,
         1.0856e-01,  8.2026e-03,  2.6212e-01,  2.6812e-01,  3.2997e-01,
        -6.5431e-01, -2.5992e-01, -2.8814e-02, -4.4315e-01, -3.3224e-01,
         7.9504e-02, -2.0987e-01, -7.6230e-01,  2.9356e-01, -3.1997e-01,
        -5.3639e-01, -3.1728e-01,  1.5493e-01, -4.1793e-01, -2.1286e-01,
         5.9072e-01, -6.3091e-01, -1.2783e-01,  5.1296e-01, -7.3888e-02,
        -6.0379e-01, -1.3534e-02,  4.5741e-01,  1.5891e-01,  3.7653e-01,
        -1.9471e-01, -2.8366e-01,  1.5151e-02,  1.4395e-01, -5.3682e-02,
         9.3069e-02,  1.3803e-01, -9.1722e-03,  6.8126e-03, -5.1569e-01,
         3.3131e-01, -7.6867e-03,  2.8123e-01,  3.0629e-01,  4.6738e-01,
         3.8399e-01,  1.0482e-01,  3.6536e-01, -1.9875e-01,  3.0407e-01,
         3.0106e-01,  3.1572e-02,  3.7536e-02, -1.5588e-01,  1.6955e-01,
        -1.0689e-01], device='cuda:5', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([256]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100], device='cuda:5',
       grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([256, 256]) Parameter containing:
tensor([[ 0.4291, -0.0802, -0.2324,  ..., -0.0609, -0.2293,  0.2362],
        [ 0.0072,  0.3236, -0.3177,  ..., -0.6955,  0.1161, -0.3569],
        [-0.3702, -0.1020, -0.0222,  ..., -0.1157,  0.3218,  0.0711],
        ...,
        [-0.0774,  0.1223,  0.0417,  ..., -0.4335,  0.0328,  0.1872],
        [-0.6486,  0.2865, -0.0789,  ...,  0.4990,  0.0854,  0.3098],
        [-0.3082, -0.2109,  0.3177,  ..., -0.4717, -0.5745, -0.0346]],
       device='cuda:5', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([256, 256]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([256]) Parameter containing:
tensor([-1.8443e-01, -2.1578e-01, -2.2051e-01,  1.8673e-01,  3.6908e-01,
         3.4870e-01,  5.2506e-01,  5.6390e-05,  8.4301e-02, -4.4840e-01,
         4.4414e-01,  4.4231e-02,  8.4119e-02,  1.5525e-01, -4.2693e-01,
        -1.9005e-01,  5.4822e-01,  3.9408e-01,  2.1291e-02, -8.7279e-01,
        -5.1559e-01,  1.1799e-02,  1.6148e-02,  3.9324e-01, -7.8860e-02,
         2.4853e-01, -1.8001e-01, -1.3779e-01, -2.5321e-01, -2.6411e-01,
        -4.7761e-01,  1.8340e-01,  4.7092e-01,  1.7264e-01,  3.5516e-01,
        -1.0892e-01,  2.6207e-01,  3.7542e-01,  1.8001e-01, -4.3193e-01,
         2.0109e-01,  8.6724e-01,  1.0444e-01,  3.5617e-01, -5.5209e-01,
         4.9645e-02, -1.7967e-01, -3.4096e-01, -1.3297e-01,  1.2962e-01,
         1.2329e-01,  8.3894e-02,  1.3692e-01, -9.7702e-02,  4.1007e-01,
         1.8709e-01,  5.6997e-01, -4.6868e-01,  1.4244e-01, -3.0671e-01,
         3.1244e-03, -1.3257e-01, -2.2100e-01, -1.0050e-01,  1.8291e-03,
        -5.1519e-01,  3.5514e-01, -6.1307e-01,  1.3136e-01, -2.1369e-01,
        -5.6291e-01, -2.9284e-01,  2.6486e-01,  4.6136e-01, -3.4336e-01,
        -2.0938e-02,  4.0332e-01, -4.8921e-02, -4.2022e-01, -6.8280e-01,
         2.9024e-01,  3.4607e-02,  1.1592e-01, -2.2672e-01, -7.6860e-01,
        -2.4476e-01, -1.3270e-01, -6.2252e-02,  1.9073e-01,  3.1870e-01,
        -3.2130e-01,  8.2252e-02, -8.0867e-02, -7.1124e-02, -3.2400e-01,
        -2.2875e-01,  2.0238e-01,  1.0360e-01,  2.2677e-01, -1.8711e-01,
        -1.0493e-01,  1.2534e-01, -2.2738e-01, -1.2087e-01,  3.1377e-01,
        -3.7010e-01, -1.9423e-01, -2.4794e-01, -1.5370e-01, -1.5819e-01,
         3.3041e-01, -4.8389e-01,  5.5189e-01,  4.0426e-01, -4.4486e-01,
        -6.1314e-01, -6.7910e-02, -4.1903e-02,  1.6625e-01,  5.6093e-02,
        -1.3822e-02,  8.0092e-01,  1.9615e-02, -7.7541e-01, -4.0941e-02,
         5.5105e-03,  4.9579e-01,  1.1676e-01,  4.5972e-01,  2.2020e-01,
        -1.1356e-02,  1.1259e-01, -6.2689e-01,  5.7297e-01, -4.2929e-01,
        -4.3509e-02, -3.0675e-01, -9.4240e-02,  1.4859e-01, -1.1397e-01,
        -3.0814e-01, -1.4753e-02,  3.4847e-01,  5.3324e-01, -5.5235e-01,
         2.4612e-01, -2.4753e-01,  3.1987e-01,  6.4844e-02, -2.3890e-01,
        -1.2140e-01, -9.4842e-02,  4.1394e-01, -3.5692e-01, -2.5443e-02,
         9.1436e-01, -3.8255e-01, -3.4500e-01,  2.0884e-01, -5.4827e-02,
        -1.0588e-02, -1.7682e-02, -1.7662e-01,  4.5665e-01, -3.8460e-01,
         1.5377e-01,  2.1021e-01, -2.2189e-01, -1.9075e-01,  5.8681e-02,
        -6.4813e-02, -5.5884e-02, -4.8706e-01,  1.4231e-01, -2.3168e-01,
         3.2289e-01,  1.8271e-02,  4.4667e-01,  8.7086e-01, -3.7068e-02,
         2.3850e-01, -3.0077e-01, -2.7473e-01, -8.3153e-02,  5.2311e-01,
        -9.6211e-02, -1.7647e-01, -2.3507e-01,  2.6949e-01,  3.3192e-01,
         2.8318e-01,  4.3594e-01, -3.9376e-01, -2.4347e-01, -8.6367e-02,
         3.8419e-01, -7.7253e-01,  5.2675e-01,  1.6860e-01,  7.4694e-02,
         5.1228e-01, -2.0018e-03,  6.5111e-01,  4.0656e-01, -1.4037e-02,
         1.6894e-01,  2.1062e-01, -4.4814e-02, -3.6461e-01, -3.8275e-01,
         6.7062e-02, -3.8131e-01, -1.2800e-01, -9.4917e-02, -3.7395e-02,
         3.9838e-02, -3.8536e-01,  3.4285e-04, -3.8586e-01,  6.3123e-01,
         3.6742e-01,  2.9934e-01,  2.4998e-02,  1.0775e-01, -3.5167e-02,
         7.7190e-02, -1.9822e-01,  2.6497e-01,  5.5082e-01,  3.3871e-01,
        -2.2265e-01,  5.1586e-01,  8.2842e-02,  5.8535e-01,  7.5003e-02,
         3.7528e-01,  1.0125e-01,  4.9134e-01,  2.6886e-01, -3.0981e-01,
        -3.8785e-01, -9.9132e-02,  1.8724e-01, -2.7522e-01,  2.1941e-01,
         6.2220e-01, -3.0737e-01,  1.3207e-01, -2.7318e-02, -6.3811e-01,
         5.4717e-01, -2.5621e-01, -5.3466e-01, -1.4789e-01,  1.6443e-01,
        -2.9549e-01], device='cuda:5', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([256]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100], device='cuda:5',
       grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([256, 256]) Parameter containing:
tensor([[ 0.7213, -0.1118,  0.5195,  ...,  0.2371, -0.2096, -0.1518],
        [-0.1977,  0.0258,  0.1710,  ..., -0.0411,  0.0294,  0.0119],
        [-0.7677, -0.2053,  0.1546,  ...,  0.1217, -0.3203, -0.1613],
        ...,
        [-0.1076,  0.0300, -0.5523,  ...,  0.0930, -0.1153, -0.1606],
        [ 0.3622, -0.1375,  0.1171,  ...,  0.2577, -0.1088,  0.4559],
        [ 0.4231, -0.0389,  0.0736,  ..., -0.3082, -0.0750,  0.1036]],
       device='cuda:5', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([256, 256]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:5', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([256]) Parameter containing:
tensor([ 2.4186e-01, -1.3716e-01,  7.3800e-01, -5.7575e-01, -1.4242e-01,
         7.4657e-02, -1.7738e-01,  1.0940e-02,  5.8711e-01, -1.6210e-01,
         4.1696e-01,  6.3709e-02,  1.7850e-01,  1.2733e-01, -2.3888e-01,
        -2.5624e-01, -1.6752e-02, -3.2804e-01, -2.3486e-01, -1.7283e-01,
        -6.3063e-01,  3.7983e-01,  3.4775e-01,  2.2710e-01, -1.3133e-02,
        -2.7242e-01, -1.2390e-01,  3.6263e-02, -4.2616e-01, -5.7370e-01,
        -4.7061e-01,  1.4074e-01, -5.6009e-02,  4.5916e-01, -6.9336e-02,
        -2.1958e-01, -8.4320e-02, -2.4364e-04,  1.1797e-01, -2.5423e-02,
         1.2845e-01, -1.9020e-02,  1.2878e-01,  1.1003e-01,  5.0790e-01,
        -3.3442e-01,  2.3988e-01, -3.7193e-01,  3.9686e-01,  7.9731e-01,
        -1.3855e-01, -3.9009e-01, -1.1969e-01,  3.5569e-01, -3.8865e-01,
        -8.3380e-04,  1.8749e-01,  5.3033e-01, -3.8398e-01,  5.9169e-01,
        -2.6824e-01, -5.4681e-01, -6.0677e-01,  2.8146e-01,  5.4718e-01,
         2.1616e-01,  1.7816e-01,  3.1767e-01, -1.0415e-01,  8.6346e-02,
        -3.9324e-01, -5.3936e-02,  4.4692e-01,  2.5306e-01, -1.6114e-01,
        -2.5449e-01, -3.7804e-02, -2.2334e-01, -2.0741e-02,  2.9842e-01,
         2.8048e-01, -5.7159e-01, -1.9294e-01, -3.7394e-01,  5.8372e-01,
         2.1554e-01, -4.3990e-01, -3.7866e-01, -3.8189e-02,  6.5674e-01,
         1.7591e-01, -2.3626e-02,  3.2730e-02, -7.6497e-02, -7.7010e-01,
        -4.6440e-01,  6.3543e-01, -4.6235e-02,  4.3081e-01, -2.1791e-01,
         2.0349e-02, -1.3517e-02, -1.5544e-01,  2.6865e-01, -1.3685e-01,
         1.6276e-02, -3.8962e-02,  3.2614e-01,  9.5226e-02,  2.8166e-01,
         8.7303e-03, -4.5345e-01, -1.3619e-01,  3.7340e-01,  1.9053e-01,
        -9.6410e-02,  4.7171e-01,  3.9936e-02,  4.4176e-01,  5.0293e-01,
         1.4533e-01,  2.5598e-01,  1.3194e-01, -3.0385e-01,  1.1225e-01,
         2.6125e-01, -4.1791e-01, -4.2681e-01, -5.2173e-01, -5.5178e-01,
         2.5362e-02, -2.5408e-01, -8.9299e-02,  5.3591e-01,  4.8064e-01,
        -2.6032e-01, -3.3521e-01,  4.6098e-01,  2.3350e-01,  2.7315e-01,
        -1.5237e-01,  5.7192e-02, -2.4153e-01,  2.5761e-01, -7.0759e-01,
         5.3026e-01, -3.2673e-01,  3.5713e-02,  1.1010e-01, -7.6822e-02,
        -3.8479e-01,  9.8121e-03,  4.6827e-01,  1.3830e-01, -2.4106e-01,
         5.2530e-01, -3.8934e-01, -1.4104e-01, -2.4382e-01,  3.6980e-01,
         2.4150e-01, -1.8699e-01, -2.7132e-01, -1.0686e-01, -7.0976e-01,
         1.8091e-01,  1.4220e-01,  1.0492e-01, -4.3483e-03, -1.5082e-01,
        -9.4652e-02, -3.2487e-01,  1.0421e-01, -1.5903e-01, -2.6158e-01,
        -4.2830e-01,  1.7834e-01,  1.1553e-02,  3.9469e-01,  1.7598e-01,
        -4.5541e-01, -2.5147e-01,  6.4108e-02,  3.2168e-01,  5.0295e-01,
        -7.2575e-01,  8.9172e-02, -1.6985e-02, -2.3323e-01, -4.0907e-01,
        -1.1502e-01,  1.6057e-01,  5.7565e-01,  3.2092e-01,  4.5098e-01,
         2.7287e-03,  5.2513e-01,  3.9127e-01, -2.3872e-01,  3.9474e-01,
        -7.1148e-01,  2.2805e-01, -1.1565e-01, -1.0446e-01, -4.4549e-02,
         1.5241e-01,  2.3614e-01, -4.2282e-01,  1.4297e-01,  2.8354e-01,
        -1.1463e-01,  1.6701e-01,  9.5301e-01, -1.6696e-02, -1.9518e-02,
        -3.4143e-01,  1.4666e-01, -1.7071e-01,  5.7129e-01, -5.7836e-02,
        -2.2403e-01,  3.1074e-02, -1.0299e-01, -6.5107e-02, -2.2912e-02,
         1.6906e-01, -2.9867e-01,  3.8521e-01, -1.3690e-01, -1.6398e-01,
         2.4639e-01,  3.1972e-01,  2.7648e-01,  1.9873e-01, -1.4325e-01,
         3.0994e-01, -3.3636e-01, -1.1084e-01, -2.9189e-01,  5.5675e-01,
        -1.8525e-01, -5.1814e-02, -5.1827e-01, -1.3827e-01, -6.9804e-01,
        -2.1803e-01, -1.9922e-01, -4.4275e-01,  4.5800e-01,  1.8600e-01,
        -7.8632e-01,  3.2888e-01,  2.3879e-02,  2.5628e-01, -8.3394e-02,
         3.4800e-01], device='cuda:5', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([256]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100], device='cuda:5',
       grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 256]) Parameter containing:
tensor([[ 4.2296e-02,  2.4743e-01,  1.2249e-02,  1.6850e-01,  1.6473e-01,
         -1.0106e-03,  3.6415e-01, -2.7448e-01, -1.7367e-01, -1.6828e-01,
         -9.9286e-01,  2.9866e-01, -4.1138e-01,  4.2350e-02, -3.5021e-01,
         -2.1996e-01,  6.1941e-02,  5.6432e-01,  8.3550e-02,  1.5524e-01,
          3.6316e-02, -7.8069e-03,  1.5315e-01,  1.8096e-01,  1.2299e-01,
         -2.1478e-01, -6.0627e-01,  1.4583e-01,  1.1841e-01,  1.1143e-01,
         -9.3443e-02, -4.5166e-01, -8.7006e-02,  1.8057e-01, -1.4976e-01,
          4.2957e-01, -2.4956e-01,  7.3844e-02, -3.0138e-01, -1.8017e-01,
         -2.7933e-01, -3.0416e-01, -1.8094e-01,  1.6210e-01, -4.1403e-01,
          4.8681e-03, -1.6025e-01, -3.2805e-01, -1.9491e-01, -5.9324e-01,
          1.3016e-01, -3.0362e-01,  4.0876e-01, -5.1767e-02, -6.5148e-01,
          7.3587e-01,  2.8455e-01,  1.0238e-02,  4.7830e-01,  9.1853e-02,
         -9.5112e-02,  3.3021e-02, -1.9865e-01,  2.4166e-01, -3.2379e-01,
         -1.0918e-01,  4.9459e-01, -3.6090e-01, -2.9805e-01,  2.4859e-01,
          7.5413e-02,  7.6729e-01, -1.5848e-01, -6.7192e-02,  1.6112e-01,
         -3.7667e-02,  5.7274e-01,  6.0497e-02,  3.2748e-01,  1.5611e-01,
          9.0015e-02,  4.5961e-01, -2.7062e-01, -4.8497e-02, -1.1325e-01,
         -4.1995e-01,  1.9599e-01,  8.3707e-02,  1.0230e+00,  4.7991e-01,
         -7.9022e-01, -5.8974e-01, -8.5174e-01, -4.4219e-01, -7.8125e-02,
         -1.7044e-01, -2.5366e-01, -5.4536e-01, -7.7202e-03,  2.8284e-01,
          2.7640e-01,  5.9448e-01,  3.8518e-01,  5.1420e-01, -6.8279e-01,
          7.5530e-03,  1.4377e-01,  1.9877e-01,  2.1811e-01,  1.1702e-01,
         -1.9907e-01,  2.3941e-01,  5.0517e-01,  1.9059e-01,  2.6262e-01,
         -3.2300e-01, -8.8350e-02, -8.9250e-02, -1.4961e-01,  8.9517e-02,
         -2.7923e-01,  2.7398e-01,  3.3800e-01,  8.2093e-03, -3.0646e-01,
         -4.3076e-02,  2.1860e-02,  4.5226e-01,  1.6127e-01, -4.7139e-01,
         -5.0111e-01, -5.5751e-02,  1.3081e-01,  1.4993e-01, -1.1007e-02,
         -2.2711e-01,  4.6947e-01,  2.1830e-01,  6.7539e-02, -3.3544e-01,
         -2.8732e-02, -4.0688e-02, -5.5573e-01,  9.6202e-01,  8.7933e-01,
         -4.1588e-01,  6.7107e-01, -5.6612e-02, -9.3760e-02,  3.8301e-02,
         -1.2431e-01,  7.0808e-01,  1.1304e-01,  3.6728e-02,  2.7642e-01,
          3.0512e-01,  4.5441e-02, -1.3956e-01, -6.8374e-02, -4.6000e-01,
         -2.6585e-02,  2.3245e-01, -5.1199e-01, -2.2989e-01, -3.6915e-01,
         -3.0027e-01, -1.1706e-01, -4.3182e-01,  3.1908e-01,  1.9884e-01,
          4.9869e-01, -2.2167e-01,  3.2239e-01,  3.3217e-01, -1.2581e-01,
         -1.9388e-01, -3.0382e-01,  1.4743e-01, -2.9879e-01,  9.2975e-02,
          2.0740e-02, -7.0983e-01, -7.2764e-02,  3.9096e-01,  3.3742e-01,
         -5.9988e-02,  4.7008e-02,  6.3028e-02,  2.8675e-01,  2.5844e-01,
         -4.7671e-02, -6.4004e-02,  3.0019e-01,  4.4043e-02, -1.0474e-01,
          1.3580e-01,  2.4007e-01,  3.2469e-01,  7.4086e-02,  5.5255e-02,
         -1.8992e-01, -3.8775e-01,  4.4702e-01, -5.2473e-01,  2.1387e-01,
         -8.1484e-01, -2.9343e-02, -4.4841e-02,  2.5025e-02,  8.7773e-02,
          2.0610e-01,  2.0874e-01,  4.8400e-01,  1.2740e+00, -3.0241e-01,
         -6.9983e-02,  1.9357e-02,  3.0097e-01,  2.7540e-02,  4.3727e-01,
          4.9520e-01,  4.8612e-01, -2.1272e-01,  2.6504e-01, -1.0462e-01,
         -1.2851e-01, -1.5356e-02,  6.3581e-04,  3.7774e-01, -5.4587e-01,
          1.3702e-01,  2.9936e-01, -3.5645e-01,  3.9233e-02,  1.5746e-01,
          2.1152e-01, -1.0129e-01, -1.7776e-01, -4.4052e-01,  4.8282e-01,
         -3.0138e-01,  2.0432e-02,  4.1013e-01,  4.1947e-01, -4.3150e-01,
         -2.2442e-01,  3.9667e-01, -1.1994e-01,  6.8371e-01, -3.4186e-01,
         -2.7708e-01,  2.9798e-01, -5.9095e-02, -1.9994e-01,  5.5068e-01,
          2.4202e-01]], device='cuda:5', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 256]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100]], device='cuda:5',
       grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([0.1072], device='cuda:5', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:5', grad_fn=<AddBackward0>)
Using device: cuda:5
===== Training profile sineasy10-3x256-s05 - 1 =====
[0:00:01.836223] epoch: 0 | elbo: 233791768.47999996 | train_rmse: 88.3972 | val_rmse: 88.3244 | val_ll: -141.4979
[0:01:39.803075] epoch: 50 | elbo: 5459368.194999999 | train_rmse: 14.6172 | val_rmse: 15.3868 | val_ll: -7.7749
[0:03:20.323172] epoch: 100 | elbo: 3371615.55 | train_rmse: 10.8647 | val_rmse: 12.1063 | val_ll: -6.147
[0:05:01.289024] epoch: 150 | elbo: 2513107.18 | train_rmse: 8.9278 | val_rmse: 10.531 | val_ll: -5.5558
[0:06:37.494393] epoch: 200 | elbo: 2025884.0550000002 | train_rmse: 7.6083 | val_rmse: 9.5357 | val_ll: -5.2081
[0:08:13.117433] epoch: 250 | elbo: 1710768.2862500001 | train_rmse: 6.6055 | val_rmse: 8.7833 | val_ll: -4.9605
[0:09:49.764215] epoch: 300 | elbo: 1487284.7025000001 | train_rmse: 5.8081 | val_rmse: 8.2051 | val_ll: -4.8407
[0:11:26.634913] epoch: 350 | elbo: 1312013.4325 | train_rmse: 5.1178 | val_rmse: 7.7334 | val_ll: -4.7532
[0:13:04.403150] epoch: 400 | elbo: 1174581.3462500002 | train_rmse: 4.5528 | val_rmse: 7.3049 | val_ll: -4.6948
[0:14:42.695995] epoch: 450 | elbo: 1071866.89875 | train_rmse: 4.0445 | val_rmse: 6.9528 | val_ll: -4.6264
[0:16:20.287033] epoch: 500 | elbo: 981722.728125 | train_rmse: 3.5912 | val_rmse: 6.6478 | val_ll: -4.5737
[0:17:58.962890] epoch: 550 | elbo: 909354.795625 | train_rmse: 3.19 | val_rmse: 6.3527 | val_ll: -4.5146
[0:19:37.391468] epoch: 600 | elbo: 851957.725 | train_rmse: 2.8187 | val_rmse: 6.0915 | val_ll: -4.5006
[0:21:15.303081] epoch: 650 | elbo: 803425.3556250001 | train_rmse: 2.5027 | val_rmse: 5.8578 | val_ll: -4.4594
[0:22:52.731040] epoch: 700 | elbo: 762455.4375 | train_rmse: 2.216 | val_rmse: 5.6393 | val_ll: -4.4309
[0:24:30.157100] epoch: 750 | elbo: 729600.619375 | train_rmse: 1.9598 | val_rmse: 5.4304 | val_ll: -4.391
[0:26:07.668150] epoch: 800 | elbo: 702355.9450000001 | train_rmse: 1.7421 | val_rmse: 5.2424 | val_ll: -4.3966
[0:27:45.740838] epoch: 850 | elbo: 677735.858125 | train_rmse: 1.5053 | val_rmse: 5.0518 | val_ll: -4.3448
[0:29:25.079735] epoch: 900 | elbo: 657810.971875 | train_rmse: 1.3337 | val_rmse: 4.8809 | val_ll: -4.3147
[0:31:05.537749] epoch: 950 | elbo: 640430.2656250001 | train_rmse: 1.1711 | val_rmse: 4.704 | val_ll: -4.2136
[0:32:44.108897] epoch: 1000 | elbo: 625617.6118750002 | train_rmse: 1.0313 | val_rmse: 4.5547 | val_ll: -4.1883
[0:34:21.005044] epoch: 1050 | elbo: 612959.7974999999 | train_rmse: 0.906 | val_rmse: 4.396 | val_ll: -4.172
[0:36:00.338889] epoch: 1100 | elbo: 601197.7506250001 | train_rmse: 0.8213 | val_rmse: 4.252 | val_ll: -4.1225
[0:37:39.153715] epoch: 1150 | elbo: 590569.441875 | train_rmse: 0.7146 | val_rmse: 4.0895 | val_ll: -4.0679
[0:39:17.831996] epoch: 1200 | elbo: 580916.4456249999 | train_rmse: 0.6415 | val_rmse: 3.9377 | val_ll: -3.9492
[0:40:55.391563] epoch: 1250 | elbo: 571851.87125 | train_rmse: 0.5837 | val_rmse: 3.7796 | val_ll: -3.8707
[0:42:33.957249] epoch: 1300 | elbo: 563545.4506249999 | train_rmse: 0.5414 | val_rmse: 3.6334 | val_ll: -3.8028
[0:44:11.740964] epoch: 1350 | elbo: 555225.95875 | train_rmse: 0.4917 | val_rmse: 3.4771 | val_ll: -3.703
[0:45:49.059271] epoch: 1400 | elbo: 546701.63125 | train_rmse: 0.4643 | val_rmse: 3.3225 | val_ll: -3.6209
[0:47:27.084904] epoch: 1450 | elbo: 538882.4743749999 | train_rmse: 0.44 | val_rmse: 3.1645 | val_ll: -3.4839
[0:49:06.387872] epoch: 1500 | elbo: 530686.32125 | train_rmse: 0.4235 | val_rmse: 3.0083 | val_ll: -3.3705
[0:50:43.699754] epoch: 1550 | elbo: 523069.1603125 | train_rmse: 0.4076 | val_rmse: 2.8483 | val_ll: -3.2272
[0:52:20.686497] epoch: 1600 | elbo: 515198.86781250004 | train_rmse: 0.391 | val_rmse: 2.6993 | val_ll: -3.0632
[0:53:58.215468] epoch: 1650 | elbo: 507495.89562500006 | train_rmse: 0.391 | val_rmse: 2.5527 | val_ll: -2.9623
[0:55:37.297750] epoch: 1700 | elbo: 499306.7078125 | train_rmse: 0.3859 | val_rmse: 2.4119 | val_ll: -2.8484
[0:57:16.327682] epoch: 1750 | elbo: 491852.255 | train_rmse: 0.3992 | val_rmse: 2.2744 | val_ll: -2.6932
[0:58:55.491609] epoch: 1800 | elbo: 484084.65843749995 | train_rmse: 0.3713 | val_rmse: 2.1393 | val_ll: -2.555
[1:00:32.224631] epoch: 1850 | elbo: 476316.2421875001 | train_rmse: 0.3704 | val_rmse: 2.0221 | val_ll: -2.452
[1:02:08.092278] epoch: 1900 | elbo: 468520.26781249995 | train_rmse: 0.3657 | val_rmse: 1.9034 | val_ll: -2.3148
[1:03:44.875808] epoch: 1950 | elbo: 460860.99874999997 | train_rmse: 0.3616 | val_rmse: 1.8014 | val_ll: -2.2157
[1:05:25.086610] epoch: 2000 | elbo: 453323.98687499994 | train_rmse: 0.3645 | val_rmse: 1.6993 | val_ll: -2.1138
[1:07:03.469871] epoch: 2050 | elbo: 445560.33499999996 | train_rmse: 0.3613 | val_rmse: 1.6058 | val_ll: -2.0292
[1:08:43.239489] epoch: 2100 | elbo: 438084.2959375001 | train_rmse: 0.3631 | val_rmse: 1.522 | val_ll: -1.933
[1:10:21.426183] epoch: 2150 | elbo: 430603.9540624999 | train_rmse: 0.3633 | val_rmse: 1.45 | val_ll: -1.873
[1:12:00.380005] epoch: 2200 | elbo: 423141.4756250001 | train_rmse: 0.3614 | val_rmse: 1.3773 | val_ll: -1.7942
[1:13:38.722460] epoch: 2250 | elbo: 415670.33343749994 | train_rmse: 0.3648 | val_rmse: 1.315 | val_ll: -1.7225
[1:15:16.494837] epoch: 2300 | elbo: 408392.5928125 | train_rmse: 0.3629 | val_rmse: 1.257 | val_ll: -1.6722
[1:16:54.962676] epoch: 2350 | elbo: 401041.8565625 | train_rmse: 0.3643 | val_rmse: 1.2092 | val_ll: -1.6389
[1:18:33.897765] epoch: 2400 | elbo: 393867.7165625 | train_rmse: 0.365 | val_rmse: 1.1632 | val_ll: -1.5757
[1:20:12.900312] epoch: 2450 | elbo: 386825.46124999993 | train_rmse: 0.366 | val_rmse: 1.1188 | val_ll: -1.5299
[1:21:51.013588] epoch: 2500 | elbo: 379622.26968749997 | train_rmse: 0.3706 | val_rmse: 1.0788 | val_ll: -1.4855
[1:23:29.514995] epoch: 2550 | elbo: 372365.03343750007 | train_rmse: 0.3748 | val_rmse: 1.0482 | val_ll: -1.4538
[1:25:09.552168] epoch: 2600 | elbo: 365609.1225 | train_rmse: 0.3773 | val_rmse: 1.0119 | val_ll: -1.4075
[1:26:49.044371] epoch: 2650 | elbo: 358508.72062499996 | train_rmse: 0.3816 | val_rmse: 0.9804 | val_ll: -1.3701
[1:28:28.574857] epoch: 2700 | elbo: 351687.1140625 | train_rmse: 0.3865 | val_rmse: 0.9558 | val_ll: -1.3344
[1:30:07.065623] epoch: 2750 | elbo: 344831.14625 | train_rmse: 0.3882 | val_rmse: 0.9321 | val_ll: -1.3121
[1:31:43.634549] epoch: 2800 | elbo: 338000.595625 | train_rmse: 0.3932 | val_rmse: 0.9106 | val_ll: -1.2916
[1:33:20.364821] epoch: 2850 | elbo: 331314.31875 | train_rmse: 0.4031 | val_rmse: 0.8923 | val_ll: -1.2649
[1:34:57.201597] epoch: 2900 | elbo: 324467.5959375 | train_rmse: 0.4012 | val_rmse: 0.87 | val_ll: -1.2365
[1:36:34.352302] epoch: 2950 | elbo: 317746.3978125 | train_rmse: 0.4044 | val_rmse: 0.8562 | val_ll: -1.2225
[1:38:14.325390] epoch: 3000 | elbo: 311142.11906249996 | train_rmse: 0.4091 | val_rmse: 0.8405 | val_ll: -1.209
[1:39:52.651694] epoch: 3050 | elbo: 304463.2890625 | train_rmse: 0.4134 | val_rmse: 0.8244 | val_ll: -1.1865
[1:41:32.025210] epoch: 3100 | elbo: 297813.2765625 | train_rmse: 0.4159 | val_rmse: 0.8119 | val_ll: -1.1695
[1:43:11.597837] epoch: 3150 | elbo: 291224.7015625 | train_rmse: 0.422 | val_rmse: 0.7986 | val_ll: -1.1563
[1:44:50.265377] epoch: 3200 | elbo: 284663.8478125 | train_rmse: 0.4267 | val_rmse: 0.7902 | val_ll: -1.1377
[1:46:29.831319] epoch: 3250 | elbo: 278148.55937500007 | train_rmse: 0.4298 | val_rmse: 0.779 | val_ll: -1.1236
[1:48:09.548302] epoch: 3300 | elbo: 271560.91625 | train_rmse: 0.4346 | val_rmse: 0.769 | val_ll: -1.1184
[1:49:48.589626] epoch: 3350 | elbo: 265114.00999999995 | train_rmse: 0.4412 | val_rmse: 0.7592 | val_ll: -1.0998
[1:51:25.930304] epoch: 3400 | elbo: 258696.55109375 | train_rmse: 0.4453 | val_rmse: 0.7524 | val_ll: -1.0956
[1:53:04.996636] epoch: 3450 | elbo: 252296.14296875003 | train_rmse: 0.4507 | val_rmse: 0.7434 | val_ll: -1.0792
[1:54:44.213153] epoch: 3500 | elbo: 245822.99390624993 | train_rmse: 0.4567 | val_rmse: 0.7351 | val_ll: -1.0713
[1:56:23.043327] epoch: 3550 | elbo: 239425.14359374996 | train_rmse: 0.4625 | val_rmse: 0.7272 | val_ll: -1.0578
[1:58:02.220716] epoch: 3600 | elbo: 232961.81609374998 | train_rmse: 0.4647 | val_rmse: 0.7242 | val_ll: -1.0516
[1:59:41.189240] epoch: 3650 | elbo: 226656.6546875 | train_rmse: 0.4684 | val_rmse: 0.7176 | val_ll: -1.048
[2:01:19.744523] epoch: 3700 | elbo: 220271.02046874998 | train_rmse: 0.4736 | val_rmse: 0.713 | val_ll: -1.0393
[2:02:57.053435] epoch: 3750 | elbo: 213940.05312499995 | train_rmse: 0.4758 | val_rmse: 0.7076 | val_ll: -1.0388
[2:04:34.397247] epoch: 3800 | elbo: 207714.8796875 | train_rmse: 0.4806 | val_rmse: 0.7023 | val_ll: -1.0288
[2:06:11.882603] epoch: 3850 | elbo: 201453.48046875003 | train_rmse: 0.4829 | val_rmse: 0.6986 | val_ll: -1.0332
[2:07:49.724217] epoch: 3900 | elbo: 195246.70875000002 | train_rmse: 0.4884 | val_rmse: 0.6951 | val_ll: -1.0231
[2:09:26.068400] epoch: 3950 | elbo: 189133.871875 | train_rmse: 0.4891 | val_rmse: 0.6906 | val_ll: -1.0211
[2:11:02.922927] epoch: 4000 | elbo: 182995.76859375 | train_rmse: 0.4926 | val_rmse: 0.6884 | val_ll: -1.0149
[2:12:41.035781] epoch: 4050 | elbo: 176897.14406249998 | train_rmse: 0.4962 | val_rmse: 0.684 | val_ll: -1.0096
[2:14:18.836035] epoch: 4100 | elbo: 170926.48140624998 | train_rmse: 0.4984 | val_rmse: 0.6826 | val_ll: -1.012
[2:15:58.965065] epoch: 4150 | elbo: 164913.96812499998 | train_rmse: 0.5026 | val_rmse: 0.6779 | val_ll: -1.0056
[2:17:37.438434] epoch: 4200 | elbo: 159153.91953124997 | train_rmse: 0.5031 | val_rmse: 0.6759 | val_ll: -1.011
[2:19:15.603925] epoch: 4250 | elbo: 153366.95124999998 | train_rmse: 0.5058 | val_rmse: 0.6723 | val_ll: -1.004
[2:20:52.831149] epoch: 4300 | elbo: 147629.311875 | train_rmse: 0.5096 | val_rmse: 0.6688 | val_ll: -1.0046
[2:22:29.909633] epoch: 4350 | elbo: 142051.76953125 | train_rmse: 0.5105 | val_rmse: 0.6688 | val_ll: -1.0024
[2:24:07.738106] epoch: 4400 | elbo: 136449.02437499998 | train_rmse: 0.5127 | val_rmse: 0.6659 | val_ll: -1.008
[2:25:44.938825] epoch: 4450 | elbo: 131057.38023437498 | train_rmse: 0.5151 | val_rmse: 0.6637 | val_ll: -1.0071
[2:27:23.495666] epoch: 4500 | elbo: 125832.60171875001 | train_rmse: 0.515 | val_rmse: 0.6628 | val_ll: -1.0037
[2:29:00.761818] epoch: 4550 | elbo: 120658.21085937499 | train_rmse: 0.5175 | val_rmse: 0.6605 | val_ll: -1.0028
[2:30:37.824603] epoch: 4600 | elbo: 115590.51906250001 | train_rmse: 0.5189 | val_rmse: 0.6591 | val_ll: -0.9975
[2:32:17.288256] epoch: 4650 | elbo: 110654.503515625 | train_rmse: 0.5196 | val_rmse: 0.6576 | val_ll: -1.0003
[2:33:57.119859] epoch: 4700 | elbo: 106062.470859375 | train_rmse: 0.5204 | val_rmse: 0.6549 | val_ll: -0.9883
[2:35:35.198726] epoch: 4750 | elbo: 101433.79046874998 | train_rmse: 0.5204 | val_rmse: 0.6549 | val_ll: -0.9837
[2:37:12.428566] epoch: 4800 | elbo: 97017.923359375 | train_rmse: 0.5211 | val_rmse: 0.6542 | val_ll: -0.9889
[2:38:51.643411] epoch: 4850 | elbo: 92714.93687500001 | train_rmse: 0.5202 | val_rmse: 0.6505 | val_ll: -0.9767
[2:40:29.700271] epoch: 4900 | elbo: 88605.191484375 | train_rmse: 0.5209 | val_rmse: 0.6508 | val_ll: -0.9786
[2:42:07.837281] epoch: 4950 | elbo: 84610.93046875001 | train_rmse: 0.5179 | val_rmse: 0.649 | val_ll: -0.9728
[2:43:45.431510] epoch: 5000 | elbo: 80869.675390625 | train_rmse: 0.5169 | val_rmse: 0.648 | val_ll: -0.9706
[2:45:22.969529] epoch: 5050 | elbo: 77207.51609374999 | train_rmse: 0.5162 | val_rmse: 0.646 | val_ll: -0.966
[2:47:01.422487] epoch: 5100 | elbo: 73749.55945312501 | train_rmse: 0.5157 | val_rmse: 0.6467 | val_ll: -0.9667
[2:48:39.315928] epoch: 5150 | elbo: 70470.901171875 | train_rmse: 0.5154 | val_rmse: 0.6452 | val_ll: -0.964
[2:50:17.961197] epoch: 5200 | elbo: 67325.2996875 | train_rmse: 0.5145 | val_rmse: 0.6443 | val_ll: -0.9638
[2:51:56.652022] epoch: 5250 | elbo: 64286.09734375001 | train_rmse: 0.5124 | val_rmse: 0.6424 | val_ll: -0.9607
[2:53:34.524579] epoch: 5300 | elbo: 61529.808515624994 | train_rmse: 0.5104 | val_rmse: 0.6407 | val_ll: -0.9588
[2:55:13.320571] epoch: 5350 | elbo: 58813.289804687505 | train_rmse: 0.5098 | val_rmse: 0.6396 | val_ll: -0.9535
[2:56:50.925069] epoch: 5400 | elbo: 56340.276171875 | train_rmse: 0.5093 | val_rmse: 0.6389 | val_ll: -0.9521
[2:58:29.425696] epoch: 5450 | elbo: 53954.87042968751 | train_rmse: 0.508 | val_rmse: 0.638 | val_ll: -0.9519
[3:00:08.676416] epoch: 5500 | elbo: 51808.824375000004 | train_rmse: 0.5072 | val_rmse: 0.6372 | val_ll: -0.9484
[3:01:47.074490] epoch: 5550 | elbo: 49731.369921875004 | train_rmse: 0.5056 | val_rmse: 0.636 | val_ll: -0.9483
[3:03:25.591727] epoch: 5600 | elbo: 47753.678828125005 | train_rmse: 0.506 | val_rmse: 0.6358 | val_ll: -0.946
[3:05:04.058027] epoch: 5650 | elbo: 46027.58675781251 | train_rmse: 0.5044 | val_rmse: 0.6351 | val_ll: -0.9393
[3:06:40.509353] epoch: 5700 | elbo: 44365.612148437496 | train_rmse: 0.5034 | val_rmse: 0.6334 | val_ll: -0.9381
[3:08:17.201638] epoch: 5750 | elbo: 42817.8594921875 | train_rmse: 0.5023 | val_rmse: 0.6319 | val_ll: -0.9356
[3:09:54.620900] epoch: 5800 | elbo: 41454.18582031251 | train_rmse: 0.5016 | val_rmse: 0.6298 | val_ll: -0.933
[3:11:33.431476] epoch: 5850 | elbo: 40191.7577734375 | train_rmse: 0.5024 | val_rmse: 0.6317 | val_ll: -0.9341
[3:13:10.813910] epoch: 5900 | elbo: 38982.56253906251 | train_rmse: 0.5 | val_rmse: 0.6297 | val_ll: -0.9303
[3:14:51.817678] epoch: 5950 | elbo: 37946.594257812496 | train_rmse: 0.4996 | val_rmse: 0.6282 | val_ll: -0.9244
[3:16:29.046660] epoch: 6000 | elbo: 36937.186953125 | train_rmse: 0.4999 | val_rmse: 0.6276 | val_ll: -0.9243
[3:18:07.791187] epoch: 6050 | elbo: 36050.70437500001 | train_rmse: 0.4989 | val_rmse: 0.6272 | val_ll: -0.9188
[3:19:46.046672] epoch: 6100 | elbo: 35209.3590234375 | train_rmse: 0.4976 | val_rmse: 0.6256 | val_ll: -0.9179
[3:21:22.840317] epoch: 6150 | elbo: 34486.740859375 | train_rmse: 0.4967 | val_rmse: 0.6263 | val_ll: -0.9224
[3:23:01.786311] epoch: 6200 | elbo: 33799.4534765625 | train_rmse: 0.4958 | val_rmse: 0.6243 | val_ll: -0.9186
[3:24:39.672776] epoch: 6250 | elbo: 33173.59447265625 | train_rmse: 0.4965 | val_rmse: 0.6251 | val_ll: -0.9183
[3:26:17.775848] epoch: 6300 | elbo: 32630.105624999997 | train_rmse: 0.4964 | val_rmse: 0.6231 | val_ll: -0.9151
[3:27:56.857483] epoch: 6350 | elbo: 32153.26658203125 | train_rmse: 0.4945 | val_rmse: 0.6233 | val_ll: -0.9164
[3:29:35.301316] epoch: 6400 | elbo: 31593.955429687503 | train_rmse: 0.4944 | val_rmse: 0.6231 | val_ll: -0.9162
[3:31:11.968748] epoch: 6450 | elbo: 31174.879511718744 | train_rmse: 0.4944 | val_rmse: 0.6226 | val_ll: -0.9129
[3:32:49.440460] epoch: 6500 | elbo: 30774.063828125003 | train_rmse: 0.493 | val_rmse: 0.6208 | val_ll: -0.9092
[3:34:27.519382] epoch: 6550 | elbo: 30389.090253906244 | train_rmse: 0.4933 | val_rmse: 0.6205 | val_ll: -0.9149
[3:36:06.492501] epoch: 6600 | elbo: 30055.6711328125 | train_rmse: 0.4918 | val_rmse: 0.6197 | val_ll: -0.9102
[3:37:44.384480] epoch: 6650 | elbo: 29698.54640625 | train_rmse: 0.4911 | val_rmse: 0.6193 | val_ll: -0.9083
[3:39:22.617652] epoch: 6700 | elbo: 29458.8268359375 | train_rmse: 0.491 | val_rmse: 0.6188 | val_ll: -0.9139
[3:41:01.637540] epoch: 6750 | elbo: 29084.9970703125 | train_rmse: 0.4914 | val_rmse: 0.6202 | val_ll: -0.9143
[3:42:40.440960] epoch: 6800 | elbo: 28897.016933593746 | train_rmse: 0.4898 | val_rmse: 0.6179 | val_ll: -0.9033
[3:44:18.303503] epoch: 6850 | elbo: 28640.195253906255 | train_rmse: 0.4888 | val_rmse: 0.6184 | val_ll: -0.909
[3:45:56.225871] epoch: 6900 | elbo: 28388.717109375 | train_rmse: 0.4889 | val_rmse: 0.6181 | val_ll: -0.9068
[3:47:34.903561] epoch: 6950 | elbo: 28157.428593749995 | train_rmse: 0.4874 | val_rmse: 0.6162 | val_ll: -0.9014
[3:49:14.964335] epoch: 7000 | elbo: 28005.608945312495 | train_rmse: 0.4881 | val_rmse: 0.6163 | val_ll: -0.9018
[3:50:55.243518] epoch: 7050 | elbo: 27790.35865234375 | train_rmse: 0.4883 | val_rmse: 0.6161 | val_ll: -0.9038
[3:52:33.921856] epoch: 7100 | elbo: 27585.1150390625 | train_rmse: 0.4867 | val_rmse: 0.6156 | val_ll: -0.9087
[3:54:11.611664] epoch: 7150 | elbo: 27386.3771875 | train_rmse: 0.486 | val_rmse: 0.6147 | val_ll: -0.8966
[3:55:49.357814] epoch: 7200 | elbo: 27243.703847656252 | train_rmse: 0.4858 | val_rmse: 0.6157 | val_ll: -0.9057
[3:57:25.741161] epoch: 7250 | elbo: 27061.145371093753 | train_rmse: 0.4853 | val_rmse: 0.6143 | val_ll: -0.9002
[3:59:03.812118] epoch: 7300 | elbo: 26903.176835937495 | train_rmse: 0.4853 | val_rmse: 0.6136 | val_ll: -0.8953
[4:00:39.777465] epoch: 7350 | elbo: 26767.810507812497 | train_rmse: 0.4856 | val_rmse: 0.6139 | val_ll: -0.8968
[4:02:18.101426] epoch: 7400 | elbo: 26626.378613281246 | train_rmse: 0.484 | val_rmse: 0.6135 | val_ll: -0.8988
[4:03:55.445764] epoch: 7450 | elbo: 26477.0700390625 | train_rmse: 0.4838 | val_rmse: 0.6125 | val_ll: -0.8956
[4:05:34.419280] epoch: 7500 | elbo: 26312.01787109375 | train_rmse: 0.4825 | val_rmse: 0.6132 | val_ll: -0.8945
[4:07:12.529432] epoch: 7550 | elbo: 26219.093769531253 | train_rmse: 0.4827 | val_rmse: 0.6129 | val_ll: -0.8997
[4:08:52.240568] epoch: 7600 | elbo: 26113.368359375 | train_rmse: 0.4818 | val_rmse: 0.6132 | val_ll: -0.8932
[4:10:30.388778] epoch: 7650 | elbo: 25951.57900390625 | train_rmse: 0.4814 | val_rmse: 0.6126 | val_ll: -0.8962
[4:12:08.397832] epoch: 7700 | elbo: 25869.8676171875 | train_rmse: 0.4807 | val_rmse: 0.6123 | val_ll: -0.8887
[4:13:48.152898] epoch: 7750 | elbo: 25738.0732421875 | train_rmse: 0.4812 | val_rmse: 0.6119 | val_ll: -0.8923
[4:15:27.584210] epoch: 7800 | elbo: 25659.64837890625 | train_rmse: 0.4793 | val_rmse: 0.6126 | val_ll: -0.8923
[4:17:06.587389] epoch: 7850 | elbo: 25537.679609375 | train_rmse: 0.4803 | val_rmse: 0.6121 | val_ll: -0.8901
[4:18:45.757204] epoch: 7900 | elbo: 25373.26955078125 | train_rmse: 0.4797 | val_rmse: 0.6119 | val_ll: -0.8884
[4:20:25.207435] epoch: 7950 | elbo: 25333.24138671875 | train_rmse: 0.4789 | val_rmse: 0.6119 | val_ll: -0.887
[4:22:02.363146] epoch: 8000 | elbo: 25219.34044921875 | train_rmse: 0.4794 | val_rmse: 0.6117 | val_ll: -0.8889
[4:23:39.803161] epoch: 8050 | elbo: 25071.9617578125 | train_rmse: 0.4784 | val_rmse: 0.6116 | val_ll: -0.8884
[4:25:17.144372] epoch: 8100 | elbo: 25015.921757812503 | train_rmse: 0.4788 | val_rmse: 0.6112 | val_ll: -0.8866
[4:26:55.699104] epoch: 8150 | elbo: 24934.94724609375 | train_rmse: 0.4803 | val_rmse: 0.6123 | val_ll: -0.8945
[4:28:34.198683] epoch: 8200 | elbo: 24822.0862109375 | train_rmse: 0.4772 | val_rmse: 0.6112 | val_ll: -0.891
[4:30:11.598484] epoch: 8250 | elbo: 24742.5355859375 | train_rmse: 0.4776 | val_rmse: 0.6107 | val_ll: -0.8883
[4:31:50.156862] epoch: 8300 | elbo: 24654.671679687497 | train_rmse: 0.4779 | val_rmse: 0.6112 | val_ll: -0.8853
[4:33:27.291489] epoch: 8350 | elbo: 24565.857753906246 | train_rmse: 0.4762 | val_rmse: 0.6111 | val_ll: -0.8858
[4:35:05.546894] epoch: 8400 | elbo: 24509.205625000002 | train_rmse: 0.4776 | val_rmse: 0.6119 | val_ll: -0.8898
[4:36:43.487065] epoch: 8450 | elbo: 24410.562109374998 | train_rmse: 0.4762 | val_rmse: 0.6102 | val_ll: -0.8901
[4:38:22.595083] epoch: 8500 | elbo: 24337.9105859375 | train_rmse: 0.4764 | val_rmse: 0.6103 | val_ll: -0.8911
[4:40:01.924245] epoch: 8550 | elbo: 24291.83173828125 | train_rmse: 0.4768 | val_rmse: 0.6099 | val_ll: -0.8895
[4:41:39.616262] epoch: 8600 | elbo: 24188.220878906253 | train_rmse: 0.4759 | val_rmse: 0.6094 | val_ll: -0.8897
[4:43:17.805039] epoch: 8650 | elbo: 24107.171035156247 | train_rmse: 0.4759 | val_rmse: 0.6103 | val_ll: -0.8891
[4:44:54.695624] epoch: 8700 | elbo: 24045.315761718746 | train_rmse: 0.4768 | val_rmse: 0.6117 | val_ll: -0.895
[4:46:31.625432] epoch: 8750 | elbo: 23941.34943359375 | train_rmse: 0.475 | val_rmse: 0.6098 | val_ll: -0.8889
[4:48:07.791212] epoch: 8800 | elbo: 23917.176015625002 | train_rmse: 0.4748 | val_rmse: 0.6098 | val_ll: -0.8876
[4:49:43.754183] epoch: 8850 | elbo: 23788.93794921875 | train_rmse: 0.4749 | val_rmse: 0.6098 | val_ll: -0.8897
[4:51:21.588797] epoch: 8900 | elbo: 23702.249003906247 | train_rmse: 0.4739 | val_rmse: 0.6105 | val_ll: -0.8911
[4:52:59.231198] epoch: 8950 | elbo: 23695.40646484375 | train_rmse: 0.4745 | val_rmse: 0.6107 | val_ll: -0.8854
[4:54:36.247164] epoch: 9000 | elbo: 23611.239199218755 | train_rmse: 0.4732 | val_rmse: 0.6093 | val_ll: -0.8888
[4:56:13.760450] epoch: 9050 | elbo: 23559.89634765625 | train_rmse: 0.4733 | val_rmse: 0.6104 | val_ll: -0.8885
[4:57:51.568005] epoch: 9100 | elbo: 23487.374980468747 | train_rmse: 0.4732 | val_rmse: 0.6099 | val_ll: -0.8903
[4:59:30.629798] epoch: 9150 | elbo: 23434.73037109375 | train_rmse: 0.4737 | val_rmse: 0.611 | val_ll: -0.8893
[5:01:10.241414] epoch: 9200 | elbo: 23343.928125 | train_rmse: 0.4724 | val_rmse: 0.6103 | val_ll: -0.8885
[5:02:48.007251] epoch: 9250 | elbo: 23395.565917968754 | train_rmse: 0.4727 | val_rmse: 0.6101 | val_ll: -0.8909
[5:04:24.398179] epoch: 9300 | elbo: 23311.961542968747 | train_rmse: 0.4723 | val_rmse: 0.6094 | val_ll: -0.8896
[5:06:01.558413] epoch: 9350 | elbo: 23241.250019531253 | train_rmse: 0.4721 | val_rmse: 0.6096 | val_ll: -0.8858
[5:07:40.060491] epoch: 9400 | elbo: 23179.5321875 | train_rmse: 0.4722 | val_rmse: 0.6097 | val_ll: -0.8911
[5:09:16.457303] epoch: 9450 | elbo: 23128.28787109375 | train_rmse: 0.472 | val_rmse: 0.6099 | val_ll: -0.8904
[5:10:55.053329] epoch: 9500 | elbo: 23078.3662109375 | train_rmse: 0.4717 | val_rmse: 0.6095 | val_ll: -0.8877
[5:12:33.864464] epoch: 9550 | elbo: 23034.3092578125 | train_rmse: 0.4718 | val_rmse: 0.609 | val_ll: -0.8898
[5:14:12.245471] epoch: 9600 | elbo: 22984.753710937504 | train_rmse: 0.4714 | val_rmse: 0.6085 | val_ll: -0.8856
[5:15:51.555483] epoch: 9650 | elbo: 22954.80794921875 | train_rmse: 0.4715 | val_rmse: 0.6084 | val_ll: -0.8857
[5:17:30.318338] epoch: 9700 | elbo: 22872.923593749998 | train_rmse: 0.4716 | val_rmse: 0.6086 | val_ll: -0.8872
[5:19:09.052342] epoch: 9750 | elbo: 22956.863906249997 | train_rmse: 0.4719 | val_rmse: 0.6084 | val_ll: -0.8865
[5:20:44.241483] epoch: 9800 | elbo: 22817.072343750006 | train_rmse: 0.4713 | val_rmse: 0.6085 | val_ll: -0.8848
[5:22:21.752776] epoch: 9850 | elbo: 22828.558378906248 | train_rmse: 0.4719 | val_rmse: 0.6083 | val_ll: -0.8852
[5:23:56.695590] epoch: 9900 | elbo: 22740.456738281246 | train_rmse: 0.4717 | val_rmse: 0.6079 | val_ll: -0.8852
[5:25:31.652449] epoch: 9950 | elbo: 22716.833105468748 | train_rmse: 0.4718 | val_rmse: 0.6081 | val_ll: -0.8871
Training finished in 5:27:05.064299 seconds
Saved SVI model to tests/dataset-tests/sineasy10-10k-s05/models/sineasy10-3x256-s05/checkpoint_1.pt
File Size is 1.0322160720825195 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=256, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=256, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:5 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-10k-s05/models/sineasy10-3x256-s05/checkpoint_1.pt
using device: cuda:5
====== evaluating profile sineasy10-3x256-s05 - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:02:48.933934
End time: 2023-07-12 14:26:46.572499
Total time: 5:29:58.323278
