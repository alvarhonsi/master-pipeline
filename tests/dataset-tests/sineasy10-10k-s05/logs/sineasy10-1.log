Start time: 2023-07-11 13:39:25.053649
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=1024, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=1024, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.3 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([1024, 10]) Parameter containing:
tensor([[-0.2089,  0.1508,  0.1643,  ...,  0.2689,  0.2351, -0.3329],
        [ 0.2061,  0.0734, -0.9134,  ...,  0.2025,  0.1104, -0.0875],
        [-0.0934,  0.5361, -0.2912,  ...,  0.1764,  0.0960, -0.3162],
        ...,
        [ 0.0451,  0.0931,  0.0445,  ..., -0.0960, -0.2719,  0.3048],
        [-0.3908,  0.1906, -0.1231,  ..., -0.1292,  0.0310,  0.1810],
        [-0.0436, -0.3058, -0.1012,  ..., -0.1576,  0.2240, -0.3060]],
       device='cuda:4', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([1024, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([1024]) Parameter containing:
tensor([-0.5753, -0.3101,  0.2168,  ..., -0.2042,  0.0234,  0.0770],
       device='cuda:4', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([1024]) tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([1024, 1024]) Parameter containing:
tensor([[ 0.0352, -0.0762,  0.1614,  ...,  0.1166, -0.3852,  0.3676],
        [-0.1264,  0.2264,  0.1796,  ..., -0.5443, -0.3937,  0.0275],
        [-0.7284,  0.3423,  0.0622,  ...,  0.3707,  0.4784,  0.1133],
        ...,
        [ 0.3014, -0.5653,  0.4463,  ...,  0.2374,  0.3848,  0.1976],
        [ 0.1413, -0.3396, -0.2743,  ..., -0.4204,  0.2095,  0.0190],
        [-0.1254,  0.0109,  0.6786,  ..., -0.0577, -0.0040, -0.2138]],
       device='cuda:4', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([1024, 1024]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([1024]) Parameter containing:
tensor([-0.1085, -0.0696,  0.4176,  ..., -0.3476,  0.6824,  0.4097],
       device='cuda:4', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([1024]) tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([1024, 1024]) Parameter containing:
tensor([[ 0.2941, -0.2311,  0.2470,  ...,  0.1197, -0.5038, -0.3242],
        [ 0.3758,  0.1970, -0.3776,  ..., -0.2569, -0.5728, -0.2872],
        [-0.0497, -0.1804, -0.2681,  ...,  0.4026, -0.1131,  0.2799],
        ...,
        [ 0.2761, -0.1192, -0.4168,  ..., -0.6176,  0.2410,  0.1452],
        [-0.2655, -0.4898, -0.0059,  ..., -0.3315,  0.3965,  0.2428],
        [-0.3038, -0.4560,  0.0440,  ..., -0.1564,  0.1564,  0.0968]],
       device='cuda:4', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([1024, 1024]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([1024]) Parameter containing:
tensor([-0.2846,  0.3795, -0.3625,  ..., -0.3964,  0.1820,  0.3978],
       device='cuda:4', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([1024]) tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 1024]) Parameter containing:
tensor([[-0.4354,  0.1229,  0.3068,  ...,  0.0374,  0.2633,  0.3091]],
       device='cuda:4', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 1024]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([0.1311], device='cuda:4', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:4', grad_fn=<AddBackward0>)
Using device: cuda:4
===== Training profile sineasy10-3x1024-s03 - 1 =====
[0:00:01.973226] epoch: 0 | elbo: 11818746398.720003 | train_rmse: 378.2093 | val_rmse: 382.5185 | val_ll: -46.8501
[0:01:41.966367] epoch: 50 | elbo: 365424353.28000003 | train_rmse: 69.1537 | val_rmse: 88.4724 | val_ll: -7.3653
[0:03:21.941459] epoch: 100 | elbo: 203782364.48 | train_rmse: 46.0887 | val_rmse: 75.1669 | val_ll: -6.7302
[0:05:02.467168] epoch: 150 | elbo: 135590269.27999997 | train_rmse: 32.9762 | val_rmse: 69.4597 | val_ll: -6.532
[0:06:41.776020] epoch: 200 | elbo: 102148291.44000001 | train_rmse: 24.0223 | val_rmse: 66.2665 | val_ll: -6.478
[0:08:24.620308] epoch: 250 | elbo: 80697774.08 | train_rmse: 17.5878 | val_rmse: 64.093 | val_ll: -6.5501
[0:10:04.999207] epoch: 300 | elbo: 67761560.03999999 | train_rmse: 12.8815 | val_rmse: 62.4102 | val_ll: -6.601
[0:11:46.084797] epoch: 350 | elbo: 58678013.83999999 | train_rmse: 9.414 | val_rmse: 61.2096 | val_ll: -6.7143
[0:13:27.031380] epoch: 400 | elbo: 52126398.6 | train_rmse: 7.0069 | val_rmse: 59.9685 | val_ll: -6.7992
[0:15:08.960224] epoch: 450 | elbo: 46998364.04 | train_rmse: 5.9318 | val_rmse: 58.6573 | val_ll: -6.9051
[0:16:50.453907] epoch: 500 | elbo: 42820751.4 | train_rmse: 4.7626 | val_rmse: 57.4525 | val_ll: -7.042
[0:18:30.957904] epoch: 550 | elbo: 39075383.52 | train_rmse: 4.2441 | val_rmse: 55.9923 | val_ll: -7.1331
[0:20:09.052306] epoch: 600 | elbo: 35758748.720000006 | train_rmse: 3.9761 | val_rmse: 54.5698 | val_ll: -7.3734
[0:21:49.544508] epoch: 650 | elbo: 32797218.860000003 | train_rmse: 4.0563 | val_rmse: 53.0155 | val_ll: -7.5139
[0:23:31.422954] epoch: 700 | elbo: 30152576.940000005 | train_rmse: 3.6173 | val_rmse: 51.5762 | val_ll: -7.7532
[0:25:12.689119] epoch: 750 | elbo: 27704420.98 | train_rmse: 3.6077 | val_rmse: 50.0233 | val_ll: -7.9273
[0:26:54.114307] epoch: 800 | elbo: 25660082.119999997 | train_rmse: 3.3613 | val_rmse: 48.5679 | val_ll: -8.1906
[0:28:35.483673] epoch: 850 | elbo: 23643952.859999996 | train_rmse: 3.3895 | val_rmse: 47.1969 | val_ll: -8.4188
[0:30:16.264700] epoch: 900 | elbo: 22063587.96 | train_rmse: 3.2754 | val_rmse: 45.9547 | val_ll: -8.7163
[0:31:57.816826] epoch: 950 | elbo: 20727323.700000003 | train_rmse: 3.0085 | val_rmse: 44.4512 | val_ll: -8.8852
[0:33:38.431494] epoch: 1000 | elbo: 19385236.360000003 | train_rmse: 2.6819 | val_rmse: 43.1748 | val_ll: -9.1343
[0:35:19.090094] epoch: 1050 | elbo: 18321070.94 | train_rmse: 2.8868 | val_rmse: 41.8728 | val_ll: -9.542
[0:36:59.886299] epoch: 1100 | elbo: 17290692.04 | train_rmse: 2.5422 | val_rmse: 40.7013 | val_ll: -9.7712
[0:38:39.999461] epoch: 1150 | elbo: 16494434.489999998 | train_rmse: 2.6726 | val_rmse: 39.6275 | val_ll: -9.9831
[0:40:22.313824] epoch: 1200 | elbo: 15692152.030000001 | train_rmse: 2.272 | val_rmse: 38.4898 | val_ll: -10.3036
[0:42:02.796314] epoch: 1250 | elbo: 15022953.850000003 | train_rmse: 2.2746 | val_rmse: 37.3291 | val_ll: -10.4907
[0:43:43.267999] epoch: 1300 | elbo: 14399504.160000002 | train_rmse: 2.1235 | val_rmse: 36.3311 | val_ll: -10.7801
[0:45:23.562397] epoch: 1350 | elbo: 13974970.0 | train_rmse: 2.494 | val_rmse: 35.5297 | val_ll: -11.0923
[0:47:05.473167] epoch: 1400 | elbo: 13427008.430000002 | train_rmse: 2.0275 | val_rmse: 34.5247 | val_ll: -11.5822
[0:48:45.997138] epoch: 1450 | elbo: 13041381.62 | train_rmse: 2.3033 | val_rmse: 33.755 | val_ll: -12.0578
[0:50:25.704061] epoch: 1500 | elbo: 12643127.100000001 | train_rmse: 1.9757 | val_rmse: 32.8884 | val_ll: -12.3532
[0:52:07.808280] epoch: 1550 | elbo: 12304414.87 | train_rmse: 1.7263 | val_rmse: 31.9988 | val_ll: -12.842
[0:53:47.643386] epoch: 1600 | elbo: 12063780.620000001 | train_rmse: 1.7371 | val_rmse: 31.1542 | val_ll: -13.1408
[0:55:28.118439] epoch: 1650 | elbo: 11797651.989999998 | train_rmse: 1.7447 | val_rmse: 30.3689 | val_ll: -13.5746
[0:57:08.861705] epoch: 1700 | elbo: 11555340.75 | train_rmse: 1.6684 | val_rmse: 29.5612 | val_ll: -13.8649
[0:58:49.636566] epoch: 1750 | elbo: 11357960.790000001 | train_rmse: 1.5695 | val_rmse: 28.8263 | val_ll: -14.4527
[1:00:29.126406] epoch: 1800 | elbo: 11128498.400000002 | train_rmse: 1.5184 | val_rmse: 28.0627 | val_ll: -14.9594
[1:02:08.922132] epoch: 1850 | elbo: 10958663.02 | train_rmse: 1.552 | val_rmse: 27.3706 | val_ll: -15.1248
[1:03:49.384734] epoch: 1900 | elbo: 10778114.239999998 | train_rmse: 1.477 | val_rmse: 26.6968 | val_ll: -15.524
[1:05:29.515215] epoch: 1950 | elbo: 10631509.6 | train_rmse: 1.414 | val_rmse: 25.9756 | val_ll: -15.8042
[1:07:08.622305] epoch: 2000 | elbo: 10491801.6 | train_rmse: 1.3029 | val_rmse: 25.2679 | val_ll: -16.0434
[1:08:49.996355] epoch: 2050 | elbo: 10345620.43 | train_rmse: 1.3522 | val_rmse: 24.6361 | val_ll: -16.6659
[1:10:29.787838] epoch: 2100 | elbo: 10223394.34 | train_rmse: 1.3189 | val_rmse: 23.9511 | val_ll: -16.8531
[1:12:09.052919] epoch: 2150 | elbo: 10104644.709999999 | train_rmse: 1.1498 | val_rmse: 23.3443 | val_ll: -17.1573
[1:13:51.392104] epoch: 2200 | elbo: 10002326.919999998 | train_rmse: 1.1859 | val_rmse: 22.749 | val_ll: -17.6555
[1:15:32.513075] epoch: 2250 | elbo: 9900573.03 | train_rmse: 1.2402 | val_rmse: 22.1621 | val_ll: -17.9397
[1:17:13.561980] epoch: 2300 | elbo: 9782462.02 | train_rmse: 1.1758 | val_rmse: 21.5046 | val_ll: -18.5945
[1:18:54.946459] epoch: 2350 | elbo: 9671921.950000001 | train_rmse: 1.0181 | val_rmse: 20.8985 | val_ll: -18.325
[1:20:37.475842] epoch: 2400 | elbo: 9580215.97 | train_rmse: 1.084 | val_rmse: 20.2559 | val_ll: -18.356
[1:22:18.519054] epoch: 2450 | elbo: 9483519.42 | train_rmse: 1.0677 | val_rmse: 19.6803 | val_ll: -18.36
[1:23:59.445558] epoch: 2500 | elbo: 9378248.41 | train_rmse: 0.9759 | val_rmse: 19.1198 | val_ll: -18.4537
[1:25:38.022415] epoch: 2550 | elbo: 9292027.51 | train_rmse: 0.9532 | val_rmse: 18.6104 | val_ll: -18.3998
[1:27:19.092926] epoch: 2600 | elbo: 9181847.559999999 | train_rmse: 0.9039 | val_rmse: 18.0512 | val_ll: -18.2874
[1:28:59.040413] epoch: 2650 | elbo: 9095112.290000001 | train_rmse: 0.8718 | val_rmse: 17.5217 | val_ll: -18.0972
[1:30:38.916715] epoch: 2700 | elbo: 9001930.139999999 | train_rmse: 0.8998 | val_rmse: 16.9936 | val_ll: -17.9358
[1:32:19.387666] epoch: 2750 | elbo: 8905479.419999998 | train_rmse: 0.8236 | val_rmse: 16.4671 | val_ll: -17.6353
[1:33:59.731707] epoch: 2800 | elbo: 8815102.4 | train_rmse: 0.8576 | val_rmse: 15.9449 | val_ll: -17.328
[1:35:41.802741] epoch: 2850 | elbo: 8721227.94 | train_rmse: 0.801 | val_rmse: 15.4617 | val_ll: -17.187
[1:37:21.187453] epoch: 2900 | elbo: 8625502.26 | train_rmse: 0.8819 | val_rmse: 14.957 | val_ll: -16.7844
[1:39:01.624391] epoch: 2950 | elbo: 8530245.660000002 | train_rmse: 0.8388 | val_rmse: 14.4624 | val_ll: -16.2131
[1:40:44.171786] epoch: 3000 | elbo: 8437174.24 | train_rmse: 0.7077 | val_rmse: 13.9761 | val_ll: -16.0983
[1:42:26.073152] epoch: 3050 | elbo: 8334921.63 | train_rmse: 0.6967 | val_rmse: 13.4826 | val_ll: -15.3406
[1:44:07.085590] epoch: 3100 | elbo: 8242219.45 | train_rmse: 0.6802 | val_rmse: 13.0122 | val_ll: -15.0472
[1:45:47.618670] epoch: 3150 | elbo: 8141314.5 | train_rmse: 0.6882 | val_rmse: 12.5426 | val_ll: -14.6494
[1:47:28.052645] epoch: 3200 | elbo: 8041758.275 | train_rmse: 0.6216 | val_rmse: 12.0782 | val_ll: -14.1778
[1:49:08.202239] epoch: 3250 | elbo: 7941458.7700000005 | train_rmse: 0.6085 | val_rmse: 11.6615 | val_ll: -13.9298
[1:50:49.330553] epoch: 3300 | elbo: 7843532.864999999 | train_rmse: 0.597 | val_rmse: 11.2513 | val_ll: -13.7107
[1:52:29.998403] epoch: 3350 | elbo: 7741673.784999999 | train_rmse: 0.6349 | val_rmse: 10.873 | val_ll: -13.2281
[1:54:11.382940] epoch: 3400 | elbo: 7638838.51 | train_rmse: 0.5552 | val_rmse: 10.4927 | val_ll: -12.857
[1:55:52.425194] epoch: 3450 | elbo: 7537595.615 | train_rmse: 0.5394 | val_rmse: 10.1232 | val_ll: -12.5741
[1:57:33.038481] epoch: 3500 | elbo: 7437053.839999999 | train_rmse: 0.5269 | val_rmse: 9.7166 | val_ll: -12.0191
[1:59:14.473701] epoch: 3550 | elbo: 7335754.735000001 | train_rmse: 0.4993 | val_rmse: 9.3482 | val_ll: -11.5579
[2:00:54.746732] epoch: 3600 | elbo: 7232410.294999999 | train_rmse: 0.5014 | val_rmse: 8.9388 | val_ll: -10.9609
[2:02:34.275202] epoch: 3650 | elbo: 7126493.349999999 | train_rmse: 0.5062 | val_rmse: 8.5646 | val_ll: -10.5498
[2:04:14.427209] epoch: 3700 | elbo: 7022834.290000001 | train_rmse: 0.4744 | val_rmse: 8.1894 | val_ll: -10.1269
[2:05:55.178387] epoch: 3750 | elbo: 6918094.74 | train_rmse: 0.4548 | val_rmse: 7.8331 | val_ll: -9.5495
[2:07:36.576486] epoch: 3800 | elbo: 6814040.505 | train_rmse: 0.4196 | val_rmse: 7.4944 | val_ll: -9.1611
[2:09:15.774615] epoch: 3850 | elbo: 6709254.084999999 | train_rmse: 0.412 | val_rmse: 7.1619 | val_ll: -8.8345
[2:10:56.898819] epoch: 3900 | elbo: 6601980.135 | train_rmse: 0.391 | val_rmse: 6.8681 | val_ll: -8.6861
[2:12:37.857236] epoch: 3950 | elbo: 6496084.955 | train_rmse: 0.3988 | val_rmse: 6.5792 | val_ll: -8.2053
[2:14:17.313538] epoch: 4000 | elbo: 6390810.1899999995 | train_rmse: 0.3946 | val_rmse: 6.3006 | val_ll: -8.0618
[2:15:57.773600] epoch: 4050 | elbo: 6286642.275 | train_rmse: 0.3691 | val_rmse: 6.04 | val_ll: -7.7092
[2:17:36.597633] epoch: 4100 | elbo: 6180944.909999999 | train_rmse: 0.356 | val_rmse: 5.7881 | val_ll: -7.4401
[2:19:16.260872] epoch: 4150 | elbo: 6075415.369999999 | train_rmse: 0.3548 | val_rmse: 5.566 | val_ll: -7.158
[2:20:55.986269] epoch: 4200 | elbo: 5971541.720000001 | train_rmse: 0.3366 | val_rmse: 5.3687 | val_ll: -6.987
[2:22:35.809639] epoch: 4250 | elbo: 5866205.105 | train_rmse: 0.3232 | val_rmse: 5.1815 | val_ll: -6.7958
[2:24:16.929813] epoch: 4300 | elbo: 5763328.85 | train_rmse: 0.3197 | val_rmse: 4.9944 | val_ll: -6.5806
[2:25:56.979828] epoch: 4350 | elbo: 5657879.0 | train_rmse: 0.3264 | val_rmse: 4.8145 | val_ll: -6.4442
[2:27:37.338830] epoch: 4400 | elbo: 5554768.645 | train_rmse: 0.2969 | val_rmse: 4.6578 | val_ll: -6.2801
[2:29:16.969697] epoch: 4450 | elbo: 5451499.699999998 | train_rmse: 0.2993 | val_rmse: 4.4989 | val_ll: -6.1676
[2:30:56.550579] epoch: 4500 | elbo: 5350172.345000001 | train_rmse: 0.3023 | val_rmse: 4.3605 | val_ll: -5.9932
[2:32:35.630175] epoch: 4550 | elbo: 5246491.5 | train_rmse: 0.2936 | val_rmse: 4.2331 | val_ll: -5.7807
[2:34:14.913330] epoch: 4600 | elbo: 5145066.75 | train_rmse: 0.2859 | val_rmse: 4.1188 | val_ll: -5.6699
[2:35:55.067025] epoch: 4650 | elbo: 5043360.915000001 | train_rmse: 0.3141 | val_rmse: 4.0151 | val_ll: -5.6163
[2:37:35.914305] epoch: 4700 | elbo: 4943422.385 | train_rmse: 0.281 | val_rmse: 3.9171 | val_ll: -5.4428
[2:39:18.069672] epoch: 4750 | elbo: 4843353.075 | train_rmse: 0.2735 | val_rmse: 3.826 | val_ll: -5.3524
[2:40:59.933191] epoch: 4800 | elbo: 4744225.3149999995 | train_rmse: 0.2758 | val_rmse: 3.7492 | val_ll: -5.2401
[2:42:40.829211] epoch: 4850 | elbo: 4644745.375000001 | train_rmse: 0.2757 | val_rmse: 3.6851 | val_ll: -5.1041
[2:44:22.987886] epoch: 4900 | elbo: 4546718.444999999 | train_rmse: 0.2824 | val_rmse: 3.6156 | val_ll: -5.0921
[2:46:02.910352] epoch: 4950 | elbo: 4450589.494999999 | train_rmse: 0.2751 | val_rmse: 3.5506 | val_ll: -4.9327
[2:47:43.232760] epoch: 5000 | elbo: 4354093.09 | train_rmse: 0.2754 | val_rmse: 3.4944 | val_ll: -4.8868
[2:49:24.311264] epoch: 5050 | elbo: 4258637.3950000005 | train_rmse: 0.2853 | val_rmse: 3.438 | val_ll: -4.7872
[2:51:05.637248] epoch: 5100 | elbo: 4163469.960000001 | train_rmse: 0.2946 | val_rmse: 3.3927 | val_ll: -4.688
[2:52:45.348259] epoch: 5150 | elbo: 4069808.7225 | train_rmse: 0.2899 | val_rmse: 3.3439 | val_ll: -4.6159
[2:54:26.811249] epoch: 5200 | elbo: 3977262.185 | train_rmse: 0.2902 | val_rmse: 3.2969 | val_ll: -4.4942
[2:56:09.105700] epoch: 5250 | elbo: 3885165.9324999996 | train_rmse: 0.2962 | val_rmse: 3.2585 | val_ll: -4.4234
[2:57:50.097150] epoch: 5300 | elbo: 3793575.2675000005 | train_rmse: 0.2951 | val_rmse: 3.2163 | val_ll: -4.3127
[2:59:31.976468] epoch: 5350 | elbo: 3703722.54 | train_rmse: 0.2961 | val_rmse: 3.1741 | val_ll: -4.2407
[3:01:13.387182] epoch: 5400 | elbo: 3613693.9875 | train_rmse: 0.3066 | val_rmse: 3.1362 | val_ll: -4.1958
[3:02:54.148502] epoch: 5450 | elbo: 3525153.7950000004 | train_rmse: 0.3067 | val_rmse: 3.1058 | val_ll: -4.126
[3:04:34.874443] epoch: 5500 | elbo: 3437943.534999999 | train_rmse: 0.3104 | val_rmse: 3.0744 | val_ll: -4.0175
[3:06:16.409103] epoch: 5550 | elbo: 3352743.8899999997 | train_rmse: 0.329 | val_rmse: 3.0495 | val_ll: -3.9536
[3:07:57.543359] epoch: 5600 | elbo: 3267427.1775 | train_rmse: 0.327 | val_rmse: 3.0162 | val_ll: -3.8512
[3:09:39.985394] epoch: 5650 | elbo: 3183259.8425000003 | train_rmse: 0.3339 | val_rmse: 2.9854 | val_ll: -3.7958
[3:11:21.972909] epoch: 5700 | elbo: 3101190.675 | train_rmse: 0.3382 | val_rmse: 2.9589 | val_ll: -3.7225
[3:13:02.102285] epoch: 5750 | elbo: 3018998.4699999997 | train_rmse: 0.3362 | val_rmse: 2.9292 | val_ll: -3.6861
[3:14:43.071040] epoch: 5800 | elbo: 2938153.1125 | train_rmse: 0.3428 | val_rmse: 2.9034 | val_ll: -3.6645
[3:16:24.253232] epoch: 5850 | elbo: 2859101.7049999996 | train_rmse: 0.3433 | val_rmse: 2.875 | val_ll: -3.5623
[3:18:04.582371] epoch: 5900 | elbo: 2781320.505 | train_rmse: 0.3511 | val_rmse: 2.8536 | val_ll: -3.5382
[3:19:44.294461] epoch: 5950 | elbo: 2705067.4225 | train_rmse: 0.3585 | val_rmse: 2.828 | val_ll: -3.4814
[3:21:24.250066] epoch: 6000 | elbo: 2630138.115 | train_rmse: 0.3599 | val_rmse: 2.7996 | val_ll: -3.4536
[3:23:05.207689] epoch: 6050 | elbo: 2554728.61 | train_rmse: 0.3655 | val_rmse: 2.7762 | val_ll: -3.3889
[3:24:47.903387] epoch: 6100 | elbo: 2482427.5949999997 | train_rmse: 0.3671 | val_rmse: 2.7541 | val_ll: -3.3654
[3:26:28.713610] epoch: 6150 | elbo: 2411132.5174999996 | train_rmse: 0.3812 | val_rmse: 2.7335 | val_ll: -3.3638
[3:28:09.802838] epoch: 6200 | elbo: 2341588.1499999994 | train_rmse: 0.3904 | val_rmse: 2.7079 | val_ll: -3.3132
[3:29:49.874554] epoch: 6250 | elbo: 2273117.3975 | train_rmse: 0.4056 | val_rmse: 2.6865 | val_ll: -3.2658
[3:31:32.061599] epoch: 6300 | elbo: 2205582.985 | train_rmse: 0.3973 | val_rmse: 2.6575 | val_ll: -3.2749
[3:33:11.977920] epoch: 6350 | elbo: 2139846.88 | train_rmse: 0.4058 | val_rmse: 2.6258 | val_ll: -3.1567
[3:34:53.041983] epoch: 6400 | elbo: 2075824.7899999998 | train_rmse: 0.4015 | val_rmse: 2.6036 | val_ll: -3.1431
[3:36:34.222101] epoch: 6450 | elbo: 2012820.5475 | train_rmse: 0.4052 | val_rmse: 2.5704 | val_ll: -3.1388
[3:38:15.363049] epoch: 6500 | elbo: 1951383.5212499998 | train_rmse: 0.4157 | val_rmse: 2.5472 | val_ll: -3.0952
[3:39:57.396896] epoch: 6550 | elbo: 1891759.18875 | train_rmse: 0.4228 | val_rmse: 2.5143 | val_ll: -3.0425
[3:41:38.917707] epoch: 6600 | elbo: 1833222.9375 | train_rmse: 0.4211 | val_rmse: 2.4822 | val_ll: -3.008
[3:43:18.652656] epoch: 6650 | elbo: 1776809.6487500002 | train_rmse: 0.4249 | val_rmse: 2.4522 | val_ll: -2.9462
[3:44:58.672193] epoch: 6700 | elbo: 1720636.2025 | train_rmse: 0.4313 | val_rmse: 2.4182 | val_ll: -2.8918
[3:46:38.663718] epoch: 6750 | elbo: 1666869.12875 | train_rmse: 0.4304 | val_rmse: 2.386 | val_ll: -2.9112
[3:48:18.663469] epoch: 6800 | elbo: 1614425.2762499999 | train_rmse: 0.4298 | val_rmse: 2.3551 | val_ll: -2.8445
[3:49:59.029073] epoch: 6850 | elbo: 1563674.35 | train_rmse: 0.4452 | val_rmse: 2.3197 | val_ll: -2.8303
[3:51:38.156558] epoch: 6900 | elbo: 1513516.905 | train_rmse: 0.4314 | val_rmse: 2.2879 | val_ll: -2.8142
[3:53:18.463023] epoch: 6950 | elbo: 1465123.7150000003 | train_rmse: 0.4414 | val_rmse: 2.2508 | val_ll: -2.767
[3:54:58.972520] epoch: 7000 | elbo: 1418495.4425 | train_rmse: 0.4376 | val_rmse: 2.2178 | val_ll: -2.7244
[3:56:40.570651] epoch: 7050 | elbo: 1373158.39375 | train_rmse: 0.4447 | val_rmse: 2.177 | val_ll: -2.6858
[3:58:21.989427] epoch: 7100 | elbo: 1328147.0387500003 | train_rmse: 0.4408 | val_rmse: 2.1326 | val_ll: -2.6112
[4:00:04.291677] epoch: 7150 | elbo: 1285656.6475000002 | train_rmse: 0.4477 | val_rmse: 2.0969 | val_ll: -2.582
[4:01:44.868415] epoch: 7200 | elbo: 1243541.0999999999 | train_rmse: 0.4427 | val_rmse: 2.0572 | val_ll: -2.5454
[4:03:27.376684] epoch: 7250 | elbo: 1204452.025 | train_rmse: 0.4459 | val_rmse: 2.0169 | val_ll: -2.4993
[4:05:08.010573] epoch: 7300 | elbo: 1164222.40625 | train_rmse: 0.4402 | val_rmse: 1.9703 | val_ll: -2.4301
[4:06:49.135134] epoch: 7350 | elbo: 1126762.5037500001 | train_rmse: 0.4552 | val_rmse: 1.9303 | val_ll: -2.4199
[4:08:29.070082] epoch: 7400 | elbo: 1089698.55125 | train_rmse: 0.441 | val_rmse: 1.885 | val_ll: -2.3502
[4:10:08.537687] epoch: 7450 | elbo: 1054978.20875 | train_rmse: 0.4393 | val_rmse: 1.8412 | val_ll: -2.2918
[4:11:48.654563] epoch: 7500 | elbo: 1020528.3087500001 | train_rmse: 0.4403 | val_rmse: 1.8001 | val_ll: -2.2841
[4:13:28.461683] epoch: 7550 | elbo: 987449.4225 | train_rmse: 0.452 | val_rmse: 1.7588 | val_ll: -2.2391
[4:15:08.009851] epoch: 7600 | elbo: 955999.6368750001 | train_rmse: 0.4341 | val_rmse: 1.7147 | val_ll: -2.1948
[4:16:46.706255] epoch: 7650 | elbo: 925273.87375 | train_rmse: 0.4311 | val_rmse: 1.6774 | val_ll: -2.1516
[4:18:26.026299] epoch: 7700 | elbo: 896135.8418749999 | train_rmse: 0.4327 | val_rmse: 1.6371 | val_ll: -2.0965
[4:20:05.418948] epoch: 7750 | elbo: 867632.66625 | train_rmse: 0.4276 | val_rmse: 1.598 | val_ll: -2.0664
[4:21:45.952661] epoch: 7800 | elbo: 841494.0743749999 | train_rmse: 0.4289 | val_rmse: 1.5571 | val_ll: -2.0144
[4:23:26.702111] epoch: 7850 | elbo: 813780.1699999999 | train_rmse: 0.4253 | val_rmse: 1.5219 | val_ll: -1.9578
[4:25:08.563802] epoch: 7900 | elbo: 789151.7393750001 | train_rmse: 0.4231 | val_rmse: 1.4889 | val_ll: -1.932
[4:26:49.519718] epoch: 7950 | elbo: 765646.7906249999 | train_rmse: 0.4252 | val_rmse: 1.4519 | val_ll: -1.9157
[4:28:28.503021] epoch: 8000 | elbo: 741644.6025 | train_rmse: 0.4223 | val_rmse: 1.4199 | val_ll: -1.8939
[4:30:08.027502] epoch: 8050 | elbo: 718787.490625 | train_rmse: 0.4214 | val_rmse: 1.3931 | val_ll: -1.8584
[4:31:47.968027] epoch: 8100 | elbo: 697876.72625 | train_rmse: 0.4185 | val_rmse: 1.3594 | val_ll: -1.7949
[4:33:28.249354] epoch: 8150 | elbo: 676943.090625 | train_rmse: 0.4179 | val_rmse: 1.3274 | val_ll: -1.7465
[4:35:10.805819] epoch: 8200 | elbo: 657833.7418750001 | train_rmse: 0.4135 | val_rmse: 1.2983 | val_ll: -1.7279
[4:36:53.753072] epoch: 8250 | elbo: 637952.824375 | train_rmse: 0.4151 | val_rmse: 1.2719 | val_ll: -1.7352
[4:38:35.761221] epoch: 8300 | elbo: 620242.8400000001 | train_rmse: 0.4133 | val_rmse: 1.2384 | val_ll: -1.7024
[4:40:15.160185] epoch: 8350 | elbo: 602075.5525 | train_rmse: 0.407 | val_rmse: 1.2103 | val_ll: -1.6685
[4:41:56.357540] epoch: 8400 | elbo: 585403.391875 | train_rmse: 0.4058 | val_rmse: 1.1838 | val_ll: -1.6169
[4:43:38.020298] epoch: 8450 | elbo: 568636.393125 | train_rmse: 0.4106 | val_rmse: 1.161 | val_ll: -1.644
[4:45:19.343555] epoch: 8500 | elbo: 553783.146875 | train_rmse: 0.4099 | val_rmse: 1.1426 | val_ll: -1.6302
[4:47:00.500168] epoch: 8550 | elbo: 537692.9887500001 | train_rmse: 0.402 | val_rmse: 1.1127 | val_ll: -1.6038
[4:48:42.135097] epoch: 8600 | elbo: 523180.35031249997 | train_rmse: 0.3986 | val_rmse: 1.0948 | val_ll: -1.5922
[4:50:23.662790] epoch: 8650 | elbo: 509878.929375 | train_rmse: 0.3982 | val_rmse: 1.0748 | val_ll: -1.5839
[4:52:06.099903] epoch: 8700 | elbo: 495848.6759375 | train_rmse: 0.392 | val_rmse: 1.0563 | val_ll: -1.5793
[4:53:46.531621] epoch: 8750 | elbo: 483151.49281250016 | train_rmse: 0.3963 | val_rmse: 1.0438 | val_ll: -1.575
[4:55:27.398006] epoch: 8800 | elbo: 470392.70062500006 | train_rmse: 0.3894 | val_rmse: 1.0255 | val_ll: -1.5716
[4:57:07.617136] epoch: 8850 | elbo: 458373.0434375 | train_rmse: 0.3888 | val_rmse: 1.0077 | val_ll: -1.5542
[4:58:48.531954] epoch: 8900 | elbo: 446494.71281249996 | train_rmse: 0.3868 | val_rmse: 0.9923 | val_ll: -1.5544
[5:00:28.695624] epoch: 8950 | elbo: 435570.3378125001 | train_rmse: 0.3779 | val_rmse: 0.9788 | val_ll: -1.5372
[5:02:09.561209] epoch: 9000 | elbo: 424470.54781250004 | train_rmse: 0.376 | val_rmse: 0.9629 | val_ll: -1.5083
[5:03:51.553720] epoch: 9050 | elbo: 414344.8471874999 | train_rmse: 0.3744 | val_rmse: 0.9549 | val_ll: -1.5191
[5:05:34.646598] epoch: 9100 | elbo: 404777.32937500003 | train_rmse: 0.3714 | val_rmse: 0.9462 | val_ll: -1.5428
[5:07:14.345295] epoch: 9150 | elbo: 394470.3975000001 | train_rmse: 0.3684 | val_rmse: 0.9352 | val_ll: -1.5109
[5:08:59.815054] epoch: 9200 | elbo: 386149.475 | train_rmse: 0.3621 | val_rmse: 0.9246 | val_ll: -1.5036
[5:11:07.120020] epoch: 9250 | elbo: 375879.1449999999 | train_rmse: 0.3576 | val_rmse: 0.9138 | val_ll: -1.5056
[5:12:49.177202] epoch: 9300 | elbo: 367670.7315625 | train_rmse: 0.3585 | val_rmse: 0.9069 | val_ll: -1.5018
[5:14:30.208067] epoch: 9350 | elbo: 359550.34593750007 | train_rmse: 0.3502 | val_rmse: 0.8957 | val_ll: -1.5107
[5:16:08.674982] epoch: 9400 | elbo: 351173.55343749997 | train_rmse: 0.3483 | val_rmse: 0.8886 | val_ll: -1.4828
[5:17:48.432611] epoch: 9450 | elbo: 344710.8346875 | train_rmse: 0.3477 | val_rmse: 0.8816 | val_ll: -1.4999
[5:19:26.566056] epoch: 9500 | elbo: 335956.12500000006 | train_rmse: 0.3437 | val_rmse: 0.8747 | val_ll: -1.5215
[5:21:06.338816] epoch: 9550 | elbo: 328865.5421875 | train_rmse: 0.339 | val_rmse: 0.8654 | val_ll: -1.4989
[5:22:44.650951] epoch: 9600 | elbo: 322038.8234375 | train_rmse: 0.3376 | val_rmse: 0.8598 | val_ll: -1.5148
[5:24:23.339800] epoch: 9650 | elbo: 315484.174375 | train_rmse: 0.3325 | val_rmse: 0.8518 | val_ll: -1.5184
[5:26:01.418915] epoch: 9700 | elbo: 308497.181875 | train_rmse: 0.3298 | val_rmse: 0.8447 | val_ll: -1.4946
[5:27:40.754110] epoch: 9750 | elbo: 303041.04125 | train_rmse: 0.3228 | val_rmse: 0.8378 | val_ll: -1.5236
[5:29:18.358493] epoch: 9800 | elbo: 296831.63531250006 | train_rmse: 0.3226 | val_rmse: 0.8305 | val_ll: -1.5163
[5:30:56.557268] epoch: 9850 | elbo: 291203.41312499996 | train_rmse: 0.32 | val_rmse: 0.8239 | val_ll: -1.5029
[5:32:34.748607] epoch: 9900 | elbo: 285029.92000000004 | train_rmse: 0.3141 | val_rmse: 0.8168 | val_ll: -1.5077
[5:34:14.430546] epoch: 9950 | elbo: 279882.2496875 | train_rmse: 0.3125 | val_rmse: 0.8085 | val_ll: -1.4902
Training finished in 5:35:51.749401 seconds
Saved SVI model to tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-3x1024-s03/checkpoint_1.pt
File Size is 16.11424732208252 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=1024, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=1024, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.3 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-3x1024-s03/checkpoint_1.pt
using device: cuda:4
====== evaluating profile sineasy10-3x1024-s03 - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:02:49.455603
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=1024, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=1024, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([1024, 10]) Parameter containing:
tensor([[-0.2089,  0.1508,  0.1643,  ...,  0.2689,  0.2351, -0.3329],
        [ 0.2061,  0.0734, -0.9134,  ...,  0.2025,  0.1104, -0.0875],
        [-0.0934,  0.5361, -0.2912,  ...,  0.1764,  0.0960, -0.3162],
        ...,
        [ 0.0451,  0.0931,  0.0445,  ..., -0.0960, -0.2719,  0.3048],
        [-0.3908,  0.1906, -0.1231,  ..., -0.1292,  0.0310,  0.1810],
        [-0.0436, -0.3058, -0.1012,  ..., -0.1576,  0.2240, -0.3060]],
       device='cuda:4', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([1024, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([1024]) Parameter containing:
tensor([-0.5753, -0.3101,  0.2168,  ..., -0.2042,  0.0234,  0.0770],
       device='cuda:4', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([1024]) tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([1024, 1024]) Parameter containing:
tensor([[ 0.0352, -0.0762,  0.1614,  ...,  0.1166, -0.3852,  0.3676],
        [-0.1264,  0.2264,  0.1796,  ..., -0.5443, -0.3937,  0.0275],
        [-0.7284,  0.3423,  0.0622,  ...,  0.3707,  0.4784,  0.1133],
        ...,
        [ 0.3014, -0.5653,  0.4463,  ...,  0.2374,  0.3848,  0.1976],
        [ 0.1413, -0.3396, -0.2743,  ..., -0.4204,  0.2095,  0.0190],
        [-0.1254,  0.0109,  0.6786,  ..., -0.0577, -0.0040, -0.2138]],
       device='cuda:4', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([1024, 1024]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([1024]) Parameter containing:
tensor([-0.1085, -0.0696,  0.4176,  ..., -0.3476,  0.6824,  0.4097],
       device='cuda:4', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([1024]) tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([1024, 1024]) Parameter containing:
tensor([[ 0.2941, -0.2311,  0.2470,  ...,  0.1197, -0.5038, -0.3242],
        [ 0.3758,  0.1970, -0.3776,  ..., -0.2569, -0.5728, -0.2872],
        [-0.0497, -0.1804, -0.2681,  ...,  0.4026, -0.1131,  0.2799],
        ...,
        [ 0.2761, -0.1192, -0.4168,  ..., -0.6176,  0.2410,  0.1452],
        [-0.2655, -0.4898, -0.0059,  ..., -0.3315,  0.3965,  0.2428],
        [-0.3038, -0.4560,  0.0440,  ..., -0.1564,  0.1564,  0.0968]],
       device='cuda:4', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([1024, 1024]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([1024]) Parameter containing:
tensor([-0.2846,  0.3795, -0.3625,  ..., -0.3964,  0.1820,  0.3978],
       device='cuda:4', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([1024]) tensor([0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100], device='cuda:4',
       grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 1024]) Parameter containing:
tensor([[-0.4354,  0.1229,  0.3068,  ...,  0.0374,  0.2633,  0.3091]],
       device='cuda:4', requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 1024]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([0.1311], device='cuda:4', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:4', grad_fn=<AddBackward0>)
likelihood_guide.likelihood._scale.loc torch.Size([]) Parameter containing:
tensor(0.2947, device='cuda:4', requires_grad=True)
likelihood_guide.likelihood._scale.scale torch.Size([]) tensor(0.0100, device='cuda:4', grad_fn=<AddBackward0>)
Using device: cuda:4
===== Training profile sineasy10-3x1024-sl - 1 =====
[0:00:02.015213] epoch: 0 | elbo: 598649006.72 | train_rmse: 377.8703 | val_rmse: 382.252 | val_ll: -46.9585
[0:01:52.111881] epoch: 50 | elbo: 26269960.199999996 | train_rmse: 69.4285 | val_rmse: 88.564 | val_ll: -7.3774
[0:03:42.812326] epoch: 100 | elbo: 18163553.04 | train_rmse: 46.7068 | val_rmse: 75.2545 | val_ll: -6.726
[0:05:32.637709] epoch: 150 | elbo: 14856618.62 | train_rmse: 33.5982 | val_rmse: 69.6273 | val_ll: -6.5461
[0:07:21.641949] epoch: 200 | elbo: 13150659.669999998 | train_rmse: 24.8774 | val_rmse: 66.3907 | val_ll: -6.4753
[0:09:10.793336] epoch: 250 | elbo: 12119940.17 | train_rmse: 18.4627 | val_rmse: 64.2117 | val_ll: -6.4919
[0:11:00.710016] epoch: 300 | elbo: 11496783.2 | train_rmse: 13.6937 | val_rmse: 62.6349 | val_ll: -6.5165
[0:12:51.885608] epoch: 350 | elbo: 11012330.640000002 | train_rmse: 10.2104 | val_rmse: 61.1488 | val_ll: -6.5542
[0:14:42.532341] epoch: 400 | elbo: 10671059.580000002 | train_rmse: 7.823 | val_rmse: 59.7983 | val_ll: -6.6197
[0:16:31.968327] epoch: 450 | elbo: 10388384.34 | train_rmse: 6.4016 | val_rmse: 58.6396 | val_ll: -6.708
[0:18:22.405535] epoch: 500 | elbo: 10150742.66 | train_rmse: 5.5188 | val_rmse: 57.1897 | val_ll: -6.77
[0:20:11.722553] epoch: 550 | elbo: 9921063.120000001 | train_rmse: 4.7915 | val_rmse: 55.8558 | val_ll: -6.8015
[0:22:02.465849] epoch: 600 | elbo: 9722952.3 | train_rmse: 4.6438 | val_rmse: 54.3025 | val_ll: -6.8621
[0:23:52.717132] epoch: 650 | elbo: 9541707.0 | train_rmse: 4.5471 | val_rmse: 52.8795 | val_ll: -6.9706
[0:25:43.947886] epoch: 700 | elbo: 9349437.940000001 | train_rmse: 4.1908 | val_rmse: 51.2853 | val_ll: -6.9624
[0:27:34.894562] epoch: 750 | elbo: 9188344.489999998 | train_rmse: 4.0962 | val_rmse: 49.6058 | val_ll: -7.0512
[0:29:22.805892] epoch: 800 | elbo: 9024898.219999999 | train_rmse: 3.9012 | val_rmse: 47.9448 | val_ll: -7.0108
[0:31:11.308882] epoch: 850 | elbo: 8863535.190000001 | train_rmse: 3.6591 | val_rmse: 46.1541 | val_ll: -7.0105
[0:33:01.863550] epoch: 900 | elbo: 8718424.66 | train_rmse: 3.4648 | val_rmse: 44.4918 | val_ll: -6.9301
[0:34:52.085948] epoch: 950 | elbo: 8577521.44 | train_rmse: 4.063 | val_rmse: 42.7843 | val_ll: -6.8959
[0:36:41.902239] epoch: 1000 | elbo: 8434402.094999999 | train_rmse: 3.5697 | val_rmse: 41.0591 | val_ll: -6.8132
[0:38:30.843302] epoch: 1050 | elbo: 8298322.840000001 | train_rmse: 3.2193 | val_rmse: 39.4 | val_ll: -6.692
[0:40:20.937143] epoch: 1100 | elbo: 8163287.664999999 | train_rmse: 3.3578 | val_rmse: 37.5986 | val_ll: -6.5765
[0:42:08.509699] epoch: 1150 | elbo: 8034683.935 | train_rmse: 3.1687 | val_rmse: 35.8964 | val_ll: -6.3856
[0:43:56.535850] epoch: 1200 | elbo: 7902271.37 | train_rmse: 3.0602 | val_rmse: 34.1787 | val_ll: -6.2182
[0:45:45.005062] epoch: 1250 | elbo: 7772403.220000002 | train_rmse: 2.9856 | val_rmse: 32.5007 | val_ll: -6.0734
[0:47:32.741955] epoch: 1300 | elbo: 7647023.3 | train_rmse: 3.365 | val_rmse: 30.7991 | val_ll: -5.8647
[0:49:21.893497] epoch: 1350 | elbo: 7515846.8149999995 | train_rmse: 3.1695 | val_rmse: 29.0902 | val_ll: -5.7141
[0:51:09.535015] epoch: 1400 | elbo: 7386742.845000001 | train_rmse: 2.9931 | val_rmse: 27.3369 | val_ll: -5.5443
[0:52:56.834421] epoch: 1450 | elbo: 7261553.365 | train_rmse: 2.7481 | val_rmse: 25.63 | val_ll: -5.4113
[0:54:44.837584] epoch: 1500 | elbo: 7137736.010000001 | train_rmse: 2.5705 | val_rmse: 23.9036 | val_ll: -5.2131
[0:56:34.716449] epoch: 1550 | elbo: 7014042.109999999 | train_rmse: 2.8501 | val_rmse: 22.182 | val_ll: -5.0438
[0:58:25.195566] epoch: 1600 | elbo: 6893158.3999999985 | train_rmse: 2.4973 | val_rmse: 20.4226 | val_ll: -4.8633
[1:00:15.705762] epoch: 1650 | elbo: 6771058.590000001 | train_rmse: 2.4126 | val_rmse: 18.8224 | val_ll: -4.6995
[1:02:05.575435] epoch: 1700 | elbo: 6652339.870000001 | train_rmse: 2.3218 | val_rmse: 17.0037 | val_ll: -4.5354
[1:03:55.485063] epoch: 1750 | elbo: 6534649.139999999 | train_rmse: 2.3402 | val_rmse: 15.4183 | val_ll: -4.3679
[1:05:45.088830] epoch: 1800 | elbo: 6418103.575 | train_rmse: 2.2331 | val_rmse: 13.798 | val_ll: -4.1948
[1:07:34.468779] epoch: 1850 | elbo: 6303093.46 | train_rmse: 2.16 | val_rmse: 12.3026 | val_ll: -4.0287
[1:09:25.015363] epoch: 1900 | elbo: 6188991.859999999 | train_rmse: 2.1331 | val_rmse: 10.9922 | val_ll: -3.8672
[1:11:13.453745] epoch: 1950 | elbo: 6075894.694999999 | train_rmse: 2.0531 | val_rmse: 9.8059 | val_ll: -3.7268
[1:13:02.849588] epoch: 2000 | elbo: 5963590.745 | train_rmse: 2.0189 | val_rmse: 8.6951 | val_ll: -3.575
[1:14:53.071466] epoch: 2050 | elbo: 5852257.405 | train_rmse: 1.9872 | val_rmse: 7.6627 | val_ll: -3.4265
[1:16:42.852700] epoch: 2100 | elbo: 5741396.225 | train_rmse: 1.9365 | val_rmse: 6.7618 | val_ll: -3.2872
[1:18:32.710315] epoch: 2150 | elbo: 5631131.930000001 | train_rmse: 1.8489 | val_rmse: 5.9513 | val_ll: -3.1511
[1:20:22.461512] epoch: 2200 | elbo: 5521562.720000001 | train_rmse: 1.8065 | val_rmse: 5.2592 | val_ll: -3.0362
[1:22:12.483281] epoch: 2250 | elbo: 5412321.164999999 | train_rmse: 1.7833 | val_rmse: 4.7154 | val_ll: -2.9482
[1:24:01.935491] epoch: 2300 | elbo: 5303267.700000001 | train_rmse: 1.7541 | val_rmse: 4.3083 | val_ll: -2.88
[1:25:50.647677] epoch: 2350 | elbo: 5194380.665 | train_rmse: 1.713 | val_rmse: 3.9879 | val_ll: -2.8289
[1:27:40.659937] epoch: 2400 | elbo: 5085455.15 | train_rmse: 1.6798 | val_rmse: 3.7601 | val_ll: -2.7887
[1:29:30.438151] epoch: 2450 | elbo: 4976472.385 | train_rmse: 1.6592 | val_rmse: 3.5924 | val_ll: -2.7575
[1:31:20.245607] epoch: 2500 | elbo: 4867378.795 | train_rmse: 1.6318 | val_rmse: 3.4493 | val_ll: -2.7255
[1:33:09.669002] epoch: 2550 | elbo: 4758121.045 | train_rmse: 1.6247 | val_rmse: 3.3437 | val_ll: -2.7
[1:34:58.806135] epoch: 2600 | elbo: 4648721.4 | train_rmse: 1.6108 | val_rmse: 3.2547 | val_ll: -2.6764
[1:36:47.717133] epoch: 2650 | elbo: 4539247.630000001 | train_rmse: 1.6065 | val_rmse: 3.1722 | val_ll: -2.6484
[1:38:36.922175] epoch: 2700 | elbo: 4429702.33 | train_rmse: 1.5956 | val_rmse: 3.1198 | val_ll: -2.6254
[1:40:27.919439] epoch: 2750 | elbo: 4320294.03 | train_rmse: 1.5747 | val_rmse: 3.0397 | val_ll: -2.5964
[1:42:19.748594] epoch: 2800 | elbo: 4210988.93 | train_rmse: 1.5541 | val_rmse: 2.9669 | val_ll: -2.5689
[1:44:10.070494] epoch: 2850 | elbo: 4102002.175 | train_rmse: 1.5502 | val_rmse: 2.8977 | val_ll: -2.5405
[1:46:01.640647] epoch: 2900 | elbo: 3993348.0575 | train_rmse: 1.5291 | val_rmse: 2.8281 | val_ll: -2.5127
[1:47:51.935143] epoch: 2950 | elbo: 3885162.2049999996 | train_rmse: 1.5309 | val_rmse: 2.7696 | val_ll: -2.4886
[1:49:41.433704] epoch: 3000 | elbo: 3777522.0699999994 | train_rmse: 1.5129 | val_rmse: 2.7057 | val_ll: -2.4635
[1:51:31.003996] epoch: 3050 | elbo: 3670483.2574999994 | train_rmse: 1.4978 | val_rmse: 2.6403 | val_ll: -2.44
[1:53:21.826809] epoch: 3100 | elbo: 3563991.0575 | train_rmse: 1.4836 | val_rmse: 2.5924 | val_ll: -2.4247
[1:55:12.272297] epoch: 3150 | elbo: 3458145.8649999998 | train_rmse: 1.4793 | val_rmse: 2.5396 | val_ll: -2.4017
[1:57:04.401608] epoch: 3200 | elbo: 3352868.9975 | train_rmse: 1.4806 | val_rmse: 2.4923 | val_ll: -2.3904
[1:58:56.248957] epoch: 3250 | elbo: 3248286.2975000003 | train_rmse: 1.4665 | val_rmse: 2.4467 | val_ll: -2.3808
[2:00:47.086824] epoch: 3300 | elbo: 3144213.7375 | train_rmse: 1.4807 | val_rmse: 2.407 | val_ll: -2.3704
[2:02:37.879388] epoch: 3350 | elbo: 3040758.4825000004 | train_rmse: 1.4601 | val_rmse: 2.3544 | val_ll: -2.3561
[2:04:27.650609] epoch: 3400 | elbo: 2937872.62 | train_rmse: 1.4823 | val_rmse: 2.3235 | val_ll: -2.3476
[2:06:16.824006] epoch: 3450 | elbo: 2835565.5874999994 | train_rmse: 1.4752 | val_rmse: 2.2792 | val_ll: -2.3414
[2:08:04.192485] epoch: 3500 | elbo: 2733932.5524999993 | train_rmse: 1.4909 | val_rmse: 2.2406 | val_ll: -2.3351
[2:09:52.559410] epoch: 3550 | elbo: 2632856.7449999996 | train_rmse: 1.4938 | val_rmse: 2.2034 | val_ll: -2.3264
[2:11:41.121848] epoch: 3600 | elbo: 2532578.7800000003 | train_rmse: 1.4901 | val_rmse: 2.1549 | val_ll: -2.3167
[2:13:29.154616] epoch: 3650 | elbo: 2432902.44 | train_rmse: 1.4936 | val_rmse: 2.1172 | val_ll: -2.3134
[2:15:17.777884] epoch: 3700 | elbo: 2334075.165 | train_rmse: 1.5131 | val_rmse: 2.0934 | val_ll: -2.311
[2:17:06.595379] epoch: 3750 | elbo: 2236008.4525 | train_rmse: 1.5359 | val_rmse: 2.0707 | val_ll: -2.3085
[2:18:57.017314] epoch: 3800 | elbo: 2138911.6925 | train_rmse: 1.5378 | val_rmse: 2.0363 | val_ll: -2.2993
[2:20:48.100145] epoch: 3850 | elbo: 2042704.04375 | train_rmse: 1.5545 | val_rmse: 2.0118 | val_ll: -2.2992
[2:22:39.019378] epoch: 3900 | elbo: 1947583.7137499999 | train_rmse: 1.5615 | val_rmse: 1.9749 | val_ll: -2.2976
[2:24:29.256959] epoch: 3950 | elbo: 1853630.7175 | train_rmse: 1.579 | val_rmse: 1.9672 | val_ll: -2.2945
[2:26:18.473707] epoch: 4000 | elbo: 1760869.3162499997 | train_rmse: 1.598 | val_rmse: 1.944 | val_ll: -2.2932
[2:28:06.955846] epoch: 4050 | elbo: 1669524.04375 | train_rmse: 1.6013 | val_rmse: 1.9236 | val_ll: -2.2923
[2:29:56.615760] epoch: 4100 | elbo: 1579620.2762499996 | train_rmse: 1.6144 | val_rmse: 1.9166 | val_ll: -2.2938
[2:31:47.710552] epoch: 4150 | elbo: 1491386.3074999999 | train_rmse: 1.6522 | val_rmse: 1.9175 | val_ll: -2.2962
[2:33:37.699918] epoch: 4200 | elbo: 1404877.49125 | train_rmse: 1.6627 | val_rmse: 1.9071 | val_ll: -2.2997
[2:35:28.144853] epoch: 4250 | elbo: 1320315.2037499999 | train_rmse: 1.6926 | val_rmse: 1.9153 | val_ll: -2.3053
[2:37:19.240101] epoch: 4300 | elbo: 1237795.4699999997 | train_rmse: 1.7178 | val_rmse: 1.9242 | val_ll: -2.3124
[2:39:09.646441] epoch: 4350 | elbo: 1157529.10625 | train_rmse: 1.7494 | val_rmse: 1.9367 | val_ll: -2.318
[2:41:00.831887] epoch: 4400 | elbo: 1079651.65125 | train_rmse: 1.7684 | val_rmse: 1.9487 | val_ll: -2.3258
[2:42:50.448286] epoch: 4450 | elbo: 1004344.318125 | train_rmse: 1.8081 | val_rmse: 1.9697 | val_ll: -2.3342
[2:44:40.184853] epoch: 4500 | elbo: 931888.555625 | train_rmse: 1.8363 | val_rmse: 1.9856 | val_ll: -2.3444
[2:46:30.298713] epoch: 4550 | elbo: 862276.0487500001 | train_rmse: 1.879 | val_rmse: 2.0171 | val_ll: -2.3554
[2:48:21.989196] epoch: 4600 | elbo: 795777.1993750001 | train_rmse: 1.9104 | val_rmse: 2.04 | val_ll: -2.366
[2:50:13.229590] epoch: 4650 | elbo: 732399.3375 | train_rmse: 1.9435 | val_rmse: 2.0717 | val_ll: -2.3787
[2:52:04.775019] epoch: 4700 | elbo: 672441.6581250001 | train_rmse: 1.9759 | val_rmse: 2.0901 | val_ll: -2.386
[2:53:55.758576] epoch: 4750 | elbo: 615901.77875 | train_rmse: 2.0152 | val_rmse: 2.1215 | val_ll: -2.3964
[2:55:47.129218] epoch: 4800 | elbo: 562629.811875 | train_rmse: 2.0525 | val_rmse: 2.1484 | val_ll: -2.4074
[2:57:37.539452] epoch: 4850 | elbo: 512757.58812499995 | train_rmse: 2.0874 | val_rmse: 2.1799 | val_ll: -2.419
[2:59:27.459910] epoch: 4900 | elbo: 466062.240625 | train_rmse: 2.1198 | val_rmse: 2.2046 | val_ll: -2.4281
[3:01:17.909734] epoch: 4950 | elbo: 422669.38 | train_rmse: 2.1447 | val_rmse: 2.2265 | val_ll: -2.4388
[3:03:07.588104] epoch: 5000 | elbo: 382325.71249999997 | train_rmse: 2.1709 | val_rmse: 2.2509 | val_ll: -2.445
[3:04:55.816302] epoch: 5050 | elbo: 345093.9659375 | train_rmse: 2.2012 | val_rmse: 2.268 | val_ll: -2.4547
[3:06:44.085384] epoch: 5100 | elbo: 310870.68 | train_rmse: 2.2321 | val_rmse: 2.2913 | val_ll: -2.459
[3:08:32.528447] epoch: 5150 | elbo: 279556.88375000004 | train_rmse: 2.2533 | val_rmse: 2.3101 | val_ll: -2.4714
[3:10:20.706526] epoch: 5200 | elbo: 251110.02078124997 | train_rmse: 2.2724 | val_rmse: 2.3309 | val_ll: -2.4786
[3:12:13.424862] epoch: 5250 | elbo: 225409.43828125 | train_rmse: 2.3046 | val_rmse: 2.3565 | val_ll: -2.489
[3:14:03.647285] epoch: 5300 | elbo: 202222.45750000002 | train_rmse: 2.3301 | val_rmse: 2.3762 | val_ll: -2.4976
[3:15:52.839387] epoch: 5350 | elbo: 181538.68312500004 | train_rmse: 2.3513 | val_rmse: 2.4031 | val_ll: -2.5085
[3:17:42.656346] epoch: 5400 | elbo: 163178.13265624997 | train_rmse: 2.3744 | val_rmse: 2.4241 | val_ll: -2.5145
[3:19:32.531127] epoch: 5450 | elbo: 146949.68812500002 | train_rmse: 2.3949 | val_rmse: 2.4435 | val_ll: -2.5248
[3:21:24.523057] epoch: 5500 | elbo: 132741.19265625 | train_rmse: 2.4302 | val_rmse: 2.4792 | val_ll: -2.534
[3:23:15.469657] epoch: 5550 | elbo: 120299.97648437503 | train_rmse: 2.4569 | val_rmse: 2.5125 | val_ll: -2.5421
[3:25:06.229517] epoch: 5600 | elbo: 109469.93976562501 | train_rmse: 2.4863 | val_rmse: 2.5246 | val_ll: -2.5489
[3:26:56.933163] epoch: 5650 | elbo: 100170.19273437501 | train_rmse: 2.5264 | val_rmse: 2.5607 | val_ll: -2.5543
[3:28:46.464333] epoch: 5700 | elbo: 92212.35492187501 | train_rmse: 2.5548 | val_rmse: 2.5866 | val_ll: -2.5647
[3:30:36.112038] epoch: 5750 | elbo: 85246.57804687499 | train_rmse: 2.5978 | val_rmse: 2.6362 | val_ll: -2.5698
[3:32:24.319781] epoch: 5800 | elbo: 79469.218515625 | train_rmse: 2.629 | val_rmse: 2.6674 | val_ll: -2.579
[3:34:13.409806] epoch: 5850 | elbo: 74498.14601562501 | train_rmse: 2.6558 | val_rmse: 2.7013 | val_ll: -2.584
[3:36:01.764229] epoch: 5900 | elbo: 70234.91515625 | train_rmse: 2.698 | val_rmse: 2.7409 | val_ll: -2.5902
[3:37:51.918691] epoch: 5950 | elbo: 66766.56703125002 | train_rmse: 2.731 | val_rmse: 2.7697 | val_ll: -2.5965
[3:39:42.735817] epoch: 6000 | elbo: 63689.25777343749 | train_rmse: 2.7626 | val_rmse: 2.794 | val_ll: -2.6037
[3:41:32.656766] epoch: 6050 | elbo: 61094.57839843751 | train_rmse: 2.7622 | val_rmse: 2.8024 | val_ll: -2.6087
[3:43:21.383362] epoch: 6100 | elbo: 58941.06734375 | train_rmse: 2.7981 | val_rmse: 2.8401 | val_ll: -2.6124
[3:45:10.248171] epoch: 6150 | elbo: 57076.806484374996 | train_rmse: 2.8336 | val_rmse: 2.8629 | val_ll: -2.6199
[3:47:00.518461] epoch: 6200 | elbo: 55554.351679687505 | train_rmse: 2.8568 | val_rmse: 2.8882 | val_ll: -2.6254
[3:48:52.629703] epoch: 6250 | elbo: 54158.1819921875 | train_rmse: 2.8713 | val_rmse: 2.9017 | val_ll: -2.6294
[3:50:43.796764] epoch: 6300 | elbo: 53059.339921875 | train_rmse: 2.8862 | val_rmse: 2.9249 | val_ll: -2.6376
[3:52:34.417425] epoch: 6350 | elbo: 52109.69324218751 | train_rmse: 2.8933 | val_rmse: 2.9414 | val_ll: -2.6359
[3:54:24.082496] epoch: 6400 | elbo: 51217.93554687499 | train_rmse: 2.9293 | val_rmse: 2.9602 | val_ll: -2.6436
[3:56:16.460348] epoch: 6450 | elbo: 50526.2083203125 | train_rmse: 2.9423 | val_rmse: 2.9728 | val_ll: -2.6451
[3:58:06.298399] epoch: 6500 | elbo: 49869.079882812504 | train_rmse: 2.9464 | val_rmse: 2.9868 | val_ll: -2.649
[3:59:57.129150] epoch: 6550 | elbo: 49268.255937500006 | train_rmse: 2.9445 | val_rmse: 2.9839 | val_ll: -2.6503
[4:01:47.420002] epoch: 6600 | elbo: 48864.5769921875 | train_rmse: 2.9683 | val_rmse: 3.017 | val_ll: -2.655
[4:03:38.839597] epoch: 6650 | elbo: 48276.4823828125 | train_rmse: 2.9896 | val_rmse: 3.0279 | val_ll: -2.6622
[4:05:28.372493] epoch: 6700 | elbo: 47939.2269921875 | train_rmse: 2.9917 | val_rmse: 3.0312 | val_ll: -2.6602
[4:07:18.971341] epoch: 6750 | elbo: 47585.044140625 | train_rmse: 3.0081 | val_rmse: 3.0359 | val_ll: -2.6649
[4:09:09.208791] epoch: 6800 | elbo: 47299.957421875006 | train_rmse: 3.0294 | val_rmse: 3.0641 | val_ll: -2.6659
[4:10:58.066315] epoch: 6850 | elbo: 46938.36671875 | train_rmse: 3.0406 | val_rmse: 3.0719 | val_ll: -2.6693
[4:12:48.576920] epoch: 6900 | elbo: 46778.8782421875 | train_rmse: 3.0655 | val_rmse: 3.1045 | val_ll: -2.6732
[4:14:38.907637] epoch: 6950 | elbo: 46425.374296874994 | train_rmse: 3.0537 | val_rmse: 3.0858 | val_ll: -2.6683
[4:16:28.647573] epoch: 7000 | elbo: 46158.69960937499 | train_rmse: 3.054 | val_rmse: 3.0972 | val_ll: -2.6764
[4:18:17.148873] epoch: 7050 | elbo: 45954.75910156251 | train_rmse: 3.0649 | val_rmse: 3.1139 | val_ll: -2.6805
[4:20:07.288962] epoch: 7100 | elbo: 45646.69683593751 | train_rmse: 3.0961 | val_rmse: 3.1272 | val_ll: -2.6819
[4:21:55.503861] epoch: 7150 | elbo: 45536.496914062496 | train_rmse: 3.0835 | val_rmse: 3.1212 | val_ll: -2.6772
[4:23:43.946340] epoch: 7200 | elbo: 45361.9266796875 | train_rmse: 3.088 | val_rmse: 3.1357 | val_ll: -2.6805
[4:25:33.009688] epoch: 7250 | elbo: 45156.562031249996 | train_rmse: 3.0869 | val_rmse: 3.1242 | val_ll: -2.6807
[4:27:22.108942] epoch: 7300 | elbo: 45062.8376171875 | train_rmse: 3.0994 | val_rmse: 3.131 | val_ll: -2.6839
[4:29:11.219557] epoch: 7350 | elbo: 44760.037968749995 | train_rmse: 3.0899 | val_rmse: 3.1313 | val_ll: -2.6806
[4:31:00.481273] epoch: 7400 | elbo: 44567.08453125 | train_rmse: 3.114 | val_rmse: 3.1505 | val_ll: -2.6863
[4:32:48.583541] epoch: 7450 | elbo: 44567.42250000001 | train_rmse: 3.1082 | val_rmse: 3.1413 | val_ll: -2.684
[4:34:36.979744] epoch: 7500 | elbo: 44312.711679687505 | train_rmse: 3.0726 | val_rmse: 3.1167 | val_ll: -2.6831
[4:36:26.044576] epoch: 7550 | elbo: 44202.2619140625 | train_rmse: 3.083 | val_rmse: 3.1291 | val_ll: -2.6815
[4:38:15.419524] epoch: 7600 | elbo: 44030.248984375 | train_rmse: 3.1112 | val_rmse: 3.1501 | val_ll: -2.6821
[4:40:04.195795] epoch: 7650 | elbo: 43866.13367187501 | train_rmse: 3.1215 | val_rmse: 3.1599 | val_ll: -2.6847
[4:41:54.181488] epoch: 7700 | elbo: 43817.503359375005 | train_rmse: 3.0978 | val_rmse: 3.1423 | val_ll: -2.6819
[4:43:44.457203] epoch: 7750 | elbo: 43739.64390625 | train_rmse: 3.1257 | val_rmse: 3.1539 | val_ll: -2.6861
[4:45:33.943600] epoch: 7800 | elbo: 43805.308046875005 | train_rmse: 3.1092 | val_rmse: 3.1575 | val_ll: -2.6803
[4:47:22.649482] epoch: 7850 | elbo: 43350.4005859375 | train_rmse: 3.1145 | val_rmse: 3.1545 | val_ll: -2.6811
[4:49:11.138925] epoch: 7900 | elbo: 43327.843984374995 | train_rmse: 3.1092 | val_rmse: 3.162 | val_ll: -2.6901
[4:50:59.303681] epoch: 7950 | elbo: 43303.5239453125 | train_rmse: 3.0964 | val_rmse: 3.1359 | val_ll: -2.6806
[4:52:47.746676] epoch: 8000 | elbo: 43199.7839453125 | train_rmse: 3.0904 | val_rmse: 3.1292 | val_ll: -2.6785
[4:54:36.455018] epoch: 8050 | elbo: 42959.37628906249 | train_rmse: 3.1412 | val_rmse: 3.176 | val_ll: -2.6841
[4:56:24.117299] epoch: 8100 | elbo: 42861.09484375 | train_rmse: 3.1593 | val_rmse: 3.1905 | val_ll: -2.6851
[4:58:13.265383] epoch: 8150 | elbo: 42734.702578125 | train_rmse: 3.0714 | val_rmse: 3.1179 | val_ll: -2.6721
[5:00:02.043992] epoch: 8200 | elbo: 42735.843007812495 | train_rmse: 3.1102 | val_rmse: 3.1524 | val_ll: -2.6755
[5:01:51.559377] epoch: 8250 | elbo: 42540.759257812504 | train_rmse: 3.0921 | val_rmse: 3.1271 | val_ll: -2.6718
[5:03:39.823400] epoch: 8300 | elbo: 42576.42984375 | train_rmse: 3.122 | val_rmse: 3.159 | val_ll: -2.6782
[5:05:28.481651] epoch: 8350 | elbo: 42424.5126171875 | train_rmse: 3.1131 | val_rmse: 3.1464 | val_ll: -2.6739
[5:07:18.748346] epoch: 8400 | elbo: 42401.796171875 | train_rmse: 3.1236 | val_rmse: 3.1579 | val_ll: -2.6772
[5:09:07.479616] epoch: 8450 | elbo: 42253.9105078125 | train_rmse: 3.0932 | val_rmse: 3.1329 | val_ll: -2.67
[5:10:56.372262] epoch: 8500 | elbo: 42228.048828125 | train_rmse: 3.0703 | val_rmse: 3.1125 | val_ll: -2.6695
[5:12:45.583551] epoch: 8550 | elbo: 42115.7428125 | train_rmse: 3.0985 | val_rmse: 3.1385 | val_ll: -2.667
[5:14:33.653969] epoch: 8600 | elbo: 41918.9774609375 | train_rmse: 3.0567 | val_rmse: 3.1012 | val_ll: -2.6621
[5:16:22.473162] epoch: 8650 | elbo: 42241.560937500006 | train_rmse: 3.0646 | val_rmse: 3.1131 | val_ll: -2.6603
[5:18:11.742675] epoch: 8700 | elbo: 41961.176601562496 | train_rmse: 3.0397 | val_rmse: 3.0862 | val_ll: -2.6613
[5:19:59.542517] epoch: 8750 | elbo: 41952.564296874996 | train_rmse: 3.0722 | val_rmse: 3.1025 | val_ll: -2.6587
[5:21:48.757009] epoch: 8800 | elbo: 41873.8094921875 | train_rmse: 3.0374 | val_rmse: 3.0779 | val_ll: -2.6566
[5:23:37.643901] epoch: 8850 | elbo: 41661.819765625005 | train_rmse: 3.0825 | val_rmse: 3.1167 | val_ll: -2.6579
[5:25:27.571186] epoch: 8900 | elbo: 41779.881992187504 | train_rmse: 3.0503 | val_rmse: 3.0922 | val_ll: -2.6523
[5:27:15.203558] epoch: 8950 | elbo: 41534.5075 | train_rmse: 3.021 | val_rmse: 3.0627 | val_ll: -2.6479
[5:29:04.033766] epoch: 9000 | elbo: 41606.052890625 | train_rmse: 3.0295 | val_rmse: 3.0662 | val_ll: -2.6501
[5:30:54.207541] epoch: 9050 | elbo: 41494.4175390625 | train_rmse: 2.9933 | val_rmse: 3.0348 | val_ll: -2.6448
[5:32:43.235390] epoch: 9100 | elbo: 41421.25015625 | train_rmse: 2.9741 | val_rmse: 3.0199 | val_ll: -2.6373
[5:34:33.941755] epoch: 9150 | elbo: 41452.8821875 | train_rmse: 2.9637 | val_rmse: 3.0061 | val_ll: -2.6378
[5:36:25.016559] epoch: 9200 | elbo: 41940.821054687505 | train_rmse: 2.9651 | val_rmse: 3.0107 | val_ll: -2.6353
[5:38:13.138118] epoch: 9250 | elbo: 41825.6825390625 | train_rmse: 2.9619 | val_rmse: 2.9965 | val_ll: -2.6324
[5:40:01.860419] epoch: 9300 | elbo: 41621.3544140625 | train_rmse: 2.9752 | val_rmse: 3.0293 | val_ll: -2.6331
[5:41:50.804190] epoch: 9350 | elbo: 41348.492109375 | train_rmse: 2.9735 | val_rmse: 3.0126 | val_ll: -2.6298
[5:43:40.647922] epoch: 9400 | elbo: 41232.16921875 | train_rmse: 2.9826 | val_rmse: 3.0231 | val_ll: -2.6317
[5:45:30.546345] epoch: 9450 | elbo: 41290.185312500005 | train_rmse: 2.9478 | val_rmse: 2.9898 | val_ll: -2.6239
[5:47:20.868601] epoch: 9500 | elbo: 41153.84570312501 | train_rmse: 2.946 | val_rmse: 2.9874 | val_ll: -2.6211
[5:49:10.319944] epoch: 9550 | elbo: 41075.9621875 | train_rmse: 2.917 | val_rmse: 2.9585 | val_ll: -2.6155
[5:50:59.659945] epoch: 9600 | elbo: 41038.091289062504 | train_rmse: 2.9006 | val_rmse: 2.944 | val_ll: -2.6139
[5:52:49.781824] epoch: 9650 | elbo: 41078.9159375 | train_rmse: 2.928 | val_rmse: 2.9664 | val_ll: -2.6136
[5:54:40.667285] epoch: 9700 | elbo: 41068.649375 | train_rmse: 2.9294 | val_rmse: 2.9663 | val_ll: -2.6115
[5:56:31.903637] epoch: 9750 | elbo: 41044.701328125004 | train_rmse: 2.9367 | val_rmse: 2.9816 | val_ll: -2.6105
[5:58:23.839082] epoch: 9800 | elbo: 40849.4421875 | train_rmse: 2.841 | val_rmse: 2.8901 | val_ll: -2.5974
[6:00:12.837640] epoch: 9850 | elbo: 40808.4208984375 | train_rmse: 2.8926 | val_rmse: 2.9284 | val_ll: -2.6021
[6:02:02.602571] epoch: 9900 | elbo: 41193.06265625 | train_rmse: 2.8872 | val_rmse: 2.9201 | val_ll: -2.5923
[6:03:55.335099] epoch: 9950 | elbo: 40874.152578124995 | train_rmse: 2.9243 | val_rmse: 2.9612 | val_ll: -2.5998
Training finished in 6:05:43.506921 seconds
Saved SVI model to tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-3x1024-sl/checkpoint_1.pt
File Size is 16.11479091644287 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=1024, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=1024, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-3x1024-sl/checkpoint_1.pt
using device: cuda:4
====== evaluating profile sineasy10-3x1024-sl - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:03:10.705690
End time: 2023-07-12 01:27:07.312372
Total time: 11:47:42.258715
