Start time: 2023-07-12 08:56:45.255625
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=128, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=128, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([128, 10]) Parameter containing:
tensor([[-0.1309,  0.4018, -0.3607,  ...,  0.1962, -0.3473,  0.1844],
        [ 0.2061, -0.2933, -0.1633,  ..., -0.4561,  0.2183,  0.0023],
        [ 0.0935,  0.1614, -0.0209,  ...,  0.5980, -0.4529,  0.2204],
        ...,
        [-0.3368, -0.1740,  0.0505,  ...,  0.1874,  0.0128, -0.3635],
        [ 0.2012,  0.0368,  0.5587,  ...,  0.6524,  0.8998,  0.4101],
        [-0.3913, -0.3969,  0.2764,  ..., -0.1484, -0.5085, -0.0900]],
       device='cuda:4', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([128, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([128]) Parameter containing:
tensor([-0.0198,  0.0933, -0.1901, -0.7636, -0.1344,  0.3460,  0.5122, -0.5639,
         0.3772,  0.0759,  0.0984,  0.0023,  0.1699,  0.1371, -0.0685,  0.4537,
         0.0337, -0.1075,  0.0768, -0.0253,  0.1986, -0.0976, -0.1172, -0.0627,
         0.2212, -0.0236,  0.3256, -0.1504, -0.0014,  0.4249, -0.0445,  0.4863,
        -0.1585, -0.0212, -0.3622, -0.2220,  0.0259,  0.0944,  0.3267,  0.1323,
        -0.0653, -0.2458, -0.0114, -0.2203, -0.2898, -0.2324,  0.1505, -0.4926,
        -0.4202,  0.5794, -0.0762,  0.7232, -0.2638, -0.0462,  0.0553, -0.4309,
         0.2050,  0.0430, -0.0906,  0.1572,  0.0895,  0.0764, -0.3911, -0.3078,
         0.2264, -0.4343, -0.1761, -0.0776, -0.3724,  0.4879,  0.1375, -0.3922,
         0.0324, -0.1047,  0.4235, -0.1076,  0.4466, -0.6138,  0.0179,  0.2153,
        -0.0756,  0.0651,  0.4433, -0.1266, -0.1656,  0.5768,  0.1025, -0.2412,
         0.1629,  0.3808, -0.4179, -0.0818,  0.5907, -0.4133,  0.0648,  0.1782,
        -0.3875, -0.1680,  0.1454,  0.4574,  0.1332,  0.2627, -0.1947, -0.0283,
        -0.0261,  0.1323, -0.0537, -0.5318, -0.5824, -0.3788, -0.0632, -0.5245,
         0.0911, -0.2628,  0.1615,  0.4207, -0.1517,  0.3060, -0.1709,  0.3502,
        -0.2526, -0.0024, -0.2281,  0.0316, -0.1790, -0.5513,  0.1696, -0.2829],
       device='cuda:4', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([128]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100], device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([128, 128]) Parameter containing:
tensor([[-0.0931,  0.4273, -0.8398,  ...,  0.1706, -0.3268,  0.3337],
        [ 0.5176, -0.0608,  0.6673,  ..., -0.0609,  0.6313, -0.1143],
        [-0.4583, -0.2869,  0.1796,  ..., -0.3907, -0.2166, -0.7272],
        ...,
        [ 0.3602, -0.2946, -0.4929,  ...,  0.0343, -0.3358,  0.3108],
        [-0.2519, -0.1807, -0.1557,  ..., -0.4017, -0.2391, -0.3613],
        [ 0.7190, -0.2449,  0.0324,  ...,  0.3052,  0.2361, -0.2555]],
       device='cuda:4', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([128, 128]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([128]) Parameter containing:
tensor([ 5.9379e-01,  5.3620e-05,  3.1918e-01, -1.8916e-01,  1.0524e-01,
        -1.9184e-01, -1.8251e-01, -3.4578e-01,  2.9920e-01, -2.6694e-01,
        -5.7438e-01,  9.2058e-02, -7.1625e-01, -3.6890e-01,  1.8029e-02,
        -2.8555e-01, -1.4914e-01,  8.8998e-02,  4.8252e-01,  1.4993e-01,
        -9.8130e-02,  2.1040e-01,  4.0796e-01,  1.6242e-01, -5.5886e-01,
        -3.6425e-01, -2.3758e-01,  5.2027e-02,  7.7464e-02,  7.2415e-02,
         5.0746e-03, -3.7605e-01,  4.1605e-01, -8.6058e-02, -6.0411e-02,
         5.9128e-02, -1.1314e-01, -6.3535e-02,  5.4787e-02, -3.3028e-01,
        -5.1648e-02, -2.5057e-01, -5.7988e-02, -1.4263e-02, -8.6035e-03,
        -1.4232e-01, -1.7480e-01,  2.9354e-01,  1.0869e-01, -2.3783e-01,
        -3.0574e-02, -1.0280e-01,  1.1478e-01,  3.6353e-01, -1.2331e-01,
        -3.9820e-01,  3.3551e-02, -7.3607e-01, -4.9884e-01, -3.3232e-01,
         1.1341e-01,  1.5227e-01, -4.4421e-01, -4.0265e-01,  3.5325e-02,
        -2.6552e-01,  3.8467e-01,  1.7164e-01,  3.7115e-02, -1.9839e-02,
        -2.5416e-01, -1.7189e-01,  2.8652e-01,  1.3294e-01, -1.1461e-01,
         8.6740e-02, -6.9263e-02, -2.4340e-01,  2.2491e-01, -2.9264e-01,
        -3.9643e-01,  6.0608e-01,  6.2048e-02, -1.1807e-01,  2.8155e-01,
         1.3274e-02,  1.1058e-01,  7.7843e-02, -3.0978e-01, -6.1830e-02,
        -2.8113e-01, -1.4930e-01, -1.2294e-01, -2.8596e-01,  2.3661e-01,
        -3.2340e-02,  1.7060e-01, -2.6542e-02,  1.4671e-01,  4.9122e-01,
         2.6251e-02,  2.4226e-01, -2.0907e-01, -7.7467e-02, -1.6144e-01,
         5.8255e-01, -1.7431e-01, -2.5961e-02,  2.2837e-02,  9.2036e-02,
        -6.6938e-01,  3.2239e-01, -1.8250e-01, -5.8053e-02,  1.5420e-02,
        -2.9638e-01, -2.7146e-01, -6.5556e-02, -3.9610e-01,  3.4164e-01,
         2.7464e-02, -1.0415e-01,  1.8931e-01,  2.5800e-01,  2.5234e-01,
         4.4143e-02, -2.7148e-01,  5.8566e-01], device='cuda:4',
       requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([128]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100], device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([128, 128]) Parameter containing:
tensor([[ 3.4104e-04,  2.7854e-01,  2.1582e-01,  ...,  7.1189e-01,
          2.9042e-01,  2.6209e-01],
        [ 3.0380e-01, -1.5955e-01, -1.6758e-01,  ...,  1.7021e-02,
         -2.0398e-01, -3.6530e-02],
        [ 1.7904e-01,  2.7236e-01,  2.4810e-01,  ...,  3.5063e-01,
          1.1256e-01,  2.1731e-01],
        ...,
        [ 3.2942e-02,  1.2594e-01, -5.2404e-02,  ...,  3.2112e-01,
         -1.2583e-01, -8.1995e-02],
        [ 4.9774e-02, -6.7172e-01,  1.3161e-01,  ...,  7.8815e-02,
          2.2424e-01, -1.4656e-02],
        [-7.7829e-03, -1.8218e-01,  1.3137e-01,  ...,  1.3204e-01,
         -5.7637e-01,  4.3810e-01]], device='cuda:4', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([128, 128]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([128]) Parameter containing:
tensor([-0.1039,  0.0743, -0.3046, -0.2308,  0.4249, -0.2538,  0.0537,  0.1139,
        -0.0204,  0.2472,  0.1587,  0.2443,  0.6702,  0.0224,  0.0053, -0.2819,
        -0.3468,  0.3111, -0.0165,  0.1161,  0.2422,  0.0387,  0.1074,  0.0171,
        -0.1102, -0.3777, -0.0725,  0.1678,  0.0892,  0.0986, -0.5596,  0.4830,
         0.2654, -0.0416, -0.2034,  0.1616, -0.0022, -0.0806, -0.3816,  0.1223,
         0.1606, -0.5083,  0.4225, -0.1768,  0.0541,  0.4937,  0.3737, -0.5722,
        -0.1869, -0.0662, -0.1587,  0.6752, -0.4164, -0.1648,  0.0367, -0.0252,
        -0.1296,  0.1255,  0.8089,  0.2623,  0.4414, -0.1754, -0.0084,  0.0529,
         0.1381,  0.3417, -0.1472, -0.3089, -0.5445,  0.1043, -0.2629,  0.2722,
         0.0994, -0.2730,  0.6542,  0.5115, -0.1426, -0.1372,  0.1372,  0.3702,
        -0.6000, -0.4369, -0.3358, -0.0011,  0.1969,  0.5428,  0.1042, -0.1661,
        -0.5480, -0.2994,  0.3592,  0.1472,  0.2282, -0.6618, -0.4377,  0.3151,
         0.1395,  0.0913, -0.2002, -0.4619, -0.0915, -0.0503, -0.2340,  0.4743,
         0.2731,  0.1609, -0.2690,  0.1027, -0.1807,  0.6365, -0.5104, -0.4988,
        -0.0909, -0.3365, -0.3300, -0.1560,  0.0257, -0.4020,  0.5514,  0.3689,
         0.1959,  0.1504, -0.2519, -0.2699,  0.4027, -0.0471,  0.3302, -0.0618],
       device='cuda:4', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([128]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100], device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 128]) Parameter containing:
tensor([[ 3.1297e-01, -8.1658e-02, -1.5861e-01,  2.1111e-02,  2.4617e-01,
         -3.9167e-01, -2.7794e-01, -4.5505e-03,  6.3153e-01, -1.3052e-01,
         -2.8572e-01, -7.4014e-02, -1.2065e-01, -1.1132e-01,  3.1352e-01,
          1.9341e-01,  8.9471e-02,  5.6878e-02, -7.2571e-01,  2.8012e-01,
          5.2716e-01, -1.2700e-01, -2.7374e-01, -2.0516e-01, -2.8213e-01,
          2.6137e-01, -4.0541e-01, -4.8099e-01,  9.5972e-02,  5.6497e-02,
         -1.9056e-01, -1.1366e-01,  4.9381e-01, -5.9334e-02,  1.6921e-01,
         -3.3347e-01,  3.3271e-01, -1.3951e-01, -1.9128e-01,  3.4637e-01,
         -2.8658e-01,  1.9662e-01, -1.8521e-01, -3.9635e-04, -3.9969e-01,
         -1.9194e-01,  7.8340e-02,  5.3219e-02,  1.4025e-01, -4.3802e-01,
         -2.9073e-02, -3.0945e-01, -2.4406e-01, -1.7518e-01,  4.5779e-01,
          2.7115e-02,  5.3700e-01, -2.5942e-01,  4.7281e-02, -7.1550e-02,
         -2.9438e-01, -5.2998e-01,  2.3515e-01, -5.1073e-01,  3.7216e-01,
          3.0867e-01,  6.5061e-01,  8.2319e-02, -3.3640e-01, -2.4827e-01,
          1.9343e-01, -1.4489e-01,  1.0726e-01,  4.0789e-01,  2.3521e-01,
         -6.2704e-03, -5.3330e-01,  6.3388e-02, -3.7467e-01, -2.2680e-01,
          5.2892e-01, -2.6903e-01, -3.9157e-01,  2.7412e-01,  6.8295e-02,
          1.7405e-01,  6.4513e-01, -2.3529e-01,  5.1924e-01,  8.2867e-01,
          2.5939e-01,  1.3027e-01,  3.1868e-01,  4.0735e-01,  2.2059e-01,
          1.3463e-01, -1.2654e-02, -2.4451e-01, -3.4255e-01,  2.0842e-02,
          2.3806e-01,  6.7997e-01, -3.3314e-01,  5.1806e-01, -5.7118e-01,
         -2.2799e-01,  4.3545e-01, -2.6532e-01, -2.0608e-01,  2.1541e-01,
         -5.1399e-01, -3.3783e-02,  9.0008e-02,  3.8722e-02, -4.0285e-01,
          6.1259e-01,  2.5920e-02, -3.9047e-01,  3.4246e-01,  2.1298e-01,
          1.5561e-01, -4.0622e-01, -4.6946e-01,  4.5474e-01, -1.2647e-02,
          4.6974e-01, -1.1638e-01,  1.3040e-02]], device='cuda:4',
       requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 128]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100]], device='cuda:4', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([0.2879], device='cuda:4', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:4', grad_fn=<AddBackward0>)
Using device: cuda:4
===== Training profile sineasy10-3x128-s05 - 1 =====
[0:00:01.853041] epoch: 0 | elbo: 4409706.097499999 | train_rmse: 13.7241 | val_rmse: 13.7463 | val_ll: -29.7538
[0:01:37.433458] epoch: 50 | elbo: 550811.80875 | train_rmse: 4.2067 | val_rmse: 4.4974 | val_ll: -5.3825
[0:03:14.076714] epoch: 100 | elbo: 370513.13468749996 | train_rmse: 3.0694 | val_rmse: 3.4547 | val_ll: -4.2481
[0:04:51.140309] epoch: 150 | elbo: 298428.4715625 | train_rmse: 2.4891 | val_rmse: 2.9358 | val_ll: -3.6774
[0:06:28.378492] epoch: 200 | elbo: 258412.73203125 | train_rmse: 2.1166 | val_rmse: 2.6155 | val_ll: -3.374
[0:08:04.095579] epoch: 250 | elbo: 234188.48796874998 | train_rmse: 1.8634 | val_rmse: 2.3952 | val_ll: -3.1927
[0:09:39.909356] epoch: 300 | elbo: 217466.07875000002 | train_rmse: 1.6603 | val_rmse: 2.2354 | val_ll: -3.03
[0:11:17.196409] epoch: 350 | elbo: 204481.7775 | train_rmse: 1.4923 | val_rmse: 2.1053 | val_ll: -2.9215
[0:12:53.366090] epoch: 400 | elbo: 194345.78984375 | train_rmse: 1.3599 | val_rmse: 1.9974 | val_ll: -2.8381
[0:14:29.291985] epoch: 450 | elbo: 185833.04281250003 | train_rmse: 1.248 | val_rmse: 1.9034 | val_ll: -2.7209
[0:16:05.213776] epoch: 500 | elbo: 179029.27437499998 | train_rmse: 1.1503 | val_rmse: 1.8199 | val_ll: -2.647
[0:17:43.177204] epoch: 550 | elbo: 173048.96328125003 | train_rmse: 1.0638 | val_rmse: 1.7489 | val_ll: -2.5573
[0:19:21.246371] epoch: 600 | elbo: 168057.05859375 | train_rmse: 0.9917 | val_rmse: 1.6887 | val_ll: -2.4983
[0:20:58.289327] epoch: 650 | elbo: 163472.42234375 | train_rmse: 0.9213 | val_rmse: 1.6289 | val_ll: -2.4534
[0:22:35.044451] epoch: 700 | elbo: 159711.51499999998 | train_rmse: 0.8661 | val_rmse: 1.5778 | val_ll: -2.3915
[0:24:11.121056] epoch: 750 | elbo: 156088.96921874996 | train_rmse: 0.8094 | val_rmse: 1.5285 | val_ll: -2.2962
[0:25:46.712865] epoch: 800 | elbo: 152951.405 | train_rmse: 0.7678 | val_rmse: 1.4847 | val_ll: -2.2364
[0:27:22.689345] epoch: 850 | elbo: 150107.45609375002 | train_rmse: 0.7204 | val_rmse: 1.4395 | val_ll: -2.1603
[0:28:58.526696] epoch: 900 | elbo: 147652.174375 | train_rmse: 0.6843 | val_rmse: 1.4027 | val_ll: -2.1065
[0:30:35.517582] epoch: 950 | elbo: 145155.22999999998 | train_rmse: 0.6524 | val_rmse: 1.3695 | val_ll: -2.0459
[0:32:12.587821] epoch: 1000 | elbo: 142882.56859375 | train_rmse: 0.6242 | val_rmse: 1.3331 | val_ll: -1.9884
[0:33:50.582309] epoch: 1050 | elbo: 140797.29843749997 | train_rmse: 0.5975 | val_rmse: 1.2996 | val_ll: -1.9193
[0:35:27.477371] epoch: 1100 | elbo: 138737.51796874998 | train_rmse: 0.5788 | val_rmse: 1.2737 | val_ll: -1.8635
[0:37:04.541485] epoch: 1150 | elbo: 136943.74453125003 | train_rmse: 0.5574 | val_rmse: 1.2438 | val_ll: -1.8269
[0:38:41.286676] epoch: 1200 | elbo: 135040.29828125 | train_rmse: 0.5406 | val_rmse: 1.2173 | val_ll: -1.7838
[0:40:18.066006] epoch: 1250 | elbo: 133244.0725 | train_rmse: 0.5249 | val_rmse: 1.1895 | val_ll: -1.7377
[0:41:54.075484] epoch: 1300 | elbo: 131625.874609375 | train_rmse: 0.5182 | val_rmse: 1.1669 | val_ll: -1.6807
[0:43:30.544242] epoch: 1350 | elbo: 129988.19953125002 | train_rmse: 0.5041 | val_rmse: 1.1448 | val_ll: -1.6429
[0:45:06.434624] epoch: 1400 | elbo: 128279.41742187498 | train_rmse: 0.4981 | val_rmse: 1.1209 | val_ll: -1.6178
[0:46:42.599464] epoch: 1450 | elbo: 126760.87382812498 | train_rmse: 0.491 | val_rmse: 1.1024 | val_ll: -1.586
[0:48:19.115452] epoch: 1500 | elbo: 125165.632734375 | train_rmse: 0.4849 | val_rmse: 1.0817 | val_ll: -1.5397
[0:49:55.111406] epoch: 1550 | elbo: 123669.12648437498 | train_rmse: 0.4791 | val_rmse: 1.0634 | val_ll: -1.5124
[0:51:29.853677] epoch: 1600 | elbo: 122068.49773437502 | train_rmse: 0.4779 | val_rmse: 1.0408 | val_ll: -1.4768
[0:53:05.620979] epoch: 1650 | elbo: 120619.378984375 | train_rmse: 0.4733 | val_rmse: 1.0239 | val_ll: -1.452
[0:54:42.129245] epoch: 1700 | elbo: 119143.72140625 | train_rmse: 0.4711 | val_rmse: 1.0095 | val_ll: -1.4281
[0:56:18.857194] epoch: 1750 | elbo: 117621.545703125 | train_rmse: 0.4717 | val_rmse: 0.9948 | val_ll: -1.4077
[0:57:55.617812] epoch: 1800 | elbo: 116161.929609375 | train_rmse: 0.4711 | val_rmse: 0.9799 | val_ll: -1.3873
[0:59:32.526182] epoch: 1850 | elbo: 114746.1121875 | train_rmse: 0.4734 | val_rmse: 0.9664 | val_ll: -1.3697
[1:01:06.956249] epoch: 1900 | elbo: 113170.91968749999 | train_rmse: 0.4731 | val_rmse: 0.952 | val_ll: -1.3551
[1:02:42.518436] epoch: 1950 | elbo: 111815.21070312502 | train_rmse: 0.4734 | val_rmse: 0.9389 | val_ll: -1.3292
[1:04:19.717277] epoch: 2000 | elbo: 110360.11085937501 | train_rmse: 0.4741 | val_rmse: 0.9293 | val_ll: -1.3255
[1:05:56.706549] epoch: 2050 | elbo: 108766.06515624998 | train_rmse: 0.4756 | val_rmse: 0.9159 | val_ll: -1.3042
[1:07:33.923267] epoch: 2100 | elbo: 107411.93000000001 | train_rmse: 0.4772 | val_rmse: 0.9055 | val_ll: -1.29
[1:09:11.085909] epoch: 2150 | elbo: 105939.80765625 | train_rmse: 0.4812 | val_rmse: 0.8952 | val_ll: -1.2859
[1:10:50.016549] epoch: 2200 | elbo: 104495.969921875 | train_rmse: 0.4867 | val_rmse: 0.885 | val_ll: -1.2654
[1:12:27.813707] epoch: 2250 | elbo: 102976.557109375 | train_rmse: 0.487 | val_rmse: 0.8751 | val_ll: -1.2461
[1:14:05.044310] epoch: 2300 | elbo: 101637.155546875 | train_rmse: 0.4914 | val_rmse: 0.8664 | val_ll: -1.2389
[1:15:41.194957] epoch: 2350 | elbo: 100181.4490625 | train_rmse: 0.4937 | val_rmse: 0.8577 | val_ll: -1.231
[1:17:17.621663] epoch: 2400 | elbo: 98757.93796874999 | train_rmse: 0.4983 | val_rmse: 0.8467 | val_ll: -1.2177
[1:18:54.052667] epoch: 2450 | elbo: 97262.6 | train_rmse: 0.5026 | val_rmse: 0.8411 | val_ll: -1.2116
[1:20:30.455109] epoch: 2500 | elbo: 95882.46273437499 | train_rmse: 0.5074 | val_rmse: 0.8321 | val_ll: -1.2064
[1:22:06.234680] epoch: 2550 | elbo: 94488.343984375 | train_rmse: 0.5098 | val_rmse: 0.8248 | val_ll: -1.184
[1:23:42.001102] epoch: 2600 | elbo: 92927.61859375 | train_rmse: 0.5131 | val_rmse: 0.8167 | val_ll: -1.1775
[1:25:18.599723] epoch: 2650 | elbo: 91496.60328124999 | train_rmse: 0.516 | val_rmse: 0.8098 | val_ll: -1.1746
[1:26:54.086095] epoch: 2700 | elbo: 90109.81109375 | train_rmse: 0.5223 | val_rmse: 0.8031 | val_ll: -1.1654
[1:28:30.615506] epoch: 2750 | elbo: 88623.57921875 | train_rmse: 0.526 | val_rmse: 0.7979 | val_ll: -1.1635
[1:30:06.931726] epoch: 2800 | elbo: 87131.105234375 | train_rmse: 0.5292 | val_rmse: 0.7906 | val_ll: -1.1488
[1:31:42.431929] epoch: 2850 | elbo: 85633.806640625 | train_rmse: 0.5303 | val_rmse: 0.7839 | val_ll: -1.1453
[1:33:19.227898] epoch: 2900 | elbo: 84146.889296875 | train_rmse: 0.5367 | val_rmse: 0.7778 | val_ll: -1.1409
[1:34:54.567687] epoch: 2950 | elbo: 82575.39773437499 | train_rmse: 0.5363 | val_rmse: 0.7726 | val_ll: -1.1349
[1:36:30.503902] epoch: 3000 | elbo: 81151.87109375 | train_rmse: 0.5399 | val_rmse: 0.7657 | val_ll: -1.1282
[1:38:08.729016] epoch: 3050 | elbo: 79666.9715625 | train_rmse: 0.5431 | val_rmse: 0.7615 | val_ll: -1.1262
[1:39:44.449025] epoch: 3100 | elbo: 78140.83875000001 | train_rmse: 0.5451 | val_rmse: 0.7555 | val_ll: -1.1194
[1:41:19.924977] epoch: 3150 | elbo: 76589.81187499998 | train_rmse: 0.547 | val_rmse: 0.7518 | val_ll: -1.1123
[1:42:57.617462] epoch: 3200 | elbo: 75078.84671874999 | train_rmse: 0.5484 | val_rmse: 0.7468 | val_ll: -1.1141
[1:44:33.892556] epoch: 3250 | elbo: 73645.00578125 | train_rmse: 0.5493 | val_rmse: 0.7429 | val_ll: -1.1057
[1:46:11.625971] epoch: 3300 | elbo: 72061.94078125 | train_rmse: 0.5516 | val_rmse: 0.7399 | val_ll: -1.1038
[1:47:49.117353] epoch: 3350 | elbo: 70640.2003125 | train_rmse: 0.553 | val_rmse: 0.7358 | val_ll: -1.1046
[1:49:27.422879] epoch: 3400 | elbo: 69130.32210937499 | train_rmse: 0.5534 | val_rmse: 0.7336 | val_ll: -1.1032
[1:51:05.483487] epoch: 3450 | elbo: 67597.333125 | train_rmse: 0.5548 | val_rmse: 0.7288 | val_ll: -1.0936
[1:52:42.229533] epoch: 3500 | elbo: 66021.90078125 | train_rmse: 0.5548 | val_rmse: 0.7257 | val_ll: -1.093
[1:54:18.560635] epoch: 3550 | elbo: 64514.441796875 | train_rmse: 0.5554 | val_rmse: 0.7235 | val_ll: -1.0916
[1:55:56.295668] epoch: 3600 | elbo: 62954.34542968749 | train_rmse: 0.5542 | val_rmse: 0.7192 | val_ll: -1.0849
[1:57:32.983730] epoch: 3650 | elbo: 61468.88824218749 | train_rmse: 0.5545 | val_rmse: 0.715 | val_ll: -1.0826
[1:59:11.158419] epoch: 3700 | elbo: 59922.115703125 | train_rmse: 0.5546 | val_rmse: 0.7126 | val_ll: -1.0798
[2:00:47.702707] epoch: 3750 | elbo: 58462.07796875001 | train_rmse: 0.5546 | val_rmse: 0.7111 | val_ll: -1.0805
[2:02:25.412862] epoch: 3800 | elbo: 56844.653046875006 | train_rmse: 0.5534 | val_rmse: 0.709 | val_ll: -1.0763
[2:04:00.614343] epoch: 3850 | elbo: 55435.636640624994 | train_rmse: 0.5525 | val_rmse: 0.7065 | val_ll: -1.0769
[2:05:38.361238] epoch: 3900 | elbo: 53966.17371093751 | train_rmse: 0.5527 | val_rmse: 0.705 | val_ll: -1.0752
[2:07:16.601954] epoch: 3950 | elbo: 52516.14031250001 | train_rmse: 0.5527 | val_rmse: 0.7035 | val_ll: -1.0788
[2:08:54.020123] epoch: 4000 | elbo: 51065.99250000001 | train_rmse: 0.5513 | val_rmse: 0.7028 | val_ll: -1.0738
[2:10:30.632562] epoch: 4050 | elbo: 49626.325625 | train_rmse: 0.5518 | val_rmse: 0.7008 | val_ll: -1.0663
[2:12:07.737705] epoch: 4100 | elbo: 48223.65386718751 | train_rmse: 0.5491 | val_rmse: 0.6982 | val_ll: -1.0677
[2:13:45.395236] epoch: 4150 | elbo: 46824.143828124994 | train_rmse: 0.5492 | val_rmse: 0.6977 | val_ll: -1.0689
[2:15:22.719853] epoch: 4200 | elbo: 45485.5088671875 | train_rmse: 0.5483 | val_rmse: 0.6958 | val_ll: -1.0706
[2:16:58.572740] epoch: 4250 | elbo: 44192.2702734375 | train_rmse: 0.5472 | val_rmse: 0.6945 | val_ll: -1.0655
[2:18:35.830660] epoch: 4300 | elbo: 42915.074375000004 | train_rmse: 0.5461 | val_rmse: 0.6913 | val_ll: -1.0589
[2:20:12.740594] epoch: 4350 | elbo: 41683.773828125 | train_rmse: 0.545 | val_rmse: 0.6899 | val_ll: -1.0568
[2:21:49.097555] epoch: 4400 | elbo: 40490.2904296875 | train_rmse: 0.5434 | val_rmse: 0.6898 | val_ll: -1.0573
[2:23:24.815757] epoch: 4450 | elbo: 39372.812343749996 | train_rmse: 0.5421 | val_rmse: 0.687 | val_ll: -1.0542
[2:25:00.895862] epoch: 4500 | elbo: 38280.3148828125 | train_rmse: 0.5408 | val_rmse: 0.6858 | val_ll: -1.0586
[2:26:38.649393] epoch: 4550 | elbo: 37192.4334375 | train_rmse: 0.5397 | val_rmse: 0.6841 | val_ll: -1.0542
[2:28:16.127881] epoch: 4600 | elbo: 36227.675664062495 | train_rmse: 0.5374 | val_rmse: 0.6823 | val_ll: -1.0516
[2:29:53.088751] epoch: 4650 | elbo: 35262.241953125005 | train_rmse: 0.5355 | val_rmse: 0.6813 | val_ll: -1.052
[2:31:28.393624] epoch: 4700 | elbo: 34325.3098046875 | train_rmse: 0.5351 | val_rmse: 0.6793 | val_ll: -1.052
[2:33:06.209557] epoch: 4750 | elbo: 33501.24828125 | train_rmse: 0.5331 | val_rmse: 0.6761 | val_ll: -1.0471
[2:34:42.562475] epoch: 4800 | elbo: 32728.7517578125 | train_rmse: 0.5317 | val_rmse: 0.6756 | val_ll: -1.0482
[2:36:19.664532] epoch: 4850 | elbo: 31979.70677734375 | train_rmse: 0.5314 | val_rmse: 0.6742 | val_ll: -1.0497
[2:37:56.072217] epoch: 4900 | elbo: 31266.548496093747 | train_rmse: 0.5293 | val_rmse: 0.6722 | val_ll: -1.0459
[2:39:33.481333] epoch: 4950 | elbo: 30619.322812500002 | train_rmse: 0.5295 | val_rmse: 0.6707 | val_ll: -1.0481
[2:41:10.290434] epoch: 5000 | elbo: 29986.768886718753 | train_rmse: 0.5277 | val_rmse: 0.6693 | val_ll: -1.0385
[2:42:46.029390] epoch: 5050 | elbo: 29435.769531249993 | train_rmse: 0.527 | val_rmse: 0.6673 | val_ll: -1.0386
[2:44:21.824923] epoch: 5100 | elbo: 28866.664882812496 | train_rmse: 0.5256 | val_rmse: 0.6672 | val_ll: -1.0424
[2:45:57.654004] epoch: 5150 | elbo: 28334.025449218752 | train_rmse: 0.5256 | val_rmse: 0.6667 | val_ll: -1.0443
[2:47:33.309789] epoch: 5200 | elbo: 27930.500566406252 | train_rmse: 0.5238 | val_rmse: 0.6641 | val_ll: -1.0427
[2:49:08.897388] epoch: 5250 | elbo: 27409.327441406243 | train_rmse: 0.5238 | val_rmse: 0.6627 | val_ll: -1.0403
[2:50:45.077475] epoch: 5300 | elbo: 27011.94138671875 | train_rmse: 0.5235 | val_rmse: 0.6628 | val_ll: -1.0382
[2:52:20.840425] epoch: 5350 | elbo: 26627.056796875004 | train_rmse: 0.5211 | val_rmse: 0.6615 | val_ll: -1.0385
[2:53:57.254642] epoch: 5400 | elbo: 26225.128339843748 | train_rmse: 0.5212 | val_rmse: 0.6595 | val_ll: -1.0349
[2:55:33.159688] epoch: 5450 | elbo: 25899.328867187498 | train_rmse: 0.5201 | val_rmse: 0.6594 | val_ll: -1.034
[2:57:09.103861] epoch: 5500 | elbo: 25617.80744140625 | train_rmse: 0.5191 | val_rmse: 0.6578 | val_ll: -1.0327
[2:58:45.082449] epoch: 5550 | elbo: 25336.423593749998 | train_rmse: 0.5186 | val_rmse: 0.6587 | val_ll: -1.0349
[3:00:21.105619] epoch: 5600 | elbo: 25060.56662109375 | train_rmse: 0.5178 | val_rmse: 0.6583 | val_ll: -1.037
[3:01:57.401838] epoch: 5650 | elbo: 24779.587578125 | train_rmse: 0.5176 | val_rmse: 0.6561 | val_ll: -1.0309
[3:03:33.661621] epoch: 5700 | elbo: 24541.27705078125 | train_rmse: 0.517 | val_rmse: 0.6558 | val_ll: -1.0364
[3:05:11.090004] epoch: 5750 | elbo: 24337.06361328125 | train_rmse: 0.5168 | val_rmse: 0.6555 | val_ll: -1.0316
[3:06:47.731611] epoch: 5800 | elbo: 24107.48443359375 | train_rmse: 0.5152 | val_rmse: 0.6558 | val_ll: -1.0326
[3:08:23.458404] epoch: 5850 | elbo: 23920.4013671875 | train_rmse: 0.5148 | val_rmse: 0.6551 | val_ll: -1.0333
[3:09:58.926080] epoch: 5900 | elbo: 23736.581542968754 | train_rmse: 0.5137 | val_rmse: 0.6543 | val_ll: -1.0349
[3:11:34.729395] epoch: 5950 | elbo: 23537.657773437502 | train_rmse: 0.5136 | val_rmse: 0.6533 | val_ll: -1.0286
[3:13:11.916124] epoch: 6000 | elbo: 23433.544453125003 | train_rmse: 0.5139 | val_rmse: 0.6522 | val_ll: -1.0289
[3:14:49.095963] epoch: 6050 | elbo: 23276.4506640625 | train_rmse: 0.5126 | val_rmse: 0.6524 | val_ll: -1.0287
[3:16:26.870994] epoch: 6100 | elbo: 23085.7909765625 | train_rmse: 0.5133 | val_rmse: 0.6527 | val_ll: -1.0302
[3:18:03.662602] epoch: 6150 | elbo: 22958.459960937507 | train_rmse: 0.5123 | val_rmse: 0.6522 | val_ll: -1.0339
[3:19:40.491408] epoch: 6200 | elbo: 22813.92025390625 | train_rmse: 0.5128 | val_rmse: 0.6519 | val_ll: -1.0318
[3:21:18.360192] epoch: 6250 | elbo: 22702.03046875 | train_rmse: 0.5125 | val_rmse: 0.652 | val_ll: -1.0306
[3:22:56.832628] epoch: 6300 | elbo: 22608.22474609375 | train_rmse: 0.5121 | val_rmse: 0.6507 | val_ll: -1.0282
[3:24:34.445191] epoch: 6350 | elbo: 22489.641640625 | train_rmse: 0.5117 | val_rmse: 0.6502 | val_ll: -1.028
[3:26:10.082710] epoch: 6400 | elbo: 22346.833730468752 | train_rmse: 0.511 | val_rmse: 0.6502 | val_ll: -1.0288
[3:27:46.097944] epoch: 6450 | elbo: 22271.456894531253 | train_rmse: 0.5112 | val_rmse: 0.6494 | val_ll: -1.0267
[3:29:23.615309] epoch: 6500 | elbo: 22185.639453125 | train_rmse: 0.5118 | val_rmse: 0.6495 | val_ll: -1.0265
[3:31:00.812977] epoch: 6550 | elbo: 22119.909765625 | train_rmse: 0.5113 | val_rmse: 0.6488 | val_ll: -1.0242
[3:32:36.606277] epoch: 6600 | elbo: 21967.62451171875 | train_rmse: 0.5107 | val_rmse: 0.6487 | val_ll: -1.028
[3:34:11.977206] epoch: 6650 | elbo: 21855.776250000003 | train_rmse: 0.5104 | val_rmse: 0.6475 | val_ll: -1.0219
[3:35:49.048721] epoch: 6700 | elbo: 21793.63640625 | train_rmse: 0.511 | val_rmse: 0.6473 | val_ll: -1.0185
[3:37:27.024811] epoch: 6750 | elbo: 21710.6849609375 | train_rmse: 0.5112 | val_rmse: 0.647 | val_ll: -1.0218
[3:39:03.279998] epoch: 6800 | elbo: 21605.82587890625 | train_rmse: 0.5105 | val_rmse: 0.6467 | val_ll: -1.0212
[3:40:40.962962] epoch: 6850 | elbo: 21561.71578125 | train_rmse: 0.5103 | val_rmse: 0.6464 | val_ll: -1.0208
[3:42:19.125419] epoch: 6900 | elbo: 21500.15220703125 | train_rmse: 0.5108 | val_rmse: 0.6458 | val_ll: -1.0141
[3:43:55.533333] epoch: 6950 | elbo: 21413.3607421875 | train_rmse: 0.5106 | val_rmse: 0.6455 | val_ll: -1.0166
[3:45:33.005898] epoch: 7000 | elbo: 21373.238828125 | train_rmse: 0.5104 | val_rmse: 0.6457 | val_ll: -1.022
[3:47:11.069247] epoch: 7050 | elbo: 21244.9257421875 | train_rmse: 0.5103 | val_rmse: 0.6447 | val_ll: -1.0148
[3:48:48.048435] epoch: 7100 | elbo: 21199.210390624998 | train_rmse: 0.5104 | val_rmse: 0.6446 | val_ll: -1.0131
[3:50:25.711145] epoch: 7150 | elbo: 21104.14865234375 | train_rmse: 0.5111 | val_rmse: 0.6444 | val_ll: -1.0109
[3:52:03.121422] epoch: 7200 | elbo: 21087.509414062493 | train_rmse: 0.5111 | val_rmse: 0.6436 | val_ll: -1.0167
[3:53:39.997446] epoch: 7250 | elbo: 21011.620761718747 | train_rmse: 0.5108 | val_rmse: 0.6432 | val_ll: -1.0154
[3:55:16.564859] epoch: 7300 | elbo: 20915.094999999998 | train_rmse: 0.5103 | val_rmse: 0.6433 | val_ll: -1.0121
[3:56:52.154823] epoch: 7350 | elbo: 20891.764160156243 | train_rmse: 0.5107 | val_rmse: 0.6432 | val_ll: -1.0119
[3:58:28.622367] epoch: 7400 | elbo: 20851.923261718748 | train_rmse: 0.5099 | val_rmse: 0.6415 | val_ll: -1.0086
[4:00:04.782360] epoch: 7450 | elbo: 20790.346523437496 | train_rmse: 0.5098 | val_rmse: 0.6423 | val_ll: -1.0121
[4:01:41.556702] epoch: 7500 | elbo: 20719.472050781253 | train_rmse: 0.5101 | val_rmse: 0.6417 | val_ll: -1.0112
[4:03:17.732758] epoch: 7550 | elbo: 20620.1597265625 | train_rmse: 0.5104 | val_rmse: 0.6406 | val_ll: -1.01
[4:04:54.302381] epoch: 7600 | elbo: 20606.4296484375 | train_rmse: 0.5101 | val_rmse: 0.6395 | val_ll: -1.0104
[4:06:30.486506] epoch: 7650 | elbo: 20550.65505859375 | train_rmse: 0.5099 | val_rmse: 0.6402 | val_ll: -1.0076
[4:08:05.910810] epoch: 7700 | elbo: 20506.57515625 | train_rmse: 0.509 | val_rmse: 0.64 | val_ll: -1.0088
[4:09:42.400889] epoch: 7750 | elbo: 20452.60509765625 | train_rmse: 0.5097 | val_rmse: 0.6391 | val_ll: -1.0071
[4:11:20.633151] epoch: 7800 | elbo: 20347.284667968754 | train_rmse: 0.5092 | val_rmse: 0.6393 | val_ll: -1.0066
[4:12:58.299658] epoch: 7850 | elbo: 20319.949921875002 | train_rmse: 0.5089 | val_rmse: 0.6385 | val_ll: -1.0039
[4:14:37.356077] epoch: 7900 | elbo: 20257.2168359375 | train_rmse: 0.5087 | val_rmse: 0.6389 | val_ll: -1.0043
[4:16:15.705630] epoch: 7950 | elbo: 20200.663417968746 | train_rmse: 0.5092 | val_rmse: 0.6383 | val_ll: -1.0038
[4:17:53.255947] epoch: 8000 | elbo: 20154.2942578125 | train_rmse: 0.508 | val_rmse: 0.6375 | val_ll: -1.0014
[4:19:30.860054] epoch: 8050 | elbo: 20092.212128906252 | train_rmse: 0.5086 | val_rmse: 0.637 | val_ll: -0.9992
[4:21:07.881708] epoch: 8100 | elbo: 20076.9973828125 | train_rmse: 0.5082 | val_rmse: 0.6361 | val_ll: -0.9973
[4:22:45.055173] epoch: 8150 | elbo: 19992.24396484375 | train_rmse: 0.508 | val_rmse: 0.6362 | val_ll: -0.9946
[4:24:22.158148] epoch: 8200 | elbo: 19957.340937499997 | train_rmse: 0.5079 | val_rmse: 0.6359 | val_ll: -0.995
[4:25:58.887061] epoch: 8250 | elbo: 19913.0579296875 | train_rmse: 0.5085 | val_rmse: 0.6353 | val_ll: -0.9946
[4:27:36.793981] epoch: 8300 | elbo: 19872.2498828125 | train_rmse: 0.5075 | val_rmse: 0.635 | val_ll: -0.9978
[4:29:13.447394] epoch: 8350 | elbo: 19822.180585937505 | train_rmse: 0.5066 | val_rmse: 0.6347 | val_ll: -0.9937
[4:30:50.284870] epoch: 8400 | elbo: 19740.80771484375 | train_rmse: 0.5066 | val_rmse: 0.6345 | val_ll: -0.9875
[4:32:26.361720] epoch: 8450 | elbo: 19743.65044921875 | train_rmse: 0.5062 | val_rmse: 0.635 | val_ll: -0.9908
[4:34:01.305437] epoch: 8500 | elbo: 19652.851425781253 | train_rmse: 0.507 | val_rmse: 0.6338 | val_ll: -0.9957
[4:35:36.083648] epoch: 8550 | elbo: 19677.610625000005 | train_rmse: 0.5066 | val_rmse: 0.6343 | val_ll: -0.9954
[4:37:11.452386] epoch: 8600 | elbo: 19588.808945312503 | train_rmse: 0.5072 | val_rmse: 0.6332 | val_ll: -0.9913
[4:38:48.247179] epoch: 8650 | elbo: 19571.264296874997 | train_rmse: 0.5059 | val_rmse: 0.6333 | val_ll: -0.9907
[4:40:23.677458] epoch: 8700 | elbo: 19503.921679687497 | train_rmse: 0.506 | val_rmse: 0.6335 | val_ll: -0.9917
[4:41:59.275173] epoch: 8750 | elbo: 19517.89130859375 | train_rmse: 0.5065 | val_rmse: 0.6333 | val_ll: -0.9928
[4:43:37.719029] epoch: 8800 | elbo: 19480.755507812504 | train_rmse: 0.5064 | val_rmse: 0.6328 | val_ll: -0.992
[4:45:13.764976] epoch: 8850 | elbo: 19425.9305859375 | train_rmse: 0.506 | val_rmse: 0.6326 | val_ll: -0.9926
[4:46:51.381928] epoch: 8900 | elbo: 19390.874765625 | train_rmse: 0.5053 | val_rmse: 0.6324 | val_ll: -0.9876
[4:48:27.036651] epoch: 8950 | elbo: 19364.945781249997 | train_rmse: 0.506 | val_rmse: 0.6327 | val_ll: -0.9896
[4:50:02.701618] epoch: 9000 | elbo: 19310.41595703125 | train_rmse: 0.5056 | val_rmse: 0.6319 | val_ll: -0.9903
[4:51:39.798465] epoch: 9050 | elbo: 19287.28228515625 | train_rmse: 0.5059 | val_rmse: 0.632 | val_ll: -0.9903
[4:53:17.584085] epoch: 9100 | elbo: 19241.055546875003 | train_rmse: 0.5059 | val_rmse: 0.6315 | val_ll: -0.9878
[4:54:54.876581] epoch: 9150 | elbo: 19205.68861328125 | train_rmse: 0.5056 | val_rmse: 0.6315 | val_ll: -0.9881
[4:56:30.849718] epoch: 9200 | elbo: 19227.502812500003 | train_rmse: 0.5053 | val_rmse: 0.6317 | val_ll: -0.9894
[4:58:06.378577] epoch: 9250 | elbo: 19123.144921875 | train_rmse: 0.5058 | val_rmse: 0.6311 | val_ll: -0.9915
[4:59:43.807717] epoch: 9300 | elbo: 19141.440585937504 | train_rmse: 0.5063 | val_rmse: 0.631 | val_ll: -0.9903
[5:01:21.029893] epoch: 9350 | elbo: 19064.44599609375 | train_rmse: 0.5057 | val_rmse: 0.6308 | val_ll: -0.9876
[5:02:58.094841] epoch: 9400 | elbo: 19013.07205078125 | train_rmse: 0.5055 | val_rmse: 0.6305 | val_ll: -0.9911
[5:04:35.560309] epoch: 9450 | elbo: 19000.047695312503 | train_rmse: 0.5054 | val_rmse: 0.6306 | val_ll: -0.9878
[5:06:12.564798] epoch: 9500 | elbo: 18990.70427734375 | train_rmse: 0.5057 | val_rmse: 0.6309 | val_ll: -0.9878
[5:07:49.405769] epoch: 9550 | elbo: 18927.757558593752 | train_rmse: 0.5052 | val_rmse: 0.6303 | val_ll: -0.986
[5:09:25.377862] epoch: 9600 | elbo: 18906.86869140625 | train_rmse: 0.5047 | val_rmse: 0.6299 | val_ll: -0.9886
[5:11:01.775601] epoch: 9650 | elbo: 18825.3416015625 | train_rmse: 0.5051 | val_rmse: 0.6302 | val_ll: -0.9929
[5:12:38.256126] epoch: 9700 | elbo: 18815.3033984375 | train_rmse: 0.5055 | val_rmse: 0.6297 | val_ll: -0.987
[5:14:14.869371] epoch: 9750 | elbo: 18764.51412109375 | train_rmse: 0.505 | val_rmse: 0.63 | val_ll: -0.9889
[5:15:52.257597] epoch: 9800 | elbo: 18735.520390625003 | train_rmse: 0.505 | val_rmse: 0.629 | val_ll: -0.9871
[5:17:31.019128] epoch: 9850 | elbo: 18740.73818359375 | train_rmse: 0.5045 | val_rmse: 0.6299 | val_ll: -0.989
[5:19:07.698231] epoch: 9900 | elbo: 18694.158203124996 | train_rmse: 0.5042 | val_rmse: 0.63 | val_ll: -0.9896
[5:20:42.420652] epoch: 9950 | elbo: 18641.77671875 | train_rmse: 0.5043 | val_rmse: 0.6299 | val_ll: -0.9908
Training finished in 5:22:14.834122 seconds
Saved SVI model to tests/dataset-tests/sineasy10-10k-s05/models/sineasy10-3x128-s05/checkpoint_1.pt
File Size is 0.26848316192626953 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=128, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=128, out_features=128, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=128, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:4 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-10k-s05/models/sineasy10-3x128-s05/checkpoint_1.pt
using device: cuda:4
====== evaluating profile sineasy10-3x128-s05 - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:02:44.762833
End time: 2023-07-12 14:21:49.204254
Total time: 5:25:03.948626
