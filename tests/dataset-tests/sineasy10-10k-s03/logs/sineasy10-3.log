Start time: 2023-07-11 13:39:29.643604
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (5): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:6 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.3 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.1835,  0.1500, -0.3897,  ...,  0.3757, -0.0117,  0.2174],
        [ 0.1416,  0.1143, -0.9134,  ..., -0.1834,  0.9524,  0.0090],
        [ 0.1579, -0.1001, -0.0361,  ...,  0.3049,  0.0464, -0.4223],
        ...,
        [-0.3368,  0.1723, -0.0942,  ...,  0.1908, -0.1991,  0.4137],
        [-0.3908,  0.1906, -0.1231,  ..., -0.3708,  0.4022,  0.7265],
        [ 0.0302, -0.0700,  0.4324,  ...,  0.1134, -0.2829, -0.6205]],
       device='cuda:6', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-1.7823e-01,  5.0294e-01, -5.8265e-01,  1.3545e-01, -1.3441e-01,
         7.4781e-02, -4.0722e-01, -3.0516e-02,  6.6712e-01, -5.9947e-02,
         6.5102e-01, -5.5580e-01,  4.0616e-01,  5.3144e-01,  3.7917e-01,
        -3.8019e-02, -3.0921e-01, -1.0750e-01, -2.1006e-02, -7.8506e-02,
        -2.1181e-01,  3.8762e-01, -5.6436e-02, -8.5820e-02,  9.6550e-02,
        -1.6387e-01, -1.8363e-01,  1.4983e-01,  3.3374e-01,  2.3622e-01,
         2.6034e-01, -1.3371e-01, -4.1936e-01, -8.8920e-02,  4.0596e-03,
        -5.7847e-01,  4.8493e-02, -1.3689e-01,  2.4492e-01,  4.4066e-04,
        -1.0737e-01, -1.4807e-02, -1.1360e-02, -1.3731e-01, -1.3159e-02,
         3.7334e-01, -2.6897e-01,  1.5575e-01, -1.3909e-01,  1.5582e-01,
        -5.6248e-02,  2.3335e-01, -8.2304e-02,  3.9618e-01,  1.7103e-01,
         2.5977e-02,  6.3141e-02,  1.0835e-01, -3.9917e-01,  1.7405e-01,
         3.7544e-01, -2.5445e-01, -6.6561e-02, -4.5028e-02,  1.7191e-01,
         2.1605e-01, -4.3711e-01, -1.2801e-01,  1.9165e-01,  4.8792e-01,
        -5.8266e-01, -3.8354e-01,  4.4461e-02,  5.3406e-01,  5.3733e-01,
         2.1702e-02,  4.6778e-01,  1.0149e-01,  1.6570e-01,  5.7628e-02,
         3.6372e-02,  6.5085e-02, -4.9143e-01, -5.6108e-01,  2.7023e-01,
        -7.7419e-02,  1.2483e-01,  6.0247e-02,  1.4922e-01, -1.7925e-01,
        -5.3547e-01,  7.9142e-01,  3.5786e-01,  1.8893e-02,  4.7649e-01,
         1.1538e-01, -7.1701e-01,  8.9792e-02, -6.7074e-01,  1.1091e-01,
         1.3316e-01, -1.1757e-01, -6.5029e-01, -1.2018e-01, -3.0556e-01,
         1.8410e-01, -5.9650e-01, -2.5510e-01, -5.9243e-01,  3.6801e-01,
        -3.7799e-01,  1.4594e-01, -5.2876e-01,  2.5049e-01, -3.8332e-01,
         3.4906e-01, -2.2868e-01, -1.3695e-01, -1.2946e-01, -3.4877e-02,
         3.8910e-01, -2.2079e-01, -5.8160e-01, -1.0523e-01,  6.1920e-01,
        -5.5133e-01, -1.3600e-01, -4.2393e-01,  9.0929e-03,  3.2082e-01,
         2.6637e-02, -1.5651e-01, -2.0505e-01,  6.5848e-01,  4.0317e-01,
         5.6654e-01, -3.4408e-01,  3.6509e-01,  2.7423e-01, -4.4687e-01,
         1.2560e-01,  3.4853e-01,  5.3357e-02,  6.9369e-03, -5.7181e-01,
        -1.6457e-01,  4.5253e-01, -2.2041e-01,  4.8412e-01, -1.3008e-01,
        -1.1653e-01, -6.2746e-02, -1.8840e-01, -2.0879e-01, -2.0041e-01,
        -4.6775e-02,  1.0068e-01, -1.4608e-01,  2.0044e-01,  6.8071e-01,
         2.7867e-01,  3.0461e-01,  2.6189e-01, -1.2851e-01,  2.6575e-01,
         7.6081e-02,  1.1064e-01,  5.6273e-01, -3.6695e-01,  6.9775e-02,
        -1.5413e-01,  2.0223e-01, -5.5553e-01, -2.8271e-01,  2.9218e-01,
        -4.1837e-01,  1.5541e-01,  3.4703e-01, -1.6147e-01,  2.6572e-01,
         2.0150e-01, -2.0140e-01,  5.8032e-02, -3.0731e-01, -3.2231e-01,
        -2.9253e-01,  3.9034e-01, -3.0781e-01,  2.1896e-01, -1.4347e-01,
        -2.0774e-01, -1.9129e-02, -7.8975e-02,  6.1907e-02,  2.0551e-01,
         7.5416e-01, -3.6009e-01, -3.9572e-01, -1.6817e-02, -2.9420e-01,
         2.0326e-01, -1.0500e-01,  2.6212e-01, -5.6098e-02,  3.0379e-01,
        -7.1649e-01,  2.5382e-01,  6.4169e-02, -2.4733e-02, -3.3224e-01,
         1.7891e-01, -1.1408e-01,  4.3750e-01,  6.1905e-01, -3.2717e-01,
         2.8304e-01,  5.8473e-02,  4.1732e-02, -4.1793e-01,  3.1945e-02,
         5.2301e-01, -4.7642e-01,  3.0189e-01, -1.0101e-02, -3.8748e-01,
         1.9294e-02, -1.9180e-01,  1.3966e-01,  6.0038e-01,  6.9607e-01,
        -1.7796e-01, -2.8247e-01,  1.5151e-02,  1.1559e-01, -2.4949e-01,
         2.4764e-01, -1.8608e-01, -1.8710e-01,  2.1646e-01, -5.1569e-01,
        -3.3553e-02,  2.0670e-01,  4.7244e-01, -5.4790e-02,  3.4801e-01,
         4.7386e-01,  7.4456e-02, -3.4960e-02,  1.3684e-02, -1.9884e-02,
         4.7523e-01,  3.1636e-01,  2.8267e-01, -6.1449e-01,  2.6332e-01,
        -4.5052e-01,  3.3751e-01, -1.7049e-01, -3.5623e-01, -7.4953e-01,
         2.8320e-02, -1.2254e-01,  5.1217e-01, -3.9512e-01, -4.9254e-02,
         2.5675e-01,  1.4440e-01,  8.9358e-02,  1.6988e-01,  2.8435e-01,
        -1.3521e-01,  2.9503e-01, -1.1815e-01, -3.0179e-01,  1.8413e-02,
         7.8128e-02,  1.9856e-01, -9.7554e-02, -6.9772e-01,  5.0231e-01,
         5.4229e-01,  3.0727e-01,  7.2248e-01, -2.5087e-01,  5.6369e-02,
         3.8292e-01, -4.4517e-02, -6.4815e-02, -6.2449e-02, -1.6913e-01,
         1.1095e-01,  2.5666e-01,  2.5901e-02, -3.2260e-01, -3.3425e-01,
         7.6750e-02, -4.5165e-01, -5.5429e-01,  1.4742e-01, -3.9479e-01,
        -6.6243e-02,  4.2477e-01,  3.3378e-01, -2.2579e-01,  1.1150e-01,
         2.1304e-01, -4.4511e-01,  6.6918e-01, -2.6922e-01, -1.6936e-01,
        -1.3958e-01, -3.6294e-01, -3.5499e-01,  3.1946e-01, -3.3084e-01,
        -8.8676e-02, -2.9500e-01, -3.7574e-02, -3.6834e-01, -2.2618e-01,
         1.5549e-01, -7.3923e-01, -1.8274e-01, -2.2317e-01, -5.4449e-01,
         2.2825e-01,  6.1362e-01, -6.3564e-01,  2.0346e-01, -2.5278e-01,
        -1.8720e-01,  6.0705e-01,  7.6132e-02, -2.4135e-01, -1.2162e-01,
         3.7631e-01,  1.3138e-01,  1.2385e-01,  7.0855e-01,  2.7663e-01,
        -3.2371e-03,  4.4839e-01,  1.0249e-01, -2.4125e-01,  1.8269e-01,
        -1.8524e-01,  6.7911e-02,  1.8687e-01, -3.3226e-01,  2.7731e-01,
        -6.1685e-02, -6.4994e-01, -2.7982e-01,  5.6103e-01,  1.2059e-01,
         1.0904e-01, -2.7520e-01, -2.3984e-01, -2.0639e-01,  9.0588e-02,
         3.5252e-01,  8.6946e-03, -2.7902e-01, -2.1374e-01,  2.9388e-01,
         4.4775e-02, -3.5772e-01,  8.5682e-02,  9.1087e-02, -2.2566e-01,
         4.2784e-01,  2.5430e-01, -1.4201e-02, -4.1120e-01,  1.5796e-01,
        -1.0250e-01,  1.9909e-03,  2.4264e-01, -3.5435e-01,  9.4867e-02,
        -2.6800e-01, -2.3458e-02,  4.8180e-01,  4.3842e-01, -4.5777e-01,
        -5.3493e-01,  2.5293e-01, -1.7988e-01, -3.4911e-01,  3.7370e-01,
         7.1683e-02,  1.4495e-01, -3.0386e-01,  2.9548e-01,  1.9367e-01,
        -6.5042e-02, -3.4505e-01, -2.1409e-01,  5.1218e-01,  1.3953e-01,
        -1.9668e-01, -2.3720e-01, -2.8874e-01, -4.9342e-01,  1.4879e-01,
        -1.6595e-01, -4.6694e-02, -7.1695e-02,  1.4346e-01,  6.3961e-01,
         9.5283e-02,  2.3878e-01, -6.3243e-02,  1.9631e-03, -3.1289e-01,
         3.4219e-01,  1.4886e-01, -6.9626e-02, -3.6541e-01, -2.1746e-01,
        -2.1174e-02, -6.0332e-02,  2.1608e-01,  1.0489e-01,  4.5518e-01,
        -2.5834e-01, -2.4532e-01, -5.9218e-01, -4.0317e-01, -1.4641e-01,
         1.7277e-01, -2.5228e-01, -1.4424e-01,  8.9234e-02,  2.8347e-01,
         5.5036e-01, -1.1967e-01,  1.0841e-02,  1.4467e-01, -3.9940e-01,
        -8.6467e-02,  1.7574e-01,  1.5059e-01,  1.5513e-01, -2.8242e-02,
         2.0815e-01,  1.6971e-01, -1.3287e-01,  3.4010e-01,  1.1939e-01,
        -2.7150e-02, -4.6605e-01, -1.4778e-01,  1.9036e-01, -3.3692e-01,
         1.9554e-02, -9.2924e-03,  3.9853e-02,  1.3913e-01,  2.6812e-01,
        -8.9671e-02,  5.4844e-01,  8.7221e-02,  2.6845e-01,  4.7170e-02,
         6.9888e-01, -1.1909e-01, -2.0987e-01, -3.6266e-01,  1.2260e-01,
        -4.1352e-01, -2.1393e-01, -2.5586e-01,  3.3608e-01, -1.7097e-01,
        -2.1286e-01,  5.1641e-01,  4.9373e-01, -1.8716e-01,  5.7447e-01,
         1.9947e-01,  4.8937e-02,  2.3979e-01,  3.8402e-02, -2.4368e-01,
         2.6271e-01, -7.3902e-02, -8.6151e-01,  3.6611e-01,  6.4215e-01,
         3.8981e-01, -1.4609e-01,  4.2651e-01,  5.7394e-03, -3.9965e-01,
         2.3539e-02,  3.8235e-01, -6.5336e-02, -9.7194e-02,  3.0629e-01,
         9.4631e-01,  3.5458e-01,  5.3717e-01,  1.3678e-01, -6.8224e-01,
        -9.7560e-02,  4.5199e-02, -4.2750e-01, -1.9899e-01, -4.2412e-01,
         2.3418e-02,  2.3419e-01], device='cuda:6', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1477, -0.0802,  0.0741,  ..., -0.2312, -0.2779,  0.0872],
        [ 0.0822, -0.2462, -0.3271,  ..., -0.0950,  0.1078,  0.6187],
        [ 0.3417,  0.1422, -0.1896,  ..., -0.0823,  0.2866,  0.0515],
        ...,
        [ 0.5165,  0.3619, -0.5512,  ...,  0.5884,  0.1077, -0.6799],
        [-0.1900,  0.3652,  0.2294,  ..., -0.0391,  0.1073,  0.3460],
        [-0.6463,  0.2119,  0.2474,  ..., -0.1109,  0.2080, -0.3046]],
       device='cuda:6', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-1.7304e-02, -7.4371e-01,  4.8193e-01, -5.7575e-01, -1.4242e-01,
         9.0947e-02, -2.8964e-01,  3.3412e-01,  5.8711e-01, -4.6437e-01,
         5.2750e-01, -4.8782e-02,  3.2854e-01, -5.6577e-01, -9.4810e-02,
        -7.4957e-01, -8.9866e-02, -7.3507e-01,  1.0449e-01,  6.8090e-02,
        -6.3063e-01, -1.0982e-01,  5.9980e-01, -4.0583e-01,  2.5301e-01,
        -4.0862e-01,  2.2811e-01, -1.4941e-01, -4.2616e-01,  1.9282e-01,
        -2.6936e-01, -3.7600e-02, -3.2158e-01,  4.3963e-01, -2.3129e-01,
        -2.4136e-01,  9.2466e-02, -1.9063e-01,  2.4385e-01,  5.5556e-02,
        -4.9878e-02, -2.6260e-02,  1.9846e-01,  1.1003e-01,  1.4934e-01,
        -1.1645e-01,  5.2751e-01, -3.7193e-01,  3.3210e-01, -4.8605e-01,
         7.3373e-02,  2.1925e-01,  1.8722e-01, -3.3734e-01,  2.8190e-02,
        -3.4027e-01,  1.9335e-01,  1.8271e-01,  3.5423e-01,  4.2895e-01,
        -4.2429e-02,  3.7924e-01,  1.4194e-02, -8.8860e-02,  3.6483e-01,
         2.6001e-02,  3.1037e-01,  4.8087e-02,  4.9200e-02, -1.2883e-01,
        -1.4996e-01,  3.0200e-01,  2.5841e-01, -3.9988e-02, -1.3636e-02,
        -1.0147e-02, -2.5595e-01, -5.8228e-02,  3.4675e-01,  1.4846e-01,
        -1.1782e-01, -4.1372e-01, -1.9294e-01, -3.8107e-01,  5.3011e-01,
         4.3008e-01,  3.6775e-01,  3.1596e-01, -7.3831e-01,  6.5674e-01,
         1.7591e-01, -2.2728e-01,  2.1152e-01, -7.6497e-02, -7.9657e-01,
        -3.5461e-01, -2.9145e-01,  3.2658e-01,  3.0738e-01, -4.1942e-01,
         1.4252e-01, -2.0938e-01,  2.4464e-01, -2.5563e-01,  5.5339e-02,
         2.6314e-02,  4.4030e-01,  2.9581e-01, -1.9017e-01,  4.9778e-01,
        -3.2663e-01, -3.4342e-01, -4.7281e-02,  3.4712e-01,  2.2902e-01,
         2.6529e-01,  4.6699e-01, -6.6848e-02,  6.7599e-01,  1.1846e-01,
         1.4533e-01,  2.3124e-01,  1.3194e-01,  2.9267e-01,  1.2637e-01,
         7.5708e-01, -5.3138e-01, -3.7185e-01, -1.7431e-01, -2.8134e-01,
         2.5362e-02, -1.7231e-01,  2.3303e-01,  3.5459e-01, -1.0472e-01,
        -1.2351e-01, -1.2743e-02, -8.9211e-02,  7.6929e-02,  4.1401e-01,
         2.8890e-03,  4.9753e-01,  4.9041e-04,  3.3155e-01, -6.3475e-01,
         2.4530e-01, -7.0779e-02,  1.4113e-01, -3.1092e-02,  2.9879e-01,
        -7.7595e-01,  4.5914e-02,  4.6827e-01,  1.3830e-01, -6.2100e-01,
         1.0129e-01, -8.7237e-01, -2.7344e-01, -2.4382e-01,  1.9606e-02,
        -2.5932e-02, -1.3276e-01, -6.0999e-02, -2.2408e-01, -1.2813e-01,
         4.5391e-01,  3.5208e-01,  4.4332e-01, -4.3483e-03,  2.1009e-01,
        -1.1654e-01, -3.2487e-01, -2.3709e-01, -1.2858e-01, -1.8470e-01,
        -4.2830e-01, -2.7153e-01,  1.8645e-01,  2.7568e-01,  4.6523e-01,
        -7.9293e-01, -4.0920e-01, -3.9577e-01,  1.4098e-01, -1.1561e-01,
         9.9970e-03,  2.0233e-02, -4.4423e-01,  5.0118e-01, -4.0907e-01,
         3.0411e-01,  3.3983e-01,  4.8734e-01, -1.0515e-01, -1.9493e-01,
         1.2675e-01, -1.3493e-01,  5.3422e-01, -6.8750e-01,  6.2515e-01,
        -3.3760e-01,  2.3088e-01, -1.8721e-01, -1.2790e-01,  4.6638e-01,
         5.6233e-01,  2.3614e-01, -3.6404e-01,  5.4913e-01, -2.8064e-01,
        -1.2519e-01,  2.2452e-02,  5.4213e-01, -1.6696e-02,  3.1207e-03,
         9.1642e-02,  8.9236e-02, -1.7071e-01,  2.0936e-01, -5.7836e-02,
        -3.4964e-01,  6.7236e-02,  2.9235e-01,  5.0427e-01, -2.0986e-01,
         1.6906e-01,  1.8073e-01,  2.1306e-01, -1.3690e-01,  1.0679e-01,
         1.7766e-01,  2.6185e-01,  1.9269e-01, -6.3151e-01,  7.1693e-02,
         3.0994e-01, -3.4274e-01, -1.1084e-01, -4.6322e-01,  4.6479e-01,
         5.1805e-01,  2.2691e-01,  1.4708e-01, -1.4369e-01,  2.3180e-01,
        -6.0684e-01, -3.4615e-01, -9.8005e-02, -1.4337e-02,  2.3830e-01,
        -2.7290e-01,  5.0760e-01, -4.4781e-02, -4.6378e-02, -3.9196e-01,
         3.4800e-01,  3.4621e-01, -5.9266e-02, -6.3678e-01, -2.7233e-01,
        -9.5937e-02, -3.8816e-02,  2.7815e-01,  1.0940e-02,  2.3537e-01,
        -2.3845e-02, -5.0769e-02,  6.3709e-02, -9.4922e-02,  6.9529e-01,
        -3.8007e-01, -3.6726e-01,  1.5587e-01, -1.4668e-01, -2.9612e-01,
        -2.0268e-01,  3.0045e-01,  5.0118e-01,  1.0445e-01, -4.3017e-01,
        -1.3133e-02, -4.7267e-01,  1.0596e-01,  3.4572e-01,  5.6988e-01,
        -3.9530e-01,  1.7752e-01, -1.8111e-01, -2.1140e-01,  5.0470e-01,
         3.6142e-01, -1.8509e-01, -2.0833e-01, -2.4364e-04,  2.7748e-01,
        -3.9475e-01, -1.4549e-01, -7.9127e-02,  5.1687e-01,  1.2715e-02,
         1.6593e-01, -1.5096e-01,  2.1595e-01, -3.0374e-01,  2.2438e-01,
         1.1915e+00, -1.7656e-01,  2.1653e-01, -4.3415e-01, -8.2995e-02,
         9.6594e-02,  7.7716e-03,  7.6154e-01,  2.1252e-01, -2.5723e-01,
        -2.2339e-02, -2.8503e-01, -6.7069e-01, -6.0677e-01,  2.8146e-01,
         3.1510e-01,  2.1616e-01,  2.0367e-01,  1.6425e-01, -2.1851e-01,
         1.5391e-01, -3.7612e-01, -3.1791e-01,  3.8760e-01, -4.1480e-01,
        -1.6114e-01, -2.5449e-01, -4.5605e-01, -9.8087e-02,  7.9350e-03,
        -4.3156e-01,  5.3155e-01,  1.1264e-01,  2.4581e-01,  1.2908e-01,
         2.5088e-02, -1.9425e-01,  1.8834e-01, -4.1726e-01,  1.9716e-01,
         2.2442e-02,  1.7662e-01, -3.2100e-02, -4.5868e-01,  3.1658e-02,
        -1.5456e-01, -4.6440e-01,  9.0809e-02,  2.2375e-01,  4.6116e-01,
         1.5411e-01, -2.9951e-01,  2.5007e-01, -5.0178e-01, -1.6389e-01,
        -2.7900e-01,  5.3930e-02,  6.6744e-02, -3.2384e-01,  9.5226e-02,
        -5.5849e-01,  4.2804e-01,  3.5015e-02, -1.1938e-01, -1.4371e-01,
        -4.8624e-01, -3.8124e-01,  8.9776e-01, -8.2407e-02,  2.7250e-01,
         1.1459e-01,  1.2936e-01,  2.5598e-01,  1.4239e-01,  6.7955e-02,
         4.4034e-02,  1.0860e-01, -4.9753e-01,  6.5137e-02, -1.1185e+00,
        -1.4087e-01,  3.1329e-01,  9.7088e-02, -1.0664e-01, -4.4843e-01,
         4.8064e-01, -3.4439e-01, -2.5299e-01,  4.7642e-01, -9.8902e-02,
         8.9830e-02, -1.5237e-01,  1.5722e-02, -3.6142e-01,  1.1983e-01,
         1.0132e-01,  8.7161e-02,  1.2674e-02,  2.4100e-01,  3.2719e-01,
         1.7056e-01, -3.3684e-02, -5.5951e-02, -1.2346e-01,  9.4406e-02,
        -1.3117e-02,  3.2713e-01, -7.7023e-02, -1.4104e-01,  3.4217e-01,
         1.7002e-01,  2.4150e-01,  3.5155e-01,  1.3955e-01,  2.2147e-01,
        -2.7118e-01,  1.1829e-02,  2.2552e-02,  2.1026e-02,  2.3369e-02,
         5.2346e-01,  2.8261e-02, -2.6856e-01, -3.8788e-01, -3.6790e-01,
        -2.5159e-01,  1.4458e-01,  2.6003e-01, -6.3867e-02,  7.0197e-02,
        -1.3925e-01,  3.9376e-03, -5.1241e-02,  5.1188e-01,  1.5008e-01,
         4.6375e-01, -7.4895e-02, -3.1592e-02,  2.6546e-01,  1.5406e-01,
         6.1295e-01, -5.8630e-01, -1.0401e-01,  5.7565e-01,  3.0136e-01,
         2.0434e-01,  7.4667e-02,  5.2513e-01,  1.7537e-01, -2.3197e-01,
         3.9474e-01, -3.0074e-01, -1.3381e-01,  1.9023e-01, -1.5827e-01,
        -1.7900e-01,  3.2330e-01,  2.9406e-01, -5.1262e-01, -4.6590e-01,
         2.8354e-01, -5.7071e-01,  1.6701e-01,  3.4173e-01, -1.7136e-02,
         2.9176e-01, -1.0677e+00, -2.3840e-01, -2.3391e-01,  3.9123e-01,
         8.9054e-03,  1.6719e-01, -7.3138e-02,  3.1225e-01, -1.4398e-01,
         4.2196e-01, -4.3965e-01, -1.7356e-01,  5.5655e-01,  2.6707e-01,
        -8.8729e-02, -2.0944e-01,  1.4981e-01,  2.7648e-01,  3.8481e-01,
        -1.4325e-01,  4.7390e-01,  4.9885e-01, -1.0093e-01, -2.6477e-02,
         8.3113e-01,  1.4151e-01, -4.6067e-01, -2.1335e-01, -1.3827e-01,
        -1.9782e-01,  3.5200e-01,  4.1190e-02, -3.3886e-01, -3.2043e-01,
         1.8600e-01,  9.3158e-02, -1.0647e-01,  2.4555e-01,  2.5628e-01,
        -2.3356e-01,  2.1775e-01], device='cuda:6', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.2788,  0.4712, -0.3558,  ...,  0.0852,  0.0957,  0.2607],
        [-0.1484, -0.2478,  0.0234,  ...,  0.0594,  0.5778,  0.1728],
        [-0.1258, -0.0337,  0.0160,  ...,  0.0029,  0.5243,  0.3411],
        ...,
        [-0.1701,  0.2458, -0.2898,  ..., -0.4678, -0.1546, -0.2172],
        [-0.3523,  0.2947, -0.2264,  ..., -0.2111,  0.4472, -0.2963],
        [-0.0756, -0.2880,  0.2236,  ..., -0.1032,  0.3290,  0.5057]],
       device='cuda:6', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6095e-01, -2.2319e-01,  1.1103e-01, -2.3292e-01,  6.2166e-06,
        -1.5685e-01, -8.2054e-01, -3.1450e-01, -3.6688e-01, -2.4558e-01,
        -1.1119e-01,  3.1010e-01,  4.9447e-01,  5.8524e-02, -2.0650e-01,
        -3.1385e-02, -5.9704e-03,  5.4822e-01,  2.6223e-01,  5.0320e-01,
        -9.8184e-02, -7.3414e-02, -7.1630e-01, -9.9733e-02,  2.7497e-03,
         2.6648e-01,  2.8489e-01, -4.2355e-01, -2.4236e-01,  2.1705e-01,
        -5.7162e-01, -1.0769e-01,  7.3977e-02,  3.0081e-02, -4.1515e-01,
        -4.7219e-01,  2.1122e-01,  4.0944e-01, -1.2078e-01,  1.7836e-01,
         1.0095e+00,  4.1193e-01,  3.1660e-01,  3.8632e-01, -2.9621e-01,
        -2.5063e-01, -9.8408e-02, -5.0758e-01,  2.3404e-01, -1.0582e-01,
         2.3921e-01, -3.2840e-01, -5.1646e-01,  1.3984e-01,  1.0987e-01,
         2.7113e-01,  4.0299e-01, -5.9984e-01,  2.9657e-01, -1.2630e-01,
        -8.4542e-02, -2.4016e-01, -6.3478e-01, -7.7057e-01,  8.2179e-03,
         1.3906e-02, -4.2099e-01, -4.7980e-01, -3.7086e-02,  6.4549e-02,
         8.9574e-02, -3.0327e-01,  6.2960e-02, -2.8747e-01,  4.0641e-01,
         2.7348e-01,  2.7422e-01, -4.6330e-02,  8.4286e-03,  5.7026e-03,
        -1.4329e-01, -1.0820e-01,  2.9727e-01, -1.3750e-01, -3.8899e-01,
        -3.9868e-01, -1.2433e-01,  8.4280e-02, -2.8246e-01,  3.9499e-01,
         4.8705e-02, -9.7446e-02,  2.1331e-01,  8.5474e-01, -7.5803e-03,
         1.6337e-01,  1.3875e-01, -2.0394e-01, -2.9539e-01, -8.0152e-02,
         4.4473e-01,  1.4239e-01,  4.7914e-02, -5.3566e-01,  8.5798e-02,
        -3.1580e-01, -1.6829e-01,  4.4699e-01,  2.0898e-02, -1.8093e-01,
        -3.4566e-01, -7.8551e-01, -4.5781e-01, -1.2586e-01,  3.2521e-01,
         1.1650e-01,  3.1360e-01,  1.7587e-01,  8.2772e-02,  2.6422e-01,
        -4.4335e-01, -5.5629e-02,  1.6321e-01, -5.7374e-02,  1.6760e-01,
        -6.6611e-01, -4.6156e-01, -2.9009e-01, -3.4648e-01,  2.5283e-02,
        -6.7641e-01, -1.5736e-01, -1.8773e-01, -3.5351e-02, -2.5819e-01,
        -9.7859e-02,  1.2043e-01,  7.5777e-01, -3.3025e-01,  1.6098e-01,
        -4.6054e-01, -6.1820e-01, -1.3218e-01,  3.4195e-01, -9.2533e-02,
         9.5837e-02, -2.7968e-01,  1.9094e-01,  6.6922e-01, -2.3402e-02,
         4.0333e-01, -3.1954e-01, -1.2860e-01, -1.4148e-01,  8.7744e-03,
        -4.2406e-01, -1.1015e-01, -2.4406e-01,  6.5505e-02, -2.3290e-01,
         1.0324e-01, -2.6453e-01,  1.2824e-01,  3.1069e-01,  2.4128e-02,
         2.8166e-01,  2.7599e-02,  3.8887e-01, -3.8325e-01,  2.3562e-02,
        -2.0804e-01, -3.0988e-01,  3.6945e-01, -1.4312e-02,  3.7924e-01,
        -3.8777e-03, -4.3181e-02, -3.5745e-01, -3.6740e-02,  3.6153e-01,
         3.5577e-01, -1.3028e-01,  3.1018e-01, -2.7162e-02, -6.7490e-01,
        -3.9503e-01,  1.4966e-01, -2.2544e-02, -6.3600e-03,  2.3867e-01,
         5.6061e-02,  6.8928e-01,  5.2488e-01, -1.1731e-01, -7.3006e-02,
         2.1148e-01,  5.1771e-01, -4.9143e-01, -2.4439e-01, -3.5833e-01,
        -8.8262e-03,  4.1680e-03, -8.3199e-02, -6.7976e-01, -3.4704e-01,
         3.3973e-01,  2.3142e-01,  2.7203e-01, -1.9648e-01,  7.6699e-02,
         1.2691e-02, -2.2158e-01, -2.5528e-01, -1.4726e-01, -3.7426e-02,
         9.5982e-02, -3.5082e-01, -4.1665e-01, -1.3127e-01, -1.8484e-01,
        -4.5049e-01,  1.1465e-01, -3.9104e-01,  1.3391e-01, -1.2047e-01,
         2.3434e-02,  7.0874e-01, -8.7442e-01, -1.0756e-01,  1.1349e-01,
        -4.0147e-01, -4.4457e-01, -1.1596e-01, -4.8422e-01,  1.4537e-02,
         6.7207e-02,  1.1576e-01,  2.3008e-01, -1.6026e-01, -9.9004e-02,
        -4.3666e-01,  5.7932e-01,  2.2296e-02, -1.1045e-01,  2.0033e-01,
         1.7564e-02,  2.1350e-01, -6.8457e-01,  5.1056e-01,  9.6357e-02,
        -3.4199e-01,  7.5500e-01,  8.1252e-01,  1.0723e-03,  3.4866e-01,
        -6.2976e-01, -9.1457e-02, -4.6801e-01,  7.6142e-02,  4.0938e-01,
        -1.1158e-01,  2.2524e-01,  5.3375e-01, -9.1241e-02,  6.9274e-01,
        -2.3365e-01,  1.3238e-01,  1.7037e-01, -3.2833e-02, -1.4657e-01,
         2.9560e-01,  4.0805e-02,  2.3687e-01,  2.2300e-01, -4.3533e-01,
        -1.1714e-01, -9.1614e-02, -1.3742e-01,  3.0131e-01,  6.4255e-02,
         1.6029e-01, -1.1636e-01, -1.7743e-01,  7.8712e-02, -1.9594e-01,
        -1.2638e-01, -5.5543e-03, -3.8076e-02, -4.5537e-01,  3.5469e-01,
         3.0777e-01,  1.9096e-01, -1.9797e-01,  1.2205e-01, -6.0301e-01,
         2.1815e-01, -4.1466e-01,  2.8104e-01,  2.2488e-01,  2.1195e-01,
         6.2088e-01, -3.6156e-02,  3.3130e-01,  4.1718e-01,  1.6908e-01,
         2.7611e-01, -2.8473e-01, -4.3983e-01, -3.5778e-01, -2.0570e-01,
         3.3422e-01,  7.7666e-02,  2.7353e-01, -2.2929e-01,  1.2507e-01,
        -2.6608e-01,  2.7955e-01, -6.9187e-02,  2.0744e-01,  6.2913e-01,
        -4.7449e-02, -7.7963e-02, -4.2061e-02,  1.1904e-01,  2.7021e-01,
         2.4009e-01, -2.0361e-01, -5.1800e-01,  4.2063e-01,  7.7571e-02,
        -3.7487e-01, -1.0334e-01, -2.8719e-01,  1.6545e-01,  6.4722e-01,
         2.0639e-01,  6.9956e-02, -1.2094e-01,  1.9477e-01, -2.2353e-01,
        -2.8806e-01, -5.5906e-02, -4.3698e-01,  2.4996e-01, -3.2311e-01,
        -7.6289e-03,  3.2002e-01,  5.8519e-02, -3.7117e-01,  1.3092e-01,
        -3.5996e-01, -3.2252e-01,  8.4701e-02, -1.9236e-01, -1.1628e-01,
        -1.6623e-01,  1.3229e-01,  3.9279e-02, -3.4568e-01,  2.8687e-01,
         1.0365e-04,  3.4252e-01, -4.1116e-01,  1.1817e-01, -6.7696e-02,
        -9.3700e-02, -3.3893e-02, -5.0696e-01,  4.5188e-01,  5.5303e-02,
        -2.3416e-01,  2.4825e-01,  4.7991e-01, -8.8871e-02,  1.6317e-01,
        -4.9000e-02,  6.1902e-02,  3.1430e-01, -4.0040e-01,  2.2202e-01,
         5.3194e-01, -6.7501e-01, -4.1207e-02, -2.2033e-01, -5.4734e-01,
        -3.7292e-01, -2.1342e-02, -2.9803e-01,  3.5128e-01,  4.5594e-01,
         1.0907e-01, -3.4712e-01, -1.0442e-01,  4.1210e-02, -1.7895e-01,
         2.1669e-01, -5.9524e-01, -4.8311e-01,  1.7753e-01,  1.1325e-01,
        -3.0399e-01,  1.1610e-01,  5.3027e-01, -3.3722e-01,  2.2936e-02,
        -4.5464e-01, -2.3298e-01, -3.1652e-02,  4.0282e-01,  4.0792e-01,
         4.4835e-02,  1.8021e-01,  2.1295e-01, -1.3891e-01,  2.5734e-01,
        -3.9804e-01,  3.5444e-02, -2.1211e-01, -2.1667e-01, -8.0195e-01,
         4.3522e-01, -6.2927e-03,  3.0222e-01, -6.3260e-02, -1.7499e-01,
        -4.2224e-01, -2.0100e-02, -1.7991e-01, -2.7192e-01, -9.7680e-02,
         6.7486e-01, -1.2820e-01,  1.9117e-01, -2.1720e-01,  4.1311e-01,
        -3.9284e-01,  2.2391e-01, -4.3222e-02, -1.8916e-01, -1.3163e-01,
        -1.2097e-01, -2.3999e-02, -7.0565e-02,  3.2697e-01,  2.1047e-01,
         3.0648e-01,  1.0702e-01,  2.5062e-01, -1.3994e-01, -1.4183e-01,
        -3.1237e-01,  1.4234e-01, -3.8979e-01, -7.1782e-01,  4.5149e-01,
         6.1514e-02,  2.9931e-01,  4.6365e-01,  1.8079e-01,  3.4996e-02,
        -1.5266e-01, -3.2438e-02,  1.8433e-03,  3.0434e-01, -7.2029e-02,
        -1.1054e-01, -2.6927e-01, -1.2103e-01,  1.8328e-01, -2.9967e-01,
        -2.0482e-01,  5.9222e-01,  2.7238e-01, -9.4106e-02, -1.2597e-01,
        -1.4423e-03, -3.7636e-02,  2.0951e-01, -1.2990e-01, -6.9982e-02,
        -2.9833e-01,  1.3813e-01,  4.8790e-01,  4.4359e-01, -5.4234e-01,
        -2.2232e-02, -4.4852e-01, -1.9540e-01, -8.1884e-02,  2.7409e-02,
         1.1340e-01,  2.6882e-01, -1.5397e-01, -3.6771e-01,  9.0708e-02,
        -3.3893e-01, -5.7200e-01,  1.9194e-01, -2.6039e-01, -2.3555e-01,
         2.3619e-01, -1.7284e-01,  1.8537e-01,  2.6478e-01,  1.6624e-01,
        -3.5986e-01,  2.2010e-01, -1.9533e-01, -5.0837e-03, -2.3669e-01,
        -4.9835e-01, -2.9042e-01], device='cuda:6', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.4.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[ 0.3972,  0.3975, -0.1944,  ...,  0.0451, -0.0539,  0.7222],
        [-0.2872,  0.1173, -0.2501,  ..., -0.7904,  0.3061,  0.1683],
        [-0.6917, -0.0403,  0.4510,  ..., -0.0319,  0.1355, -0.4603],
        ...,
        [ 0.2451,  0.1871,  0.3339,  ...,  0.4404,  0.3697,  0.4092],
        [-0.0307, -0.1040,  0.1707,  ..., -0.4251, -0.1476,  0.0106],
        [ 0.1185, -0.0095, -0.1427,  ...,  0.2278, -0.2117, -0.1130]],
       device='cuda:6', requires_grad=True)
net_guide.net.4.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.4.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 4.4475e-01,  3.6144e-01, -1.1450e-01, -2.4392e-01,  1.2549e-04,
        -6.1602e-01, -1.8097e-01,  1.4764e-01, -2.6854e-02, -5.4401e-03,
         5.1756e-01, -4.6437e-01, -1.8168e-01,  3.8752e-01,  3.5092e-01,
        -1.1618e-02, -3.2023e-01, -8.1183e-01,  1.3987e-01,  9.5674e-02,
        -7.8190e-02, -2.5810e-01,  1.0539e-01,  2.2763e-01, -6.7652e-01,
         2.3447e-01,  4.6840e-01, -2.3928e-01, -2.4765e-01, -7.0773e-01,
        -1.5755e-01,  3.1882e-01, -1.6785e-01,  2.5806e-02,  2.0974e-02,
        -3.5404e-02, -2.9708e-01, -4.3045e-01, -1.0476e-01, -3.2180e-02,
        -7.5227e-01, -9.8177e-03, -1.5571e-01,  2.7464e-01, -4.5045e-01,
        -3.4861e-01, -2.9475e-01,  4.4677e-02, -1.0678e-01, -3.7808e-01,
         6.4116e-01, -4.6914e-02, -5.2397e-01, -6.8028e-01,  3.5681e-01,
         2.3569e-01,  1.9865e-01,  9.3342e-02,  1.6636e-01,  2.0993e-01,
         7.2304e-01,  4.4697e-01, -2.6788e-01, -6.1836e-01, -4.4078e-01,
        -4.0112e-01,  1.7758e-01, -5.9292e-01,  3.8209e-02,  1.9440e-01,
        -1.2991e-01,  2.2282e-01,  6.1046e-01,  1.8568e-01,  6.9125e-02,
         1.7023e-01, -1.5555e-01, -5.9963e-03,  4.7534e-01, -1.7507e-01,
         5.3105e-02,  4.3135e-03, -2.5559e-01, -5.6766e-02, -4.8608e-01,
         2.0069e-01, -2.4802e-01,  3.5508e-01,  1.0954e-01, -3.8773e-01,
        -3.1113e-01,  1.8710e-01, -1.2680e-01, -2.0622e-01,  4.5339e-02,
        -2.5946e-01,  5.0477e-01,  5.1521e-01,  1.3531e-01, -5.9095e-01,
        -2.2722e-01, -2.3506e-01, -1.5470e-01,  9.9686e-02,  1.2623e-01,
        -3.0513e-01,  1.1126e-01,  3.1947e-01,  2.2652e-01,  4.3865e-01,
        -4.8350e-01,  1.2663e-01,  6.9973e-01,  4.2156e-01,  3.9497e-01,
         4.8228e-01, -1.4544e-02,  6.6089e-02, -3.2270e-01,  1.8496e-01,
        -6.4085e-01,  6.6356e-02, -7.2457e-02, -1.0624e-01,  7.3890e-01,
        -1.7485e-01, -7.7767e-02,  9.2708e-02,  2.7175e-01, -1.2201e-01,
        -4.6412e-01,  4.8653e-01, -5.8952e-01, -6.5663e-02,  4.4469e-01,
        -3.1152e-01,  9.7899e-03,  7.8209e-02, -1.8253e-01, -3.5991e-01,
        -6.4486e-02,  2.5975e-01, -3.1176e-01,  4.2651e-01,  3.3564e-01,
        -8.2698e-02,  1.3545e-01,  2.9710e-01,  2.3860e-01,  4.5077e-02,
        -2.5717e-01, -1.3302e-01, -1.9664e-01, -1.1102e-01,  9.3340e-04,
         3.0869e-02, -2.7693e-01,  4.8447e-01,  2.2304e-01, -5.4449e-01,
         1.0799e-02,  2.4213e-01,  9.1926e-02, -3.6836e-01,  3.2216e-01,
         4.2376e-02,  1.4548e-01, -3.6697e-01,  7.6295e-02,  1.8639e-01,
         1.3183e-01, -1.3762e-01, -3.5483e-01, -4.7133e-01, -7.8375e-01,
         3.7509e-01,  1.2997e-01,  2.5223e-01, -3.8667e-01,  1.5255e-01,
         9.0708e-02, -2.8837e-01, -5.2596e-01,  9.8965e-02, -3.9886e-01,
        -1.4391e-01,  1.7685e-01,  2.6134e-01, -2.1760e-03,  3.5670e-01,
         4.1758e-02,  2.9275e-01,  5.3972e-01, -2.4669e-01,  5.9261e-02,
         1.5390e-01,  4.2318e-01,  1.4982e-01,  4.5447e-02,  1.4033e-01,
         4.2405e-01,  2.4106e-01,  6.2641e-01, -1.3967e-02, -2.7233e-01,
         3.6634e-01, -1.7130e-01, -1.8776e-01,  1.7243e-01,  6.4639e-02,
         6.2848e-03, -1.1050e-01,  1.7108e-01, -1.1251e-01,  5.6565e-02,
        -3.7758e-02, -1.3211e-01,  7.2665e-02, -8.2688e-01,  6.0895e-01,
         8.3208e-02,  2.1755e-02,  2.5938e-01,  2.1753e-01,  2.7769e-01,
        -6.3326e-01, -4.3266e-01, -3.5545e-01, -3.9938e-01, -5.6347e-01,
         1.5599e-01,  5.7306e-01, -1.3195e-01,  2.2053e-01, -9.5506e-03,
        -2.3780e-02,  7.1511e-02,  8.5472e-02,  1.7517e-01,  3.7166e-02,
         3.5972e-01, -7.4823e-02,  1.9533e-01, -6.1701e-02, -2.0607e-01,
        -4.6264e-01,  6.6320e-01,  4.5489e-01,  2.5214e-01,  6.1603e-02,
         8.5759e-01, -4.0811e-01, -3.4778e-02,  1.2184e-01,  2.1137e-01,
         2.6092e-01,  3.6810e-02, -2.1603e-01,  8.5053e-02,  2.4489e-02,
        -1.9505e-01,  1.8906e-01,  2.1770e-02,  4.5019e-01, -2.7461e-02,
        -4.9028e-01,  1.1000e-01,  2.5487e-01,  2.0283e-01,  2.0706e-01,
         2.7613e-01,  5.1031e-01, -4.2380e-01, -8.5125e-02, -7.8093e-02,
        -5.8900e-01, -1.6003e-02, -7.0565e-02, -1.2087e-01,  5.5262e-02,
         2.8586e-01,  1.2440e-01, -4.8662e-01,  1.8083e-01, -4.2738e-01,
        -2.9717e-01, -2.6578e-01, -5.6899e-02, -1.2857e-01, -6.1140e-01,
         2.6007e-01,  2.2614e-01, -4.0477e-01, -1.7880e-01,  4.7570e-02,
         3.4840e-01, -2.0457e-01, -1.4111e-01, -1.1104e-01,  3.4986e-01,
        -1.9213e-01,  5.7263e-02,  2.3448e-01, -6.4622e-01,  1.4370e-01,
        -1.1014e-01,  1.7775e-01, -6.6721e-02, -7.3215e-02,  4.1639e-01,
        -2.3362e-01, -4.7994e-01, -3.2203e-01,  4.8508e-02, -1.6467e-01,
         1.9693e-01,  2.2466e-01, -5.3818e-01, -1.1354e-01,  4.1831e-01,
         4.6900e-01, -5.8902e-01,  1.4684e-01,  3.1720e-02,  1.8682e-01,
        -4.3080e-01,  1.0570e-01,  9.6972e-02, -2.2679e-01,  2.7080e-01,
         1.3210e-01, -1.1888e-01, -5.8431e-02,  4.5414e-01,  6.3704e-01,
        -2.4955e-01,  7.2121e-01, -7.8818e-01, -2.3358e-01,  3.6407e-01,
         4.2606e-01,  2.6438e-02, -3.6964e-01,  2.8929e-01,  7.3134e-02,
         1.8462e-01,  3.7197e-01, -2.1571e-01,  3.5894e-03, -1.4738e-02,
        -1.3259e-01,  2.7961e-01,  3.8464e-01, -5.6448e-01, -1.5659e-01,
        -4.2161e-01, -3.6394e-01, -5.8040e-02,  1.6679e-01,  7.1192e-02,
        -3.9058e-02,  1.2789e-01, -1.9547e-03, -6.5694e-01,  7.3774e-02,
        -2.1124e-01, -3.0008e-01, -4.1642e-01,  2.4662e-01, -8.5819e-01,
        -5.6277e-01,  2.2659e-01, -5.6136e-02, -5.5959e-02,  3.5772e-01,
        -1.0577e-01,  2.7679e-02, -4.1154e-01,  4.8989e-01, -5.0976e-02,
         3.1705e-01,  1.6724e-01,  4.9883e-01,  1.5334e-02,  7.5586e-01,
         4.6219e-01, -2.2346e-01,  2.1232e-01, -4.8906e-03, -2.0199e-01,
        -4.9326e-01, -5.2733e-01,  7.3226e-01,  2.9899e-01,  1.1054e-01,
         3.3293e-01, -6.1675e-01, -9.3133e-02, -6.4463e-03,  2.0708e-01,
         1.6003e-02,  2.2934e-02, -3.3969e-01, -2.2151e-01,  4.3836e-01,
        -1.8864e-01,  6.2207e-01,  1.8566e-01, -4.7078e-02, -3.8708e-01,
         2.9575e-01,  3.4586e-01,  2.8151e-01,  4.0881e-01, -3.1909e-01,
        -1.2016e-02,  3.6700e-01,  8.9362e-01,  1.0899e-01, -3.1441e-01,
        -3.5461e-01, -5.2737e-02,  1.8424e-01, -3.7431e-02,  5.0358e-01,
         1.5099e-01,  4.6746e-01,  1.9825e-01,  1.5831e-01, -1.0754e-01,
        -3.2662e-01, -5.4435e-01, -1.0324e+00, -1.8485e-01,  4.5123e-01,
        -2.4724e-01,  6.8222e-01, -1.5530e-01, -7.0688e-01, -4.6669e-01,
         2.9320e-02, -4.6186e-01,  4.1776e-01,  3.4229e-01, -3.0385e-02,
        -8.0790e-02,  2.2717e-01, -2.6278e-01,  1.8914e-01, -2.8767e-01,
         4.8736e-01,  2.2062e-01,  4.4707e-01, -8.0776e-02, -4.9020e-01,
         7.2708e-01,  8.4201e-01, -1.1518e-01,  4.2012e-01, -1.5906e-01,
         4.2478e-01, -1.2544e-01,  7.0801e-01, -2.0662e-01,  1.0479e-03,
         3.3033e-01, -2.8293e-01, -4.6725e-02,  3.6571e-01, -1.0842e-01,
        -2.3855e-01, -4.4600e-02,  1.3386e-01,  2.0090e-01,  2.6327e-01,
         1.8721e-01,  1.1485e-01, -2.6928e-01, -1.4087e-01,  3.2804e-01,
         3.9866e-02, -4.8909e-01,  3.2394e-02, -8.1096e-02, -3.0059e-01,
         1.4012e-01, -5.1191e-02,  3.5098e-02, -2.2219e-01,  3.0374e-01,
         2.3092e-01,  7.6172e-02, -2.4229e-01,  3.8007e-01,  1.6584e-01,
         2.3549e-01, -7.6023e-02, -1.8484e-01, -4.7856e-01, -2.2454e-01,
        -9.9628e-02,  2.2643e-01,  2.5690e-01,  1.8767e-01, -9.0638e-02,
        -1.1447e-01,  1.9966e-01,  2.5066e-01, -3.2082e-01,  8.2066e-02,
        -4.2536e-01, -2.5967e-01], device='cuda:6', requires_grad=True)
net_guide.net.4.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.5.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[-1.0850e-01, -3.2437e-02,  3.0019e-01,  4.3196e-02, -6.3372e-01,
         -3.3287e-01,  4.4848e-01,  5.5570e-02,  1.6951e-01,  4.1299e-01,
          6.8224e-02, -7.8521e-01, -1.3507e-01,  1.0049e-01,  6.1530e-01,
          2.5091e-01,  1.9211e-01,  1.0617e-01,  1.9316e-02, -1.3024e-01,
         -2.3101e-02, -7.6611e-02,  2.3131e-01,  3.3316e-01,  4.4644e-01,
         -2.8877e-01,  2.7720e-02, -1.8144e-01, -7.1458e-02,  1.1109e-01,
         -5.6830e-01,  4.3281e-02,  1.8366e-01,  2.9948e-01,  5.6387e-01,
         -5.0546e-01, -2.0859e-01,  3.0861e-01, -7.6326e-03, -3.1844e-01,
         -2.4351e-02, -5.4891e-01, -4.7614e-02,  2.7510e-01, -2.6595e-01,
          2.2862e-01, -3.4607e-01,  3.3979e-01,  1.5860e-01, -1.7544e-01,
          1.1554e-01, -1.6722e-01, -9.7347e-02, -1.5868e-01,  2.1417e-01,
         -1.5670e-01, -4.6661e-02, -8.6404e-01, -5.3488e-02, -1.9579e-02,
          5.0147e-01,  5.6139e-01, -2.1901e-01,  4.6868e-01,  3.2021e-01,
          2.7793e-01, -1.9917e-01, -8.5893e-02,  1.1933e-01,  2.8878e-01,
          3.0152e-01,  6.3660e-02,  2.6252e-01,  8.7769e-02,  4.0801e-01,
          1.5201e-01, -5.6985e-03,  2.9471e-01, -5.4099e-02, -7.6201e-02,
          2.8265e-01, -5.7994e-03, -1.3028e-01,  2.7282e-01, -3.6792e-02,
         -1.3243e-01,  5.0516e-02, -1.6269e-01, -4.2939e-02, -5.8002e-01,
          3.6390e-01, -1.4941e-01, -1.2267e-01, -3.6136e-01, -2.6709e-01,
          1.1774e-01,  1.3052e-02,  1.2631e-01, -2.2191e-01,  7.4665e-02,
          3.9686e-01, -5.5781e-01, -4.8272e-04, -1.3709e-01,  1.1541e-01,
          1.7262e-01, -5.2397e-02,  3.3917e-01,  1.9644e-01, -1.9934e-01,
          7.1267e-01, -1.2326e-01, -1.3505e-01, -7.8416e-03, -7.6050e-03,
         -2.4662e-01, -5.0325e-01, -1.4076e-02,  5.3634e-02, -2.6596e-01,
          7.3265e-02,  3.0560e-01,  1.3884e-01, -2.8343e-01, -3.0456e-01,
          7.5212e-01,  1.6735e-01,  1.2092e-02, -4.0076e-01,  2.9580e-01,
          8.3340e-01,  4.1606e-01, -3.4315e-01, -1.7640e-01,  4.1191e-01,
          3.5745e-01, -2.4257e-01,  4.1225e-01, -2.2538e-01, -2.0629e-01,
         -9.6486e-02,  1.0862e-01,  4.0714e-01, -5.3818e-01, -1.5390e-01,
          1.7397e-01,  1.0301e-01,  5.7806e-01,  3.3346e-01, -2.3932e-01,
          4.5320e-01, -1.3016e-02, -5.2145e-01, -5.6489e-02,  3.5112e-01,
          5.7114e-01, -1.1636e-02, -4.9745e-02, -2.4929e-01, -3.3273e-01,
         -3.4026e-02, -1.1258e-01, -4.1306e-01,  2.9287e-01, -2.1514e-01,
         -3.2297e-01, -5.4955e-02, -7.6313e-02,  3.4035e-01,  2.4616e-01,
         -1.4924e-01,  1.2508e-01, -4.8104e-01, -5.3232e-01, -2.9488e-01,
          6.8902e-01,  7.9384e-02, -2.5630e-01, -4.1115e-02,  5.5178e-02,
         -6.9295e-02, -6.6557e-01,  3.5499e-01,  2.9702e-01,  1.7054e-01,
         -3.1809e-01,  8.4619e-02, -5.1074e-01,  4.5727e-01,  1.3339e-01,
          4.0036e-02,  1.6209e-01,  1.7631e-01,  4.4296e-01,  5.8510e-01,
          2.2028e-02, -5.4119e-02, -1.1853e-01,  2.9013e-01,  1.9826e-01,
         -2.3869e-01, -1.1117e-01,  1.0972e-01,  6.2709e-02, -2.9770e-01,
         -1.0695e-01, -2.5326e-01,  1.0779e-01,  2.2636e-01,  3.2961e-01,
         -2.5559e-02, -2.0595e-02, -7.1178e-02,  3.9900e-01, -2.0020e-01,
         -6.7302e-02, -1.6543e-01, -1.9165e-01, -3.5209e-02, -1.5219e-01,
          3.1959e-02,  4.3578e-01, -1.6665e-01, -4.1990e-01,  3.7993e-01,
         -3.1873e-01, -1.9283e-02, -5.7463e-01, -3.3115e-01,  1.9142e-01,
          1.6335e-01,  3.2683e-02,  4.6556e-01,  1.9627e-01, -9.7838e-02,
         -2.2165e-01,  1.6565e-01,  1.0474e-01, -3.8947e-03,  4.3811e-01,
          2.6431e-01, -3.1664e-01, -3.3014e-02,  1.6046e-01, -1.1067e-01,
         -4.7155e-02, -2.1011e-01, -1.0583e-01, -3.5272e-01, -2.8083e-01,
          8.5041e-02, -5.0180e-02, -6.6570e-02, -4.3827e-01,  6.7214e-02,
          1.9761e-01,  5.7398e-01, -2.5194e-01, -4.2360e-01,  3.6045e-02,
         -3.0959e-01, -2.1142e-01,  5.4424e-03, -8.6232e-02,  3.2023e-01,
          1.3200e-01, -3.1210e-02, -4.9988e-01, -4.1524e-01, -3.9624e-01,
         -1.3779e-01, -3.0710e-02,  3.2168e-01, -7.2443e-02,  2.1549e-01,
          1.9120e-01, -4.8191e-01, -6.7570e-01,  3.2350e-01,  4.1595e-02,
          4.8446e-01, -4.3998e-01, -1.2918e-01,  4.9133e-02,  2.7971e-02,
          1.1812e-01,  6.0091e-02, -1.1925e-01, -3.8564e-01, -3.1125e-01,
         -1.5661e-01,  1.4617e-01, -9.0550e-02, -1.0962e-01, -5.9114e-02,
         -4.6785e-01,  1.5449e-02,  6.2447e-01, -6.4142e-01, -1.6555e-01,
         -3.8697e-02,  1.1704e-01, -2.1731e-01,  5.4156e-01, -3.5426e-01,
          4.8549e-02,  1.8264e-01, -5.6583e-01,  1.0575e-02,  7.0160e-02,
         -3.1165e-01, -2.3043e-01, -2.4740e-01,  2.8845e-01,  4.2257e-01,
          3.1047e-01, -7.2539e-01, -1.2941e-01, -1.4144e-01, -5.2191e-01,
         -1.3356e-01,  5.0648e-01, -3.2648e-01, -1.4667e-02, -2.3648e-01,
          3.6654e-02,  2.9567e-01, -4.9300e-01, -1.6160e-01, -2.2306e-01,
          2.3058e-01, -1.2992e-01, -3.4732e-01,  5.7994e-01, -6.4977e-01,
          8.8053e-01, -9.6221e-02,  1.6494e-01, -5.5928e-01, -1.0835e-01,
         -1.9018e-02,  6.7185e-02,  6.9397e-01,  3.6010e-01,  2.4774e-01,
         -1.3119e-01, -1.8662e-01, -2.2597e-01,  2.8511e-01,  5.5908e-01,
          9.7109e-02,  6.7138e-02, -4.7032e-01, -1.1221e-01, -1.7902e-02,
          5.6024e-01, -3.3041e-01, -6.7954e-01,  2.5849e-01,  5.1331e-01,
          7.7621e-01,  3.1852e-01,  3.3756e-01,  1.3549e-01,  5.1973e-01,
          3.8575e-01, -5.2404e-01, -2.6165e-01, -2.9003e-01, -1.7087e-01,
         -5.4384e-01, -6.0909e-02,  7.7000e-01,  1.1822e-01, -2.8712e-01,
          3.2194e-01,  1.2971e-01, -8.9460e-02,  2.8319e-01, -2.3125e-01,
          1.9225e-01,  1.6002e-01, -1.0360e-01,  3.3191e-01,  4.2876e-03,
         -5.4064e-02, -3.1146e-02,  2.4382e-01,  3.3738e-01, -1.7495e-01,
         -3.2156e-01, -2.5674e-01,  3.8949e-01,  5.4471e-02,  3.3728e-01,
         -3.3105e-01, -1.1671e-01,  1.7571e-01,  9.1090e-02, -5.0711e-01,
         -1.7015e-01,  5.1145e-02, -8.7895e-02, -1.1488e-01, -2.2075e-02,
         -3.8229e-01,  1.2051e-01, -4.1673e-01, -7.7555e-01, -2.7774e-01,
          1.3865e-01,  2.6542e-02, -4.8062e-01,  1.0781e-01,  4.6789e-02,
          9.3492e-02, -1.2328e-01,  7.5618e-02,  1.4277e-02,  1.8394e-02,
          1.3441e-02, -1.9965e-01, -3.1640e-02,  3.2855e-01,  5.6578e-01,
          1.4922e-01, -2.9974e-01, -2.4794e-02,  5.3582e-01,  2.5645e-02,
          1.7244e-01,  2.3546e-01,  3.0874e-01, -2.1049e-01, -4.8337e-02,
          5.4756e-01, -3.6268e-02,  2.5606e-01, -1.9752e-02,  5.5304e-02,
         -4.8693e-01,  4.5077e-01,  4.5243e-01, -2.2446e-01, -2.0173e-01,
         -5.4934e-01,  1.6357e-01,  7.3961e-02,  4.6122e-02, -1.9678e-01,
          4.1288e-01,  2.6719e-02, -1.4570e-02,  1.0793e-01,  2.5667e-01,
         -2.4594e-01, -2.2976e-01, -1.3281e-01,  2.9465e-01, -1.0658e-01,
          4.3821e-01,  5.5401e-01,  2.4183e-01, -3.2653e-01,  3.4061e-01,
          2.2944e-01, -2.2144e-01, -3.5676e-01, -3.1328e-01, -1.6151e-01,
          4.6745e-01,  2.1477e-01,  5.2155e-02, -2.0512e-01,  1.1477e-01,
         -1.7113e-01, -1.9347e-03,  2.0078e-01,  5.1022e-01,  1.2511e-01,
         -6.4220e-02,  5.5920e-01,  2.7994e-01, -1.5520e-01,  2.9498e-01,
         -4.2167e-02,  6.0416e-01,  1.8755e-01,  2.9129e-01,  1.2149e-01,
         -5.6154e-02,  4.9783e-01, -3.8104e-01, -1.5784e-01, -1.4870e-01,
         -2.1498e-01,  1.4377e-01,  9.8715e-02,  1.0881e-01, -3.4904e-01,
         -5.4688e-01, -3.8609e-01, -3.7619e-01,  5.9031e-02, -6.6309e-01,
         -2.7696e-01, -3.1811e-01, -5.2695e-01, -4.5306e-01, -3.5929e-01,
          5.4525e-01,  5.1683e-01]], device='cuda:6', requires_grad=True)
net_guide.net.5.weight.scale torch.Size([1, 512]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.5.bias.loc torch.Size([1]) Parameter containing:
tensor([0.5208], device='cuda:6', requires_grad=True)
net_guide.net.5.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:6', grad_fn=<AddBackward0>)
Using device: cuda:6
===== Training profile sineasy10-4x512-s03 - 1 =====
[0:00:02.175383] epoch: 0 | elbo: 20307841730.559998 | train_rmse: 448.1544 | val_rmse: 451.5146 | val_ll: -20.6979
[0:02:01.979929] epoch: 50 | elbo: 717353642.88 | train_rmse: 93.5684 | val_rmse: 129.1129 | val_ll: -7.2341
[0:03:56.974170] epoch: 100 | elbo: 383077159.03999996 | train_rmse: 58.8662 | val_rmse: 108.9792 | val_ll: -6.9489
[0:05:53.972406] epoch: 150 | elbo: 252795036.0 | train_rmse: 41.1549 | val_rmse: 99.0821 | val_ll: -6.9151
[0:07:53.933629] epoch: 200 | elbo: 184785798.4 | train_rmse: 30.1784 | val_rmse: 92.4374 | val_ll: -6.8648
[0:09:52.502484] epoch: 250 | elbo: 143465518.96 | train_rmse: 22.3989 | val_rmse: 87.5457 | val_ll: -6.9352
[0:11:52.442976] epoch: 300 | elbo: 114921069.75999999 | train_rmse: 16.9971 | val_rmse: 83.2627 | val_ll: -6.9508
[0:13:51.461254] epoch: 350 | elbo: 95272335.6 | train_rmse: 13.2703 | val_rmse: 79.7858 | val_ll: -7.0267
[0:15:48.627904] epoch: 400 | elbo: 80836666.08 | train_rmse: 10.6287 | val_rmse: 76.4149 | val_ll: -7.1251
[0:17:45.867539] epoch: 450 | elbo: 68345641.4 | train_rmse: 8.7457 | val_rmse: 73.1075 | val_ll: -7.2354
[0:19:44.092773] epoch: 500 | elbo: 57624574.4 | train_rmse: 7.2381 | val_rmse: 69.6864 | val_ll: -7.3385
[0:21:42.563692] epoch: 550 | elbo: 50039435.2 | train_rmse: 6.3954 | val_rmse: 66.4559 | val_ll: -7.4157
[0:23:39.404590] epoch: 600 | elbo: 42657262.6 | train_rmse: 5.596 | val_rmse: 63.1334 | val_ll: -7.5109
[0:25:36.574009] epoch: 650 | elbo: 36190961.8 | train_rmse: 4.9583 | val_rmse: 59.9511 | val_ll: -7.6349
[0:27:35.479966] epoch: 700 | elbo: 31223675.4 | train_rmse: 4.5084 | val_rmse: 56.8234 | val_ll: -7.7256
[0:29:34.950777] epoch: 750 | elbo: 26991661.46 | train_rmse: 4.0712 | val_rmse: 53.7282 | val_ll: -7.7825
[0:31:33.398271] epoch: 800 | elbo: 23365828.38 | train_rmse: 3.7545 | val_rmse: 50.785 | val_ll: -7.9223
[0:33:30.549190] epoch: 850 | elbo: 20241254.32 | train_rmse: 3.6748 | val_rmse: 47.8214 | val_ll: -7.8898
[0:35:28.143484] epoch: 900 | elbo: 17725811.459999997 | train_rmse: 3.2833 | val_rmse: 45.133 | val_ll: -7.9764
[0:37:26.157583] epoch: 950 | elbo: 15315233.709999999 | train_rmse: 2.9516 | val_rmse: 42.5314 | val_ll: -8.0238
[0:39:24.203616] epoch: 1000 | elbo: 13538694.209999999 | train_rmse: 2.6611 | val_rmse: 40.0599 | val_ll: -8.1745
[0:41:20.212254] epoch: 1050 | elbo: 12004012.52 | train_rmse: 2.4405 | val_rmse: 37.7663 | val_ll: -8.1781
[0:43:16.530211] epoch: 1100 | elbo: 10745205.13 | train_rmse: 2.2927 | val_rmse: 35.5703 | val_ll: -8.2848
[0:45:11.531608] epoch: 1150 | elbo: 9540171.059999999 | train_rmse: 2.1115 | val_rmse: 33.5898 | val_ll: -8.5194
[0:47:07.920586] epoch: 1200 | elbo: 8681836.62 | train_rmse: 1.9732 | val_rmse: 31.7404 | val_ll: -8.6746
[0:49:04.712273] epoch: 1250 | elbo: 7846061.57 | train_rmse: 1.7845 | val_rmse: 30.0328 | val_ll: -8.7101
[0:51:02.449216] epoch: 1300 | elbo: 7223441.745 | train_rmse: 1.605 | val_rmse: 28.3113 | val_ll: -8.8273
[0:52:59.120585] epoch: 1350 | elbo: 6650279.949999999 | train_rmse: 1.546 | val_rmse: 26.7019 | val_ll: -9.0139
[0:54:56.976898] epoch: 1400 | elbo: 6210623.399999999 | train_rmse: 1.4892 | val_rmse: 25.0687 | val_ll: -9.2856
[0:56:54.715620] epoch: 1450 | elbo: 5805596.550000001 | train_rmse: 1.2708 | val_rmse: 23.6145 | val_ll: -9.5322
[0:58:51.102561] epoch: 1500 | elbo: 5435343.610000001 | train_rmse: 1.2085 | val_rmse: 22.1313 | val_ll: -9.6706
[1:00:47.565078] epoch: 1550 | elbo: 5169082.19 | train_rmse: 1.0998 | val_rmse: 20.7045 | val_ll: -9.7681
[1:02:45.083239] epoch: 1600 | elbo: 4925493.045 | train_rmse: 1.0274 | val_rmse: 19.4658 | val_ll: -9.918
[1:04:42.856547] epoch: 1650 | elbo: 4707568.8149999995 | train_rmse: 0.9713 | val_rmse: 18.2421 | val_ll: -10.1081
[1:06:39.242104] epoch: 1700 | elbo: 4518630.185 | train_rmse: 0.8881 | val_rmse: 17.1304 | val_ll: -10.124
[1:08:37.227816] epoch: 1750 | elbo: 4360765.624999999 | train_rmse: 0.856 | val_rmse: 16.0598 | val_ll: -10.2022
[1:10:34.130908] epoch: 1800 | elbo: 4241366.36 | train_rmse: 0.7619 | val_rmse: 15.1039 | val_ll: -10.4004
[1:12:31.977691] epoch: 1850 | elbo: 4120740.1349999993 | train_rmse: 0.7087 | val_rmse: 14.2114 | val_ll: -10.5345
[1:14:27.470713] epoch: 1900 | elbo: 4020498.877499999 | train_rmse: 0.6865 | val_rmse: 13.3305 | val_ll: -10.5063
[1:16:22.476055] epoch: 1950 | elbo: 3930050.9425 | train_rmse: 0.6531 | val_rmse: 12.5333 | val_ll: -10.6263
[1:18:17.637608] epoch: 2000 | elbo: 3847140.6225 | train_rmse: 0.5897 | val_rmse: 11.7994 | val_ll: -10.8453
[1:20:15.161596] epoch: 2050 | elbo: 3774789.4475000002 | train_rmse: 0.5758 | val_rmse: 11.1134 | val_ll: -10.9163
[1:22:11.319885] epoch: 2100 | elbo: 3708275.075 | train_rmse: 0.5004 | val_rmse: 10.4944 | val_ll: -10.8556
[1:24:08.351827] epoch: 2150 | elbo: 3645988.7074999996 | train_rmse: 0.5059 | val_rmse: 9.9128 | val_ll: -10.704
[1:26:05.945781] epoch: 2200 | elbo: 3590821.2024999997 | train_rmse: 0.4689 | val_rmse: 9.3824 | val_ll: -10.6217
[1:28:03.364433] epoch: 2250 | elbo: 3537480.7125 | train_rmse: 0.4308 | val_rmse: 8.9021 | val_ll: -10.7508
[1:29:59.376844] epoch: 2300 | elbo: 3484654.625 | train_rmse: 0.4122 | val_rmse: 8.4567 | val_ll: -10.424
[1:31:55.762002] epoch: 2350 | elbo: 3437939.0275 | train_rmse: 0.3822 | val_rmse: 8.025 | val_ll: -10.1542
[1:33:50.123664] epoch: 2400 | elbo: 3389786.6700000004 | train_rmse: 0.3902 | val_rmse: 7.6179 | val_ll: -10.1004
[1:35:46.972838] epoch: 2450 | elbo: 3343160.955 | train_rmse: 0.3626 | val_rmse: 7.2504 | val_ll: -9.8999
[1:37:44.750769] epoch: 2500 | elbo: 3297299.1975 | train_rmse: 0.343 | val_rmse: 6.9041 | val_ll: -9.5898
[1:39:39.709568] epoch: 2550 | elbo: 3253655.8124999995 | train_rmse: 0.3345 | val_rmse: 6.5956 | val_ll: -9.4122
[1:41:34.657216] epoch: 2600 | elbo: 3211000.5825000005 | train_rmse: 0.3317 | val_rmse: 6.3011 | val_ll: -9.1854
[1:43:29.380273] epoch: 2650 | elbo: 3168022.47 | train_rmse: 0.3099 | val_rmse: 6.0355 | val_ll: -9.1726
[1:45:27.720378] epoch: 2700 | elbo: 3125455.005 | train_rmse: 0.2914 | val_rmse: 5.7936 | val_ll: -8.9493
[1:47:26.770615] epoch: 2750 | elbo: 3082960.9475000002 | train_rmse: 0.2759 | val_rmse: 5.5648 | val_ll: -8.7821
[1:49:25.007905] epoch: 2800 | elbo: 3041791.8275 | train_rmse: 0.2775 | val_rmse: 5.359 | val_ll: -8.6531
[1:51:21.068563] epoch: 2850 | elbo: 3001521.3975 | train_rmse: 0.259 | val_rmse: 5.1647 | val_ll: -8.4742
[1:53:19.354725] epoch: 2900 | elbo: 2960337.7649999997 | train_rmse: 0.2566 | val_rmse: 4.9932 | val_ll: -8.3663
[1:55:14.725470] epoch: 2950 | elbo: 2919699.3400000003 | train_rmse: 0.2561 | val_rmse: 4.8243 | val_ll: -8.078
[1:57:12.372031] epoch: 3000 | elbo: 2879348.555 | train_rmse: 0.2397 | val_rmse: 4.6894 | val_ll: -7.9677
[1:59:10.606474] epoch: 3050 | elbo: 2839770.16 | train_rmse: 0.2391 | val_rmse: 4.5559 | val_ll: -7.6338
[2:01:08.122267] epoch: 3100 | elbo: 2800414.5675 | train_rmse: 0.2356 | val_rmse: 4.4423 | val_ll: -7.5902
[2:03:04.063849] epoch: 3150 | elbo: 2760657.7525 | train_rmse: 0.2264 | val_rmse: 4.3208 | val_ll: -7.4038
[2:05:00.373399] epoch: 3200 | elbo: 2721850.17 | train_rmse: 0.2259 | val_rmse: 4.2187 | val_ll: -7.2581
[2:06:55.319287] epoch: 3250 | elbo: 2682429.6224999996 | train_rmse: 0.223 | val_rmse: 4.1282 | val_ll: -7.0177
[2:08:51.716388] epoch: 3300 | elbo: 2643647.1574999997 | train_rmse: 0.2292 | val_rmse: 4.0455 | val_ll: -6.9777
[2:10:49.968306] epoch: 3350 | elbo: 2605337.9899999998 | train_rmse: 0.2204 | val_rmse: 3.9686 | val_ll: -6.7942
[2:12:48.221769] epoch: 3400 | elbo: 2566614.6949999994 | train_rmse: 0.2203 | val_rmse: 3.8913 | val_ll: -6.7184
[2:14:45.057621] epoch: 3450 | elbo: 2529670.0149999997 | train_rmse: 0.2222 | val_rmse: 3.82 | val_ll: -6.5388
[2:16:42.203277] epoch: 3500 | elbo: 2490731.6574999997 | train_rmse: 0.2144 | val_rmse: 3.7572 | val_ll: -6.4231
[2:18:43.190323] epoch: 3550 | elbo: 2453570.335 | train_rmse: 0.2198 | val_rmse: 3.6897 | val_ll: -6.2978
[2:20:42.493858] epoch: 3600 | elbo: 2416349.6525 | train_rmse: 0.2427 | val_rmse: 3.6381 | val_ll: -6.1332
[2:22:41.546347] epoch: 3650 | elbo: 2378738.3425000003 | train_rmse: 0.2295 | val_rmse: 3.584 | val_ll: -6.0021
[2:24:39.810775] epoch: 3700 | elbo: 2341637.2725 | train_rmse: 0.2285 | val_rmse: 3.5381 | val_ll: -5.8013
[2:26:39.844345] epoch: 3750 | elbo: 2305035.2325000004 | train_rmse: 0.2262 | val_rmse: 3.4853 | val_ll: -5.7437
[2:28:39.197252] epoch: 3800 | elbo: 2268471.3375 | train_rmse: 0.2388 | val_rmse: 3.4397 | val_ll: -5.5463
[2:30:37.444997] epoch: 3850 | elbo: 2231682.5125 | train_rmse: 0.2363 | val_rmse: 3.3941 | val_ll: -5.414
[2:32:34.920495] epoch: 3900 | elbo: 2196002.7124999994 | train_rmse: 0.2393 | val_rmse: 3.3583 | val_ll: -5.3041
[2:34:31.326513] epoch: 3950 | elbo: 2159866.065 | train_rmse: 0.2419 | val_rmse: 3.3154 | val_ll: -5.2486
[2:36:30.167031] epoch: 4000 | elbo: 2124683.6075000004 | train_rmse: 0.2466 | val_rmse: 3.2686 | val_ll: -5.055
[2:38:30.125329] epoch: 4050 | elbo: 2089466.3524999998 | train_rmse: 0.2503 | val_rmse: 3.2322 | val_ll: -4.9103
[2:40:28.876714] epoch: 4100 | elbo: 2053996.5325000002 | train_rmse: 0.2597 | val_rmse: 3.1987 | val_ll: -4.8515
[2:42:27.596519] epoch: 4150 | elbo: 2018886.5999999996 | train_rmse: 0.2569 | val_rmse: 3.1619 | val_ll: -4.7265
[2:44:24.298889] epoch: 4200 | elbo: 1984418.3724999998 | train_rmse: 0.2655 | val_rmse: 3.1301 | val_ll: -4.6151
[2:46:20.899889] epoch: 4250 | elbo: 1949814.7550000001 | train_rmse: 0.2679 | val_rmse: 3.0998 | val_ll: -4.4822
[2:48:17.387100] epoch: 4300 | elbo: 1915962.2087499998 | train_rmse: 0.2762 | val_rmse: 3.0588 | val_ll: -4.412
[2:50:14.422760] epoch: 4350 | elbo: 1881957.3337499998 | train_rmse: 0.2818 | val_rmse: 3.0267 | val_ll: -4.2702
[2:52:09.024155] epoch: 4400 | elbo: 1848105.6975000002 | train_rmse: 0.2861 | val_rmse: 2.999 | val_ll: -4.1686
[2:54:04.519439] epoch: 4450 | elbo: 1814817.98 | train_rmse: 0.3014 | val_rmse: 2.9671 | val_ll: -4.1106
[2:55:59.449642] epoch: 4500 | elbo: 1781975.04375 | train_rmse: 0.307 | val_rmse: 2.9391 | val_ll: -4.0426
[2:57:56.720616] epoch: 4550 | elbo: 1749502.3575000004 | train_rmse: 0.3147 | val_rmse: 2.9117 | val_ll: -3.9221
[2:59:54.265548] epoch: 4600 | elbo: 1717284.1262499995 | train_rmse: 0.311 | val_rmse: 2.888 | val_ll: -3.8716
[3:01:51.720291] epoch: 4650 | elbo: 1685024.8412499998 | train_rmse: 0.3178 | val_rmse: 2.8525 | val_ll: -3.7641
[3:03:48.290668] epoch: 4700 | elbo: 1653374.0374999999 | train_rmse: 0.3337 | val_rmse: 2.8298 | val_ll: -3.7417
[3:05:44.241653] epoch: 4750 | elbo: 1621312.25625 | train_rmse: 0.334 | val_rmse: 2.7973 | val_ll: -3.6148
[3:07:42.021488] epoch: 4800 | elbo: 1591231.04875 | train_rmse: 0.348 | val_rmse: 2.7687 | val_ll: -3.5498
[3:09:38.474351] epoch: 4850 | elbo: 1560373.7012500002 | train_rmse: 0.3544 | val_rmse: 2.7482 | val_ll: -3.5765
[3:11:37.441581] epoch: 4900 | elbo: 1530084.6412499999 | train_rmse: 0.3569 | val_rmse: 2.7168 | val_ll: -3.4389
[3:13:35.079153] epoch: 4950 | elbo: 1500105.6349999998 | train_rmse: 0.3698 | val_rmse: 2.6941 | val_ll: -3.3873
[3:15:32.847564] epoch: 5000 | elbo: 1469828.3487500001 | train_rmse: 0.3693 | val_rmse: 2.664 | val_ll: -3.3415
[3:17:28.151202] epoch: 5050 | elbo: 1440733.065 | train_rmse: 0.3769 | val_rmse: 2.6315 | val_ll: -3.2702
[3:19:22.762345] epoch: 5100 | elbo: 1411187.22125 | train_rmse: 0.3919 | val_rmse: 2.6071 | val_ll: -3.2242
[3:21:17.992411] epoch: 5150 | elbo: 1381489.145 | train_rmse: 0.3879 | val_rmse: 2.5727 | val_ll: -3.1406
[3:23:13.207786] epoch: 5200 | elbo: 1353594.1724999999 | train_rmse: 0.4071 | val_rmse: 2.5536 | val_ll: -3.1383
[3:25:09.177967] epoch: 5250 | elbo: 1325052.0312499998 | train_rmse: 0.4119 | val_rmse: 2.5186 | val_ll: -3.1053
[3:27:06.537629] epoch: 5300 | elbo: 1296899.7075 | train_rmse: 0.3985 | val_rmse: 2.4803 | val_ll: -3.013
[3:29:03.123115] epoch: 5350 | elbo: 1268619.9049999998 | train_rmse: 0.4143 | val_rmse: 2.4499 | val_ll: -2.9715
[3:30:59.845086] epoch: 5400 | elbo: 1241752.41375 | train_rmse: 0.421 | val_rmse: 2.4176 | val_ll: -2.949
[3:32:55.440591] epoch: 5450 | elbo: 1214789.6275 | train_rmse: 0.4225 | val_rmse: 2.3804 | val_ll: -2.8716
[3:34:50.753692] epoch: 5500 | elbo: 1187994.82375 | train_rmse: 0.4353 | val_rmse: 2.3526 | val_ll: -2.8305
[3:36:46.504061] epoch: 5550 | elbo: 1160819.1124999998 | train_rmse: 0.4276 | val_rmse: 2.3127 | val_ll: -2.8074
[3:38:43.046480] epoch: 5600 | elbo: 1135313.0 | train_rmse: 0.4464 | val_rmse: 2.2826 | val_ll: -2.735
[3:40:40.860878] epoch: 5650 | elbo: 1108231.48125 | train_rmse: 0.4424 | val_rmse: 2.2399 | val_ll: -2.6644
[3:42:36.002452] epoch: 5700 | elbo: 1082796.315 | train_rmse: 0.4418 | val_rmse: 2.2052 | val_ll: -2.6183
[3:44:30.459139] epoch: 5750 | elbo: 1057865.0475 | train_rmse: 0.4479 | val_rmse: 2.1639 | val_ll: -2.598
[3:46:25.733671] epoch: 5800 | elbo: 1033220.3018749999 | train_rmse: 0.4569 | val_rmse: 2.1267 | val_ll: -2.5363
[3:48:22.760914] epoch: 5850 | elbo: 1008281.9762500001 | train_rmse: 0.4632 | val_rmse: 2.0854 | val_ll: -2.4835
[3:50:19.072722] epoch: 5900 | elbo: 983522.9962500001 | train_rmse: 0.4531 | val_rmse: 2.0457 | val_ll: -2.455
[3:52:16.228647] epoch: 5950 | elbo: 959917.0731250001 | train_rmse: 0.4643 | val_rmse: 2.0015 | val_ll: -2.4024
[3:54:14.618549] epoch: 6000 | elbo: 936701.484375 | train_rmse: 0.4699 | val_rmse: 1.9646 | val_ll: -2.3612
[3:56:11.713162] epoch: 6050 | elbo: 912588.1050000001 | train_rmse: 0.4716 | val_rmse: 1.923 | val_ll: -2.3117
[3:58:08.268808] epoch: 6100 | elbo: 889804.878125 | train_rmse: 0.4697 | val_rmse: 1.883 | val_ll: -2.2902
[4:00:04.864377] epoch: 6150 | elbo: 867315.286875 | train_rmse: 0.4694 | val_rmse: 1.8405 | val_ll: -2.2251
[4:02:00.488884] epoch: 6200 | elbo: 844600.1262500001 | train_rmse: 0.4831 | val_rmse: 1.8027 | val_ll: -2.2047
[4:03:57.278372] epoch: 6250 | elbo: 822720.2649999999 | train_rmse: 0.4745 | val_rmse: 1.7615 | val_ll: -2.1677
[4:05:57.398285] epoch: 6300 | elbo: 800931.2537499999 | train_rmse: 0.4788 | val_rmse: 1.7208 | val_ll: -2.1429
[4:07:54.862059] epoch: 6350 | elbo: 779661.1443749999 | train_rmse: 0.4805 | val_rmse: 1.6827 | val_ll: -2.0871
[4:09:50.799597] epoch: 6400 | elbo: 758569.1325000001 | train_rmse: 0.4867 | val_rmse: 1.6493 | val_ll: -2.0864
[4:11:48.744663] epoch: 6450 | elbo: 738286.49875 | train_rmse: 0.4835 | val_rmse: 1.6057 | val_ll: -2.0371
[4:13:45.777993] epoch: 6500 | elbo: 717950.6706249999 | train_rmse: 0.4846 | val_rmse: 1.5697 | val_ll: -1.9934
[4:15:42.418137] epoch: 6550 | elbo: 698140.4025 | train_rmse: 0.4849 | val_rmse: 1.538 | val_ll: -1.9696
[4:17:38.930353] epoch: 6600 | elbo: 678098.1881250001 | train_rmse: 0.4859 | val_rmse: 1.4994 | val_ll: -1.9309
[4:19:36.676686] epoch: 6650 | elbo: 658751.83375 | train_rmse: 0.4916 | val_rmse: 1.466 | val_ll: -1.919
[4:21:32.424073] epoch: 6700 | elbo: 640373.6431250001 | train_rmse: 0.4924 | val_rmse: 1.4317 | val_ll: -1.8864
[4:23:28.235116] epoch: 6750 | elbo: 621736.715625 | train_rmse: 0.4847 | val_rmse: 1.3971 | val_ll: -1.8492
[4:25:25.368596] epoch: 6800 | elbo: 603982.1425 | train_rmse: 0.4842 | val_rmse: 1.3637 | val_ll: -1.812
[4:27:21.291761] epoch: 6850 | elbo: 586509.08375 | train_rmse: 0.486 | val_rmse: 1.3368 | val_ll: -1.7992
[4:29:17.378181] epoch: 6900 | elbo: 568979.5912499999 | train_rmse: 0.4891 | val_rmse: 1.3125 | val_ll: -1.7858
[4:31:13.735694] epoch: 6950 | elbo: 552336.5012500001 | train_rmse: 0.4835 | val_rmse: 1.2823 | val_ll: -1.7413
[4:33:10.888189] epoch: 7000 | elbo: 536020.689375 | train_rmse: 0.4856 | val_rmse: 1.2589 | val_ll: -1.7478
[4:35:08.456767] epoch: 7050 | elbo: 519870.41593749996 | train_rmse: 0.481 | val_rmse: 1.2349 | val_ll: -1.7328
[4:37:04.873967] epoch: 7100 | elbo: 504798.17281250004 | train_rmse: 0.4845 | val_rmse: 1.214 | val_ll: -1.7483
[4:39:02.478464] epoch: 7150 | elbo: 489023.68656249996 | train_rmse: 0.474 | val_rmse: 1.1837 | val_ll: -1.7129
[4:40:57.520837] epoch: 7200 | elbo: 474344.633125 | train_rmse: 0.4769 | val_rmse: 1.1654 | val_ll: -1.7045
[4:42:54.350004] epoch: 7250 | elbo: 460185.3440625 | train_rmse: 0.4757 | val_rmse: 1.1441 | val_ll: -1.7137
[4:44:52.467279] epoch: 7300 | elbo: 445732.4846875 | train_rmse: 0.471 | val_rmse: 1.1241 | val_ll: -1.7047
[4:46:50.190068] epoch: 7350 | elbo: 432227.22874999995 | train_rmse: 0.4664 | val_rmse: 1.1022 | val_ll: -1.6772
[4:48:46.430862] epoch: 7400 | elbo: 418846.541875 | train_rmse: 0.4631 | val_rmse: 1.0824 | val_ll: -1.6735
[4:50:42.598691] epoch: 7450 | elbo: 405821.7040624999 | train_rmse: 0.4628 | val_rmse: 1.0661 | val_ll: -1.6605
[4:52:41.149004] epoch: 7500 | elbo: 393614.01999999996 | train_rmse: 0.4568 | val_rmse: 1.0505 | val_ll: -1.6543
[4:54:36.433254] epoch: 7550 | elbo: 381709.17156250007 | train_rmse: 0.4548 | val_rmse: 1.0349 | val_ll: -1.6604
[4:56:31.804140] epoch: 7600 | elbo: 369887.641875 | train_rmse: 0.4527 | val_rmse: 1.0214 | val_ll: -1.6566
[4:58:27.562562] epoch: 7650 | elbo: 358377.30281250004 | train_rmse: 0.4564 | val_rmse: 1.0109 | val_ll: -1.6691
[5:00:23.226339] epoch: 7700 | elbo: 347510.30874999997 | train_rmse: 0.4416 | val_rmse: 0.9912 | val_ll: -1.6369
[5:02:19.838032] epoch: 7750 | elbo: 337344.4015625 | train_rmse: 0.4396 | val_rmse: 0.9818 | val_ll: -1.6445
[5:04:16.702671] epoch: 7800 | elbo: 327016.3665625 | train_rmse: 0.4381 | val_rmse: 0.967 | val_ll: -1.6184
[5:06:13.398789] epoch: 7850 | elbo: 317364.0040625 | train_rmse: 0.4397 | val_rmse: 0.9574 | val_ll: -1.648
[5:08:10.483625] epoch: 7900 | elbo: 307934.0065625 | train_rmse: 0.4297 | val_rmse: 0.943 | val_ll: -1.6117
[5:10:30.970626] epoch: 7950 | elbo: 298874.28875 | train_rmse: 0.427 | val_rmse: 0.9327 | val_ll: -1.6142
[5:12:36.981009] epoch: 8000 | elbo: 290585.35312499997 | train_rmse: 0.4259 | val_rmse: 0.9236 | val_ll: -1.6367
[5:14:35.690702] epoch: 8050 | elbo: 282009.21625000006 | train_rmse: 0.4171 | val_rmse: 0.9078 | val_ll: -1.5625
[5:16:31.950288] epoch: 8100 | elbo: 273902.5553125 | train_rmse: 0.4126 | val_rmse: 0.897 | val_ll: -1.5725
[5:18:28.739744] epoch: 8150 | elbo: 266363.9584375 | train_rmse: 0.4166 | val_rmse: 0.8907 | val_ll: -1.5892
[5:20:23.597313] epoch: 8200 | elbo: 258964.665625 | train_rmse: 0.4063 | val_rmse: 0.876 | val_ll: -1.5539
[5:22:18.458612] epoch: 8250 | elbo: 251974.87390624997 | train_rmse: 0.3995 | val_rmse: 0.8655 | val_ll: -1.53
[5:24:12.435075] epoch: 8300 | elbo: 245049.29562499997 | train_rmse: 0.3985 | val_rmse: 0.8586 | val_ll: -1.5353
[5:26:09.614359] epoch: 8350 | elbo: 238917.846875 | train_rmse: 0.3916 | val_rmse: 0.8464 | val_ll: -1.5078
[5:28:04.447159] epoch: 8400 | elbo: 232703.41390625 | train_rmse: 0.397 | val_rmse: 0.8425 | val_ll: -1.5166
[5:29:58.614071] epoch: 8450 | elbo: 226711.1921875 | train_rmse: 0.3865 | val_rmse: 0.8297 | val_ll: -1.4707
[5:31:51.907085] epoch: 8500 | elbo: 220500.31671875 | train_rmse: 0.3852 | val_rmse: 0.8222 | val_ll: -1.4399
[5:33:46.182153] epoch: 8550 | elbo: 215135.53015625002 | train_rmse: 0.3798 | val_rmse: 0.8135 | val_ll: -1.4254
[5:35:39.306028] epoch: 8600 | elbo: 210192.41140624997 | train_rmse: 0.3805 | val_rmse: 0.8097 | val_ll: -1.4533
[5:37:31.629853] epoch: 8650 | elbo: 204881.96578124998 | train_rmse: 0.3744 | val_rmse: 0.8012 | val_ll: -1.4147
[5:39:25.376396] epoch: 8700 | elbo: 200257.601875 | train_rmse: 0.3712 | val_rmse: 0.7935 | val_ll: -1.4129
[5:41:18.815312] epoch: 8750 | elbo: 195381.67453125 | train_rmse: 0.3705 | val_rmse: 0.7869 | val_ll: -1.3879
[5:43:12.254182] epoch: 8800 | elbo: 190997.775 | train_rmse: 0.3668 | val_rmse: 0.779 | val_ll: -1.3857
[5:45:06.996450] epoch: 8850 | elbo: 186718.51343750002 | train_rmse: 0.3653 | val_rmse: 0.7734 | val_ll: -1.3814
[5:47:02.021684] epoch: 8900 | elbo: 182985.14984375 | train_rmse: 0.3642 | val_rmse: 0.7663 | val_ll: -1.3657
[5:48:56.319979] epoch: 8950 | elbo: 178907.90015625 | train_rmse: 0.3603 | val_rmse: 0.7618 | val_ll: -1.3554
[5:50:49.675540] epoch: 9000 | elbo: 175721.19390625 | train_rmse: 0.3603 | val_rmse: 0.7569 | val_ll: -1.3559
[5:52:44.632227] epoch: 9050 | elbo: 171447.62640625 | train_rmse: 0.3566 | val_rmse: 0.7501 | val_ll: -1.3433
[5:54:38.884699] epoch: 9100 | elbo: 167953.00546875 | train_rmse: 0.3544 | val_rmse: 0.7453 | val_ll: -1.3252
[5:56:35.152562] epoch: 9150 | elbo: 164844.45515624998 | train_rmse: 0.353 | val_rmse: 0.7404 | val_ll: -1.3126
[5:58:31.337465] epoch: 9200 | elbo: 161456.60843750002 | train_rmse: 0.3509 | val_rmse: 0.7346 | val_ll: -1.3051
[6:00:27.868838] epoch: 9250 | elbo: 158415.0121875 | train_rmse: 0.3517 | val_rmse: 0.7303 | val_ll: -1.2819
[6:02:24.288257] epoch: 9300 | elbo: 156416.29734375 | train_rmse: 0.348 | val_rmse: 0.7261 | val_ll: -1.3041
[6:04:17.858938] epoch: 9350 | elbo: 152643.96546875002 | train_rmse: 0.3505 | val_rmse: 0.7237 | val_ll: -1.2836
[6:06:12.256315] epoch: 9400 | elbo: 149861.83546875 | train_rmse: 0.3431 | val_rmse: 0.7152 | val_ll: -1.2607
[6:08:06.762500] epoch: 9450 | elbo: 147422.80203125 | train_rmse: 0.342 | val_rmse: 0.7117 | val_ll: -1.2536
[6:10:02.071606] epoch: 9500 | elbo: 144797.9271875 | train_rmse: 0.3411 | val_rmse: 0.7079 | val_ll: -1.2568
[6:11:58.713670] epoch: 9550 | elbo: 142424.78515625 | train_rmse: 0.3416 | val_rmse: 0.7032 | val_ll: -1.2383
[6:13:55.170722] epoch: 9600 | elbo: 139784.946875 | train_rmse: 0.3378 | val_rmse: 0.6995 | val_ll: -1.2483
[6:15:50.083939] epoch: 9650 | elbo: 138018.75281250002 | train_rmse: 0.3374 | val_rmse: 0.6953 | val_ll: -1.2122
[6:17:43.918892] epoch: 9700 | elbo: 135331.51859375 | train_rmse: 0.3361 | val_rmse: 0.6922 | val_ll: -1.2157
[6:19:39.857340] epoch: 9750 | elbo: 133438.26468750002 | train_rmse: 0.339 | val_rmse: 0.6899 | val_ll: -1.2335
[6:21:36.220851] epoch: 9800 | elbo: 131009.74710937499 | train_rmse: 0.333 | val_rmse: 0.6847 | val_ll: -1.2055
[6:23:31.323864] epoch: 9850 | elbo: 129166.64671875001 | train_rmse: 0.334 | val_rmse: 0.6801 | val_ll: -1.2035
[6:25:27.430610] epoch: 9900 | elbo: 127258.828515625 | train_rmse: 0.3339 | val_rmse: 0.678 | val_ll: -1.1824
[6:27:23.006580] epoch: 9950 | elbo: 125180.92929687502 | train_rmse: 0.3335 | val_rmse: 0.6755 | val_ll: -1.2005
Training finished in 6:29:15.878096 seconds
Saved SVI model to tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-4x512-s03/checkpoint_1.pt
File Size is 6.064614295959473 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (5): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:6 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 0.3 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-4x512-s03/checkpoint_1.pt
using device: cuda:6
====== evaluating profile sineasy10-4x512-s03 - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:03:28.068168
torch.Size([1024, 10]) torch.Size([1024, 1])
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (5): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:6 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([512, 10]) Parameter containing:
tensor([[-0.1835,  0.1500, -0.3897,  ...,  0.3757, -0.0117,  0.2174],
        [ 0.1416,  0.1143, -0.9134,  ..., -0.1834,  0.9524,  0.0090],
        [ 0.1579, -0.1001, -0.0361,  ...,  0.3049,  0.0464, -0.4223],
        ...,
        [-0.3368,  0.1723, -0.0942,  ...,  0.1908, -0.1991,  0.4137],
        [-0.3908,  0.1906, -0.1231,  ..., -0.3708,  0.4022,  0.7265],
        [ 0.0302, -0.0700,  0.4324,  ...,  0.1134, -0.2829, -0.6205]],
       device='cuda:6', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([512, 10]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-1.7823e-01,  5.0294e-01, -5.8265e-01,  1.3545e-01, -1.3441e-01,
         7.4781e-02, -4.0722e-01, -3.0516e-02,  6.6712e-01, -5.9947e-02,
         6.5102e-01, -5.5580e-01,  4.0616e-01,  5.3144e-01,  3.7917e-01,
        -3.8019e-02, -3.0921e-01, -1.0750e-01, -2.1006e-02, -7.8506e-02,
        -2.1181e-01,  3.8762e-01, -5.6436e-02, -8.5820e-02,  9.6550e-02,
        -1.6387e-01, -1.8363e-01,  1.4983e-01,  3.3374e-01,  2.3622e-01,
         2.6034e-01, -1.3371e-01, -4.1936e-01, -8.8920e-02,  4.0596e-03,
        -5.7847e-01,  4.8493e-02, -1.3689e-01,  2.4492e-01,  4.4066e-04,
        -1.0737e-01, -1.4807e-02, -1.1360e-02, -1.3731e-01, -1.3159e-02,
         3.7334e-01, -2.6897e-01,  1.5575e-01, -1.3909e-01,  1.5582e-01,
        -5.6248e-02,  2.3335e-01, -8.2304e-02,  3.9618e-01,  1.7103e-01,
         2.5977e-02,  6.3141e-02,  1.0835e-01, -3.9917e-01,  1.7405e-01,
         3.7544e-01, -2.5445e-01, -6.6561e-02, -4.5028e-02,  1.7191e-01,
         2.1605e-01, -4.3711e-01, -1.2801e-01,  1.9165e-01,  4.8792e-01,
        -5.8266e-01, -3.8354e-01,  4.4461e-02,  5.3406e-01,  5.3733e-01,
         2.1702e-02,  4.6778e-01,  1.0149e-01,  1.6570e-01,  5.7628e-02,
         3.6372e-02,  6.5085e-02, -4.9143e-01, -5.6108e-01,  2.7023e-01,
        -7.7419e-02,  1.2483e-01,  6.0247e-02,  1.4922e-01, -1.7925e-01,
        -5.3547e-01,  7.9142e-01,  3.5786e-01,  1.8893e-02,  4.7649e-01,
         1.1538e-01, -7.1701e-01,  8.9792e-02, -6.7074e-01,  1.1091e-01,
         1.3316e-01, -1.1757e-01, -6.5029e-01, -1.2018e-01, -3.0556e-01,
         1.8410e-01, -5.9650e-01, -2.5510e-01, -5.9243e-01,  3.6801e-01,
        -3.7799e-01,  1.4594e-01, -5.2876e-01,  2.5049e-01, -3.8332e-01,
         3.4906e-01, -2.2868e-01, -1.3695e-01, -1.2946e-01, -3.4877e-02,
         3.8910e-01, -2.2079e-01, -5.8160e-01, -1.0523e-01,  6.1920e-01,
        -5.5133e-01, -1.3600e-01, -4.2393e-01,  9.0929e-03,  3.2082e-01,
         2.6637e-02, -1.5651e-01, -2.0505e-01,  6.5848e-01,  4.0317e-01,
         5.6654e-01, -3.4408e-01,  3.6509e-01,  2.7423e-01, -4.4687e-01,
         1.2560e-01,  3.4853e-01,  5.3357e-02,  6.9369e-03, -5.7181e-01,
        -1.6457e-01,  4.5253e-01, -2.2041e-01,  4.8412e-01, -1.3008e-01,
        -1.1653e-01, -6.2746e-02, -1.8840e-01, -2.0879e-01, -2.0041e-01,
        -4.6775e-02,  1.0068e-01, -1.4608e-01,  2.0044e-01,  6.8071e-01,
         2.7867e-01,  3.0461e-01,  2.6189e-01, -1.2851e-01,  2.6575e-01,
         7.6081e-02,  1.1064e-01,  5.6273e-01, -3.6695e-01,  6.9775e-02,
        -1.5413e-01,  2.0223e-01, -5.5553e-01, -2.8271e-01,  2.9218e-01,
        -4.1837e-01,  1.5541e-01,  3.4703e-01, -1.6147e-01,  2.6572e-01,
         2.0150e-01, -2.0140e-01,  5.8032e-02, -3.0731e-01, -3.2231e-01,
        -2.9253e-01,  3.9034e-01, -3.0781e-01,  2.1896e-01, -1.4347e-01,
        -2.0774e-01, -1.9129e-02, -7.8975e-02,  6.1907e-02,  2.0551e-01,
         7.5416e-01, -3.6009e-01, -3.9572e-01, -1.6817e-02, -2.9420e-01,
         2.0326e-01, -1.0500e-01,  2.6212e-01, -5.6098e-02,  3.0379e-01,
        -7.1649e-01,  2.5382e-01,  6.4169e-02, -2.4733e-02, -3.3224e-01,
         1.7891e-01, -1.1408e-01,  4.3750e-01,  6.1905e-01, -3.2717e-01,
         2.8304e-01,  5.8473e-02,  4.1732e-02, -4.1793e-01,  3.1945e-02,
         5.2301e-01, -4.7642e-01,  3.0189e-01, -1.0101e-02, -3.8748e-01,
         1.9294e-02, -1.9180e-01,  1.3966e-01,  6.0038e-01,  6.9607e-01,
        -1.7796e-01, -2.8247e-01,  1.5151e-02,  1.1559e-01, -2.4949e-01,
         2.4764e-01, -1.8608e-01, -1.8710e-01,  2.1646e-01, -5.1569e-01,
        -3.3553e-02,  2.0670e-01,  4.7244e-01, -5.4790e-02,  3.4801e-01,
         4.7386e-01,  7.4456e-02, -3.4960e-02,  1.3684e-02, -1.9884e-02,
         4.7523e-01,  3.1636e-01,  2.8267e-01, -6.1449e-01,  2.6332e-01,
        -4.5052e-01,  3.3751e-01, -1.7049e-01, -3.5623e-01, -7.4953e-01,
         2.8320e-02, -1.2254e-01,  5.1217e-01, -3.9512e-01, -4.9254e-02,
         2.5675e-01,  1.4440e-01,  8.9358e-02,  1.6988e-01,  2.8435e-01,
        -1.3521e-01,  2.9503e-01, -1.1815e-01, -3.0179e-01,  1.8413e-02,
         7.8128e-02,  1.9856e-01, -9.7554e-02, -6.9772e-01,  5.0231e-01,
         5.4229e-01,  3.0727e-01,  7.2248e-01, -2.5087e-01,  5.6369e-02,
         3.8292e-01, -4.4517e-02, -6.4815e-02, -6.2449e-02, -1.6913e-01,
         1.1095e-01,  2.5666e-01,  2.5901e-02, -3.2260e-01, -3.3425e-01,
         7.6750e-02, -4.5165e-01, -5.5429e-01,  1.4742e-01, -3.9479e-01,
        -6.6243e-02,  4.2477e-01,  3.3378e-01, -2.2579e-01,  1.1150e-01,
         2.1304e-01, -4.4511e-01,  6.6918e-01, -2.6922e-01, -1.6936e-01,
        -1.3958e-01, -3.6294e-01, -3.5499e-01,  3.1946e-01, -3.3084e-01,
        -8.8676e-02, -2.9500e-01, -3.7574e-02, -3.6834e-01, -2.2618e-01,
         1.5549e-01, -7.3923e-01, -1.8274e-01, -2.2317e-01, -5.4449e-01,
         2.2825e-01,  6.1362e-01, -6.3564e-01,  2.0346e-01, -2.5278e-01,
        -1.8720e-01,  6.0705e-01,  7.6132e-02, -2.4135e-01, -1.2162e-01,
         3.7631e-01,  1.3138e-01,  1.2385e-01,  7.0855e-01,  2.7663e-01,
        -3.2371e-03,  4.4839e-01,  1.0249e-01, -2.4125e-01,  1.8269e-01,
        -1.8524e-01,  6.7911e-02,  1.8687e-01, -3.3226e-01,  2.7731e-01,
        -6.1685e-02, -6.4994e-01, -2.7982e-01,  5.6103e-01,  1.2059e-01,
         1.0904e-01, -2.7520e-01, -2.3984e-01, -2.0639e-01,  9.0588e-02,
         3.5252e-01,  8.6946e-03, -2.7902e-01, -2.1374e-01,  2.9388e-01,
         4.4775e-02, -3.5772e-01,  8.5682e-02,  9.1087e-02, -2.2566e-01,
         4.2784e-01,  2.5430e-01, -1.4201e-02, -4.1120e-01,  1.5796e-01,
        -1.0250e-01,  1.9909e-03,  2.4264e-01, -3.5435e-01,  9.4867e-02,
        -2.6800e-01, -2.3458e-02,  4.8180e-01,  4.3842e-01, -4.5777e-01,
        -5.3493e-01,  2.5293e-01, -1.7988e-01, -3.4911e-01,  3.7370e-01,
         7.1683e-02,  1.4495e-01, -3.0386e-01,  2.9548e-01,  1.9367e-01,
        -6.5042e-02, -3.4505e-01, -2.1409e-01,  5.1218e-01,  1.3953e-01,
        -1.9668e-01, -2.3720e-01, -2.8874e-01, -4.9342e-01,  1.4879e-01,
        -1.6595e-01, -4.6694e-02, -7.1695e-02,  1.4346e-01,  6.3961e-01,
         9.5283e-02,  2.3878e-01, -6.3243e-02,  1.9631e-03, -3.1289e-01,
         3.4219e-01,  1.4886e-01, -6.9626e-02, -3.6541e-01, -2.1746e-01,
        -2.1174e-02, -6.0332e-02,  2.1608e-01,  1.0489e-01,  4.5518e-01,
        -2.5834e-01, -2.4532e-01, -5.9218e-01, -4.0317e-01, -1.4641e-01,
         1.7277e-01, -2.5228e-01, -1.4424e-01,  8.9234e-02,  2.8347e-01,
         5.5036e-01, -1.1967e-01,  1.0841e-02,  1.4467e-01, -3.9940e-01,
        -8.6467e-02,  1.7574e-01,  1.5059e-01,  1.5513e-01, -2.8242e-02,
         2.0815e-01,  1.6971e-01, -1.3287e-01,  3.4010e-01,  1.1939e-01,
        -2.7150e-02, -4.6605e-01, -1.4778e-01,  1.9036e-01, -3.3692e-01,
         1.9554e-02, -9.2924e-03,  3.9853e-02,  1.3913e-01,  2.6812e-01,
        -8.9671e-02,  5.4844e-01,  8.7221e-02,  2.6845e-01,  4.7170e-02,
         6.9888e-01, -1.1909e-01, -2.0987e-01, -3.6266e-01,  1.2260e-01,
        -4.1352e-01, -2.1393e-01, -2.5586e-01,  3.3608e-01, -1.7097e-01,
        -2.1286e-01,  5.1641e-01,  4.9373e-01, -1.8716e-01,  5.7447e-01,
         1.9947e-01,  4.8937e-02,  2.3979e-01,  3.8402e-02, -2.4368e-01,
         2.6271e-01, -7.3902e-02, -8.6151e-01,  3.6611e-01,  6.4215e-01,
         3.8981e-01, -1.4609e-01,  4.2651e-01,  5.7394e-03, -3.9965e-01,
         2.3539e-02,  3.8235e-01, -6.5336e-02, -9.7194e-02,  3.0629e-01,
         9.4631e-01,  3.5458e-01,  5.3717e-01,  1.3678e-01, -6.8224e-01,
        -9.7560e-02,  4.5199e-02, -4.2750e-01, -1.9899e-01, -4.2412e-01,
         2.3418e-02,  2.3419e-01], device='cuda:6', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.1477, -0.0802,  0.0741,  ..., -0.2312, -0.2779,  0.0872],
        [ 0.0822, -0.2462, -0.3271,  ..., -0.0950,  0.1078,  0.6187],
        [ 0.3417,  0.1422, -0.1896,  ..., -0.0823,  0.2866,  0.0515],
        ...,
        [ 0.5165,  0.3619, -0.5512,  ...,  0.5884,  0.1077, -0.6799],
        [-0.1900,  0.3652,  0.2294,  ..., -0.0391,  0.1073,  0.3460],
        [-0.6463,  0.2119,  0.2474,  ..., -0.1109,  0.2080, -0.3046]],
       device='cuda:6', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-1.7304e-02, -7.4371e-01,  4.8193e-01, -5.7575e-01, -1.4242e-01,
         9.0947e-02, -2.8964e-01,  3.3412e-01,  5.8711e-01, -4.6437e-01,
         5.2750e-01, -4.8782e-02,  3.2854e-01, -5.6577e-01, -9.4810e-02,
        -7.4957e-01, -8.9866e-02, -7.3507e-01,  1.0449e-01,  6.8090e-02,
        -6.3063e-01, -1.0982e-01,  5.9980e-01, -4.0583e-01,  2.5301e-01,
        -4.0862e-01,  2.2811e-01, -1.4941e-01, -4.2616e-01,  1.9282e-01,
        -2.6936e-01, -3.7600e-02, -3.2158e-01,  4.3963e-01, -2.3129e-01,
        -2.4136e-01,  9.2466e-02, -1.9063e-01,  2.4385e-01,  5.5556e-02,
        -4.9878e-02, -2.6260e-02,  1.9846e-01,  1.1003e-01,  1.4934e-01,
        -1.1645e-01,  5.2751e-01, -3.7193e-01,  3.3210e-01, -4.8605e-01,
         7.3373e-02,  2.1925e-01,  1.8722e-01, -3.3734e-01,  2.8190e-02,
        -3.4027e-01,  1.9335e-01,  1.8271e-01,  3.5423e-01,  4.2895e-01,
        -4.2429e-02,  3.7924e-01,  1.4194e-02, -8.8860e-02,  3.6483e-01,
         2.6001e-02,  3.1037e-01,  4.8087e-02,  4.9200e-02, -1.2883e-01,
        -1.4996e-01,  3.0200e-01,  2.5841e-01, -3.9988e-02, -1.3636e-02,
        -1.0147e-02, -2.5595e-01, -5.8228e-02,  3.4675e-01,  1.4846e-01,
        -1.1782e-01, -4.1372e-01, -1.9294e-01, -3.8107e-01,  5.3011e-01,
         4.3008e-01,  3.6775e-01,  3.1596e-01, -7.3831e-01,  6.5674e-01,
         1.7591e-01, -2.2728e-01,  2.1152e-01, -7.6497e-02, -7.9657e-01,
        -3.5461e-01, -2.9145e-01,  3.2658e-01,  3.0738e-01, -4.1942e-01,
         1.4252e-01, -2.0938e-01,  2.4464e-01, -2.5563e-01,  5.5339e-02,
         2.6314e-02,  4.4030e-01,  2.9581e-01, -1.9017e-01,  4.9778e-01,
        -3.2663e-01, -3.4342e-01, -4.7281e-02,  3.4712e-01,  2.2902e-01,
         2.6529e-01,  4.6699e-01, -6.6848e-02,  6.7599e-01,  1.1846e-01,
         1.4533e-01,  2.3124e-01,  1.3194e-01,  2.9267e-01,  1.2637e-01,
         7.5708e-01, -5.3138e-01, -3.7185e-01, -1.7431e-01, -2.8134e-01,
         2.5362e-02, -1.7231e-01,  2.3303e-01,  3.5459e-01, -1.0472e-01,
        -1.2351e-01, -1.2743e-02, -8.9211e-02,  7.6929e-02,  4.1401e-01,
         2.8890e-03,  4.9753e-01,  4.9041e-04,  3.3155e-01, -6.3475e-01,
         2.4530e-01, -7.0779e-02,  1.4113e-01, -3.1092e-02,  2.9879e-01,
        -7.7595e-01,  4.5914e-02,  4.6827e-01,  1.3830e-01, -6.2100e-01,
         1.0129e-01, -8.7237e-01, -2.7344e-01, -2.4382e-01,  1.9606e-02,
        -2.5932e-02, -1.3276e-01, -6.0999e-02, -2.2408e-01, -1.2813e-01,
         4.5391e-01,  3.5208e-01,  4.4332e-01, -4.3483e-03,  2.1009e-01,
        -1.1654e-01, -3.2487e-01, -2.3709e-01, -1.2858e-01, -1.8470e-01,
        -4.2830e-01, -2.7153e-01,  1.8645e-01,  2.7568e-01,  4.6523e-01,
        -7.9293e-01, -4.0920e-01, -3.9577e-01,  1.4098e-01, -1.1561e-01,
         9.9970e-03,  2.0233e-02, -4.4423e-01,  5.0118e-01, -4.0907e-01,
         3.0411e-01,  3.3983e-01,  4.8734e-01, -1.0515e-01, -1.9493e-01,
         1.2675e-01, -1.3493e-01,  5.3422e-01, -6.8750e-01,  6.2515e-01,
        -3.3760e-01,  2.3088e-01, -1.8721e-01, -1.2790e-01,  4.6638e-01,
         5.6233e-01,  2.3614e-01, -3.6404e-01,  5.4913e-01, -2.8064e-01,
        -1.2519e-01,  2.2452e-02,  5.4213e-01, -1.6696e-02,  3.1207e-03,
         9.1642e-02,  8.9236e-02, -1.7071e-01,  2.0936e-01, -5.7836e-02,
        -3.4964e-01,  6.7236e-02,  2.9235e-01,  5.0427e-01, -2.0986e-01,
         1.6906e-01,  1.8073e-01,  2.1306e-01, -1.3690e-01,  1.0679e-01,
         1.7766e-01,  2.6185e-01,  1.9269e-01, -6.3151e-01,  7.1693e-02,
         3.0994e-01, -3.4274e-01, -1.1084e-01, -4.6322e-01,  4.6479e-01,
         5.1805e-01,  2.2691e-01,  1.4708e-01, -1.4369e-01,  2.3180e-01,
        -6.0684e-01, -3.4615e-01, -9.8005e-02, -1.4337e-02,  2.3830e-01,
        -2.7290e-01,  5.0760e-01, -4.4781e-02, -4.6378e-02, -3.9196e-01,
         3.4800e-01,  3.4621e-01, -5.9266e-02, -6.3678e-01, -2.7233e-01,
        -9.5937e-02, -3.8816e-02,  2.7815e-01,  1.0940e-02,  2.3537e-01,
        -2.3845e-02, -5.0769e-02,  6.3709e-02, -9.4922e-02,  6.9529e-01,
        -3.8007e-01, -3.6726e-01,  1.5587e-01, -1.4668e-01, -2.9612e-01,
        -2.0268e-01,  3.0045e-01,  5.0118e-01,  1.0445e-01, -4.3017e-01,
        -1.3133e-02, -4.7267e-01,  1.0596e-01,  3.4572e-01,  5.6988e-01,
        -3.9530e-01,  1.7752e-01, -1.8111e-01, -2.1140e-01,  5.0470e-01,
         3.6142e-01, -1.8509e-01, -2.0833e-01, -2.4364e-04,  2.7748e-01,
        -3.9475e-01, -1.4549e-01, -7.9127e-02,  5.1687e-01,  1.2715e-02,
         1.6593e-01, -1.5096e-01,  2.1595e-01, -3.0374e-01,  2.2438e-01,
         1.1915e+00, -1.7656e-01,  2.1653e-01, -4.3415e-01, -8.2995e-02,
         9.6594e-02,  7.7716e-03,  7.6154e-01,  2.1252e-01, -2.5723e-01,
        -2.2339e-02, -2.8503e-01, -6.7069e-01, -6.0677e-01,  2.8146e-01,
         3.1510e-01,  2.1616e-01,  2.0367e-01,  1.6425e-01, -2.1851e-01,
         1.5391e-01, -3.7612e-01, -3.1791e-01,  3.8760e-01, -4.1480e-01,
        -1.6114e-01, -2.5449e-01, -4.5605e-01, -9.8087e-02,  7.9350e-03,
        -4.3156e-01,  5.3155e-01,  1.1264e-01,  2.4581e-01,  1.2908e-01,
         2.5088e-02, -1.9425e-01,  1.8834e-01, -4.1726e-01,  1.9716e-01,
         2.2442e-02,  1.7662e-01, -3.2100e-02, -4.5868e-01,  3.1658e-02,
        -1.5456e-01, -4.6440e-01,  9.0809e-02,  2.2375e-01,  4.6116e-01,
         1.5411e-01, -2.9951e-01,  2.5007e-01, -5.0178e-01, -1.6389e-01,
        -2.7900e-01,  5.3930e-02,  6.6744e-02, -3.2384e-01,  9.5226e-02,
        -5.5849e-01,  4.2804e-01,  3.5015e-02, -1.1938e-01, -1.4371e-01,
        -4.8624e-01, -3.8124e-01,  8.9776e-01, -8.2407e-02,  2.7250e-01,
         1.1459e-01,  1.2936e-01,  2.5598e-01,  1.4239e-01,  6.7955e-02,
         4.4034e-02,  1.0860e-01, -4.9753e-01,  6.5137e-02, -1.1185e+00,
        -1.4087e-01,  3.1329e-01,  9.7088e-02, -1.0664e-01, -4.4843e-01,
         4.8064e-01, -3.4439e-01, -2.5299e-01,  4.7642e-01, -9.8902e-02,
         8.9830e-02, -1.5237e-01,  1.5722e-02, -3.6142e-01,  1.1983e-01,
         1.0132e-01,  8.7161e-02,  1.2674e-02,  2.4100e-01,  3.2719e-01,
         1.7056e-01, -3.3684e-02, -5.5951e-02, -1.2346e-01,  9.4406e-02,
        -1.3117e-02,  3.2713e-01, -7.7023e-02, -1.4104e-01,  3.4217e-01,
         1.7002e-01,  2.4150e-01,  3.5155e-01,  1.3955e-01,  2.2147e-01,
        -2.7118e-01,  1.1829e-02,  2.2552e-02,  2.1026e-02,  2.3369e-02,
         5.2346e-01,  2.8261e-02, -2.6856e-01, -3.8788e-01, -3.6790e-01,
        -2.5159e-01,  1.4458e-01,  2.6003e-01, -6.3867e-02,  7.0197e-02,
        -1.3925e-01,  3.9376e-03, -5.1241e-02,  5.1188e-01,  1.5008e-01,
         4.6375e-01, -7.4895e-02, -3.1592e-02,  2.6546e-01,  1.5406e-01,
         6.1295e-01, -5.8630e-01, -1.0401e-01,  5.7565e-01,  3.0136e-01,
         2.0434e-01,  7.4667e-02,  5.2513e-01,  1.7537e-01, -2.3197e-01,
         3.9474e-01, -3.0074e-01, -1.3381e-01,  1.9023e-01, -1.5827e-01,
        -1.7900e-01,  3.2330e-01,  2.9406e-01, -5.1262e-01, -4.6590e-01,
         2.8354e-01, -5.7071e-01,  1.6701e-01,  3.4173e-01, -1.7136e-02,
         2.9176e-01, -1.0677e+00, -2.3840e-01, -2.3391e-01,  3.9123e-01,
         8.9054e-03,  1.6719e-01, -7.3138e-02,  3.1225e-01, -1.4398e-01,
         4.2196e-01, -4.3965e-01, -1.7356e-01,  5.5655e-01,  2.6707e-01,
        -8.8729e-02, -2.0944e-01,  1.4981e-01,  2.7648e-01,  3.8481e-01,
        -1.4325e-01,  4.7390e-01,  4.9885e-01, -1.0093e-01, -2.6477e-02,
         8.3113e-01,  1.4151e-01, -4.6067e-01, -2.1335e-01, -1.3827e-01,
        -1.9782e-01,  3.5200e-01,  4.1190e-02, -3.3886e-01, -3.2043e-01,
         1.8600e-01,  9.3158e-02, -1.0647e-01,  2.4555e-01,  2.5628e-01,
        -2.3356e-01,  2.1775e-01], device='cuda:6', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[-0.2788,  0.4712, -0.3558,  ...,  0.0852,  0.0957,  0.2607],
        [-0.1484, -0.2478,  0.0234,  ...,  0.0594,  0.5778,  0.1728],
        [-0.1258, -0.0337,  0.0160,  ...,  0.0029,  0.5243,  0.3411],
        ...,
        [-0.1701,  0.2458, -0.2898,  ..., -0.4678, -0.1546, -0.2172],
        [-0.3523,  0.2947, -0.2264,  ..., -0.2111,  0.4472, -0.2963],
        [-0.0756, -0.2880,  0.2236,  ..., -0.1032,  0.3290,  0.5057]],
       device='cuda:6', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([512]) Parameter containing:
tensor([-3.6095e-01, -2.2319e-01,  1.1103e-01, -2.3292e-01,  6.2166e-06,
        -1.5685e-01, -8.2054e-01, -3.1450e-01, -3.6688e-01, -2.4558e-01,
        -1.1119e-01,  3.1010e-01,  4.9447e-01,  5.8524e-02, -2.0650e-01,
        -3.1385e-02, -5.9704e-03,  5.4822e-01,  2.6223e-01,  5.0320e-01,
        -9.8184e-02, -7.3414e-02, -7.1630e-01, -9.9733e-02,  2.7497e-03,
         2.6648e-01,  2.8489e-01, -4.2355e-01, -2.4236e-01,  2.1705e-01,
        -5.7162e-01, -1.0769e-01,  7.3977e-02,  3.0081e-02, -4.1515e-01,
        -4.7219e-01,  2.1122e-01,  4.0944e-01, -1.2078e-01,  1.7836e-01,
         1.0095e+00,  4.1193e-01,  3.1660e-01,  3.8632e-01, -2.9621e-01,
        -2.5063e-01, -9.8408e-02, -5.0758e-01,  2.3404e-01, -1.0582e-01,
         2.3921e-01, -3.2840e-01, -5.1646e-01,  1.3984e-01,  1.0987e-01,
         2.7113e-01,  4.0299e-01, -5.9984e-01,  2.9657e-01, -1.2630e-01,
        -8.4542e-02, -2.4016e-01, -6.3478e-01, -7.7057e-01,  8.2179e-03,
         1.3906e-02, -4.2099e-01, -4.7980e-01, -3.7086e-02,  6.4549e-02,
         8.9574e-02, -3.0327e-01,  6.2960e-02, -2.8747e-01,  4.0641e-01,
         2.7348e-01,  2.7422e-01, -4.6330e-02,  8.4286e-03,  5.7026e-03,
        -1.4329e-01, -1.0820e-01,  2.9727e-01, -1.3750e-01, -3.8899e-01,
        -3.9868e-01, -1.2433e-01,  8.4280e-02, -2.8246e-01,  3.9499e-01,
         4.8705e-02, -9.7446e-02,  2.1331e-01,  8.5474e-01, -7.5803e-03,
         1.6337e-01,  1.3875e-01, -2.0394e-01, -2.9539e-01, -8.0152e-02,
         4.4473e-01,  1.4239e-01,  4.7914e-02, -5.3566e-01,  8.5798e-02,
        -3.1580e-01, -1.6829e-01,  4.4699e-01,  2.0898e-02, -1.8093e-01,
        -3.4566e-01, -7.8551e-01, -4.5781e-01, -1.2586e-01,  3.2521e-01,
         1.1650e-01,  3.1360e-01,  1.7587e-01,  8.2772e-02,  2.6422e-01,
        -4.4335e-01, -5.5629e-02,  1.6321e-01, -5.7374e-02,  1.6760e-01,
        -6.6611e-01, -4.6156e-01, -2.9009e-01, -3.4648e-01,  2.5283e-02,
        -6.7641e-01, -1.5736e-01, -1.8773e-01, -3.5351e-02, -2.5819e-01,
        -9.7859e-02,  1.2043e-01,  7.5777e-01, -3.3025e-01,  1.6098e-01,
        -4.6054e-01, -6.1820e-01, -1.3218e-01,  3.4195e-01, -9.2533e-02,
         9.5837e-02, -2.7968e-01,  1.9094e-01,  6.6922e-01, -2.3402e-02,
         4.0333e-01, -3.1954e-01, -1.2860e-01, -1.4148e-01,  8.7744e-03,
        -4.2406e-01, -1.1015e-01, -2.4406e-01,  6.5505e-02, -2.3290e-01,
         1.0324e-01, -2.6453e-01,  1.2824e-01,  3.1069e-01,  2.4128e-02,
         2.8166e-01,  2.7599e-02,  3.8887e-01, -3.8325e-01,  2.3562e-02,
        -2.0804e-01, -3.0988e-01,  3.6945e-01, -1.4312e-02,  3.7924e-01,
        -3.8777e-03, -4.3181e-02, -3.5745e-01, -3.6740e-02,  3.6153e-01,
         3.5577e-01, -1.3028e-01,  3.1018e-01, -2.7162e-02, -6.7490e-01,
        -3.9503e-01,  1.4966e-01, -2.2544e-02, -6.3600e-03,  2.3867e-01,
         5.6061e-02,  6.8928e-01,  5.2488e-01, -1.1731e-01, -7.3006e-02,
         2.1148e-01,  5.1771e-01, -4.9143e-01, -2.4439e-01, -3.5833e-01,
        -8.8262e-03,  4.1680e-03, -8.3199e-02, -6.7976e-01, -3.4704e-01,
         3.3973e-01,  2.3142e-01,  2.7203e-01, -1.9648e-01,  7.6699e-02,
         1.2691e-02, -2.2158e-01, -2.5528e-01, -1.4726e-01, -3.7426e-02,
         9.5982e-02, -3.5082e-01, -4.1665e-01, -1.3127e-01, -1.8484e-01,
        -4.5049e-01,  1.1465e-01, -3.9104e-01,  1.3391e-01, -1.2047e-01,
         2.3434e-02,  7.0874e-01, -8.7442e-01, -1.0756e-01,  1.1349e-01,
        -4.0147e-01, -4.4457e-01, -1.1596e-01, -4.8422e-01,  1.4537e-02,
         6.7207e-02,  1.1576e-01,  2.3008e-01, -1.6026e-01, -9.9004e-02,
        -4.3666e-01,  5.7932e-01,  2.2296e-02, -1.1045e-01,  2.0033e-01,
         1.7564e-02,  2.1350e-01, -6.8457e-01,  5.1056e-01,  9.6357e-02,
        -3.4199e-01,  7.5500e-01,  8.1252e-01,  1.0723e-03,  3.4866e-01,
        -6.2976e-01, -9.1457e-02, -4.6801e-01,  7.6142e-02,  4.0938e-01,
        -1.1158e-01,  2.2524e-01,  5.3375e-01, -9.1241e-02,  6.9274e-01,
        -2.3365e-01,  1.3238e-01,  1.7037e-01, -3.2833e-02, -1.4657e-01,
         2.9560e-01,  4.0805e-02,  2.3687e-01,  2.2300e-01, -4.3533e-01,
        -1.1714e-01, -9.1614e-02, -1.3742e-01,  3.0131e-01,  6.4255e-02,
         1.6029e-01, -1.1636e-01, -1.7743e-01,  7.8712e-02, -1.9594e-01,
        -1.2638e-01, -5.5543e-03, -3.8076e-02, -4.5537e-01,  3.5469e-01,
         3.0777e-01,  1.9096e-01, -1.9797e-01,  1.2205e-01, -6.0301e-01,
         2.1815e-01, -4.1466e-01,  2.8104e-01,  2.2488e-01,  2.1195e-01,
         6.2088e-01, -3.6156e-02,  3.3130e-01,  4.1718e-01,  1.6908e-01,
         2.7611e-01, -2.8473e-01, -4.3983e-01, -3.5778e-01, -2.0570e-01,
         3.3422e-01,  7.7666e-02,  2.7353e-01, -2.2929e-01,  1.2507e-01,
        -2.6608e-01,  2.7955e-01, -6.9187e-02,  2.0744e-01,  6.2913e-01,
        -4.7449e-02, -7.7963e-02, -4.2061e-02,  1.1904e-01,  2.7021e-01,
         2.4009e-01, -2.0361e-01, -5.1800e-01,  4.2063e-01,  7.7571e-02,
        -3.7487e-01, -1.0334e-01, -2.8719e-01,  1.6545e-01,  6.4722e-01,
         2.0639e-01,  6.9956e-02, -1.2094e-01,  1.9477e-01, -2.2353e-01,
        -2.8806e-01, -5.5906e-02, -4.3698e-01,  2.4996e-01, -3.2311e-01,
        -7.6289e-03,  3.2002e-01,  5.8519e-02, -3.7117e-01,  1.3092e-01,
        -3.5996e-01, -3.2252e-01,  8.4701e-02, -1.9236e-01, -1.1628e-01,
        -1.6623e-01,  1.3229e-01,  3.9279e-02, -3.4568e-01,  2.8687e-01,
         1.0365e-04,  3.4252e-01, -4.1116e-01,  1.1817e-01, -6.7696e-02,
        -9.3700e-02, -3.3893e-02, -5.0696e-01,  4.5188e-01,  5.5303e-02,
        -2.3416e-01,  2.4825e-01,  4.7991e-01, -8.8871e-02,  1.6317e-01,
        -4.9000e-02,  6.1902e-02,  3.1430e-01, -4.0040e-01,  2.2202e-01,
         5.3194e-01, -6.7501e-01, -4.1207e-02, -2.2033e-01, -5.4734e-01,
        -3.7292e-01, -2.1342e-02, -2.9803e-01,  3.5128e-01,  4.5594e-01,
         1.0907e-01, -3.4712e-01, -1.0442e-01,  4.1210e-02, -1.7895e-01,
         2.1669e-01, -5.9524e-01, -4.8311e-01,  1.7753e-01,  1.1325e-01,
        -3.0399e-01,  1.1610e-01,  5.3027e-01, -3.3722e-01,  2.2936e-02,
        -4.5464e-01, -2.3298e-01, -3.1652e-02,  4.0282e-01,  4.0792e-01,
         4.4835e-02,  1.8021e-01,  2.1295e-01, -1.3891e-01,  2.5734e-01,
        -3.9804e-01,  3.5444e-02, -2.1211e-01, -2.1667e-01, -8.0195e-01,
         4.3522e-01, -6.2927e-03,  3.0222e-01, -6.3260e-02, -1.7499e-01,
        -4.2224e-01, -2.0100e-02, -1.7991e-01, -2.7192e-01, -9.7680e-02,
         6.7486e-01, -1.2820e-01,  1.9117e-01, -2.1720e-01,  4.1311e-01,
        -3.9284e-01,  2.2391e-01, -4.3222e-02, -1.8916e-01, -1.3163e-01,
        -1.2097e-01, -2.3999e-02, -7.0565e-02,  3.2697e-01,  2.1047e-01,
         3.0648e-01,  1.0702e-01,  2.5062e-01, -1.3994e-01, -1.4183e-01,
        -3.1237e-01,  1.4234e-01, -3.8979e-01, -7.1782e-01,  4.5149e-01,
         6.1514e-02,  2.9931e-01,  4.6365e-01,  1.8079e-01,  3.4996e-02,
        -1.5266e-01, -3.2438e-02,  1.8433e-03,  3.0434e-01, -7.2029e-02,
        -1.1054e-01, -2.6927e-01, -1.2103e-01,  1.8328e-01, -2.9967e-01,
        -2.0482e-01,  5.9222e-01,  2.7238e-01, -9.4106e-02, -1.2597e-01,
        -1.4423e-03, -3.7636e-02,  2.0951e-01, -1.2990e-01, -6.9982e-02,
        -2.9833e-01,  1.3813e-01,  4.8790e-01,  4.4359e-01, -5.4234e-01,
        -2.2232e-02, -4.4852e-01, -1.9540e-01, -8.1884e-02,  2.7409e-02,
         1.1340e-01,  2.6882e-01, -1.5397e-01, -3.6771e-01,  9.0708e-02,
        -3.3893e-01, -5.7200e-01,  1.9194e-01, -2.6039e-01, -2.3555e-01,
         2.3619e-01, -1.7284e-01,  1.8537e-01,  2.6478e-01,  1.6624e-01,
        -3.5986e-01,  2.2010e-01, -1.9533e-01, -5.0837e-03, -2.3669e-01,
        -4.9835e-01, -2.9042e-01], device='cuda:6', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.4.0.weight.loc torch.Size([512, 512]) Parameter containing:
tensor([[ 0.3972,  0.3975, -0.1944,  ...,  0.0451, -0.0539,  0.7222],
        [-0.2872,  0.1173, -0.2501,  ..., -0.7904,  0.3061,  0.1683],
        [-0.6917, -0.0403,  0.4510,  ..., -0.0319,  0.1355, -0.4603],
        ...,
        [ 0.2451,  0.1871,  0.3339,  ...,  0.4404,  0.3697,  0.4092],
        [-0.0307, -0.1040,  0.1707,  ..., -0.4251, -0.1476,  0.0106],
        [ 0.1185, -0.0095, -0.1427,  ...,  0.2278, -0.2117, -0.1130]],
       device='cuda:6', requires_grad=True)
net_guide.net.4.0.weight.scale torch.Size([512, 512]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.4.0.bias.loc torch.Size([512]) Parameter containing:
tensor([ 4.4475e-01,  3.6144e-01, -1.1450e-01, -2.4392e-01,  1.2549e-04,
        -6.1602e-01, -1.8097e-01,  1.4764e-01, -2.6854e-02, -5.4401e-03,
         5.1756e-01, -4.6437e-01, -1.8168e-01,  3.8752e-01,  3.5092e-01,
        -1.1618e-02, -3.2023e-01, -8.1183e-01,  1.3987e-01,  9.5674e-02,
        -7.8190e-02, -2.5810e-01,  1.0539e-01,  2.2763e-01, -6.7652e-01,
         2.3447e-01,  4.6840e-01, -2.3928e-01, -2.4765e-01, -7.0773e-01,
        -1.5755e-01,  3.1882e-01, -1.6785e-01,  2.5806e-02,  2.0974e-02,
        -3.5404e-02, -2.9708e-01, -4.3045e-01, -1.0476e-01, -3.2180e-02,
        -7.5227e-01, -9.8177e-03, -1.5571e-01,  2.7464e-01, -4.5045e-01,
        -3.4861e-01, -2.9475e-01,  4.4677e-02, -1.0678e-01, -3.7808e-01,
         6.4116e-01, -4.6914e-02, -5.2397e-01, -6.8028e-01,  3.5681e-01,
         2.3569e-01,  1.9865e-01,  9.3342e-02,  1.6636e-01,  2.0993e-01,
         7.2304e-01,  4.4697e-01, -2.6788e-01, -6.1836e-01, -4.4078e-01,
        -4.0112e-01,  1.7758e-01, -5.9292e-01,  3.8209e-02,  1.9440e-01,
        -1.2991e-01,  2.2282e-01,  6.1046e-01,  1.8568e-01,  6.9125e-02,
         1.7023e-01, -1.5555e-01, -5.9963e-03,  4.7534e-01, -1.7507e-01,
         5.3105e-02,  4.3135e-03, -2.5559e-01, -5.6766e-02, -4.8608e-01,
         2.0069e-01, -2.4802e-01,  3.5508e-01,  1.0954e-01, -3.8773e-01,
        -3.1113e-01,  1.8710e-01, -1.2680e-01, -2.0622e-01,  4.5339e-02,
        -2.5946e-01,  5.0477e-01,  5.1521e-01,  1.3531e-01, -5.9095e-01,
        -2.2722e-01, -2.3506e-01, -1.5470e-01,  9.9686e-02,  1.2623e-01,
        -3.0513e-01,  1.1126e-01,  3.1947e-01,  2.2652e-01,  4.3865e-01,
        -4.8350e-01,  1.2663e-01,  6.9973e-01,  4.2156e-01,  3.9497e-01,
         4.8228e-01, -1.4544e-02,  6.6089e-02, -3.2270e-01,  1.8496e-01,
        -6.4085e-01,  6.6356e-02, -7.2457e-02, -1.0624e-01,  7.3890e-01,
        -1.7485e-01, -7.7767e-02,  9.2708e-02,  2.7175e-01, -1.2201e-01,
        -4.6412e-01,  4.8653e-01, -5.8952e-01, -6.5663e-02,  4.4469e-01,
        -3.1152e-01,  9.7899e-03,  7.8209e-02, -1.8253e-01, -3.5991e-01,
        -6.4486e-02,  2.5975e-01, -3.1176e-01,  4.2651e-01,  3.3564e-01,
        -8.2698e-02,  1.3545e-01,  2.9710e-01,  2.3860e-01,  4.5077e-02,
        -2.5717e-01, -1.3302e-01, -1.9664e-01, -1.1102e-01,  9.3340e-04,
         3.0869e-02, -2.7693e-01,  4.8447e-01,  2.2304e-01, -5.4449e-01,
         1.0799e-02,  2.4213e-01,  9.1926e-02, -3.6836e-01,  3.2216e-01,
         4.2376e-02,  1.4548e-01, -3.6697e-01,  7.6295e-02,  1.8639e-01,
         1.3183e-01, -1.3762e-01, -3.5483e-01, -4.7133e-01, -7.8375e-01,
         3.7509e-01,  1.2997e-01,  2.5223e-01, -3.8667e-01,  1.5255e-01,
         9.0708e-02, -2.8837e-01, -5.2596e-01,  9.8965e-02, -3.9886e-01,
        -1.4391e-01,  1.7685e-01,  2.6134e-01, -2.1760e-03,  3.5670e-01,
         4.1758e-02,  2.9275e-01,  5.3972e-01, -2.4669e-01,  5.9261e-02,
         1.5390e-01,  4.2318e-01,  1.4982e-01,  4.5447e-02,  1.4033e-01,
         4.2405e-01,  2.4106e-01,  6.2641e-01, -1.3967e-02, -2.7233e-01,
         3.6634e-01, -1.7130e-01, -1.8776e-01,  1.7243e-01,  6.4639e-02,
         6.2848e-03, -1.1050e-01,  1.7108e-01, -1.1251e-01,  5.6565e-02,
        -3.7758e-02, -1.3211e-01,  7.2665e-02, -8.2688e-01,  6.0895e-01,
         8.3208e-02,  2.1755e-02,  2.5938e-01,  2.1753e-01,  2.7769e-01,
        -6.3326e-01, -4.3266e-01, -3.5545e-01, -3.9938e-01, -5.6347e-01,
         1.5599e-01,  5.7306e-01, -1.3195e-01,  2.2053e-01, -9.5506e-03,
        -2.3780e-02,  7.1511e-02,  8.5472e-02,  1.7517e-01,  3.7166e-02,
         3.5972e-01, -7.4823e-02,  1.9533e-01, -6.1701e-02, -2.0607e-01,
        -4.6264e-01,  6.6320e-01,  4.5489e-01,  2.5214e-01,  6.1603e-02,
         8.5759e-01, -4.0811e-01, -3.4778e-02,  1.2184e-01,  2.1137e-01,
         2.6092e-01,  3.6810e-02, -2.1603e-01,  8.5053e-02,  2.4489e-02,
        -1.9505e-01,  1.8906e-01,  2.1770e-02,  4.5019e-01, -2.7461e-02,
        -4.9028e-01,  1.1000e-01,  2.5487e-01,  2.0283e-01,  2.0706e-01,
         2.7613e-01,  5.1031e-01, -4.2380e-01, -8.5125e-02, -7.8093e-02,
        -5.8900e-01, -1.6003e-02, -7.0565e-02, -1.2087e-01,  5.5262e-02,
         2.8586e-01,  1.2440e-01, -4.8662e-01,  1.8083e-01, -4.2738e-01,
        -2.9717e-01, -2.6578e-01, -5.6899e-02, -1.2857e-01, -6.1140e-01,
         2.6007e-01,  2.2614e-01, -4.0477e-01, -1.7880e-01,  4.7570e-02,
         3.4840e-01, -2.0457e-01, -1.4111e-01, -1.1104e-01,  3.4986e-01,
        -1.9213e-01,  5.7263e-02,  2.3448e-01, -6.4622e-01,  1.4370e-01,
        -1.1014e-01,  1.7775e-01, -6.6721e-02, -7.3215e-02,  4.1639e-01,
        -2.3362e-01, -4.7994e-01, -3.2203e-01,  4.8508e-02, -1.6467e-01,
         1.9693e-01,  2.2466e-01, -5.3818e-01, -1.1354e-01,  4.1831e-01,
         4.6900e-01, -5.8902e-01,  1.4684e-01,  3.1720e-02,  1.8682e-01,
        -4.3080e-01,  1.0570e-01,  9.6972e-02, -2.2679e-01,  2.7080e-01,
         1.3210e-01, -1.1888e-01, -5.8431e-02,  4.5414e-01,  6.3704e-01,
        -2.4955e-01,  7.2121e-01, -7.8818e-01, -2.3358e-01,  3.6407e-01,
         4.2606e-01,  2.6438e-02, -3.6964e-01,  2.8929e-01,  7.3134e-02,
         1.8462e-01,  3.7197e-01, -2.1571e-01,  3.5894e-03, -1.4738e-02,
        -1.3259e-01,  2.7961e-01,  3.8464e-01, -5.6448e-01, -1.5659e-01,
        -4.2161e-01, -3.6394e-01, -5.8040e-02,  1.6679e-01,  7.1192e-02,
        -3.9058e-02,  1.2789e-01, -1.9547e-03, -6.5694e-01,  7.3774e-02,
        -2.1124e-01, -3.0008e-01, -4.1642e-01,  2.4662e-01, -8.5819e-01,
        -5.6277e-01,  2.2659e-01, -5.6136e-02, -5.5959e-02,  3.5772e-01,
        -1.0577e-01,  2.7679e-02, -4.1154e-01,  4.8989e-01, -5.0976e-02,
         3.1705e-01,  1.6724e-01,  4.9883e-01,  1.5334e-02,  7.5586e-01,
         4.6219e-01, -2.2346e-01,  2.1232e-01, -4.8906e-03, -2.0199e-01,
        -4.9326e-01, -5.2733e-01,  7.3226e-01,  2.9899e-01,  1.1054e-01,
         3.3293e-01, -6.1675e-01, -9.3133e-02, -6.4463e-03,  2.0708e-01,
         1.6003e-02,  2.2934e-02, -3.3969e-01, -2.2151e-01,  4.3836e-01,
        -1.8864e-01,  6.2207e-01,  1.8566e-01, -4.7078e-02, -3.8708e-01,
         2.9575e-01,  3.4586e-01,  2.8151e-01,  4.0881e-01, -3.1909e-01,
        -1.2016e-02,  3.6700e-01,  8.9362e-01,  1.0899e-01, -3.1441e-01,
        -3.5461e-01, -5.2737e-02,  1.8424e-01, -3.7431e-02,  5.0358e-01,
         1.5099e-01,  4.6746e-01,  1.9825e-01,  1.5831e-01, -1.0754e-01,
        -3.2662e-01, -5.4435e-01, -1.0324e+00, -1.8485e-01,  4.5123e-01,
        -2.4724e-01,  6.8222e-01, -1.5530e-01, -7.0688e-01, -4.6669e-01,
         2.9320e-02, -4.6186e-01,  4.1776e-01,  3.4229e-01, -3.0385e-02,
        -8.0790e-02,  2.2717e-01, -2.6278e-01,  1.8914e-01, -2.8767e-01,
         4.8736e-01,  2.2062e-01,  4.4707e-01, -8.0776e-02, -4.9020e-01,
         7.2708e-01,  8.4201e-01, -1.1518e-01,  4.2012e-01, -1.5906e-01,
         4.2478e-01, -1.2544e-01,  7.0801e-01, -2.0662e-01,  1.0479e-03,
         3.3033e-01, -2.8293e-01, -4.6725e-02,  3.6571e-01, -1.0842e-01,
        -2.3855e-01, -4.4600e-02,  1.3386e-01,  2.0090e-01,  2.6327e-01,
         1.8721e-01,  1.1485e-01, -2.6928e-01, -1.4087e-01,  3.2804e-01,
         3.9866e-02, -4.8909e-01,  3.2394e-02, -8.1096e-02, -3.0059e-01,
         1.4012e-01, -5.1191e-02,  3.5098e-02, -2.2219e-01,  3.0374e-01,
         2.3092e-01,  7.6172e-02, -2.4229e-01,  3.8007e-01,  1.6584e-01,
         2.3549e-01, -7.6023e-02, -1.8484e-01, -4.7856e-01, -2.2454e-01,
        -9.9628e-02,  2.2643e-01,  2.5690e-01,  1.8767e-01, -9.0638e-02,
        -1.1447e-01,  1.9966e-01,  2.5066e-01, -3.2082e-01,  8.2066e-02,
        -4.2536e-01, -2.5967e-01], device='cuda:6', requires_grad=True)
net_guide.net.4.0.bias.scale torch.Size([512]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.5.weight.loc torch.Size([1, 512]) Parameter containing:
tensor([[-1.0850e-01, -3.2437e-02,  3.0019e-01,  4.3196e-02, -6.3372e-01,
         -3.3287e-01,  4.4848e-01,  5.5570e-02,  1.6951e-01,  4.1299e-01,
          6.8224e-02, -7.8521e-01, -1.3507e-01,  1.0049e-01,  6.1530e-01,
          2.5091e-01,  1.9211e-01,  1.0617e-01,  1.9316e-02, -1.3024e-01,
         -2.3101e-02, -7.6611e-02,  2.3131e-01,  3.3316e-01,  4.4644e-01,
         -2.8877e-01,  2.7720e-02, -1.8144e-01, -7.1458e-02,  1.1109e-01,
         -5.6830e-01,  4.3281e-02,  1.8366e-01,  2.9948e-01,  5.6387e-01,
         -5.0546e-01, -2.0859e-01,  3.0861e-01, -7.6326e-03, -3.1844e-01,
         -2.4351e-02, -5.4891e-01, -4.7614e-02,  2.7510e-01, -2.6595e-01,
          2.2862e-01, -3.4607e-01,  3.3979e-01,  1.5860e-01, -1.7544e-01,
          1.1554e-01, -1.6722e-01, -9.7347e-02, -1.5868e-01,  2.1417e-01,
         -1.5670e-01, -4.6661e-02, -8.6404e-01, -5.3488e-02, -1.9579e-02,
          5.0147e-01,  5.6139e-01, -2.1901e-01,  4.6868e-01,  3.2021e-01,
          2.7793e-01, -1.9917e-01, -8.5893e-02,  1.1933e-01,  2.8878e-01,
          3.0152e-01,  6.3660e-02,  2.6252e-01,  8.7769e-02,  4.0801e-01,
          1.5201e-01, -5.6985e-03,  2.9471e-01, -5.4099e-02, -7.6201e-02,
          2.8265e-01, -5.7994e-03, -1.3028e-01,  2.7282e-01, -3.6792e-02,
         -1.3243e-01,  5.0516e-02, -1.6269e-01, -4.2939e-02, -5.8002e-01,
          3.6390e-01, -1.4941e-01, -1.2267e-01, -3.6136e-01, -2.6709e-01,
          1.1774e-01,  1.3052e-02,  1.2631e-01, -2.2191e-01,  7.4665e-02,
          3.9686e-01, -5.5781e-01, -4.8272e-04, -1.3709e-01,  1.1541e-01,
          1.7262e-01, -5.2397e-02,  3.3917e-01,  1.9644e-01, -1.9934e-01,
          7.1267e-01, -1.2326e-01, -1.3505e-01, -7.8416e-03, -7.6050e-03,
         -2.4662e-01, -5.0325e-01, -1.4076e-02,  5.3634e-02, -2.6596e-01,
          7.3265e-02,  3.0560e-01,  1.3884e-01, -2.8343e-01, -3.0456e-01,
          7.5212e-01,  1.6735e-01,  1.2092e-02, -4.0076e-01,  2.9580e-01,
          8.3340e-01,  4.1606e-01, -3.4315e-01, -1.7640e-01,  4.1191e-01,
          3.5745e-01, -2.4257e-01,  4.1225e-01, -2.2538e-01, -2.0629e-01,
         -9.6486e-02,  1.0862e-01,  4.0714e-01, -5.3818e-01, -1.5390e-01,
          1.7397e-01,  1.0301e-01,  5.7806e-01,  3.3346e-01, -2.3932e-01,
          4.5320e-01, -1.3016e-02, -5.2145e-01, -5.6489e-02,  3.5112e-01,
          5.7114e-01, -1.1636e-02, -4.9745e-02, -2.4929e-01, -3.3273e-01,
         -3.4026e-02, -1.1258e-01, -4.1306e-01,  2.9287e-01, -2.1514e-01,
         -3.2297e-01, -5.4955e-02, -7.6313e-02,  3.4035e-01,  2.4616e-01,
         -1.4924e-01,  1.2508e-01, -4.8104e-01, -5.3232e-01, -2.9488e-01,
          6.8902e-01,  7.9384e-02, -2.5630e-01, -4.1115e-02,  5.5178e-02,
         -6.9295e-02, -6.6557e-01,  3.5499e-01,  2.9702e-01,  1.7054e-01,
         -3.1809e-01,  8.4619e-02, -5.1074e-01,  4.5727e-01,  1.3339e-01,
          4.0036e-02,  1.6209e-01,  1.7631e-01,  4.4296e-01,  5.8510e-01,
          2.2028e-02, -5.4119e-02, -1.1853e-01,  2.9013e-01,  1.9826e-01,
         -2.3869e-01, -1.1117e-01,  1.0972e-01,  6.2709e-02, -2.9770e-01,
         -1.0695e-01, -2.5326e-01,  1.0779e-01,  2.2636e-01,  3.2961e-01,
         -2.5559e-02, -2.0595e-02, -7.1178e-02,  3.9900e-01, -2.0020e-01,
         -6.7302e-02, -1.6543e-01, -1.9165e-01, -3.5209e-02, -1.5219e-01,
          3.1959e-02,  4.3578e-01, -1.6665e-01, -4.1990e-01,  3.7993e-01,
         -3.1873e-01, -1.9283e-02, -5.7463e-01, -3.3115e-01,  1.9142e-01,
          1.6335e-01,  3.2683e-02,  4.6556e-01,  1.9627e-01, -9.7838e-02,
         -2.2165e-01,  1.6565e-01,  1.0474e-01, -3.8947e-03,  4.3811e-01,
          2.6431e-01, -3.1664e-01, -3.3014e-02,  1.6046e-01, -1.1067e-01,
         -4.7155e-02, -2.1011e-01, -1.0583e-01, -3.5272e-01, -2.8083e-01,
          8.5041e-02, -5.0180e-02, -6.6570e-02, -4.3827e-01,  6.7214e-02,
          1.9761e-01,  5.7398e-01, -2.5194e-01, -4.2360e-01,  3.6045e-02,
         -3.0959e-01, -2.1142e-01,  5.4424e-03, -8.6232e-02,  3.2023e-01,
          1.3200e-01, -3.1210e-02, -4.9988e-01, -4.1524e-01, -3.9624e-01,
         -1.3779e-01, -3.0710e-02,  3.2168e-01, -7.2443e-02,  2.1549e-01,
          1.9120e-01, -4.8191e-01, -6.7570e-01,  3.2350e-01,  4.1595e-02,
          4.8446e-01, -4.3998e-01, -1.2918e-01,  4.9133e-02,  2.7971e-02,
          1.1812e-01,  6.0091e-02, -1.1925e-01, -3.8564e-01, -3.1125e-01,
         -1.5661e-01,  1.4617e-01, -9.0550e-02, -1.0962e-01, -5.9114e-02,
         -4.6785e-01,  1.5449e-02,  6.2447e-01, -6.4142e-01, -1.6555e-01,
         -3.8697e-02,  1.1704e-01, -2.1731e-01,  5.4156e-01, -3.5426e-01,
          4.8549e-02,  1.8264e-01, -5.6583e-01,  1.0575e-02,  7.0160e-02,
         -3.1165e-01, -2.3043e-01, -2.4740e-01,  2.8845e-01,  4.2257e-01,
          3.1047e-01, -7.2539e-01, -1.2941e-01, -1.4144e-01, -5.2191e-01,
         -1.3356e-01,  5.0648e-01, -3.2648e-01, -1.4667e-02, -2.3648e-01,
          3.6654e-02,  2.9567e-01, -4.9300e-01, -1.6160e-01, -2.2306e-01,
          2.3058e-01, -1.2992e-01, -3.4732e-01,  5.7994e-01, -6.4977e-01,
          8.8053e-01, -9.6221e-02,  1.6494e-01, -5.5928e-01, -1.0835e-01,
         -1.9018e-02,  6.7185e-02,  6.9397e-01,  3.6010e-01,  2.4774e-01,
         -1.3119e-01, -1.8662e-01, -2.2597e-01,  2.8511e-01,  5.5908e-01,
          9.7109e-02,  6.7138e-02, -4.7032e-01, -1.1221e-01, -1.7902e-02,
          5.6024e-01, -3.3041e-01, -6.7954e-01,  2.5849e-01,  5.1331e-01,
          7.7621e-01,  3.1852e-01,  3.3756e-01,  1.3549e-01,  5.1973e-01,
          3.8575e-01, -5.2404e-01, -2.6165e-01, -2.9003e-01, -1.7087e-01,
         -5.4384e-01, -6.0909e-02,  7.7000e-01,  1.1822e-01, -2.8712e-01,
          3.2194e-01,  1.2971e-01, -8.9460e-02,  2.8319e-01, -2.3125e-01,
          1.9225e-01,  1.6002e-01, -1.0360e-01,  3.3191e-01,  4.2876e-03,
         -5.4064e-02, -3.1146e-02,  2.4382e-01,  3.3738e-01, -1.7495e-01,
         -3.2156e-01, -2.5674e-01,  3.8949e-01,  5.4471e-02,  3.3728e-01,
         -3.3105e-01, -1.1671e-01,  1.7571e-01,  9.1090e-02, -5.0711e-01,
         -1.7015e-01,  5.1145e-02, -8.7895e-02, -1.1488e-01, -2.2075e-02,
         -3.8229e-01,  1.2051e-01, -4.1673e-01, -7.7555e-01, -2.7774e-01,
          1.3865e-01,  2.6542e-02, -4.8062e-01,  1.0781e-01,  4.6789e-02,
          9.3492e-02, -1.2328e-01,  7.5618e-02,  1.4277e-02,  1.8394e-02,
          1.3441e-02, -1.9965e-01, -3.1640e-02,  3.2855e-01,  5.6578e-01,
          1.4922e-01, -2.9974e-01, -2.4794e-02,  5.3582e-01,  2.5645e-02,
          1.7244e-01,  2.3546e-01,  3.0874e-01, -2.1049e-01, -4.8337e-02,
          5.4756e-01, -3.6268e-02,  2.5606e-01, -1.9752e-02,  5.5304e-02,
         -4.8693e-01,  4.5077e-01,  4.5243e-01, -2.2446e-01, -2.0173e-01,
         -5.4934e-01,  1.6357e-01,  7.3961e-02,  4.6122e-02, -1.9678e-01,
          4.1288e-01,  2.6719e-02, -1.4570e-02,  1.0793e-01,  2.5667e-01,
         -2.4594e-01, -2.2976e-01, -1.3281e-01,  2.9465e-01, -1.0658e-01,
          4.3821e-01,  5.5401e-01,  2.4183e-01, -3.2653e-01,  3.4061e-01,
          2.2944e-01, -2.2144e-01, -3.5676e-01, -3.1328e-01, -1.6151e-01,
          4.6745e-01,  2.1477e-01,  5.2155e-02, -2.0512e-01,  1.1477e-01,
         -1.7113e-01, -1.9347e-03,  2.0078e-01,  5.1022e-01,  1.2511e-01,
         -6.4220e-02,  5.5920e-01,  2.7994e-01, -1.5520e-01,  2.9498e-01,
         -4.2167e-02,  6.0416e-01,  1.8755e-01,  2.9129e-01,  1.2149e-01,
         -5.6154e-02,  4.9783e-01, -3.8104e-01, -1.5784e-01, -1.4870e-01,
         -2.1498e-01,  1.4377e-01,  9.8715e-02,  1.0881e-01, -3.4904e-01,
         -5.4688e-01, -3.8609e-01, -3.7619e-01,  5.9031e-02, -6.6309e-01,
         -2.7696e-01, -3.1811e-01, -5.2695e-01, -4.5306e-01, -3.5929e-01,
          5.4525e-01,  5.1683e-01]], device='cuda:6', requires_grad=True)
net_guide.net.5.weight.scale torch.Size([1, 512]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.5.bias.loc torch.Size([1]) Parameter containing:
tensor([0.5208], device='cuda:6', requires_grad=True)
net_guide.net.5.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:6', grad_fn=<AddBackward0>)
likelihood_guide.likelihood._scale.loc torch.Size([]) Parameter containing:
tensor(0.2287, device='cuda:6', requires_grad=True)
likelihood_guide.likelihood._scale.scale torch.Size([]) tensor(0.0100, device='cuda:6', grad_fn=<AddBackward0>)
Using device: cuda:6
===== Training profile sineasy10-4x512-sl - 1 =====
[0:00:02.462176] epoch: 0 | elbo: 1155590321.9200003 | train_rmse: 448.1068 | val_rmse: 451.5236 | val_ll: -20.8913
[0:02:09.128264] epoch: 50 | elbo: 43038547.32 | train_rmse: 94.1127 | val_rmse: 129.7971 | val_ll: -7.2437
[0:04:16.000490] epoch: 100 | elbo: 24342946.02 | train_rmse: 59.7339 | val_rmse: 109.4979 | val_ll: -6.9553
[0:06:23.563264] epoch: 150 | elbo: 16985977.330000002 | train_rmse: 41.9677 | val_rmse: 99.7182 | val_ll: -6.9033
[0:08:31.357365] epoch: 200 | elbo: 13084721.95 | train_rmse: 30.9914 | val_rmse: 93.2221 | val_ll: -6.8793
[0:10:39.543566] epoch: 250 | elbo: 10793440.52 | train_rmse: 23.314 | val_rmse: 88.0858 | val_ll: -6.8998
[0:12:48.083240] epoch: 300 | elbo: 9316675.73 | train_rmse: 17.8315 | val_rmse: 84.362 | val_ll: -6.9814
[0:14:57.542853] epoch: 350 | elbo: 8192981.275 | train_rmse: 14.0126 | val_rmse: 80.8775 | val_ll: -7.0572
[0:17:05.250119] epoch: 400 | elbo: 7311108.029999998 | train_rmse: 11.0233 | val_rmse: 77.4695 | val_ll: -7.1331
[0:19:11.027600] epoch: 450 | elbo: 6685522.674999999 | train_rmse: 9.6759 | val_rmse: 74.4166 | val_ll: -7.1915
[0:21:19.159136] epoch: 500 | elbo: 6169604.08 | train_rmse: 7.9136 | val_rmse: 71.0933 | val_ll: -7.2396
[0:23:28.542884] epoch: 550 | elbo: 5680256.619999999 | train_rmse: 6.7165 | val_rmse: 68.0806 | val_ll: -7.3673
[0:25:38.644751] epoch: 600 | elbo: 5301906.945 | train_rmse: 6.1003 | val_rmse: 64.8951 | val_ll: -7.4891
[0:27:46.896944] epoch: 650 | elbo: 4974296.89 | train_rmse: 5.3118 | val_rmse: 61.7591 | val_ll: -7.4751
[0:29:54.355119] epoch: 700 | elbo: 4698613.640000001 | train_rmse: 4.8616 | val_rmse: 58.6976 | val_ll: -7.5354
[0:32:02.039641] epoch: 750 | elbo: 4471687.055 | train_rmse: 4.4892 | val_rmse: 55.6389 | val_ll: -7.5472
[0:34:08.539825] epoch: 800 | elbo: 4273994.7375 | train_rmse: 4.001 | val_rmse: 52.7015 | val_ll: -7.6444
[0:36:14.442263] epoch: 850 | elbo: 4110566.7475 | train_rmse: 3.7943 | val_rmse: 49.7813 | val_ll: -7.6361
[0:38:19.785370] epoch: 900 | elbo: 3964263.535 | train_rmse: 3.4818 | val_rmse: 47.0465 | val_ll: -7.6673
[0:40:26.698211] epoch: 950 | elbo: 3840351.3800000004 | train_rmse: 3.2735 | val_rmse: 44.3518 | val_ll: -7.6018
[0:42:31.092146] epoch: 1000 | elbo: 3737343.1049999995 | train_rmse: 3.0345 | val_rmse: 41.7046 | val_ll: -7.5012
[0:44:36.983311] epoch: 1050 | elbo: 3644543.3175 | train_rmse: 2.8374 | val_rmse: 39.275 | val_ll: -7.4848
[0:46:41.756003] epoch: 1100 | elbo: 3560420.5074999994 | train_rmse: 2.6048 | val_rmse: 36.912 | val_ll: -7.4371
[0:48:46.174330] epoch: 1150 | elbo: 3482787.59 | train_rmse: 2.4014 | val_rmse: 34.6963 | val_ll: -7.3241
[0:50:50.565430] epoch: 1200 | elbo: 3414780.9824999995 | train_rmse: 2.3473 | val_rmse: 32.4777 | val_ll: -7.1687
[0:52:55.703430] epoch: 1250 | elbo: 3352459.385 | train_rmse: 2.182 | val_rmse: 30.5285 | val_ll: -7.0592
[0:55:00.414230] epoch: 1300 | elbo: 3292241.7775 | train_rmse: 2.0355 | val_rmse: 28.477 | val_ll: -6.9434
[0:57:04.537515] epoch: 1350 | elbo: 3233802.0825000005 | train_rmse: 1.884 | val_rmse: 26.465 | val_ll: -6.7214
[0:59:08.948478] epoch: 1400 | elbo: 3177829.27 | train_rmse: 1.8482 | val_rmse: 24.4969 | val_ll: -6.5297
[1:01:13.812671] epoch: 1450 | elbo: 3124389.4675000003 | train_rmse: 1.7276 | val_rmse: 22.5947 | val_ll: -6.3015
[1:03:20.119933] epoch: 1500 | elbo: 3071690.7275 | train_rmse: 1.6301 | val_rmse: 20.6712 | val_ll: -6.0623
[1:05:25.492517] epoch: 1550 | elbo: 3021170.0575 | train_rmse: 1.5507 | val_rmse: 18.8436 | val_ll: -5.7743
[1:07:30.099436] epoch: 1600 | elbo: 2971660.1900000004 | train_rmse: 1.4653 | val_rmse: 17.0416 | val_ll: -5.482
[1:09:34.912137] epoch: 1650 | elbo: 2922960.655 | train_rmse: 1.4337 | val_rmse: 15.3645 | val_ll: -5.2132
[1:11:40.463224] epoch: 1700 | elbo: 2874967.915 | train_rmse: 1.3343 | val_rmse: 13.7784 | val_ll: -4.8956
[1:13:45.647929] epoch: 1750 | elbo: 2828408.44 | train_rmse: 1.2911 | val_rmse: 12.3145 | val_ll: -4.6377
[1:15:50.398330] epoch: 1800 | elbo: 2782534.6624999996 | train_rmse: 1.2563 | val_rmse: 11.0008 | val_ll: -4.3755
[1:17:56.762694] epoch: 1850 | elbo: 2737682.7575 | train_rmse: 1.2219 | val_rmse: 9.7932 | val_ll: -4.1293
[1:20:01.361315] epoch: 1900 | elbo: 2693381.245 | train_rmse: 1.1795 | val_rmse: 8.7209 | val_ll: -3.8852
[1:22:06.763421] epoch: 1950 | elbo: 2649937.6775 | train_rmse: 1.1482 | val_rmse: 7.7767 | val_ll: -3.6599
[1:24:11.934858] epoch: 2000 | elbo: 2607091.8475 | train_rmse: 1.1082 | val_rmse: 6.9538 | val_ll: -3.464
[1:26:17.671790] epoch: 2050 | elbo: 2564714.0974999997 | train_rmse: 1.0981 | val_rmse: 6.2265 | val_ll: -3.2927
[1:28:23.458752] epoch: 2100 | elbo: 2522630.2325 | train_rmse: 1.0644 | val_rmse: 5.6555 | val_ll: -3.1313
[1:30:28.082657] epoch: 2150 | elbo: 2481174.6574999997 | train_rmse: 1.0509 | val_rmse: 5.1849 | val_ll: -3.029
[1:32:34.032265] epoch: 2200 | elbo: 2439760.785 | train_rmse: 1.0054 | val_rmse: 4.8188 | val_ll: -2.9376
[1:34:38.749116] epoch: 2250 | elbo: 2398686.1850000005 | train_rmse: 1.0213 | val_rmse: 4.5107 | val_ll: -2.8674
[1:36:44.326159] epoch: 2300 | elbo: 2357705.33 | train_rmse: 1.021 | val_rmse: 4.2589 | val_ll: -2.7987
[1:38:50.464050] epoch: 2350 | elbo: 2316745.89 | train_rmse: 0.9939 | val_rmse: 4.0573 | val_ll: -2.7451
[1:40:57.728170] epoch: 2400 | elbo: 2275778.2474999996 | train_rmse: 0.9716 | val_rmse: 3.896 | val_ll: -2.7094
[1:43:05.657416] epoch: 2450 | elbo: 2234923.4274999993 | train_rmse: 0.9681 | val_rmse: 3.7331 | val_ll: -2.6586
[1:45:15.153283] epoch: 2500 | elbo: 2193972.1799999997 | train_rmse: 0.9667 | val_rmse: 3.6024 | val_ll: -2.6294
[1:47:23.053904] epoch: 2550 | elbo: 2153041.2199999997 | train_rmse: 0.9665 | val_rmse: 3.4714 | val_ll: -2.5943
[1:49:30.213114] epoch: 2600 | elbo: 2111938.74 | train_rmse: 0.9579 | val_rmse: 3.3739 | val_ll: -2.5667
[1:51:38.178844] epoch: 2650 | elbo: 2070966.4324999999 | train_rmse: 0.9584 | val_rmse: 3.2805 | val_ll: -2.5406
[1:53:46.585691] epoch: 2700 | elbo: 2029931.11375 | train_rmse: 0.9612 | val_rmse: 3.1764 | val_ll: -2.5127
[1:55:53.973037] epoch: 2750 | elbo: 1988945.5074999998 | train_rmse: 0.9584 | val_rmse: 3.1026 | val_ll: -2.4813
[1:58:00.372044] epoch: 2800 | elbo: 1948054.0674999997 | train_rmse: 0.953 | val_rmse: 3.021 | val_ll: -2.448
[2:00:05.492642] epoch: 2850 | elbo: 1907227.0112500004 | train_rmse: 0.9192 | val_rmse: 2.9364 | val_ll: -2.4217
[2:02:11.995229] epoch: 2900 | elbo: 1866534.44375 | train_rmse: 0.9285 | val_rmse: 2.8713 | val_ll: -2.3992
[2:04:17.406983] epoch: 2950 | elbo: 1825922.1037499998 | train_rmse: 0.9403 | val_rmse: 2.8017 | val_ll: -2.3829
[2:06:23.407885] epoch: 3000 | elbo: 1785488.48375 | train_rmse: 0.9416 | val_rmse: 2.7338 | val_ll: -2.3577
[2:08:28.106999] epoch: 3050 | elbo: 1745189.7925 | train_rmse: 0.9545 | val_rmse: 2.6692 | val_ll: -2.332
[2:10:34.330273] epoch: 3100 | elbo: 1705060.9912500002 | train_rmse: 0.9629 | val_rmse: 2.6166 | val_ll: -2.3117
[2:12:41.203020] epoch: 3150 | elbo: 1664987.4949999999 | train_rmse: 0.9823 | val_rmse: 2.553 | val_ll: -2.2983
[2:14:47.119701] epoch: 3200 | elbo: 1625115.6925000001 | train_rmse: 0.998 | val_rmse: 2.4988 | val_ll: -2.2857
[2:16:52.314545] epoch: 3250 | elbo: 1585272.57875 | train_rmse: 1.003 | val_rmse: 2.4334 | val_ll: -2.2687
[2:18:58.184211] epoch: 3300 | elbo: 1545610.3199999998 | train_rmse: 1.0383 | val_rmse: 2.3929 | val_ll: -2.2622
[2:21:04.950907] epoch: 3350 | elbo: 1506027.3325 | train_rmse: 1.0693 | val_rmse: 2.3313 | val_ll: -2.2478
[2:23:10.671207] epoch: 3400 | elbo: 1466552.7899999998 | train_rmse: 1.0862 | val_rmse: 2.2848 | val_ll: -2.2384
[2:25:16.221606] epoch: 3450 | elbo: 1427205.3125000005 | train_rmse: 1.1132 | val_rmse: 2.228 | val_ll: -2.2265
[2:27:22.806177] epoch: 3500 | elbo: 1387930.2337500001 | train_rmse: 1.1208 | val_rmse: 2.1637 | val_ll: -2.2132
[2:29:28.398771] epoch: 3550 | elbo: 1348879.3287499999 | train_rmse: 1.1567 | val_rmse: 2.1149 | val_ll: -2.2064
[2:31:33.601194] epoch: 3600 | elbo: 1309853.7875 | train_rmse: 1.1693 | val_rmse: 2.0511 | val_ll: -2.1917
[2:33:38.614144] epoch: 3650 | elbo: 1270983.6237499998 | train_rmse: 1.1835 | val_rmse: 1.9987 | val_ll: -2.1823
[2:35:44.112207] epoch: 3700 | elbo: 1232256.4387500002 | train_rmse: 1.1965 | val_rmse: 1.9341 | val_ll: -2.1691
[2:37:51.596545] epoch: 3750 | elbo: 1193648.4575 | train_rmse: 1.2061 | val_rmse: 1.8796 | val_ll: -2.1554
[2:39:57.850063] epoch: 3800 | elbo: 1155246.0612499998 | train_rmse: 1.2291 | val_rmse: 1.834 | val_ll: -2.143
[2:42:03.654964] epoch: 3850 | elbo: 1117032.9375 | train_rmse: 1.2313 | val_rmse: 1.7732 | val_ll: -2.1275
[2:44:10.317986] epoch: 3900 | elbo: 1078999.8587500001 | train_rmse: 1.2312 | val_rmse: 1.7204 | val_ll: -2.1166
[2:46:17.770281] epoch: 3950 | elbo: 1041188.2375 | train_rmse: 1.2483 | val_rmse: 1.6817 | val_ll: -2.1024
[2:48:23.337298] epoch: 4000 | elbo: 1003653.7662500001 | train_rmse: 1.2456 | val_rmse: 1.6455 | val_ll: -2.0904
[2:50:29.558838] epoch: 4050 | elbo: 966363.3225000001 | train_rmse: 1.2373 | val_rmse: 1.5939 | val_ll: -2.0759
[2:52:35.120935] epoch: 4100 | elbo: 929396.1775 | train_rmse: 1.2373 | val_rmse: 1.5661 | val_ll: -2.0662
[2:54:42.159462] epoch: 4150 | elbo: 892809.3993749998 | train_rmse: 1.2432 | val_rmse: 1.5482 | val_ll: -2.0596
[2:56:49.667480] epoch: 4200 | elbo: 856504.0525 | train_rmse: 1.2278 | val_rmse: 1.5111 | val_ll: -2.0474
[2:58:56.556658] epoch: 4250 | elbo: 820695.1881250001 | train_rmse: 1.2396 | val_rmse: 1.5011 | val_ll: -2.0452
[3:01:03.446352] epoch: 4300 | elbo: 785307.5225 | train_rmse: 1.2539 | val_rmse: 1.4895 | val_ll: -2.0413
[3:03:10.969433] epoch: 4350 | elbo: 750430.6125 | train_rmse: 1.2494 | val_rmse: 1.4712 | val_ll: -2.037
[3:05:17.376606] epoch: 4400 | elbo: 716040.588125 | train_rmse: 1.273 | val_rmse: 1.4826 | val_ll: -2.0384
[3:07:24.232581] epoch: 4450 | elbo: 682288.939375 | train_rmse: 1.2652 | val_rmse: 1.4644 | val_ll: -2.0426
[3:09:30.241855] epoch: 4500 | elbo: 649041.238125 | train_rmse: 1.2735 | val_rmse: 1.465 | val_ll: -2.0446
[3:11:37.161591] epoch: 4550 | elbo: 616552.7093750001 | train_rmse: 1.2969 | val_rmse: 1.4705 | val_ll: -2.0524
[3:13:42.589704] epoch: 4600 | elbo: 584718.60625 | train_rmse: 1.315 | val_rmse: 1.4849 | val_ll: -2.0574
[3:15:48.538428] epoch: 4650 | elbo: 553558.548125 | train_rmse: 1.333 | val_rmse: 1.4987 | val_ll: -2.0702
[3:17:54.439739] epoch: 4700 | elbo: 523247.6893750001 | train_rmse: 1.3626 | val_rmse: 1.5175 | val_ll: -2.0808
[3:20:01.161212] epoch: 4750 | elbo: 493663.94000000006 | train_rmse: 1.3677 | val_rmse: 1.5185 | val_ll: -2.091
[3:22:07.504014] epoch: 4800 | elbo: 464955.1046875 | train_rmse: 1.4115 | val_rmse: 1.5543 | val_ll: -2.1011
[3:24:12.612568] epoch: 4850 | elbo: 437060.17312500003 | train_rmse: 1.4336 | val_rmse: 1.5718 | val_ll: -2.1172
[3:26:17.596119] epoch: 4900 | elbo: 410124.195625 | train_rmse: 1.4797 | val_rmse: 1.608 | val_ll: -2.1292
[3:28:23.601540] epoch: 4950 | elbo: 383989.41156249994 | train_rmse: 1.4974 | val_rmse: 1.623 | val_ll: -2.1461
[3:30:30.715324] epoch: 5000 | elbo: 358900.7587499999 | train_rmse: 1.5581 | val_rmse: 1.6722 | val_ll: -2.1634
[3:32:37.968913] epoch: 5050 | elbo: 334755.8359375 | train_rmse: 1.5872 | val_rmse: 1.6872 | val_ll: -2.1782
[3:34:43.906395] epoch: 5100 | elbo: 311578.90625 | train_rmse: 1.6228 | val_rmse: 1.7236 | val_ll: -2.1973
[3:36:50.241441] epoch: 5150 | elbo: 289423.279375 | train_rmse: 1.6641 | val_rmse: 1.7716 | val_ll: -2.2148
[3:38:56.392243] epoch: 5200 | elbo: 268343.5615625 | train_rmse: 1.6991 | val_rmse: 1.7922 | val_ll: -2.2333
[3:41:01.602924] epoch: 5250 | elbo: 248222.45203125002 | train_rmse: 1.7469 | val_rmse: 1.8408 | val_ll: -2.2473
[3:43:07.631479] epoch: 5300 | elbo: 229215.15484374994 | train_rmse: 1.7904 | val_rmse: 1.8803 | val_ll: -2.267
[3:45:13.614550] epoch: 5350 | elbo: 211396.37296875 | train_rmse: 1.8333 | val_rmse: 1.9141 | val_ll: -2.2843
[3:47:18.801783] epoch: 5400 | elbo: 194584.97703124996 | train_rmse: 1.8801 | val_rmse: 1.9589 | val_ll: -2.3049
[3:49:25.922769] epoch: 5450 | elbo: 179234.73125 | train_rmse: 1.924 | val_rmse: 2.0042 | val_ll: -2.3162
[3:51:31.884596] epoch: 5500 | elbo: 164398.80875 | train_rmse: 1.9711 | val_rmse: 2.0384 | val_ll: -2.3365
[3:53:37.717663] epoch: 5550 | elbo: 151094.93171874998 | train_rmse: 2.001 | val_rmse: 2.0664 | val_ll: -2.3522
[3:55:43.671471] epoch: 5600 | elbo: 138672.249375 | train_rmse: 2.0363 | val_rmse: 2.0961 | val_ll: -2.3635
[3:57:49.379394] epoch: 5650 | elbo: 127410.08117187498 | train_rmse: 2.0757 | val_rmse: 2.1413 | val_ll: -2.3811
[3:59:54.845099] epoch: 5700 | elbo: 117511.95546875002 | train_rmse: 2.106 | val_rmse: 2.1569 | val_ll: -2.3966
[4:02:02.863377] epoch: 5750 | elbo: 108298.204921875 | train_rmse: 2.1371 | val_rmse: 2.1934 | val_ll: -2.4076
[4:04:11.466366] epoch: 5800 | elbo: 100183.45718750001 | train_rmse: 2.1465 | val_rmse: 2.2095 | val_ll: -2.4204
[4:06:21.230569] epoch: 5850 | elbo: 92849.809765625 | train_rmse: 2.19 | val_rmse: 2.2405 | val_ll: -2.4286
[4:08:29.099718] epoch: 5900 | elbo: 86612.733203125 | train_rmse: 2.2214 | val_rmse: 2.2802 | val_ll: -2.4404
[4:10:37.883090] epoch: 5950 | elbo: 81170.32625 | train_rmse: 2.2512 | val_rmse: 2.2975 | val_ll: -2.4509
[4:12:45.116235] epoch: 6000 | elbo: 76515.916171875 | train_rmse: 2.2553 | val_rmse: 2.2956 | val_ll: -2.4525
[4:14:50.174251] epoch: 6050 | elbo: 72380.73421875 | train_rmse: 2.2812 | val_rmse: 2.3312 | val_ll: -2.4609
[4:16:57.052936] epoch: 6100 | elbo: 68988.52640625 | train_rmse: 2.2702 | val_rmse: 2.3245 | val_ll: -2.4631
[4:19:03.840304] epoch: 6150 | elbo: 65920.2909765625 | train_rmse: 2.2955 | val_rmse: 2.3478 | val_ll: -2.4686
[4:21:08.125767] epoch: 6200 | elbo: 63545.46078125001 | train_rmse: 2.328 | val_rmse: 2.3698 | val_ll: -2.4707
[4:23:11.705375] epoch: 6250 | elbo: 61661.949492187494 | train_rmse: 2.3291 | val_rmse: 2.3827 | val_ll: -2.4734
[4:25:17.561456] epoch: 6300 | elbo: 59740.180429687505 | train_rmse: 2.3416 | val_rmse: 2.383 | val_ll: -2.4722
[4:27:22.068268] epoch: 6350 | elbo: 57965.78527343749 | train_rmse: 2.2979 | val_rmse: 2.3415 | val_ll: -2.4709
[4:29:27.464401] epoch: 6400 | elbo: 56789.130351562504 | train_rmse: 2.3452 | val_rmse: 2.392 | val_ll: -2.4733
[4:31:32.681272] epoch: 6450 | elbo: 55574.45480468749 | train_rmse: 2.3103 | val_rmse: 2.3577 | val_ll: -2.4685
[4:33:38.491010] epoch: 6500 | elbo: 54592.44394531251 | train_rmse: 2.3439 | val_rmse: 2.377 | val_ll: -2.4645
[4:35:42.115524] epoch: 6550 | elbo: 54144.10855468749 | train_rmse: 2.298 | val_rmse: 2.3459 | val_ll: -2.4659
[4:37:51.836566] epoch: 6600 | elbo: 53120.1498828125 | train_rmse: 2.3302 | val_rmse: 2.3819 | val_ll: -2.4607
[4:40:00.006141] epoch: 6650 | elbo: 52312.6316796875 | train_rmse: 2.301 | val_rmse: 2.3487 | val_ll: -2.4559
[4:42:09.261924] epoch: 6700 | elbo: 51768.387265625 | train_rmse: 2.3094 | val_rmse: 2.3534 | val_ll: -2.4564
[4:44:18.564434] epoch: 6750 | elbo: 51244.518046875 | train_rmse: 2.3074 | val_rmse: 2.3549 | val_ll: -2.4497
[4:46:23.155791] epoch: 6800 | elbo: 50588.6890234375 | train_rmse: 2.2904 | val_rmse: 2.334 | val_ll: -2.4452
[4:48:26.577050] epoch: 6850 | elbo: 50280.438203125006 | train_rmse: 2.3127 | val_rmse: 2.3632 | val_ll: -2.4478
[4:50:33.249774] epoch: 6900 | elbo: 49728.303984375 | train_rmse: 2.2887 | val_rmse: 2.3352 | val_ll: -2.4408
[4:52:39.846007] epoch: 6950 | elbo: 49317.330546875 | train_rmse: 2.2745 | val_rmse: 2.3243 | val_ll: -2.4339
[4:54:47.314263] epoch: 7000 | elbo: 48921.616445312495 | train_rmse: 2.2541 | val_rmse: 2.3081 | val_ll: -2.431
[4:56:53.155628] epoch: 7050 | elbo: 48616.3259375 | train_rmse: 2.2644 | val_rmse: 2.3005 | val_ll: -2.4285
[4:58:58.555322] epoch: 7100 | elbo: 48208.77312499999 | train_rmse: 2.2216 | val_rmse: 2.2838 | val_ll: -2.4251
[5:01:03.238920] epoch: 7150 | elbo: 47870.30499999999 | train_rmse: 2.2405 | val_rmse: 2.2938 | val_ll: -2.4211
[5:03:08.490656] epoch: 7200 | elbo: 47628.46445312501 | train_rmse: 2.223 | val_rmse: 2.2822 | val_ll: -2.4152
[5:05:15.493164] epoch: 7250 | elbo: 47142.6040234375 | train_rmse: 2.2495 | val_rmse: 2.2966 | val_ll: -2.4095
[5:07:23.269258] epoch: 7300 | elbo: 47020.113437500004 | train_rmse: 2.2263 | val_rmse: 2.2726 | val_ll: -2.404
[5:09:29.709488] epoch: 7350 | elbo: 46763.805117187505 | train_rmse: 2.2022 | val_rmse: 2.2525 | val_ll: -2.3999
[5:11:35.589330] epoch: 7400 | elbo: 46488.007265625005 | train_rmse: 2.1859 | val_rmse: 2.2454 | val_ll: -2.3942
[5:13:40.985307] epoch: 7450 | elbo: 46232.18707031249 | train_rmse: 2.1816 | val_rmse: 2.2436 | val_ll: -2.3922
[5:15:45.263245] epoch: 7500 | elbo: 46019.6930078125 | train_rmse: 2.1836 | val_rmse: 2.2501 | val_ll: -2.3865
[5:17:49.465087] epoch: 7550 | elbo: 45693.9994140625 | train_rmse: 2.1651 | val_rmse: 2.2161 | val_ll: -2.3832
[5:19:54.402094] epoch: 7600 | elbo: 45468.944570312495 | train_rmse: 2.1663 | val_rmse: 2.2167 | val_ll: -2.3764
[5:21:59.669842] epoch: 7650 | elbo: 45308.401953125 | train_rmse: 2.155 | val_rmse: 2.2083 | val_ll: -2.3758
[5:24:05.065213] epoch: 7700 | elbo: 45075.356328125 | train_rmse: 2.125 | val_rmse: 2.1827 | val_ll: -2.3646
[5:26:10.543011] epoch: 7750 | elbo: 44964.527578125 | train_rmse: 2.1231 | val_rmse: 2.1795 | val_ll: -2.3619
[5:28:15.302034] epoch: 7800 | elbo: 44890.590546875006 | train_rmse: 2.0983 | val_rmse: 2.1592 | val_ll: -2.3552
[5:30:19.170406] epoch: 7850 | elbo: 44615.198085937496 | train_rmse: 2.1131 | val_rmse: 2.1627 | val_ll: -2.3498
[5:32:24.433749] epoch: 7900 | elbo: 44402.38249999999 | train_rmse: 2.0916 | val_rmse: 2.1521 | val_ll: -2.3419
[5:34:29.816380] epoch: 7950 | elbo: 44470.724414062504 | train_rmse: 2.1023 | val_rmse: 2.1561 | val_ll: -2.3426
[5:36:34.650602] epoch: 8000 | elbo: 44485.740039062504 | train_rmse: 2.0656 | val_rmse: 2.1283 | val_ll: -2.3322
[5:38:41.514123] epoch: 8050 | elbo: 44093.0830859375 | train_rmse: 2.0739 | val_rmse: 2.1343 | val_ll: -2.3332
[5:40:47.446172] epoch: 8100 | elbo: 44047.092421875 | train_rmse: 2.038 | val_rmse: 2.1008 | val_ll: -2.3214
[5:42:52.408035] epoch: 8150 | elbo: 43834.82640625 | train_rmse: 2.0812 | val_rmse: 2.1463 | val_ll: -2.3299
[5:44:57.974418] epoch: 8200 | elbo: 43672.27796875 | train_rmse: 2.043 | val_rmse: 2.0986 | val_ll: -2.3146
[5:47:01.942195] epoch: 8250 | elbo: 43756.518281250006 | train_rmse: 2.0519 | val_rmse: 2.1037 | val_ll: -2.3126
[5:49:06.177391] epoch: 8300 | elbo: 43492.179765625 | train_rmse: 2.0389 | val_rmse: 2.0996 | val_ll: -2.306
[5:51:11.579619] epoch: 8350 | elbo: 43765.2265625 | train_rmse: 2.0185 | val_rmse: 2.0846 | val_ll: -2.3001
[5:53:16.253860] epoch: 8400 | elbo: 43195.87847656251 | train_rmse: 2.0123 | val_rmse: 2.0784 | val_ll: -2.2975
[5:55:21.093897] epoch: 8450 | elbo: 43357.379648437505 | train_rmse: 1.9988 | val_rmse: 2.0575 | val_ll: -2.2895
[5:57:24.793167] epoch: 8500 | elbo: 42925.491093749995 | train_rmse: 2.0005 | val_rmse: 2.0645 | val_ll: -2.2902
[5:59:28.381272] epoch: 8550 | elbo: 43056.85550781251 | train_rmse: 1.9898 | val_rmse: 2.0542 | val_ll: -2.2817
[6:01:31.924163] epoch: 8600 | elbo: 42814.188242187505 | train_rmse: 1.971 | val_rmse: 2.0466 | val_ll: -2.2782
[6:03:36.030411] epoch: 8650 | elbo: 42762.8597265625 | train_rmse: 1.9849 | val_rmse: 2.0471 | val_ll: -2.2785
[6:05:39.430739] epoch: 8700 | elbo: 42678.419375000005 | train_rmse: 1.9733 | val_rmse: 2.0438 | val_ll: -2.2683
[6:07:44.289478] epoch: 8750 | elbo: 42835.19968749999 | train_rmse: 1.9573 | val_rmse: 2.0238 | val_ll: -2.2663
[6:09:49.097590] epoch: 8800 | elbo: 42513.894453125 | train_rmse: 1.9638 | val_rmse: 2.023 | val_ll: -2.265
[6:11:55.558460] epoch: 8850 | elbo: 42531.103945312505 | train_rmse: 1.9551 | val_rmse: 2.0256 | val_ll: -2.2591
[6:14:00.314118] epoch: 8900 | elbo: 42378.0284375 | train_rmse: 1.9484 | val_rmse: 2.0204 | val_ll: -2.2568
[6:16:06.084286] epoch: 8950 | elbo: 42279.811171875 | train_rmse: 1.9611 | val_rmse: 2.02 | val_ll: -2.2536
[6:18:11.944138] epoch: 9000 | elbo: 42137.445429687505 | train_rmse: 1.9429 | val_rmse: 2.0069 | val_ll: -2.2492
[6:20:17.298168] epoch: 9050 | elbo: 42061.37992187501 | train_rmse: 1.9425 | val_rmse: 2.0082 | val_ll: -2.2447
[6:22:21.542348] epoch: 9100 | elbo: 42027.131796875 | train_rmse: 1.9251 | val_rmse: 1.9922 | val_ll: -2.2433
[6:24:26.322419] epoch: 9150 | elbo: 41972.4687890625 | train_rmse: 1.9221 | val_rmse: 1.992 | val_ll: -2.2379
[6:26:31.108281] epoch: 9200 | elbo: 41849.4783984375 | train_rmse: 1.9119 | val_rmse: 1.9836 | val_ll: -2.2331
[6:28:34.921262] epoch: 9250 | elbo: 41764.7346484375 | train_rmse: 1.9204 | val_rmse: 1.9955 | val_ll: -2.233
[6:30:42.645214] epoch: 9300 | elbo: 41881.6514453125 | train_rmse: 1.9171 | val_rmse: 1.9839 | val_ll: -2.2298
[6:32:49.543382] epoch: 9350 | elbo: 41821.704023437494 | train_rmse: 1.9239 | val_rmse: 1.9885 | val_ll: -2.2281
[6:34:57.087567] epoch: 9400 | elbo: 41694.888164062504 | train_rmse: 1.909 | val_rmse: 1.9782 | val_ll: -2.2278
[6:37:07.087697] epoch: 9450 | elbo: 41652.41703125 | train_rmse: 1.8942 | val_rmse: 1.9625 | val_ll: -2.2188
[6:39:15.359334] epoch: 9500 | elbo: 41485.426015624995 | train_rmse: 1.8975 | val_rmse: 1.9601 | val_ll: -2.2142
[6:41:23.362019] epoch: 9550 | elbo: 41520.887812500005 | train_rmse: 1.8993 | val_rmse: 1.9691 | val_ll: -2.2162
[6:43:30.514087] epoch: 9600 | elbo: 41419.6306640625 | train_rmse: 1.911 | val_rmse: 1.9821 | val_ll: -2.2147
[6:45:39.023718] epoch: 9650 | elbo: 41365.3187890625 | train_rmse: 1.8879 | val_rmse: 1.9622 | val_ll: -2.206
[6:47:47.752717] epoch: 9700 | elbo: 41259.87078124999 | train_rmse: 1.8939 | val_rmse: 1.9627 | val_ll: -2.2051
[6:49:55.163913] epoch: 9750 | elbo: 41232.9817578125 | train_rmse: 1.8767 | val_rmse: 1.945 | val_ll: -2.2047
[6:52:02.400476] epoch: 9800 | elbo: 41289.6228125 | train_rmse: 1.8735 | val_rmse: 1.9473 | val_ll: -2.1957
[6:54:10.840750] epoch: 9850 | elbo: 41383.6700390625 | train_rmse: 1.8662 | val_rmse: 1.9381 | val_ll: -2.1942
[6:56:17.463949] epoch: 9900 | elbo: 41081.2375390625 | train_rmse: 1.8648 | val_rmse: 1.9384 | val_ll: -2.1917
[6:58:23.905236] epoch: 9950 | elbo: 41152.778554687495 | train_rmse: 1.8701 | val_rmse: 1.9404 | val_ll: -2.1921
Training finished in 7:00:29.145328 seconds
Saved SVI model to tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-4x512-sl/checkpoint_1.pt
File Size is 6.065402030944824 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=10, out_features=512, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (4): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
  )
  (5): Linear(in_features=512, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:6 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic_gamma PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 2.0 LIKELIHOOD_SCALE: 1.0 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy10-10k-s03/models/sineasy10-4x512-sl/checkpoint_1.pt
using device: cuda:6
====== evaluating profile sineasy10-4x512-sl - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:03:46.195240
End time: 2023-07-12 03:16:35.680451
Total time: 13:37:06.036843
