Start time: 2023-07-13 18:38:19.095961
torch.Size([512, 20]) torch.Size([512, 1])
Sequential(
  (0): Linear(in_features=20, out_features=64, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=64, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:6 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Initial parameters:
net_guide.net.0.weight.loc torch.Size([64, 20]) Parameter containing:
tensor([[-0.1309,  0.4018, -0.3607,  ..., -0.4561,  0.2183,  0.0023],
        [ 0.0935,  0.1614, -0.0209,  ..., -0.0543, -0.3209, -0.2669],
        [-0.3255, -0.0794,  0.0347,  ..., -0.1676,  0.1695, -0.1385],
        ...,
        [ 0.2740, -0.6644, -0.2358,  ...,  0.3116, -0.6245,  0.1227],
        [ 0.1703, -0.1246,  0.1638,  ...,  0.1874,  0.0128, -0.3635],
        [ 0.2012,  0.0368,  0.5587,  ..., -0.1484, -0.5085, -0.0900]],
       device='cuda:6', requires_grad=True)
net_guide.net.0.weight.scale torch.Size([64, 20]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.0.bias.loc torch.Size([64]) Parameter containing:
tensor([ 0.2392, -0.0379, -0.3687, -0.8119, -0.5445,  0.3098,  0.0717, -0.5639,
        -0.3441,  0.1958,  0.1668, -0.2536, -0.1787, -0.0566, -0.0685,  0.3006,
        -0.4432,  0.0651,  0.1327, -0.0253, -0.1644, -0.0976, -0.0980, -0.0627,
         0.1629,  0.0829, -0.2601,  0.5188,  0.4388,  0.2773,  0.1520,  0.4767,
        -0.1585, -0.1049,  0.0123,  0.5252,  0.6004,  0.1246,  0.6866, -0.0283,
         0.2444,  0.1440, -0.5015, -0.3497, -0.1861, -0.2324, -0.0632, -0.4926,
        -0.3574,  0.3461,  0.1615,  0.4207, -0.2287,  0.3060,  0.0553, -0.4309,
        -0.3899,  0.0430, -0.2281, -0.1939, -0.4402, -0.5449,  0.3130, -0.1966],
       device='cuda:6', requires_grad=True)
net_guide.net.0.bias.scale torch.Size([64]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100], device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.2.0.weight.loc torch.Size([64, 64]) Parameter containing:
tensor([[-0.6811,  0.3161, -0.3270,  ...,  0.1441, -0.2110, -0.0239],
        [ 0.2203, -0.3347,  0.6441,  ...,  0.2032,  0.0659,  0.5272],
        [ 0.7038, -0.1670,  0.4566,  ..., -0.7412,  0.2807, -0.2283],
        ...,
        [ 0.1048,  0.1075,  0.0584,  ..., -0.1556,  0.2848,  0.2082],
        [-0.6225,  0.2305, -0.0193,  ...,  0.0437,  0.1458,  0.0625],
        [-0.2267,  0.0859,  0.5048,  ...,  0.2608, -0.1071, -0.0373]],
       device='cuda:6', requires_grad=True)
net_guide.net.2.0.weight.scale torch.Size([64, 64]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.2.0.bias.loc torch.Size([64]) Parameter containing:
tensor([ 0.3025,  0.1352,  0.3700,  0.1716, -0.0192,  0.1063, -0.5706, -0.4473,
         0.2992, -0.1567, -0.1146,  0.0921, -0.5697,  0.1017,  0.0600,  0.0913,
        -0.1491,  0.6061,  0.4346, -0.4054,  0.0227,  0.0837,  0.2729,  0.1709,
        -0.4732, -0.3316, -0.1909,  0.0864, -0.2754,  0.2093,  0.0051, -0.0323,
        -0.1965, -0.1512,  0.2758,  0.4396,  0.0263, -0.0469,  0.0548, -0.4210,
        -0.1051, -0.0760,  0.1387,  0.3863, -0.1109, -0.1161, -0.4074,  0.2749,
        -0.1825, -0.2378,  0.0911, -0.1028, -0.1763,  0.2711, -0.1233, -0.1339,
         0.0184, -0.5446,  0.1913,  0.0891,  0.2523, -0.3813, -0.2715,  0.4334],
       device='cuda:6', requires_grad=True)
net_guide.net.2.0.bias.scale torch.Size([64]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100], device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.3.0.weight.loc torch.Size([64, 64]) Parameter containing:
tensor([[-0.0744, -0.5360,  0.3588,  ..., -0.3374,  0.0014, -0.0660],
        [ 0.1757,  0.7729,  0.1265,  ...,  0.3470,  0.5208,  0.0036],
        [ 0.3038, -0.0595, -0.3465,  ..., -0.1033,  0.0408,  0.1115],
        ...,
        [-0.1124, -0.4488,  0.0415,  ..., -0.5524,  0.4044,  0.0128],
        [ 0.2510,  0.1914, -0.4134,  ...,  0.2205,  0.0806,  0.2155],
        [ 0.1556, -0.4279,  0.1402,  ..., -0.5084, -0.0691,  0.5523]],
       device='cuda:6', requires_grad=True)
net_guide.net.3.0.weight.scale torch.Size([64, 64]) tensor([[0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        ...,
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100],
        [0.0100, 0.0100, 0.0100,  ..., 0.0100, 0.0100, 0.0100]],
       device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.3.0.bias.loc torch.Size([64]) Parameter containing:
tensor([-0.2210,  0.1183, -0.1472, -0.3089,  0.1225,  0.5399, -0.1065,  0.2722,
         0.0994, -0.2531,  0.0213,  0.4683,  0.2405, -0.2473,  0.2104, -0.5272,
        -0.3478,  0.1336, -0.3497, -0.0275,  0.2957,  0.5428, -0.0084, -0.1596,
        -0.1397, -0.4866,  0.0664,  0.3073, -0.0928, -0.2963, -0.4377,  0.3151,
         0.1783,  0.0243, -0.5760,  0.0975, -0.3268, -0.0503, -0.2340,  0.2785,
         0.1606,  0.1490, -0.0595, -0.2117,  0.0053,  0.4937,  0.0638, -0.4911,
         0.3041, -0.0662,  0.0253,  0.6752, -0.4164, -0.8839,  0.6171,  0.2710,
         0.1959,  0.1598,  0.5000,  0.3911,  0.4414, -0.4382,  0.0797, -0.2578],
       device='cuda:6', requires_grad=True)
net_guide.net.3.0.bias.scale torch.Size([64]) tensor([0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
        0.0100], device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.4.weight.loc torch.Size([1, 64]) Parameter containing:
tensor([[ 3.1297e-01, -1.3180e-01,  4.1689e-01,  1.1147e-01, -2.3095e-01,
         -2.8177e-01, -2.7794e-01, -5.4138e-01,  1.0726e-01,  4.0789e-01,
          8.4096e-02, -2.6012e-02, -1.2065e-01,  6.3524e-03, -2.2531e-02,
          4.0374e-01,  3.8021e-01, -1.4948e-01, -3.4344e-01,  3.0148e-01,
         -2.6721e-02, -8.3000e-02,  1.4736e-01, -2.3529e-01, -1.2065e-01,
          5.9608e-01, -3.3587e-01, -3.4515e-01,  9.5972e-02,  2.4205e-01,
         -8.7279e-03,  1.3463e-01,  3.2840e-02, -2.4451e-01,  1.4154e-01,
          2.0842e-02,  2.2589e-01, -7.3635e-02, -3.2984e-01,  5.1806e-01,
         -3.8147e-01, -7.9524e-02,  1.2603e-01, -3.9635e-04, -2.0608e-01,
         -1.4884e-01, -4.4520e-02, -4.7193e-02,  9.0008e-02, -3.3710e-01,
         -5.8034e-01,  2.6689e-01, -4.3677e-02, -1.7518e-01,  4.6406e-01,
          2.7115e-02,  5.3700e-01, -3.1759e-01, -1.4925e-02,  1.0452e-01,
         -1.2647e-02,  2.2349e-02, -9.3917e-02, -5.9171e-01]], device='cuda:6',
       requires_grad=True)
net_guide.net.4.weight.scale torch.Size([1, 64]) tensor([[0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100,
         0.0100]], device='cuda:6', grad_fn=<AddBackward0>)
net_guide.net.4.bias.loc torch.Size([1]) Parameter containing:
tensor([0.2879], device='cuda:6', requires_grad=True)
net_guide.net.4.bias.scale torch.Size([1]) tensor([0.0100], device='cuda:6', grad_fn=<AddBackward0>)
Using device: cuda:6
===== Training profile sineasy20-3x64-s05 - 1 =====
[0:00:01.901690] epoch: 0 | elbo: 6555869.485000001 | train_rmse: 16.9976 | val_rmse: 17.1487 | val_ll: -153.3813
[0:01:42.924226] epoch: 50 | elbo: 1430499.63375 | train_rmse: 8.5069 | val_rmse: 8.5904 | val_ll: -48.933
[0:03:23.769170] epoch: 100 | elbo: 750796.300625 | train_rmse: 6.0791 | val_rmse: 6.3923 | val_ll: -23.2993
[0:05:04.658376] epoch: 150 | elbo: 436109.53531249997 | train_rmse: 4.5711 | val_rmse: 5.0262 | val_ll: -14.1164
[0:06:43.413011] epoch: 200 | elbo: 297493.70328125 | train_rmse: 3.7596 | val_rmse: 4.3289 | val_ll: -11.1857
[0:08:21.645007] epoch: 250 | elbo: 226857.16359375004 | train_rmse: 3.2445 | val_rmse: 3.9142 | val_ll: -10.0936
[0:09:59.079705] epoch: 300 | elbo: 182022.61640625 | train_rmse: 2.8784 | val_rmse: 3.6388 | val_ll: -9.2774
[0:11:37.865296] epoch: 350 | elbo: 152008.92570312505 | train_rmse: 2.6174 | val_rmse: 3.438 | val_ll: -9.1173
[0:13:16.309819] epoch: 400 | elbo: 131706.35304687504 | train_rmse: 2.4249 | val_rmse: 3.2849 | val_ll: -8.9097
[0:14:54.332592] epoch: 450 | elbo: 115706.39585937501 | train_rmse: 2.2588 | val_rmse: 3.1565 | val_ll: -8.9015
[0:16:32.900557] epoch: 500 | elbo: 103048.975 | train_rmse: 2.1263 | val_rmse: 3.0579 | val_ll: -8.8442
[0:18:11.094887] epoch: 550 | elbo: 92555.416328125 | train_rmse: 2.0126 | val_rmse: 2.9795 | val_ll: -8.8697
[0:19:48.546693] epoch: 600 | elbo: 85284.683359375 | train_rmse: 1.9137 | val_rmse: 2.9173 | val_ll: -9.0357
[0:21:26.834369] epoch: 650 | elbo: 78411.10687500001 | train_rmse: 1.8292 | val_rmse: 2.8651 | val_ll: -9.0867
[0:23:06.896587] epoch: 700 | elbo: 72851.80859375 | train_rmse: 1.756 | val_rmse: 2.8199 | val_ll: -9.25
[0:24:45.017552] epoch: 750 | elbo: 68427.5130859375 | train_rmse: 1.6907 | val_rmse: 2.7822 | val_ll: -9.4181
[0:26:24.220204] epoch: 800 | elbo: 64513.42652343749 | train_rmse: 1.6405 | val_rmse: 2.7533 | val_ll: -9.5659
[0:28:05.540244] epoch: 850 | elbo: 60844.390507812495 | train_rmse: 1.5856 | val_rmse: 2.7232 | val_ll: -9.6927
[0:29:46.187449] epoch: 900 | elbo: 57886.101640625006 | train_rmse: 1.5429 | val_rmse: 2.701 | val_ll: -9.8203
[0:31:25.929939] epoch: 950 | elbo: 55213.021601562505 | train_rmse: 1.4985 | val_rmse: 2.6837 | val_ll: -9.922
[0:33:06.634940] epoch: 1000 | elbo: 52972.351171875 | train_rmse: 1.4625 | val_rmse: 2.6701 | val_ll: -10.0572
[0:34:47.526989] epoch: 1050 | elbo: 50615.2048828125 | train_rmse: 1.4306 | val_rmse: 2.6588 | val_ll: -10.2406
[0:36:27.581189] epoch: 1100 | elbo: 48642.734296875 | train_rmse: 1.3974 | val_rmse: 2.6531 | val_ll: -10.4129
[0:38:08.341630] epoch: 1150 | elbo: 46689.30601562501 | train_rmse: 1.3665 | val_rmse: 2.6439 | val_ll: -10.5476
[0:39:48.345684] epoch: 1200 | elbo: 45213.54402343749 | train_rmse: 1.3378 | val_rmse: 2.6377 | val_ll: -10.6831
[0:41:29.657731] epoch: 1250 | elbo: 43726.837421874996 | train_rmse: 1.3113 | val_rmse: 2.6338 | val_ll: -10.8683
[0:43:11.688497] epoch: 1300 | elbo: 42254.05550781251 | train_rmse: 1.2869 | val_rmse: 2.6315 | val_ll: -11.0214
[0:44:54.882762] epoch: 1350 | elbo: 40750.65169921876 | train_rmse: 1.2624 | val_rmse: 2.6317 | val_ll: -11.1895
[0:46:36.259597] epoch: 1400 | elbo: 39787.624375 | train_rmse: 1.2413 | val_rmse: 2.63 | val_ll: -11.2932
[0:48:17.699241] epoch: 1450 | elbo: 38818.7244140625 | train_rmse: 1.224 | val_rmse: 2.629 | val_ll: -11.4854
[0:49:58.079200] epoch: 1500 | elbo: 37567.222851562496 | train_rmse: 1.2013 | val_rmse: 2.6307 | val_ll: -11.5952
[0:51:38.896198] epoch: 1550 | elbo: 36510.5841796875 | train_rmse: 1.1814 | val_rmse: 2.6295 | val_ll: -11.6726
[0:53:20.810861] epoch: 1600 | elbo: 35794.272128906254 | train_rmse: 1.163 | val_rmse: 2.632 | val_ll: -11.8526
[0:55:02.921698] epoch: 1650 | elbo: 35065.30783203125 | train_rmse: 1.1468 | val_rmse: 2.6298 | val_ll: -11.8929
[0:56:46.565904] epoch: 1700 | elbo: 34254.495449218746 | train_rmse: 1.1293 | val_rmse: 2.6323 | val_ll: -12.0278
[0:58:29.883582] epoch: 1750 | elbo: 33525.37498046874 | train_rmse: 1.1152 | val_rmse: 2.6336 | val_ll: -12.1022
[1:00:12.759929] epoch: 1800 | elbo: 32806.599375 | train_rmse: 1.1005 | val_rmse: 2.6332 | val_ll: -12.1875
[1:01:54.124119] epoch: 1850 | elbo: 32205.283124999994 | train_rmse: 1.091 | val_rmse: 2.638 | val_ll: -12.2634
[1:03:36.569835] epoch: 1900 | elbo: 31664.95679687501 | train_rmse: 1.0717 | val_rmse: 2.6343 | val_ll: -12.3166
[1:05:19.563631] epoch: 1950 | elbo: 31094.918320312496 | train_rmse: 1.0612 | val_rmse: 2.6385 | val_ll: -12.3621
[1:07:03.060587] epoch: 2000 | elbo: 30435.70234375 | train_rmse: 1.0467 | val_rmse: 2.6398 | val_ll: -12.4784
[1:08:45.153584] epoch: 2050 | elbo: 29952.042265625 | train_rmse: 1.0361 | val_rmse: 2.6426 | val_ll: -12.4934
[1:10:27.439307] epoch: 2100 | elbo: 29263.401054687507 | train_rmse: 1.0256 | val_rmse: 2.6462 | val_ll: -12.545
[1:12:10.543834] epoch: 2150 | elbo: 28898.756523437496 | train_rmse: 1.0128 | val_rmse: 2.6481 | val_ll: -12.5734
[1:13:54.593863] epoch: 2200 | elbo: 28380.217148437503 | train_rmse: 1.0018 | val_rmse: 2.647 | val_ll: -12.6008
[1:15:36.082597] epoch: 2250 | elbo: 27872.155117187503 | train_rmse: 0.9916 | val_rmse: 2.6519 | val_ll: -12.6971
[1:17:18.184850] epoch: 2300 | elbo: 27397.41572265625 | train_rmse: 0.9828 | val_rmse: 2.658 | val_ll: -12.671
[1:18:58.407343] epoch: 2350 | elbo: 27034.98091796875 | train_rmse: 0.9691 | val_rmse: 2.6572 | val_ll: -12.7589
[1:20:38.347991] epoch: 2400 | elbo: 26616.323554687504 | train_rmse: 0.9603 | val_rmse: 2.66 | val_ll: -12.7512
[1:22:17.382789] epoch: 2450 | elbo: 26162.1019140625 | train_rmse: 0.9522 | val_rmse: 2.665 | val_ll: -12.8436
[1:23:54.842599] epoch: 2500 | elbo: 25859.647050781245 | train_rmse: 0.9406 | val_rmse: 2.6684 | val_ll: -12.8889
[1:25:33.052986] epoch: 2550 | elbo: 25503.676679687498 | train_rmse: 0.9316 | val_rmse: 2.6718 | val_ll: -12.9002
[1:27:12.449978] epoch: 2600 | elbo: 25276.493984374996 | train_rmse: 0.9223 | val_rmse: 2.6772 | val_ll: -12.9689
[1:28:52.170962] epoch: 2650 | elbo: 24881.54787109375 | train_rmse: 0.9116 | val_rmse: 2.6792 | val_ll: -12.9794
[1:30:31.751371] epoch: 2700 | elbo: 24641.48759765624 | train_rmse: 0.9053 | val_rmse: 2.6851 | val_ll: -13.0171
[1:32:12.817630] epoch: 2750 | elbo: 24198.749746093752 | train_rmse: 0.894 | val_rmse: 2.6851 | val_ll: -13.0475
[1:33:53.098472] epoch: 2800 | elbo: 24000.272851562502 | train_rmse: 0.8858 | val_rmse: 2.6905 | val_ll: -13.0826
[1:35:34.324032] epoch: 2850 | elbo: 23598.6623828125 | train_rmse: 0.877 | val_rmse: 2.6929 | val_ll: -13.1429
[1:37:15.831719] epoch: 2900 | elbo: 23286.175576171874 | train_rmse: 0.8697 | val_rmse: 2.6981 | val_ll: -13.1511
[1:38:58.409287] epoch: 2950 | elbo: 23255.536103515624 | train_rmse: 0.8603 | val_rmse: 2.6997 | val_ll: -13.1812
[1:40:37.902369] epoch: 3000 | elbo: 22783.801445312496 | train_rmse: 0.8503 | val_rmse: 2.7035 | val_ll: -13.1992
[1:42:18.017308] epoch: 3050 | elbo: 22540.29322265625 | train_rmse: 0.8458 | val_rmse: 2.7064 | val_ll: -13.1853
[1:43:57.727913] epoch: 3100 | elbo: 22275.799589843744 | train_rmse: 0.8363 | val_rmse: 2.7102 | val_ll: -13.2391
[1:45:38.933896] epoch: 3150 | elbo: 21991.98146484375 | train_rmse: 0.8291 | val_rmse: 2.7118 | val_ll: -13.2492
[1:47:19.568604] epoch: 3200 | elbo: 21858.849150390623 | train_rmse: 0.8329 | val_rmse: 2.7157 | val_ll: -13.3084
[1:49:00.066492] epoch: 3250 | elbo: 21590.881162109374 | train_rmse: 0.8141 | val_rmse: 2.7178 | val_ll: -13.3073
[1:50:40.755281] epoch: 3300 | elbo: 21423.347822265627 | train_rmse: 0.8072 | val_rmse: 2.7197 | val_ll: -13.2945
[1:52:19.706596] epoch: 3350 | elbo: 21126.960478515623 | train_rmse: 0.8009 | val_rmse: 2.7246 | val_ll: -13.3351
[1:54:00.032760] epoch: 3400 | elbo: 20997.648535156248 | train_rmse: 0.7939 | val_rmse: 2.7256 | val_ll: -13.3971
[1:55:41.534983] epoch: 3450 | elbo: 20777.143134765625 | train_rmse: 0.7876 | val_rmse: 2.729 | val_ll: -13.3749
[1:57:21.595400] epoch: 3500 | elbo: 20526.58717773438 | train_rmse: 0.78 | val_rmse: 2.7315 | val_ll: -13.3784
[1:59:00.938625] epoch: 3550 | elbo: 20335.283496093754 | train_rmse: 0.7754 | val_rmse: 2.7356 | val_ll: -13.4224
[2:00:40.802446] epoch: 3600 | elbo: 20178.064433593747 | train_rmse: 0.7677 | val_rmse: 2.7379 | val_ll: -13.4571
[2:02:20.572376] epoch: 3650 | elbo: 19993.49203125 | train_rmse: 0.763 | val_rmse: 2.7408 | val_ll: -13.5076
[2:03:59.659752] epoch: 3700 | elbo: 19832.919228515624 | train_rmse: 0.7564 | val_rmse: 2.7449 | val_ll: -13.495
[2:05:39.312900] epoch: 3750 | elbo: 19651.924960937504 | train_rmse: 0.7519 | val_rmse: 2.747 | val_ll: -13.5635
[2:07:19.084547] epoch: 3800 | elbo: 19493.346689453123 | train_rmse: 0.7455 | val_rmse: 2.751 | val_ll: -13.6005
[2:09:01.197790] epoch: 3850 | elbo: 19312.196982421876 | train_rmse: 0.7415 | val_rmse: 2.7538 | val_ll: -13.638
[2:10:43.117303] epoch: 3900 | elbo: 19169.80077148437 | train_rmse: 0.7355 | val_rmse: 2.7578 | val_ll: -13.6479
[2:12:27.030001] epoch: 3950 | elbo: 18964.429189453123 | train_rmse: 0.7293 | val_rmse: 2.7603 | val_ll: -13.6961
[2:14:06.780085] epoch: 4000 | elbo: 18888.160058593752 | train_rmse: 0.7241 | val_rmse: 2.7642 | val_ll: -13.7342
[2:15:46.000138] epoch: 4050 | elbo: 18749.57737304687 | train_rmse: 0.7191 | val_rmse: 2.7682 | val_ll: -13.782
[2:17:24.738719] epoch: 4100 | elbo: 18598.01912109375 | train_rmse: 0.7144 | val_rmse: 2.7706 | val_ll: -13.7526
[2:19:04.731438] epoch: 4150 | elbo: 18506.178164062505 | train_rmse: 0.7091 | val_rmse: 2.7729 | val_ll: -13.8305
[2:20:42.844107] epoch: 4200 | elbo: 18199.02338867187 | train_rmse: 0.7047 | val_rmse: 2.7749 | val_ll: -13.8141
[2:22:22.353105] epoch: 4250 | elbo: 18197.913710937497 | train_rmse: 0.7013 | val_rmse: 2.779 | val_ll: -13.8761
[2:24:01.698681] epoch: 4300 | elbo: 18078.276513671877 | train_rmse: 0.6939 | val_rmse: 2.7813 | val_ll: -13.8981
[2:25:41.079176] epoch: 4350 | elbo: 17945.231826171872 | train_rmse: 0.691 | val_rmse: 2.7845 | val_ll: -13.9441
[2:27:20.273041] epoch: 4400 | elbo: 17781.381914062502 | train_rmse: 0.6856 | val_rmse: 2.7873 | val_ll: -13.9912
[2:29:00.347051] epoch: 4450 | elbo: 17752.53509765625 | train_rmse: 0.6822 | val_rmse: 2.7893 | val_ll: -14.0061
[2:30:40.294822] epoch: 4500 | elbo: 17633.573125000006 | train_rmse: 0.6753 | val_rmse: 2.7928 | val_ll: -14.0229
[2:32:18.603489] epoch: 4550 | elbo: 17486.280605468746 | train_rmse: 0.6733 | val_rmse: 2.7949 | val_ll: -14.0459
[2:33:57.551344] epoch: 4600 | elbo: 17392.2030859375 | train_rmse: 0.6716 | val_rmse: 2.7973 | val_ll: -14.1041
[2:35:36.376709] epoch: 4650 | elbo: 17234.516494140626 | train_rmse: 0.6647 | val_rmse: 2.7997 | val_ll: -14.1062
[2:37:14.976141] epoch: 4700 | elbo: 17179.101728515623 | train_rmse: 0.66 | val_rmse: 2.8019 | val_ll: -14.1062
[2:38:54.026772] epoch: 4750 | elbo: 17036.7924609375 | train_rmse: 0.6595 | val_rmse: 2.8068 | val_ll: -14.1516
[2:40:33.316600] epoch: 4800 | elbo: 17045.64837890625 | train_rmse: 0.6522 | val_rmse: 2.8075 | val_ll: -14.1966
[2:42:12.733380] epoch: 4850 | elbo: 16860.626699218745 | train_rmse: 0.6507 | val_rmse: 2.8105 | val_ll: -14.1894
[2:43:52.835518] epoch: 4900 | elbo: 16765.88640625 | train_rmse: 0.6461 | val_rmse: 2.8131 | val_ll: -14.218
[2:45:31.918303] epoch: 4950 | elbo: 16639.01309570313 | train_rmse: 0.6438 | val_rmse: 2.8148 | val_ll: -14.2453
[2:47:12.920551] epoch: 5000 | elbo: 16620.659687500003 | train_rmse: 0.6378 | val_rmse: 2.8166 | val_ll: -14.2744
[2:48:51.799710] epoch: 5050 | elbo: 16547.897880859375 | train_rmse: 0.6359 | val_rmse: 2.8193 | val_ll: -14.3022
[2:50:32.837261] epoch: 5100 | elbo: 16428.17443359375 | train_rmse: 0.6345 | val_rmse: 2.8216 | val_ll: -14.3097
[2:52:13.334673] epoch: 5150 | elbo: 16430.055566406252 | train_rmse: 0.6328 | val_rmse: 2.825 | val_ll: -14.3689
[2:53:52.840431] epoch: 5200 | elbo: 16301.73307617188 | train_rmse: 0.6252 | val_rmse: 2.8252 | val_ll: -14.3622
[2:55:31.562055] epoch: 5250 | elbo: 16172.992470703122 | train_rmse: 0.6228 | val_rmse: 2.8272 | val_ll: -14.4114
[2:57:10.853332] epoch: 5300 | elbo: 16102.956845703124 | train_rmse: 0.6217 | val_rmse: 2.8292 | val_ll: -14.4277
[2:58:49.933600] epoch: 5350 | elbo: 16046.947773437505 | train_rmse: 0.6188 | val_rmse: 2.8323 | val_ll: -14.4614
[3:00:28.299762] epoch: 5400 | elbo: 15953.574121093747 | train_rmse: 0.6199 | val_rmse: 2.8348 | val_ll: -14.5116
[3:02:08.070479] epoch: 5450 | elbo: 15894.36291992188 | train_rmse: 0.6114 | val_rmse: 2.8356 | val_ll: -14.4965
[3:03:47.805675] epoch: 5500 | elbo: 15823.407617187502 | train_rmse: 0.6094 | val_rmse: 2.8361 | val_ll: -14.5162
[3:05:26.774913] epoch: 5550 | elbo: 15763.800068359375 | train_rmse: 0.6065 | val_rmse: 2.8399 | val_ll: -14.5261
[3:07:06.758144] epoch: 5600 | elbo: 15687.56359375 | train_rmse: 0.6046 | val_rmse: 2.8401 | val_ll: -14.5779
[3:08:46.399492] epoch: 5650 | elbo: 15586.370810546878 | train_rmse: 0.598 | val_rmse: 2.8417 | val_ll: -14.5479
[3:10:24.255975] epoch: 5700 | elbo: 15549.806669921876 | train_rmse: 0.5954 | val_rmse: 2.8424 | val_ll: -14.5704
[3:12:03.996496] epoch: 5750 | elbo: 15484.085537109373 | train_rmse: 0.5949 | val_rmse: 2.8461 | val_ll: -14.6069
[3:13:42.373015] epoch: 5800 | elbo: 15450.353564453118 | train_rmse: 0.5933 | val_rmse: 2.8469 | val_ll: -14.6509
[3:15:21.061292] epoch: 5850 | elbo: 15448.92443359375 | train_rmse: 0.5871 | val_rmse: 2.8486 | val_ll: -14.6509
[3:16:58.495716] epoch: 5900 | elbo: 15251.592158203126 | train_rmse: 0.5847 | val_rmse: 2.8495 | val_ll: -14.6852
[3:18:38.469676] epoch: 5950 | elbo: 15175.319628906253 | train_rmse: 0.5812 | val_rmse: 2.8509 | val_ll: -14.699
[3:20:19.122255] epoch: 6000 | elbo: 15125.92529296875 | train_rmse: 0.5786 | val_rmse: 2.8527 | val_ll: -14.7406
[3:22:00.434761] epoch: 6050 | elbo: 15096.867050781253 | train_rmse: 0.5764 | val_rmse: 2.8557 | val_ll: -14.7639
[3:23:38.144736] epoch: 6100 | elbo: 15059.928496093755 | train_rmse: 0.5737 | val_rmse: 2.8555 | val_ll: -14.7672
[3:25:16.953647] epoch: 6150 | elbo: 15073.8069921875 | train_rmse: 0.5738 | val_rmse: 2.859 | val_ll: -14.8068
[3:26:57.449781] epoch: 6200 | elbo: 14902.551020507812 | train_rmse: 0.569 | val_rmse: 2.8581 | val_ll: -14.8036
[3:28:35.699917] epoch: 6250 | elbo: 14872.107158203125 | train_rmse: 0.5678 | val_rmse: 2.8617 | val_ll: -14.8277
[3:30:16.029670] epoch: 6300 | elbo: 14803.138828125002 | train_rmse: 0.5649 | val_rmse: 2.8628 | val_ll: -14.8417
[3:31:56.427815] epoch: 6350 | elbo: 14682.180703124997 | train_rmse: 0.561 | val_rmse: 2.8615 | val_ll: -14.8375
[3:33:38.032899] epoch: 6400 | elbo: 14656.796308593755 | train_rmse: 0.561 | val_rmse: 2.8652 | val_ll: -14.8766
[3:35:19.002514] epoch: 6450 | elbo: 14596.60478027344 | train_rmse: 0.557 | val_rmse: 2.8662 | val_ll: -14.8361
[3:37:00.696199] epoch: 6500 | elbo: 14583.819843750001 | train_rmse: 0.5559 | val_rmse: 2.8663 | val_ll: -14.8824
[3:38:42.803878] epoch: 6550 | elbo: 14529.942373046875 | train_rmse: 0.5542 | val_rmse: 2.8695 | val_ll: -14.8994
[3:40:24.224378] epoch: 6600 | elbo: 14459.541845703123 | train_rmse: 0.5506 | val_rmse: 2.8707 | val_ll: -14.9148
[3:42:07.212680] epoch: 6650 | elbo: 14475.444204101563 | train_rmse: 0.5493 | val_rmse: 2.872 | val_ll: -14.9089
[3:43:49.676242] epoch: 6700 | elbo: 14410.743164062504 | train_rmse: 0.5511 | val_rmse: 2.8722 | val_ll: -14.9418
[3:45:31.715337] epoch: 6750 | elbo: 14324.151469726561 | train_rmse: 0.5446 | val_rmse: 2.8752 | val_ll: -14.9523
[3:47:12.777493] epoch: 6800 | elbo: 14386.262866210938 | train_rmse: 0.5526 | val_rmse: 2.8784 | val_ll: -14.9403
[3:48:53.899617] epoch: 6850 | elbo: 14283.747661132811 | train_rmse: 0.5405 | val_rmse: 2.878 | val_ll: -14.97
[3:50:35.771056] epoch: 6900 | elbo: 14227.458588867186 | train_rmse: 0.5369 | val_rmse: 2.878 | val_ll: -14.9707
[3:52:14.420299] epoch: 6950 | elbo: 14127.018969726563 | train_rmse: 0.5406 | val_rmse: 2.8794 | val_ll: -14.9891
[3:53:53.840487] epoch: 7000 | elbo: 14080.107504882812 | train_rmse: 0.5334 | val_rmse: 2.8801 | val_ll: -14.9915
[3:55:34.178199] epoch: 7050 | elbo: 14057.996093749996 | train_rmse: 0.5314 | val_rmse: 2.8815 | val_ll: -15.0057
[3:57:15.243587] epoch: 7100 | elbo: 13991.785039062499 | train_rmse: 0.5287 | val_rmse: 2.8827 | val_ll: -15.0303
[3:58:55.456701] epoch: 7150 | elbo: 13951.286889648438 | train_rmse: 0.528 | val_rmse: 2.8846 | val_ll: -15.0419
[4:00:36.226855] epoch: 7200 | elbo: 13948.743691406251 | train_rmse: 0.5269 | val_rmse: 2.8862 | val_ll: -15.0686
[4:02:16.599787] epoch: 7250 | elbo: 13898.792260742188 | train_rmse: 0.5241 | val_rmse: 2.8862 | val_ll: -15.0548
[4:03:57.867161] epoch: 7300 | elbo: 13854.664370117189 | train_rmse: 0.5228 | val_rmse: 2.8882 | val_ll: -15.0642
[4:05:39.588777] epoch: 7350 | elbo: 13808.646220703124 | train_rmse: 0.5219 | val_rmse: 2.8891 | val_ll: -15.0315
[4:07:21.264739] epoch: 7400 | elbo: 13735.579833984379 | train_rmse: 0.5199 | val_rmse: 2.8899 | val_ll: -15.1011
[4:09:03.481024] epoch: 7450 | elbo: 13725.533623046877 | train_rmse: 0.5176 | val_rmse: 2.8931 | val_ll: -15.1202
[4:10:45.157560] epoch: 7500 | elbo: 13728.25712890625 | train_rmse: 0.5174 | val_rmse: 2.8933 | val_ll: -15.091
[4:12:25.874824] epoch: 7550 | elbo: 13660.405112304687 | train_rmse: 0.5137 | val_rmse: 2.8946 | val_ll: -15.1214
[4:14:07.709467] epoch: 7600 | elbo: 13583.47023925781 | train_rmse: 0.5116 | val_rmse: 2.8957 | val_ll: -15.1044
[4:15:48.201299] epoch: 7650 | elbo: 13654.439809570313 | train_rmse: 0.5089 | val_rmse: 2.8975 | val_ll: -15.1389
[4:17:30.030339] epoch: 7700 | elbo: 13562.568842773439 | train_rmse: 0.5073 | val_rmse: 2.8965 | val_ll: -15.1318
[4:19:12.558607] epoch: 7750 | elbo: 13473.926635742188 | train_rmse: 0.5055 | val_rmse: 2.8997 | val_ll: -15.156
[4:20:56.216060] epoch: 7800 | elbo: 13417.820654296878 | train_rmse: 0.5043 | val_rmse: 2.9006 | val_ll: -15.1485
[4:22:38.965166] epoch: 7850 | elbo: 13433.256567382812 | train_rmse: 0.5013 | val_rmse: 2.9 | val_ll: -15.1524
[4:24:21.622872] epoch: 7900 | elbo: 13360.041372070313 | train_rmse: 0.5002 | val_rmse: 2.9013 | val_ll: -15.1737
[4:26:04.342202] epoch: 7950 | elbo: 13332.817592773437 | train_rmse: 0.5013 | val_rmse: 2.9019 | val_ll: -15.1861
[4:27:47.106864] epoch: 8000 | elbo: 13322.161059570315 | train_rmse: 0.4971 | val_rmse: 2.9052 | val_ll: -15.1867
[4:29:29.944590] epoch: 8050 | elbo: 13283.001049804685 | train_rmse: 0.4991 | val_rmse: 2.906 | val_ll: -15.2092
[4:31:12.250709] epoch: 8100 | elbo: 13278.537749023439 | train_rmse: 0.4934 | val_rmse: 2.9054 | val_ll: -15.2067
[4:32:55.247822] epoch: 8150 | elbo: 13219.289365234374 | train_rmse: 0.4954 | val_rmse: 2.9064 | val_ll: -15.1977
[4:34:37.731832] epoch: 8200 | elbo: 13205.163818359375 | train_rmse: 0.4913 | val_rmse: 2.9078 | val_ll: -15.2194
[4:36:19.808448] epoch: 8250 | elbo: 13175.561728515628 | train_rmse: 0.4896 | val_rmse: 2.9095 | val_ll: -15.2298
[4:38:02.632015] epoch: 8300 | elbo: 13102.270146484374 | train_rmse: 0.4876 | val_rmse: 2.9094 | val_ll: -15.2393
[4:39:44.806458] epoch: 8350 | elbo: 13057.336201171876 | train_rmse: 0.4864 | val_rmse: 2.9109 | val_ll: -15.2258
[4:41:28.090564] epoch: 8400 | elbo: 13026.629799804685 | train_rmse: 0.4838 | val_rmse: 2.9114 | val_ll: -15.2463
[4:43:09.838419] epoch: 8450 | elbo: 13001.903916015623 | train_rmse: 0.4826 | val_rmse: 2.9143 | val_ll: -15.2672
[4:44:52.339340] epoch: 8500 | elbo: 13039.097495117188 | train_rmse: 0.4827 | val_rmse: 2.915 | val_ll: -15.2558
[4:46:32.870152] epoch: 8550 | elbo: 12947.078095703127 | train_rmse: 0.4797 | val_rmse: 2.9148 | val_ll: -15.2618
[4:48:15.784580] epoch: 8600 | elbo: 12930.742724609376 | train_rmse: 0.4877 | val_rmse: 2.9177 | val_ll: -15.2905
[4:49:58.990336] epoch: 8650 | elbo: 12892.040278320312 | train_rmse: 0.4775 | val_rmse: 2.9163 | val_ll: -15.2717
[4:51:41.956297] epoch: 8700 | elbo: 12888.477172851564 | train_rmse: 0.4753 | val_rmse: 2.919 | val_ll: -15.2897
[4:53:22.668696] epoch: 8750 | elbo: 12855.258857421872 | train_rmse: 0.4743 | val_rmse: 2.9209 | val_ll: -15.2906
[4:55:04.627373] epoch: 8800 | elbo: 12853.65351074219 | train_rmse: 0.4742 | val_rmse: 2.9205 | val_ll: -15.3046
[4:56:46.084736] epoch: 8850 | elbo: 12783.50163574219 | train_rmse: 0.4702 | val_rmse: 2.9214 | val_ll: -15.3134
[4:58:28.402831] epoch: 8900 | elbo: 12784.942416992188 | train_rmse: 0.4699 | val_rmse: 2.9231 | val_ll: -15.316
[5:00:10.801462] epoch: 8950 | elbo: 12719.688027343751 | train_rmse: 0.4684 | val_rmse: 2.9255 | val_ll: -15.37
[5:01:52.473517] epoch: 9000 | elbo: 12737.347026367186 | train_rmse: 0.4673 | val_rmse: 2.9244 | val_ll: -15.3366
[5:03:33.244544] epoch: 9050 | elbo: 12669.326855468753 | train_rmse: 0.4636 | val_rmse: 2.9257 | val_ll: -15.3726
[5:05:13.834866] epoch: 9100 | elbo: 12669.11541015625 | train_rmse: 0.4643 | val_rmse: 2.9269 | val_ll: -15.3338
[5:06:55.366218] epoch: 9150 | elbo: 12656.741333007812 | train_rmse: 0.4695 | val_rmse: 2.9288 | val_ll: -15.3851
[5:08:36.713925] epoch: 9200 | elbo: 12606.288984375002 | train_rmse: 0.4602 | val_rmse: 2.9287 | val_ll: -15.3744
[5:10:19.186254] epoch: 9250 | elbo: 12610.422207031253 | train_rmse: 0.4615 | val_rmse: 2.9331 | val_ll: -15.3886
[5:12:00.168283] epoch: 9300 | elbo: 12555.758110351566 | train_rmse: 0.4579 | val_rmse: 2.9324 | val_ll: -15.3973
[5:13:42.577828] epoch: 9350 | elbo: 12558.867724609376 | train_rmse: 0.4558 | val_rmse: 2.9305 | val_ll: -15.3882
[5:15:23.547976] epoch: 9400 | elbo: 12522.73303222656 | train_rmse: 0.4584 | val_rmse: 2.9349 | val_ll: -15.4141
[5:17:02.001297] epoch: 9450 | elbo: 12535.116958007815 | train_rmse: 0.4554 | val_rmse: 2.935 | val_ll: -15.4218
[5:18:41.171947] epoch: 9500 | elbo: 12516.809501953121 | train_rmse: 0.4519 | val_rmse: 2.9355 | val_ll: -15.41
[5:20:22.316616] epoch: 9550 | elbo: 12446.70969238281 | train_rmse: 0.4501 | val_rmse: 2.937 | val_ll: -15.4556
[5:22:04.737480] epoch: 9600 | elbo: 12419.508979492188 | train_rmse: 0.449 | val_rmse: 2.937 | val_ll: -15.4069
[5:23:47.149826] epoch: 9650 | elbo: 12415.825932617186 | train_rmse: 0.45 | val_rmse: 2.9385 | val_ll: -15.4661
[5:25:29.068310] epoch: 9700 | elbo: 12421.80873046875 | train_rmse: 0.4547 | val_rmse: 2.9393 | val_ll: -15.5101
[5:27:11.796615] epoch: 9750 | elbo: 12353.098325195311 | train_rmse: 0.4454 | val_rmse: 2.9409 | val_ll: -15.493
[5:28:55.009874] epoch: 9800 | elbo: 12307.513652343747 | train_rmse: 0.444 | val_rmse: 2.9406 | val_ll: -15.4906
[5:30:36.627817] epoch: 9850 | elbo: 12299.0986328125 | train_rmse: 0.4423 | val_rmse: 2.9431 | val_ll: -15.5392
[5:32:15.820454] epoch: 9900 | elbo: 12284.359892578124 | train_rmse: 0.4419 | val_rmse: 2.9424 | val_ll: -15.502
[5:33:55.754127] epoch: 9950 | elbo: 12249.083149414062 | train_rmse: 0.4392 | val_rmse: 2.9435 | val_ll: -15.5376
Training finished in 5:35:32.764533 seconds
Saved SVI model to tests/dataset-tests/sineasy20/models/sineasy20-3x64-s05/checkpoint_1.pt
File Size is 0.07903003692626953 MB
data samples:  (1000, 1000)
Sequential(
  (0): Linear(in_features=20, out_features=64, bias=True)
  (1): ReLU()
  (2): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (3): Sequential(
    (0): Linear(in_features=64, out_features=64, bias=True)
    (1): ReLU()
  )
  (4): Linear(in_features=64, out_features=1, bias=True)
)
Settings:
DEVICE: cuda:6 INFERENCE_TYPE: svi OBS_MODEL: homoskedastic PRIOR_LOC: 0.0 PRIOR_SCALE: 1.0 LIKELIHOOD_SCALE_LOC: 1.0 LIKELIHOOD_SCALE: 0.5 GUIDE_SCALE: 0.01 TRAIN_SIZE: 10000
Loaded SVI model from tests/dataset-tests/sineasy20/models/sineasy20-3x64-s05/checkpoint_1.pt
using device: cuda:6
====== evaluating profile sineasy20-3x64-s05 - 1 ======
pred samples:  (1000, 1000)
Evaluating train...
Evaluating test...
Evaluating in_domain...
Evaluating out_domain...
Eval done in 0:02:43.141053
End time: 2023-07-14 00:16:40.570777
Total time: 5:38:21.474806
